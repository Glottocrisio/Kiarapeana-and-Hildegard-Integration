

# kiara\asciinet_files.txt
.
./.git
./.git/config
./.git/description
./.git/HEAD
./.git/hooks
./.git/hooks/applypatch-msg.sample
./.git/hooks/commit-msg.sample
./.git/hooks/fsmonitor-watchman.sample
./.git/hooks/post-update.sample
./.git/hooks/pre-applypatch.sample
./.git/hooks/pre-commit.sample
./.git/hooks/pre-merge-commit.sample
./.git/hooks/pre-push.sample
./.git/hooks/pre-rebase.sample
./.git/hooks/pre-receive.sample
./.git/hooks/prepare-commit-msg.sample
./.git/hooks/push-to-checkout.sample
./.git/hooks/update.sample
./.git/index
./.git/info
./.git/info/exclude
./.git/logs
./.git/logs/HEAD
./.git/logs/refs
./.git/logs/refs/heads
./.git/logs/refs/heads/master
./.git/logs/refs/remotes
./.git/logs/refs/remotes/origin
./.git/logs/refs/remotes/origin/HEAD
./.git/objects
./.git/objects/info
./.git/objects/pack
./.git/objects/pack/pack-fe0591af46ba506f73b18acd2ce9319eb287ab0f.idx
./.git/objects/pack/pack-fe0591af46ba506f73b18acd2ce9319eb287ab0f.pack
./.git/packed-refs
./.git/refs
./.git/refs/heads
./.git/refs/heads/master
./.git/refs/remotes
./.git/refs/remotes/origin
./.git/refs/remotes/origin/HEAD
./.git/refs/tags
./.github
./.github/workflows
./.github/workflows/build-linux.yaml
./.gitignore
./build.sbt
./ci
./ci/conda
./ci/conda/conda-pkg-patch.yaml
./LICENSE
./NOTICE
./project
./project/plugins.sbt
./project/project
./project/project/target
./project/project/target/config-classes
./project/project/target/config-classes/$199aed1c4a6cadfe5bab$$anonfun$$sbtdef$1.class
./project/project/target/config-classes/$199aed1c4a6cadfe5bab$.class
./project/project/target/config-classes/$199aed1c4a6cadfe5bab.cache
./project/project/target/config-classes/$199aed1c4a6cadfe5bab.class
./project/project/target/config-classes/$240e278a9ccafc73a758$.class
./project/project/target/config-classes/$240e278a9ccafc73a758.cache
./project/project/target/config-classes/$240e278a9ccafc73a758.class
./project/project/target/config-classes/$2e0dfd9443d9c5ea9ec2$$anonfun$$sbtdef$1.class
./project/project/target/config-classes/$2e0dfd9443d9c5ea9ec2$.class
./project/project/target/config-classes/$2e0dfd9443d9c5ea9ec2.cache
./project/project/target/config-classes/$2e0dfd9443d9c5ea9ec2.class
./project/project/target/config-classes/$46abe603945a8d610ba4$.class
./project/project/target/config-classes/$46abe603945a8d610ba4.cache
./project/project/target/config-classes/$46abe603945a8d610ba4.class
./project/project/target/config-classes/$54cb83494900f1d8c1df$$anonfun$$sbtdef$1.class
./project/project/target/config-classes/$54cb83494900f1d8c1df$.class
./project/project/target/config-classes/$54cb83494900f1d8c1df.cache
./project/project/target/config-classes/$54cb83494900f1d8c1df.class
./project/project/target/config-classes/$6c333fbe8119be4b4813$.class
./project/project/target/config-classes/$6c333fbe8119be4b4813.cache
./project/project/target/config-classes/$6c333fbe8119be4b4813.class
./project/project/target/config-classes/$70c40b6651adb5f6784f$.class
./project/project/target/config-classes/$70c40b6651adb5f6784f.cache
./project/project/target/config-classes/$70c40b6651adb5f6784f.class
./project/project/target/config-classes/$7447bf20cbc259232937$$anonfun$$sbtdef$1.class
./project/project/target/config-classes/$7447bf20cbc259232937$.class
./project/project/target/config-classes/$7447bf20cbc259232937.cache
./project/project/target/config-classes/$7447bf20cbc259232937.class
./project/project/target/config-classes/$7a5bbe82ce3664cc112f$.class
./project/project/target/config-classes/$7a5bbe82ce3664cc112f.cache
./project/project/target/config-classes/$7a5bbe82ce3664cc112f.class
./project/project/target/config-classes/$7c6405447769aac4fa12$.class
./project/project/target/config-classes/$7c6405447769aac4fa12.cache
./project/project/target/config-classes/$7c6405447769aac4fa12.class
./project/project/target/config-classes/$8dbefffaed01806bbee2$.class
./project/project/target/config-classes/$8dbefffaed01806bbee2.cache
./project/project/target/config-classes/$8dbefffaed01806bbee2.class
./project/project/target/config-classes/$95376d5467fcb5c63abc$.class
./project/project/target/config-classes/$95376d5467fcb5c63abc.cache
./project/project/target/config-classes/$95376d5467fcb5c63abc.class
./project/project/target/config-classes/$a86ddade965b72b5660d$$anonfun$$sbtdef$1.class
./project/project/target/config-classes/$a86ddade965b72b5660d$.class
./project/project/target/config-classes/$a86ddade965b72b5660d.cache
./project/project/target/config-classes/$a86ddade965b72b5660d.class
./project/project/target/config-classes/$ae4096496d613bbf7c0f$$anonfun$$sbtdef$1.class
./project/project/target/config-classes/$ae4096496d613bbf7c0f$.class
./project/project/target/config-classes/$ae4096496d613bbf7c0f.cache
./project/project/target/config-classes/$ae4096496d613bbf7c0f.class
./project/project/target/config-classes/$bcc4954f441d003af0f9$.class
./project/project/target/config-classes/$bcc4954f441d003af0f9.cache
./project/project/target/config-classes/$bcc4954f441d003af0f9.class
./project/project/target/config-classes/$db7cf20348368cf2531e$$anonfun$$sbtdef$1.class
./project/project/target/config-classes/$db7cf20348368cf2531e$.class
./project/project/target/config-classes/$db7cf20348368cf2531e.cache
./project/project/target/config-classes/$db7cf20348368cf2531e.class
./project/project/target/config-classes/$e78b709a118dd216360c$$anonfun$$sbtdef$1.class
./project/project/target/config-classes/$e78b709a118dd216360c$.class
./project/project/target/config-classes/$e78b709a118dd216360c.cache
./project/project/target/config-classes/$e78b709a118dd216360c.class
./project/project/target/config-classes/$f269997d9ce3e54dd31b$$anonfun$$sbtdef$1.class
./project/project/target/config-classes/$f269997d9ce3e54dd31b$.class
./project/project/target/config-classes/$f269997d9ce3e54dd31b.cache
./project/project/target/config-classes/$f269997d9ce3e54dd31b.class
./pyasciinet
./pyasciinet/asciinet
./pyasciinet/asciinet/lib
./pyasciinet/asciinet/lib/asciigraph-assembly-0.2.1.jar
./pyasciinet/asciinet/test
./pyasciinet/asciinet/test/base.py
./pyasciinet/asciinet/test/test_ascii.py
./pyasciinet/asciinet/test/__init__.py
./pyasciinet/asciinet/_libutil.py
./pyasciinet/asciinet/__init__.py
./pyasciinet/asciinet/__version__.py
./pyasciinet/build.sh
./pyasciinet/dependencies.txt
./pyasciinet/examples
./pyasciinet/examples/example.py
./pyasciinet/ez_setup.py
./pyasciinet/LICENSE
./pyasciinet/MANIFEST.in
./pyasciinet/NOTICE
./pyasciinet/pyasciigraph.iml
./pyasciinet/README.md
./pyasciinet/requirements.sh
./pyasciinet/requirements.txt
./pyasciinet/setup.py
./README.md
./src
./src/main
./src/main/scala
./src/main/scala/com
./src/main/scala/com/ascii
./src/main/scala/com/ascii/AsciiGraph.scala
./src/main/scala/com/ascii/AsciiGraphContainer.scala
./src/main/scala/com/ascii/Server.scala


# kiara\DHARPA_Project_files.txt
.
./.editorconfig
./.envrc.disabled
./.git
./.git/config
./.git/description
./.git/HEAD
./.git/hooks
./.git/hooks/applypatch-msg.sample
./.git/hooks/commit-msg.sample
./.git/hooks/fsmonitor-watchman.sample
./.git/hooks/post-update.sample
./.git/hooks/pre-applypatch.sample
./.git/hooks/pre-commit.sample
./.git/hooks/pre-merge-commit.sample
./.git/hooks/pre-push.sample
./.git/hooks/pre-rebase.sample
./.git/hooks/pre-receive.sample
./.git/hooks/prepare-commit-msg.sample
./.git/hooks/push-to-checkout.sample
./.git/hooks/update.sample
./.git/index
./.git/info
./.git/info/exclude
./.git/logs
./.git/logs/HEAD
./.git/logs/refs
./.git/logs/refs/heads
./.git/logs/refs/heads/develop
./.git/logs/refs/remotes
./.git/logs/refs/remotes/origin
./.git/logs/refs/remotes/origin/HEAD
./.git/objects
./.git/objects/info
./.git/objects/pack
./.git/objects/pack/pack-ec7b721bbdb50d3ab4c8e67e4647bcc61580155c.idx
./.git/objects/pack/pack-ec7b721bbdb50d3ab4c8e67e4647bcc61580155c.pack
./.git/packed-refs
./.git/refs
./.git/refs/heads
./.git/refs/heads/develop
./.git/refs/remotes
./.git/refs/remotes/origin
./.git/refs/remotes/origin/HEAD
./.git/refs/tags
./.gitattributes
./.github
./.github/dependabot.yml
./.github/ISSUE_TEMPLATE
./.github/ISSUE_TEMPLATE/bug_report.md
./.github/ISSUE_TEMPLATE/feature_request.md
./.github/workflows
./.github/workflows/build-darwin.yaml
./.github/workflows/build-linux.yaml
./.github/workflows/build-windows.yaml
./.gitignore
./.gitlab-ci.yml
./.git_archival.txt
./.kiara_complete.zsh
./.pre-commit-config.yaml
./AUTHORS.rst
./CHANGELOG.md
./ci
./ci/conda
./ci/conda/airium
./ci/conda/airium/conda-pkg-patch.yaml
./ci/conda/bases
./ci/conda/bases/conda-pkg-patch.yaml
./ci/conda/conda-pkg-patch.yaml
./ci/conda/dag-cbor
./ci/conda/dag-cbor/conda-pkg-patch.yaml
./ci/conda/email-validator
./ci/conda/email-validator/conda-pkg-patch.yaml
./ci/conda/fasteners
./ci/conda/fasteners/conda-pkg-patch.yaml
./ci/conda/jupyter-client
./ci/conda/jupyter-client/conda-pkg-patch.yaml
./ci/conda/jupyter-core
./ci/conda/jupyter-core/conda-pkg-patch.yaml
./ci/conda/makkus.bases
./ci/conda/makkus.bases/conda-pkg-patch.yaml
./ci/conda/makkus.dag-cbor
./ci/conda/makkus.dag-cbor/conda-pkg-patch.yaml
./ci/conda/makkus.multiformats
./ci/conda/makkus.multiformats/multiformats.yaml
./ci/conda/makkus.multiformats-config
./ci/conda/makkus.multiformats-config/multiformats-config.yaml
./ci/conda/mknotebooks
./ci/conda/mknotebooks/conda-pkg-patch.yaml
./ci/conda/multiformats
./ci/conda/multiformats/multiformats.yaml
./ci/conda/multiformats-config
./ci/conda/multiformats-config/multiformats-config.yaml
./ci/conda/pp-ez
./ci/conda/pp-ez/conda-pkg-patch.yaml
./ci/conda/puremagic
./ci/conda/puremagic/conda-pkg-patch.yaml
./ci/conda/pydantic-to-typescript
./ci/conda/pydantic-to-typescript/conda-pkg-patch.yaml
./ci/conda/Readme.md
./ci/conda/typing-validation
./ci/conda/typing-validation/conda-pkg-patch.yaml
./commitlint.config.js
./dev
./dev/dev.ipynb
./dev/network_analysis.ipynb
./dev/script.py
./docs
./docs/design_docs
./docs/design_docs/architecture
./docs/design_docs/architecture/assumptions.md
./docs/design_docs/architecture/data
./docs/design_docs/architecture/data/data_centric_approach.ipynb
./docs/design_docs/architecture/data/data_formats.ipynb
./docs/design_docs/architecture/data/dev.ipynb
./docs/design_docs/architecture/data/index.md
./docs/design_docs/architecture/data/persistence.md
./docs/design_docs/architecture/data/requirements.ipynb
./docs/design_docs/architecture/data/result_tree.png
./docs/design_docs/architecture/decisions.md
./docs/design_docs/architecture/index.md
./docs/design_docs/architecture/metadata.md
./docs/design_docs/architecture/SUMMARY.md
./docs/design_docs/architecture/workflows
./docs/design_docs/architecture/workflows/index.md
./docs/design_docs/architecture/workflows/modularity
./docs/design_docs/architecture/workflows/modularity/modularity.ipynb
./docs/design_docs/architecture/workflows/modularity/modularity_2.ipynb
./docs/design_docs/architecture/workflows/modularity/workflows
./docs/design_docs/architecture/workflows/modularity/workflows/corpus_processing.yaml
./docs/design_docs/architecture/workflows/modularity/workflows/corpus_processing_simple.yaml
./docs/design_docs/architecture/workflows/modularity/workflows/input_files_processing.yaml
./docs/design_docs/index.md
./docs/design_docs/SUMMARY.md
./docs/development
./docs/development/index.md
./docs/development/install.md
./docs/development/modules
./docs/development/modules/render_value.md
./docs/development/modules/SUMMARY.md
./docs/development/rendering.md
./docs/development/stores.md
./docs/development/SUMMARY.md
./docs/included_components
./docs/included_components/index.md
./docs/index.md
./docs/stylesheets
./docs/stylesheets/extra.css
./docs/SUMMARY.md
./examples
./examples/data
./examples/data/.gitkeep
./examples/data/journals
./examples/data/journals/JournalEdges1902.csv
./examples/data/journals/JournalNodes1902.csv
./examples/data/journals/query.graphql
./examples/data/journals/Readme.md
./examples/data/text_corpus
./examples/data/text_corpus/La_Ragione
./examples/data/text_corpus/La_Ragione/sn84037024_1917-04-25_ed-1_seq-1_ocr.txt
./examples/data/text_corpus/La_Ragione/sn84037024_1917-04-25_ed-2_seq-1_ocr.txt
./examples/data/text_corpus/La_Ragione/sn84037024_1917-04-25_ed-3_seq-1_ocr.txt
./examples/data/text_corpus/La_Ragione/sn84037024_1917-04-25_ed-4_seq-1_ocr.txt
./examples/data/text_corpus/La_Ragione/sn84037024_1917-05-05_ed-1_seq-1_ocr.txt
./examples/data/text_corpus/La_Ragione/sn84037024_1917-05-05_ed-2_seq-1_ocr.txt
./examples/data/text_corpus/La_Ragione/sn84037024_1917-05-05_ed-3_seq-1_ocr.txt
./examples/data/text_corpus/La_Ragione/sn84037024_1917-05-05_ed-4_seq-1_ocr.txt
./examples/data/text_corpus/La_Ragione/sn84037024_1917-05-16_ed-1_seq-1_ocr.txt
./examples/data/text_corpus/La_Ragione/sn84037024_1917-05-16_ed-2_seq-1_ocr.txt
./examples/data/text_corpus/La_Ragione/sn84037024_1917-05-16_ed-3_seq-1_ocr.txt
./examples/data/text_corpus/La_Rassegna
./examples/data/text_corpus/La_Rassegna/sn84037025_1917-04-07_ed-1_seq-1_ocr.txt
./examples/data/text_corpus/La_Rassegna/sn84037025_1917-04-14_ed-1_seq-1_ocr.txt
./examples/data/text_corpus/La_Rassegna/sn84037025_1917-04-14_ed-2_seq-1_ocr.txt
./examples/data/text_corpus/La_Rassegna/sn84037025_1917-04-21_ed-1_seq-1_ocr.txt
./examples/data/text_corpus/La_Rassegna/sn84037025_1917-04-21_ed-2_seq-1_ocr.txt
./examples/data/text_corpus/Readme.md
./examples/jupyter
./examples/jupyter/install_example.ipynb
./examples/pipelines
./examples/pipelines/mock_pipeline_1.yaml
./examples/scripts
./examples/scripts/import_df.py
./LICENSE
./Makefile
./MANIFEST.in
./mkdocs.yml
./onboarding.folder_to_table.json
./pyproject.toml
./README.md
./scripts
./scripts/documentation
./scripts/documentation/gen_api_doc_pages.py
./scripts/documentation/gen_info_pages.py
./scripts/documentation/gen_schemas.py
./src
./src/kiara
./src/kiara/api.py
./src/kiara/context
./src/kiara/context/config.py
./src/kiara/context/orm.py
./src/kiara/context/runtime_config.py
./src/kiara/context/__init__.py
./src/kiara/data_types
./src/kiara/data_types/included_core_types
./src/kiara/data_types/included_core_types/filesystem.py
./src/kiara/data_types/included_core_types/internal
./src/kiara/data_types/included_core_types/internal/render_value.py
./src/kiara/data_types/included_core_types/internal/__init__.py
./src/kiara/data_types/included_core_types/metadata.py
./src/kiara/data_types/included_core_types/serialization.py
./src/kiara/data_types/included_core_types/__init__.py
./src/kiara/data_types/__init__.py
./src/kiara/defaults.py
./src/kiara/doc
./src/kiara/doc/generate_api_doc.py
./src/kiara/doc/gen_info_pages.py
./src/kiara/doc/mkdocstrings
./src/kiara/doc/mkdocstrings/collector.py
./src/kiara/doc/mkdocstrings/handler.py
./src/kiara/doc/mkdocstrings/renderer.py
./src/kiara/doc/mkdocstrings/__init__.py
./src/kiara/doc/mkdocs_macros_cli.py
./src/kiara/doc/mkdocs_macros_kiara.py
./src/kiara/doc/__init__.py
./src/kiara/exceptions.py
./src/kiara/interfaces
./src/kiara/interfaces/cli
./src/kiara/interfaces/cli/archive
./src/kiara/interfaces/cli/archive/commands.py
./src/kiara/interfaces/cli/archive/__init__.py
./src/kiara/interfaces/cli/context
./src/kiara/interfaces/cli/context/commands.py
./src/kiara/interfaces/cli/context/__init__.py
./src/kiara/interfaces/cli/data
./src/kiara/interfaces/cli/data/commands.py
./src/kiara/interfaces/cli/data/__init__.py
./src/kiara/interfaces/cli/info
./src/kiara/interfaces/cli/info/commands.py
./src/kiara/interfaces/cli/info/__init__.py
./src/kiara/interfaces/cli/module
./src/kiara/interfaces/cli/module/commands.py
./src/kiara/interfaces/cli/module/__init__.py
./src/kiara/interfaces/cli/operation
./src/kiara/interfaces/cli/operation/commands.py
./src/kiara/interfaces/cli/operation/__init__.py
./src/kiara/interfaces/cli/pipeline
./src/kiara/interfaces/cli/pipeline/commands.py
./src/kiara/interfaces/cli/pipeline/__init__.py
./src/kiara/interfaces/cli/proxy_cli.py
./src/kiara/interfaces/cli/render
./src/kiara/interfaces/cli/render/commands.py
./src/kiara/interfaces/cli/render/__init__.py
./src/kiara/interfaces/cli/run.py
./src/kiara/interfaces/cli/type
./src/kiara/interfaces/cli/type/commands.py
./src/kiara/interfaces/cli/type/__init__.py
./src/kiara/interfaces/cli/workflow
./src/kiara/interfaces/cli/workflow/commands.py
./src/kiara/interfaces/cli/workflow/__init__.py
./src/kiara/interfaces/cli/__init__.py
./src/kiara/interfaces/python_api
./src/kiara/interfaces/python_api/base_api.py
./src/kiara/interfaces/python_api/batch.py
./src/kiara/interfaces/python_api/kiara_api.py
./src/kiara/interfaces/python_api/models
./src/kiara/interfaces/python_api/models/archive.py
./src/kiara/interfaces/python_api/models/doc.py
./src/kiara/interfaces/python_api/models/info.py
./src/kiara/interfaces/python_api/models/job.py
./src/kiara/interfaces/python_api/models/workflow.py
./src/kiara/interfaces/python_api/models/__init__.py
./src/kiara/interfaces/python_api/operation.py
./src/kiara/interfaces/python_api/proxy.py
./src/kiara/interfaces/python_api/utils.py
./src/kiara/interfaces/python_api/value.py
./src/kiara/interfaces/python_api/workflow.py
./src/kiara/interfaces/python_api/__init__.py
./src/kiara/interfaces/__init__.py
./src/kiara/models
./src/kiara/models/aliases
./src/kiara/models/aliases/__init__.py
./src/kiara/models/archives.py
./src/kiara/models/context.py
./src/kiara/models/data_types.py
./src/kiara/models/documentation.py
./src/kiara/models/events
./src/kiara/models/events/alias_registry.py
./src/kiara/models/events/data_registry.py
./src/kiara/models/events/destiny_registry.py
./src/kiara/models/events/job_registry.py
./src/kiara/models/events/pipeline.py
./src/kiara/models/events/workflow_registry.py
./src/kiara/models/events/__init__.py
./src/kiara/models/filesystem.py
./src/kiara/models/metadata
./src/kiara/models/metadata/__init__.py
./src/kiara/models/module
./src/kiara/models/module/destiny.py
./src/kiara/models/module/jobs.py
./src/kiara/models/module/manifest.py
./src/kiara/models/module/operation.py
./src/kiara/models/module/persistence.py
./src/kiara/models/module/pipeline
./src/kiara/models/module/pipeline/controller.py
./src/kiara/models/module/pipeline/pipeline.py
./src/kiara/models/module/pipeline/stages.py
./src/kiara/models/module/pipeline/structure.py
./src/kiara/models/module/pipeline/value_refs.py
./src/kiara/models/module/pipeline/__init__.py
./src/kiara/models/module/__init__.py
./src/kiara/models/python_class.py
./src/kiara/models/rendering
./src/kiara/models/rendering/values.py
./src/kiara/models/rendering/__init__.py
./src/kiara/models/runtime_environment
./src/kiara/models/runtime_environment/kiara.py
./src/kiara/models/runtime_environment/operating_system.py
./src/kiara/models/runtime_environment/python.py
./src/kiara/models/runtime_environment/__init__.py
./src/kiara/models/values
./src/kiara/models/values/data_type.py
./src/kiara/models/values/lineage.py
./src/kiara/models/values/matchers.py
./src/kiara/models/values/value.py
./src/kiara/models/values/value_metadata
./src/kiara/models/values/value_metadata/included_metadata_types
./src/kiara/models/values/value_metadata/included_metadata_types/__init__.py
./src/kiara/models/values/value_metadata/__init__.py
./src/kiara/models/values/value_schema.py
./src/kiara/models/values/__init__.py
./src/kiara/models/workflow.py
./src/kiara/models/__init__.py
./src/kiara/modules
./src/kiara/modules/included_core_modules
./src/kiara/modules/included_core_modules/create_from.py
./src/kiara/modules/included_core_modules/export_as.py
./src/kiara/modules/included_core_modules/filesystem.py
./src/kiara/modules/included_core_modules/filter.py
./src/kiara/modules/included_core_modules/metadata.py
./src/kiara/modules/included_core_modules/mock.py
./src/kiara/modules/included_core_modules/pipeline.py
./src/kiara/modules/included_core_modules/pretty_print.py
./src/kiara/modules/included_core_modules/render_value.py
./src/kiara/modules/included_core_modules/serialization.py
./src/kiara/modules/included_core_modules/__init__.py
./src/kiara/modules/__init__.py
./src/kiara/operations
./src/kiara/operations/included_core_operations
./src/kiara/operations/included_core_operations/create_from.py
./src/kiara/operations/included_core_operations/export_as.py
./src/kiara/operations/included_core_operations/filter.py
./src/kiara/operations/included_core_operations/import_data.py
./src/kiara/operations/included_core_operations/metadata.py
./src/kiara/operations/included_core_operations/pipeline.py
./src/kiara/operations/included_core_operations/pretty_print.py
./src/kiara/operations/included_core_operations/render_data.py
./src/kiara/operations/included_core_operations/render_value.py
./src/kiara/operations/included_core_operations/serialize.py
./src/kiara/operations/included_core_operations/__init__.py
./src/kiara/operations/__init__.py
./src/kiara/processing
./src/kiara/processing/synchronous.py
./src/kiara/processing/__init__.py
./src/kiara/py.typed
./src/kiara/registries
./src/kiara/registries/aliases
./src/kiara/registries/aliases/archives.py
./src/kiara/registries/aliases/sqlite_store.py
./src/kiara/registries/aliases/__init__.py
./src/kiara/registries/data
./src/kiara/registries/data/data_store
./src/kiara/registries/data/data_store/filesystem_store.py
./src/kiara/registries/data/data_store/sqlite_store.py
./src/kiara/registries/data/data_store/__init__.py
./src/kiara/registries/data/__init__.py
./src/kiara/registries/environment
./src/kiara/registries/environment/__init__.py
./src/kiara/registries/events
./src/kiara/registries/events/metadata.py
./src/kiara/registries/events/registry.py
./src/kiara/registries/events/__init__.py
./src/kiara/registries/ids
./src/kiara/registries/ids/__init__.py
./src/kiara/registries/jobs
./src/kiara/registries/jobs/job_store
./src/kiara/registries/jobs/job_store/filesystem_store.py
./src/kiara/registries/jobs/job_store/sqlite_store.py
./src/kiara/registries/jobs/job_store/__init__.py
./src/kiara/registries/jobs/__init__.py
./src/kiara/registries/metadata
./src/kiara/registries/metadata/metadata_store
./src/kiara/registries/metadata/metadata_store/sqlite_store.py
./src/kiara/registries/metadata/metadata_store/__init__.py
./src/kiara/registries/metadata/__init__.py
./src/kiara/registries/models
./src/kiara/registries/models/__init__.py
./src/kiara/registries/modules
./src/kiara/registries/modules/__init__.py
./src/kiara/registries/operations
./src/kiara/registries/operations/__init__.py
./src/kiara/registries/rendering
./src/kiara/registries/rendering/__init__.py
./src/kiara/registries/templates
./src/kiara/registries/templates/__init__.py
./src/kiara/registries/types
./src/kiara/registries/types/__init__.py
./src/kiara/registries/workflows
./src/kiara/registries/workflows/archives.py
./src/kiara/registries/workflows/sqlite_store.py
./src/kiara/registries/workflows/__init__.py
./src/kiara/registries/__init__.py
./src/kiara/renderers
./src/kiara/renderers/included_renderers
./src/kiara/renderers/included_renderers/api
./src/kiara/renderers/included_renderers/api/base_api.py
./src/kiara/renderers/included_renderers/api/kiara_api.py
./src/kiara/renderers/included_renderers/api/__init__.py
./src/kiara/renderers/included_renderers/archive.py
./src/kiara/renderers/included_renderers/job.py
./src/kiara/renderers/included_renderers/pipeline.py
./src/kiara/renderers/included_renderers/value.py
./src/kiara/renderers/included_renderers/__init__.py
./src/kiara/renderers/jinja.py
./src/kiara/renderers/__init__.py
./src/kiara/resources
./src/kiara/resources/.gitkeep
./src/kiara/resources/templates
./src/kiara/resources/templates/doc_gen
./src/kiara/resources/templates/doc_gen/info_listing.j2
./src/kiara/resources/templates/render
./src/kiara/resources/templates/render/archive
./src/kiara/resources/templates/render/archive/static_page
./src/kiara/resources/templates/render/archive/static_page/page.html.j2
./src/kiara/resources/templates/render/kiara_api
./src/kiara/resources/templates/render/kiara_api/api_doc.md.j2
./src/kiara/resources/templates/render/kiara_api/kiara_api_endpoint.py.j2
./src/kiara/resources/templates/render/models
./src/kiara/resources/templates/render/models/generic_model_info.html
./src/kiara/resources/templates/render/models/info.operation.html
./src/kiara/resources/templates/render/models/info.value.html
./src/kiara/resources/templates/render/models/macros.html
./src/kiara/resources/templates/render/models/metadata.authors.html
./src/kiara/resources/templates/render/models/metadata.context.html
./src/kiara/resources/templates/render/models/metadata.documentation.html
./src/kiara/resources/templates/render/models/model_data.html
./src/kiara/resources/templates/render/pipeline
./src/kiara/resources/templates/render/pipeline/markdown
./src/kiara/resources/templates/render/pipeline/markdown/pipeline.md.j2
./src/kiara/resources/templates/render/pipeline/pipeline_info.md.j2
./src/kiara/resources/templates/render/pipeline/python_script.py.j2
./src/kiara/resources/templates/render/pipeline/static_page
./src/kiara/resources/templates/render/pipeline/static_page/page.html.j2
./src/kiara/resources/templates/render/pipeline/static_page/step.html.j2
./src/kiara/resources/tui
./src/kiara/resources/tui/pager_app.css
./src/kiara/utils
./src/kiara/utils/archives.py
./src/kiara/utils/class_loading.py
./src/kiara/utils/cli
./src/kiara/utils/cli/exceptions.py
./src/kiara/utils/cli/rich_click.py
./src/kiara/utils/cli/run.py
./src/kiara/utils/cli/__init__.py
./src/kiara/utils/concurrency.py
./src/kiara/utils/config.py
./src/kiara/utils/data.py
./src/kiara/utils/dates.py
./src/kiara/utils/db.py
./src/kiara/utils/debug.py
./src/kiara/utils/develop
./src/kiara/utils/develop/__init__.py
./src/kiara/utils/dicts.py
./src/kiara/utils/doc.py
./src/kiara/utils/downloads.py
./src/kiara/utils/files.py
./src/kiara/utils/global_metadata.py
./src/kiara/utils/graphs.py
./src/kiara/utils/hashfs
./src/kiara/utils/hashfs/__init__.py
./src/kiara/utils/hashing.py
./src/kiara/utils/html.py
./src/kiara/utils/introspection.py
./src/kiara/utils/json.py
./src/kiara/utils/metadata.py
./src/kiara/utils/models.py
./src/kiara/utils/modules.py
./src/kiara/utils/operations.py
./src/kiara/utils/output.py
./src/kiara/utils/pipelines.py
./src/kiara/utils/reflection.py
./src/kiara/utils/rendering.py
./src/kiara/utils/stores.py
./src/kiara/utils/string_vars.py
./src/kiara/utils/testing
./src/kiara/utils/testing/__init__.py
./src/kiara/utils/values.py
./src/kiara/utils/windows.py
./src/kiara/utils/yaml.py
./src/kiara/utils/__init__.py
./src/kiara/zmq
./src/kiara/zmq/client.py
./src/kiara/zmq/messages
./src/kiara/zmq/messages/__init__.py
./src/kiara/zmq/service
./src/kiara/zmq/service/__init__.py
./src/kiara/zmq/__init__.py
./src/kiara/__init__.py
./src/mkdocstrings_handlers
./src/mkdocstrings_handlers/kiara
./src/mkdocstrings_handlers/kiara/templates
./src/mkdocstrings_handlers/kiara/templates/material
./src/mkdocstrings_handlers/kiara/templates/material/.gitkeep
./src/mkdocstrings_handlers/kiara/__init__.py
./tests
./tests/conftest.py
./tests/resources
./tests/resources/archives
./tests/resources/archives/archive_create_jobs
./tests/resources/archives/archive_create_jobs/nand_true.yaml
./tests/resources/archives/nand_true.0.10.kiarchive
./tests/resources/archives/nand_true.0.10.kiarchive.json
./tests/resources/invalid_pipelines
./tests/resources/invalid_pipelines/logic_4.json
./tests/resources/kiara.config
./tests/resources/module_configs
./tests/resources/module_configs/and.json
./tests/resources/module_configs/and_wrapped.json
./tests/resources/module_configs/table_load.json
./tests/resources/pipelines
./tests/resources/pipelines/dummy
./tests/resources/pipelines/dummy/dummy_1.json
./tests/resources/pipelines/dummy/dummy_1_delay.json
./tests/resources/pipelines/logic
./tests/resources/pipelines/logic/logic_1.json
./tests/resources/pipelines/logic/logic_2.json
./tests/resources/pipelines/logic/logic_3.json
./tests/resources/pipelines/logic/logic_4.json
./tests/resources/pipelines/table_import.json
./tests/resources/pipelines/test_preseed_1.yaml
./tests/test_api
./tests/test_api/test_data_types.py
./tests/test_api/test_misc.py
./tests/test_api/test_module_types.py
./tests/test_api/test_operations.py
./tests/test_api/__init__.py
./tests/test_archives
./tests/test_archives/test_archive_export.py
./tests/test_archives/test_archive_import.py
./tests/test_cli
./tests/test_cli/test_context_subcommands.py
./tests/test_cli/test_data_subcommands.py
./tests/test_cli/test_metadata_subcommands.py
./tests/test_cli/test_misc_commands.py
./tests/test_cli/test_module_subcommands.py
./tests/test_cli/test_operation_subcommands.py
./tests/test_cli/test_pipeline_subcommands.py
./tests/test_cli/test_run_subcommand.py
./tests/test_cli/__init__.py
./tests/test_included_data_types
./tests/test_included_data_types/test_string.py
./tests/test_kiara_context.py
./tests/test_modules
./tests/test_modules/test_operations.py
./tests/test_modules/test_simple_module_exec.py
./tests/test_modules/__init__.py
./tests/test_module_instances.py
./tests/test_module_types.py
./tests/test_operations.py
./tests/test_operation_types
./tests/test_operation_types/test_extract_metadata.py
./tests/test_operation_types/test_persist_value.py
./tests/test_operation_types/test_render_value.py
./tests/test_operation_types/__init__.py
./tests/test_pipelines
./tests/test_pipelines/test_pipelines.py
./tests/test_pipelines/test_pipeline_configs.py
./tests/test_pipelines/__init__.py
./tests/test_pipeline_creation.py
./tests/test_rendering.py
./tests/test_values
./tests/test_values/test_values.py
./tests/test_values/__init__.py
./tests/utils.py
./tests/__init__.py
.
./.cruft.json
./.envrc.disabled
./.git
./.git/config
./.git/description
./.git/HEAD
./.git/hooks
./.git/hooks/applypatch-msg.sample
./.git/hooks/commit-msg.sample
./.git/hooks/fsmonitor-watchman.sample
./.git/hooks/post-update.sample
./.git/hooks/pre-applypatch.sample
./.git/hooks/pre-commit.sample
./.git/hooks/pre-merge-commit.sample
./.git/hooks/pre-push.sample
./.git/hooks/pre-rebase.sample
./.git/hooks/pre-receive.sample
./.git/hooks/prepare-commit-msg.sample
./.git/hooks/push-to-checkout.sample
./.git/hooks/update.sample
./.git/index
./.git/info
./.git/info/exclude
./.git/logs
./.git/logs/HEAD
./.git/logs/refs
./.git/logs/refs/heads
./.git/logs/refs/heads/develop
./.git/logs/refs/remotes
./.git/logs/refs/remotes/origin
./.git/logs/refs/remotes/origin/HEAD
./.git/objects
./.git/objects/info
./.git/objects/pack
./.git/objects/pack/pack-50d32aa2bfe040cdf261502e30541e3e3f19969a.idx
./.git/objects/pack/pack-50d32aa2bfe040cdf261502e30541e3e3f19969a.pack
./.git/packed-refs
./.git/refs
./.git/refs/heads
./.git/refs/heads/develop
./.git/refs/remotes
./.git/refs/remotes/origin
./.git/refs/remotes/origin/HEAD
./.git/refs/tags
./.gitattributes
./.github
./.github/ISSUE_TEMPLATE
./.github/ISSUE_TEMPLATE/bug_report.md
./.github/ISSUE_TEMPLATE/suggest-a-module.md
./.github/workflows
./.github/workflows/build-darwin.yaml
./.github/workflows/build-linux.yaml
./.github/workflows/build-windows.yaml
./.gitignore
./.git_archival.txt
./.pre-commit-config.yaml
./AUTHORS.md
./CHANGELOG.md
./ci
./ci/conda
./ci/conda/conda-pkg-patch.yaml
./commitlint.config.js
./docs
./docs/development.md
./docs/index.md
./docs/stylesheets
./docs/stylesheets/extra.css
./docs/SUMMARY.md
./docs/usage.md
./examples
./examples/data
./examples/data/gexf
./examples/data/gexf/lesmis.gexf
./examples/data/gexf/quakers.gexf
./examples/data/gexf/Readme.md
./examples/data/gml
./examples/data/gml/adjnoun.gml
./examples/data/gml/celegansneural.gml
./examples/data/gml/dolphins.gml
./examples/data/gml/football.gml
./examples/data/gml/karate.gml
./examples/data/gml/lesmis.gml
./examples/data/gml/polbooks.gml
./examples/data/gml/Readme.md
./examples/data/journals
./examples/data/journals/JournalEdges1902.csv
./examples/data/journals/JournalNodes1902.csv
./examples/data/journals/Readme.md
./examples/data/JSON
./examples/data/JSON/peacetreaties.json
./examples/data/JSON/Readme.md
./examples/data/quakers
./examples/data/quakers/quakers_edgelist.csv
./examples/data/quakers/quakers_nodelist.csv
./examples/data/quakers/Readme.md
./examples/data/Readme.md
./examples/data/simple_networks
./examples/data/simple_networks/connected
./examples/data/simple_networks/connected/SampleEdges.csv
./examples/data/simple_networks/connected/SampleNodes.csv
./examples/data/simple_networks/two_components
./examples/data/simple_networks/two_components/SampleEdges.csv
./examples/data/simple_networks/two_components/SampleNodes.csv
./examples/data/treaties
./examples/data/treaties/peace_treaties_rus.csv
./examples/data/treaties/Readme.md
./examples/jobs
./examples/jobs/create_journals_network.yaml
./examples/jobs/create_simple_disconnected_network.yaml
./examples/jobs/create_simple_network.yaml
./examples/jobs/notebooks
./examples/jobs/notebooks/create_journals_network.ipynb
./examples/jobs/notebooks/create_simple_network.ipynb
./examples/jobs/notebooks/utilities.ipynb
./examples/jobs/Readme.md
./examples/pipelines
./examples/pipelines/create_network_graph.yaml
./examples/pipelines/Readme.md
./examples/runs
./examples/runs/journal_components.run
./LICENSE
./Makefile
./MANIFEST.in
./mkdocs.yml
./pixi.toml
./pyproject.toml
./README.md
./scripts
./scripts/documentation
./scripts/documentation/gen_api_doc_pages.py
./scripts/documentation/gen_info_pages.py
./scripts/documentation/gen_module_doc.py
./src
./src/kiara_plugin
./src/kiara_plugin/network_analysis
./src/kiara_plugin/network_analysis/data_types
./src/kiara_plugin/network_analysis/data_types/__init__.py
./src/kiara_plugin/network_analysis/defaults.py
./src/kiara_plugin/network_analysis/models
./src/kiara_plugin/network_analysis/models/inputs.py
./src/kiara_plugin/network_analysis/models/metadata.py
./src/kiara_plugin/network_analysis/models/__init__.py
./src/kiara_plugin/network_analysis/modules
./src/kiara_plugin/network_analysis/modules/components.py
./src/kiara_plugin/network_analysis/modules/create.py
./src/kiara_plugin/network_analysis/modules/export.py
./src/kiara_plugin/network_analysis/modules/filters.py
./src/kiara_plugin/network_analysis/modules/rendering.py
./src/kiara_plugin/network_analysis/modules/__init__.py
./src/kiara_plugin/network_analysis/pipelines
./src/kiara_plugin/network_analysis/pipelines/.gitkeep
./src/kiara_plugin/network_analysis/pipelines/__init__.py
./src/kiara_plugin/network_analysis/py.typed
./src/kiara_plugin/network_analysis/resources
./src/kiara_plugin/network_analysis/resources/.gitkeep
./src/kiara_plugin/network_analysis/streamlit
./src/kiara_plugin/network_analysis/streamlit/components
./src/kiara_plugin/network_analysis/streamlit/components/data_import.py
./src/kiara_plugin/network_analysis/streamlit/components/__init__.py
./src/kiara_plugin/network_analysis/streamlit/__init__.py
./src/kiara_plugin/network_analysis/utils.py
./src/kiara_plugin/network_analysis/__init__.py
./tests
./tests/conftest.py
./tests/job_tests
./tests/job_tests/create_journals_network
./tests/job_tests/create_journals_network/outputs.py
./tests/job_tests/create_journals_network/outputs.yaml
./tests/resources
./tests/resources/.gitkeep
./tests/test_job_descs.py
./tests/test_kiara_modules_default.py
.
./.git
./.git/config
./.git/description
./.git/HEAD
./.git/hooks
./.git/hooks/applypatch-msg.sample
./.git/hooks/commit-msg.sample
./.git/hooks/fsmonitor-watchman.sample
./.git/hooks/post-update.sample
./.git/hooks/pre-applypatch.sample
./.git/hooks/pre-commit.sample
./.git/hooks/pre-merge-commit.sample
./.git/hooks/pre-push.sample
./.git/hooks/pre-rebase.sample
./.git/hooks/pre-receive.sample
./.git/hooks/prepare-commit-msg.sample
./.git/hooks/push-to-checkout.sample
./.git/hooks/update.sample
./.git/index
./.git/info
./.git/info/exclude
./.git/logs
./.git/logs/HEAD
./.git/logs/refs
./.git/logs/refs/heads
./.git/logs/refs/heads/main
./.git/logs/refs/remotes
./.git/logs/refs/remotes/origin
./.git/logs/refs/remotes/origin/HEAD
./.git/objects
./.git/objects/info
./.git/objects/pack
./.git/objects/pack/pack-c159c97bc25bb122279f93d0995cc25711431e4a.idx
./.git/objects/pack/pack-c159c97bc25bb122279f93d0995cc25711431e4a.pack
./.git/packed-refs
./.git/refs
./.git/refs/heads
./.git/refs/heads/main
./.git/refs/remotes
./.git/refs/remotes/origin
./.git/refs/remotes/origin/HEAD
./.git/refs/tags
./COPYING
./data
./data/JournalEdges1902.csv
./data/JournalNodes1902.csv
./OneModeNetwork.ipynb
./README.md
.
./.git
./.git/config
./.git/description
./.git/HEAD
./.git/hooks
./.git/hooks/applypatch-msg.sample
./.git/hooks/commit-msg.sample
./.git/hooks/fsmonitor-watchman.sample
./.git/hooks/post-update.sample
./.git/hooks/pre-applypatch.sample
./.git/hooks/pre-commit.sample
./.git/hooks/pre-merge-commit.sample
./.git/hooks/pre-push.sample
./.git/hooks/pre-rebase.sample
./.git/hooks/pre-receive.sample
./.git/hooks/prepare-commit-msg.sample
./.git/hooks/push-to-checkout.sample
./.git/hooks/update.sample
./.git/index
./.git/info
./.git/info/exclude
./.git/logs
./.git/logs/HEAD
./.git/logs/refs
./.git/logs/refs/heads
./.git/logs/refs/heads/master
./.git/logs/refs/remotes
./.git/logs/refs/remotes/origin
./.git/logs/refs/remotes/origin/HEAD
./.git/objects
./.git/objects/info
./.git/objects/pack
./.git/objects/pack/pack-6093d41861fdde8fe22d0ba7c6b2616e854e3208.idx
./.git/objects/pack/pack-6093d41861fdde8fe22d0ba7c6b2616e854e3208.pack
./.git/packed-refs
./.git/refs
./.git/refs/heads
./.git/refs/heads/master
./.git/refs/remotes
./.git/refs/remotes/origin
./.git/refs/remotes/origin/HEAD
./.git/refs/tags
./archived_notebook_versions
./archived_notebook_versions/pyLDAvis_relevance.ipynb
./archived_notebook_versions/topic_modelling_bigrams.ipynb
./archived_notebook_versions/topic_modelling_bigrams_colab.ipynb
./archived_notebook_versions/topic_modelling_bigrams_colab_sample.ipynb
./archived_notebook_versions/topic_modelling_colab_mallet.ipynb
./archived_notebook_versions/topic_modelling_colab_tfidf_sample.ipynb
./archived_notebook_versions/topic_modelling_gensim.ipynb
./archived_notebook_versions/topic_modelling_gensim_colab.ipynb
./archived_notebook_versions/topic_modelling_gensim_colab_sample.ipynb
./archived_notebook_versions/topic_modelling_gensim_colab_sample_w_rollingmeanpub.ipynb
./archived_notebook_versions/topic_modelling_gensim_mallet.ipynb
./archived_notebook_versions/topic_modelling_gensim_real_data.ipynb
./archived_notebook_versions/topic_modelling_mallet.ipynb
./archived_notebook_versions/topic_modelling_mallet_colab.ipynb
./archived_notebook_versions/topic_modelling_tfidf.ipynb
./archived_notebook_versions/topic_modelling_tfidf_colab.ipynb
./COPYING
./img
./img/corpus_preview.png
./img/subset_check.png
./README.md
./requirements.txt
./stop_words.csv
./tests
./tests/coherence_tests.ipynb
./tests/mallet_gensim_conversion_test_colab.ipynb
./topic_modelling.ipynb
./vis-files
./vis-files/readme.md
./vis-files/tm_1
./vis-files/tm_1/2.64b0803b.chunk.js
./vis-files/tm_1/jscode-test-2.js
./vis-files/tm_1/jscode-test.js
./vis-files/tm_1/main.8a3a0dbd.chunk.css
./vis-files/tm_1/main.c8962a70.chunk.js
./vis-files/tm_1/tm1_data_prepare_gensim.py
./vis-files/tm_1/tm1_gensim.py
./vis-files/tm_1/tm1_mallet.py
./vis-files/tm_1/tm1_tfidf.py
./vis-files/tm_1/topic_info.csv
./vis-files/tm_1/topic_proportion.csv
./vis-files/tm_coherence_chart.js
./vis-files/tm_coherence_table.js
./vis-files/tm_distribution_year_rolling.js
./vis-files/tm_publication_dist.js
./vis-files/tm_topic_dist_year.js
./vis-files/tm_topic_dist_year_bar.js
./vis-files/tm_topic_proportion_chart.js
./vis-files/vis_tm1_notebook_integration_test.ipynb
.
./.eslintrc.js
./.git
./.git/config
./.git/description
./.git/HEAD
./.git/hooks
./.git/hooks/applypatch-msg.sample
./.git/hooks/commit-msg.sample
./.git/hooks/fsmonitor-watchman.sample
./.git/hooks/post-update.sample
./.git/hooks/pre-applypatch.sample
./.git/hooks/pre-commit.sample
./.git/hooks/pre-merge-commit.sample
./.git/hooks/pre-push.sample
./.git/hooks/pre-rebase.sample
./.git/hooks/pre-receive.sample
./.git/hooks/prepare-commit-msg.sample
./.git/hooks/push-to-checkout.sample
./.git/hooks/update.sample
./.git/index
./.git/info
./.git/info/exclude
./.git/logs
./.git/logs/HEAD
./.git/logs/refs
./.git/logs/refs/heads
./.git/logs/refs/heads/master
./.git/logs/refs/remotes
./.git/logs/refs/remotes/origin
./.git/logs/refs/remotes/origin/HEAD
./.git/objects
./.git/objects/info
./.git/objects/pack
./.git/objects/pack/pack-c35b51f558f8cd65a67cd61d9eaf128c1187109a.idx
./.git/objects/pack/pack-c35b51f558f8cd65a67cd61d9eaf128c1187109a.pack
./.git/packed-refs
./.git/refs
./.git/refs/heads
./.git/refs/heads/master
./.git/refs/remotes
./.git/refs/remotes/origin
./.git/refs/remotes/origin/HEAD
./.git/refs/tags
./.gitignore
./.prettierrc.js
./.vscode
./.vscode/settings.json
./babel.config.js
./dharpa_jupyterlab_extension_example
./dharpa_jupyterlab_extension_example/__init__.py
./install.json
./package.json
./postcss.config.js
./pyproject.toml
./README.md
./setup.py
./src
./src/common
./src/common/ModelContext.tsx
./src/components
./src/components/app.scss
./src/components/App.tsx
./src/index.html
./src/index.ts
./src/jupyter
./src/jupyter/index.ts
./src/jupyter/kernelModel.ts
./src/jupyter/panel.ts
./src/jupyter/widget.tsx
./src/standalone.tsx
./tailwind.config.js
./tsconfig.json
./webpack.config.labext.js
./webpack.config.standalone.js
./yarn.lock
.
./.git
./.git/config
./.git/description
./.git/HEAD
./.git/hooks
./.git/hooks/applypatch-msg.sample
./.git/hooks/commit-msg.sample
./.git/hooks/fsmonitor-watchman.sample
./.git/hooks/post-update.sample
./.git/hooks/pre-applypatch.sample
./.git/hooks/pre-commit.sample
./.git/hooks/pre-merge-commit.sample
./.git/hooks/pre-push.sample
./.git/hooks/pre-rebase.sample
./.git/hooks/pre-receive.sample
./.git/hooks/prepare-commit-msg.sample
./.git/hooks/push-to-checkout.sample
./.git/hooks/update.sample
./.git/index
./.git/info
./.git/info/exclude
./.git/logs
./.git/logs/HEAD
./.git/logs/refs
./.git/logs/refs/heads
./.git/logs/refs/heads/master
./.git/logs/refs/remotes
./.git/logs/refs/remotes/origin
./.git/logs/refs/remotes/origin/HEAD
./.git/objects
./.git/objects/info
./.git/objects/pack
./.git/objects/pack/pack-fe0591af46ba506f73b18acd2ce9319eb287ab0f.idx
./.git/objects/pack/pack-fe0591af46ba506f73b18acd2ce9319eb287ab0f.pack
./.git/packed-refs
./.git/refs
./.git/refs/heads
./.git/refs/heads/master
./.git/refs/remotes
./.git/refs/remotes/origin
./.git/refs/remotes/origin/HEAD
./.git/refs/tags
./.github
./.github/workflows
./.github/workflows/build-linux.yaml
./.gitignore
./build.sbt
./ci
./ci/conda
./ci/conda/conda-pkg-patch.yaml
./LICENSE
./NOTICE
./project
./project/plugins.sbt
./project/project
./project/project/target
./project/project/target/config-classes
./project/project/target/config-classes/$199aed1c4a6cadfe5bab$$anonfun$$sbtdef$1.class
./project/project/target/config-classes/$199aed1c4a6cadfe5bab$.class
./project/project/target/config-classes/$199aed1c4a6cadfe5bab.cache
./project/project/target/config-classes/$199aed1c4a6cadfe5bab.class
./project/project/target/config-classes/$240e278a9ccafc73a758$.class
./project/project/target/config-classes/$240e278a9ccafc73a758.cache
./project/project/target/config-classes/$240e278a9ccafc73a758.class
./project/project/target/config-classes/$2e0dfd9443d9c5ea9ec2$$anonfun$$sbtdef$1.class
./project/project/target/config-classes/$2e0dfd9443d9c5ea9ec2$.class
./project/project/target/config-classes/$2e0dfd9443d9c5ea9ec2.cache
./project/project/target/config-classes/$2e0dfd9443d9c5ea9ec2.class
./project/project/target/config-classes/$46abe603945a8d610ba4$.class
./project/project/target/config-classes/$46abe603945a8d610ba4.cache
./project/project/target/config-classes/$46abe603945a8d610ba4.class
./project/project/target/config-classes/$54cb83494900f1d8c1df$$anonfun$$sbtdef$1.class
./project/project/target/config-classes/$54cb83494900f1d8c1df$.class
./project/project/target/config-classes/$54cb83494900f1d8c1df.cache
./project/project/target/config-classes/$54cb83494900f1d8c1df.class
./project/project/target/config-classes/$6c333fbe8119be4b4813$.class
./project/project/target/config-classes/$6c333fbe8119be4b4813.cache
./project/project/target/config-classes/$6c333fbe8119be4b4813.class
./project/project/target/config-classes/$70c40b6651adb5f6784f$.class
./project/project/target/config-classes/$70c40b6651adb5f6784f.cache
./project/project/target/config-classes/$70c40b6651adb5f6784f.class
./project/project/target/config-classes/$7447bf20cbc259232937$$anonfun$$sbtdef$1.class
./project/project/target/config-classes/$7447bf20cbc259232937$.class
./project/project/target/config-classes/$7447bf20cbc259232937.cache
./project/project/target/config-classes/$7447bf20cbc259232937.class
./project/project/target/config-classes/$7a5bbe82ce3664cc112f$.class
./project/project/target/config-classes/$7a5bbe82ce3664cc112f.cache
./project/project/target/config-classes/$7a5bbe82ce3664cc112f.class
./project/project/target/config-classes/$7c6405447769aac4fa12$.class
./project/project/target/config-classes/$7c6405447769aac4fa12.cache
./project/project/target/config-classes/$7c6405447769aac4fa12.class
./project/project/target/config-classes/$8dbefffaed01806bbee2$.class
./project/project/target/config-classes/$8dbefffaed01806bbee2.cache
./project/project/target/config-classes/$8dbefffaed01806bbee2.class
./project/project/target/config-classes/$95376d5467fcb5c63abc$.class
./project/project/target/config-classes/$95376d5467fcb5c63abc.cache
./project/project/target/config-classes/$95376d5467fcb5c63abc.class
./project/project/target/config-classes/$a86ddade965b72b5660d$$anonfun$$sbtdef$1.class
./project/project/target/config-classes/$a86ddade965b72b5660d$.class
./project/project/target/config-classes/$a86ddade965b72b5660d.cache
./project/project/target/config-classes/$a86ddade965b72b5660d.class
./project/project/target/config-classes/$ae4096496d613bbf7c0f$$anonfun$$sbtdef$1.class
./project/project/target/config-classes/$ae4096496d613bbf7c0f$.class
./project/project/target/config-classes/$ae4096496d613bbf7c0f.cache
./project/project/target/config-classes/$ae4096496d613bbf7c0f.class
./project/project/target/config-classes/$bcc4954f441d003af0f9$.class
./project/project/target/config-classes/$bcc4954f441d003af0f9.cache
./project/project/target/config-classes/$bcc4954f441d003af0f9.class
./project/project/target/config-classes/$db7cf20348368cf2531e$$anonfun$$sbtdef$1.class
./project/project/target/config-classes/$db7cf20348368cf2531e$.class
./project/project/target/config-classes/$db7cf20348368cf2531e.cache
./project/project/target/config-classes/$db7cf20348368cf2531e.class
./project/project/target/config-classes/$e78b709a118dd216360c$$anonfun$$sbtdef$1.class
./project/project/target/config-classes/$e78b709a118dd216360c$.class
./project/project/target/config-classes/$e78b709a118dd216360c.cache
./project/project/target/config-classes/$e78b709a118dd216360c.class
./project/project/target/config-classes/$f269997d9ce3e54dd31b$$anonfun$$sbtdef$1.class
./project/project/target/config-classes/$f269997d9ce3e54dd31b$.class
./project/project/target/config-classes/$f269997d9ce3e54dd31b.cache
./project/project/target/config-classes/$f269997d9ce3e54dd31b.class
./pyasciinet
./pyasciinet/asciinet
./pyasciinet/asciinet/lib
./pyasciinet/asciinet/lib/asciigraph-assembly-0.2.1.jar
./pyasciinet/asciinet/test
./pyasciinet/asciinet/test/base.py
./pyasciinet/asciinet/test/test_ascii.py
./pyasciinet/asciinet/test/__init__.py
./pyasciinet/asciinet/_libutil.py
./pyasciinet/asciinet/__init__.py
./pyasciinet/asciinet/__version__.py
./pyasciinet/build.sh
./pyasciinet/dependencies.txt
./pyasciinet/examples
./pyasciinet/examples/example.py
./pyasciinet/ez_setup.py
./pyasciinet/LICENSE
./pyasciinet/MANIFEST.in
./pyasciinet/NOTICE
./pyasciinet/pyasciigraph.iml
./pyasciinet/README.md
./pyasciinet/requirements.sh
./pyasciinet/requirements.txt
./pyasciinet/setup.py
./README.md
./src
./src/main
./src/main/scala
./src/main/scala/com
./src/main/scala/com/ascii
./src/main/scala/com/ascii/AsciiGraph.scala
./src/main/scala/com/ascii/AsciiGraphContainer.scala
./src/main/scala/com/ascii/Server.scala


# kiara\jupyterlab_extension_example_files.txt
.
./.eslintrc.js
./.git
./.git/config
./.git/description
./.git/HEAD
./.git/hooks
./.git/hooks/applypatch-msg.sample
./.git/hooks/commit-msg.sample
./.git/hooks/fsmonitor-watchman.sample
./.git/hooks/post-update.sample
./.git/hooks/pre-applypatch.sample
./.git/hooks/pre-commit.sample
./.git/hooks/pre-merge-commit.sample
./.git/hooks/pre-push.sample
./.git/hooks/pre-rebase.sample
./.git/hooks/pre-receive.sample
./.git/hooks/prepare-commit-msg.sample
./.git/hooks/push-to-checkout.sample
./.git/hooks/update.sample
./.git/index
./.git/info
./.git/info/exclude
./.git/logs
./.git/logs/HEAD
./.git/logs/refs
./.git/logs/refs/heads
./.git/logs/refs/heads/master
./.git/logs/refs/remotes
./.git/logs/refs/remotes/origin
./.git/logs/refs/remotes/origin/HEAD
./.git/objects
./.git/objects/info
./.git/objects/pack
./.git/objects/pack/pack-c35b51f558f8cd65a67cd61d9eaf128c1187109a.idx
./.git/objects/pack/pack-c35b51f558f8cd65a67cd61d9eaf128c1187109a.pack
./.git/packed-refs
./.git/refs
./.git/refs/heads
./.git/refs/heads/master
./.git/refs/remotes
./.git/refs/remotes/origin
./.git/refs/remotes/origin/HEAD
./.git/refs/tags
./.gitignore
./.prettierrc.js
./.vscode
./.vscode/settings.json
./babel.config.js
./dharpa_jupyterlab_extension_example
./dharpa_jupyterlab_extension_example/__init__.py
./install.json
./package.json
./postcss.config.js
./pyproject.toml
./README.md
./setup.py
./src
./src/common
./src/common/ModelContext.tsx
./src/components
./src/components/app.scss
./src/components/App.tsx
./src/index.html
./src/index.ts
./src/jupyter
./src/jupyter/index.ts
./src/jupyter/kernelModel.ts
./src/jupyter/panel.ts
./src/jupyter/widget.tsx
./src/standalone.tsx
./tailwind.config.js
./tsconfig.json
./webpack.config.labext.js
./webpack.config.standalone.js
./yarn.lock


# kiara\kiara_files.txt
.
./.editorconfig
./.envrc.disabled
./.git
./.git/config
./.git/description
./.git/HEAD
./.git/hooks
./.git/hooks/applypatch-msg.sample
./.git/hooks/commit-msg.sample
./.git/hooks/fsmonitor-watchman.sample
./.git/hooks/post-update.sample
./.git/hooks/pre-applypatch.sample
./.git/hooks/pre-commit.sample
./.git/hooks/pre-merge-commit.sample
./.git/hooks/pre-push.sample
./.git/hooks/pre-rebase.sample
./.git/hooks/pre-receive.sample
./.git/hooks/prepare-commit-msg.sample
./.git/hooks/push-to-checkout.sample
./.git/hooks/update.sample
./.git/index
./.git/info
./.git/info/exclude
./.git/logs
./.git/logs/HEAD
./.git/logs/refs
./.git/logs/refs/heads
./.git/logs/refs/heads/develop
./.git/logs/refs/remotes
./.git/logs/refs/remotes/origin
./.git/logs/refs/remotes/origin/HEAD
./.git/objects
./.git/objects/info
./.git/objects/pack
./.git/objects/pack/pack-ec7b721bbdb50d3ab4c8e67e4647bcc61580155c.idx
./.git/objects/pack/pack-ec7b721bbdb50d3ab4c8e67e4647bcc61580155c.pack
./.git/packed-refs
./.git/refs
./.git/refs/heads
./.git/refs/heads/develop
./.git/refs/remotes
./.git/refs/remotes/origin
./.git/refs/remotes/origin/HEAD
./.git/refs/tags
./.gitattributes
./.github
./.github/dependabot.yml
./.github/ISSUE_TEMPLATE
./.github/ISSUE_TEMPLATE/bug_report.md
./.github/ISSUE_TEMPLATE/feature_request.md
./.github/workflows
./.github/workflows/build-darwin.yaml
./.github/workflows/build-linux.yaml
./.github/workflows/build-windows.yaml
./.gitignore
./.gitlab-ci.yml
./.git_archival.txt
./.kiara_complete.zsh
./.pre-commit-config.yaml
./AUTHORS.rst
./CHANGELOG.md
./ci
./ci/conda
./ci/conda/airium
./ci/conda/airium/conda-pkg-patch.yaml
./ci/conda/bases
./ci/conda/bases/conda-pkg-patch.yaml
./ci/conda/conda-pkg-patch.yaml
./ci/conda/dag-cbor
./ci/conda/dag-cbor/conda-pkg-patch.yaml
./ci/conda/email-validator
./ci/conda/email-validator/conda-pkg-patch.yaml
./ci/conda/fasteners
./ci/conda/fasteners/conda-pkg-patch.yaml
./ci/conda/jupyter-client
./ci/conda/jupyter-client/conda-pkg-patch.yaml
./ci/conda/jupyter-core
./ci/conda/jupyter-core/conda-pkg-patch.yaml
./ci/conda/makkus.bases
./ci/conda/makkus.bases/conda-pkg-patch.yaml
./ci/conda/makkus.dag-cbor
./ci/conda/makkus.dag-cbor/conda-pkg-patch.yaml
./ci/conda/makkus.multiformats
./ci/conda/makkus.multiformats/multiformats.yaml
./ci/conda/makkus.multiformats-config
./ci/conda/makkus.multiformats-config/multiformats-config.yaml
./ci/conda/mknotebooks
./ci/conda/mknotebooks/conda-pkg-patch.yaml
./ci/conda/multiformats
./ci/conda/multiformats/multiformats.yaml
./ci/conda/multiformats-config
./ci/conda/multiformats-config/multiformats-config.yaml
./ci/conda/pp-ez
./ci/conda/pp-ez/conda-pkg-patch.yaml
./ci/conda/puremagic
./ci/conda/puremagic/conda-pkg-patch.yaml
./ci/conda/pydantic-to-typescript
./ci/conda/pydantic-to-typescript/conda-pkg-patch.yaml
./ci/conda/Readme.md
./ci/conda/typing-validation
./ci/conda/typing-validation/conda-pkg-patch.yaml
./commitlint.config.js
./dev
./dev/dev.ipynb
./dev/network_analysis.ipynb
./dev/script.py
./docs
./docs/design_docs
./docs/design_docs/architecture
./docs/design_docs/architecture/assumptions.md
./docs/design_docs/architecture/data
./docs/design_docs/architecture/data/data_centric_approach.ipynb
./docs/design_docs/architecture/data/data_formats.ipynb
./docs/design_docs/architecture/data/dev.ipynb
./docs/design_docs/architecture/data/index.md
./docs/design_docs/architecture/data/persistence.md
./docs/design_docs/architecture/data/requirements.ipynb
./docs/design_docs/architecture/data/result_tree.png
./docs/design_docs/architecture/decisions.md
./docs/design_docs/architecture/index.md
./docs/design_docs/architecture/metadata.md
./docs/design_docs/architecture/SUMMARY.md
./docs/design_docs/architecture/workflows
./docs/design_docs/architecture/workflows/index.md
./docs/design_docs/architecture/workflows/modularity
./docs/design_docs/architecture/workflows/modularity/modularity.ipynb
./docs/design_docs/architecture/workflows/modularity/modularity_2.ipynb
./docs/design_docs/architecture/workflows/modularity/workflows
./docs/design_docs/architecture/workflows/modularity/workflows/corpus_processing.yaml
./docs/design_docs/architecture/workflows/modularity/workflows/corpus_processing_simple.yaml
./docs/design_docs/architecture/workflows/modularity/workflows/input_files_processing.yaml
./docs/design_docs/index.md
./docs/design_docs/SUMMARY.md
./docs/development
./docs/development/index.md
./docs/development/install.md
./docs/development/modules
./docs/development/modules/render_value.md
./docs/development/modules/SUMMARY.md
./docs/development/rendering.md
./docs/development/stores.md
./docs/development/SUMMARY.md
./docs/included_components
./docs/included_components/index.md
./docs/index.md
./docs/stylesheets
./docs/stylesheets/extra.css
./docs/SUMMARY.md
./examples
./examples/data
./examples/data/.gitkeep
./examples/data/journals
./examples/data/journals/JournalEdges1902.csv
./examples/data/journals/JournalNodes1902.csv
./examples/data/journals/query.graphql
./examples/data/journals/Readme.md
./examples/data/text_corpus
./examples/data/text_corpus/La_Ragione
./examples/data/text_corpus/La_Ragione/sn84037024_1917-04-25_ed-1_seq-1_ocr.txt
./examples/data/text_corpus/La_Ragione/sn84037024_1917-04-25_ed-2_seq-1_ocr.txt
./examples/data/text_corpus/La_Ragione/sn84037024_1917-04-25_ed-3_seq-1_ocr.txt
./examples/data/text_corpus/La_Ragione/sn84037024_1917-04-25_ed-4_seq-1_ocr.txt
./examples/data/text_corpus/La_Ragione/sn84037024_1917-05-05_ed-1_seq-1_ocr.txt
./examples/data/text_corpus/La_Ragione/sn84037024_1917-05-05_ed-2_seq-1_ocr.txt
./examples/data/text_corpus/La_Ragione/sn84037024_1917-05-05_ed-3_seq-1_ocr.txt
./examples/data/text_corpus/La_Ragione/sn84037024_1917-05-05_ed-4_seq-1_ocr.txt
./examples/data/text_corpus/La_Ragione/sn84037024_1917-05-16_ed-1_seq-1_ocr.txt
./examples/data/text_corpus/La_Ragione/sn84037024_1917-05-16_ed-2_seq-1_ocr.txt
./examples/data/text_corpus/La_Ragione/sn84037024_1917-05-16_ed-3_seq-1_ocr.txt
./examples/data/text_corpus/La_Rassegna
./examples/data/text_corpus/La_Rassegna/sn84037025_1917-04-07_ed-1_seq-1_ocr.txt
./examples/data/text_corpus/La_Rassegna/sn84037025_1917-04-14_ed-1_seq-1_ocr.txt
./examples/data/text_corpus/La_Rassegna/sn84037025_1917-04-14_ed-2_seq-1_ocr.txt
./examples/data/text_corpus/La_Rassegna/sn84037025_1917-04-21_ed-1_seq-1_ocr.txt
./examples/data/text_corpus/La_Rassegna/sn84037025_1917-04-21_ed-2_seq-1_ocr.txt
./examples/data/text_corpus/Readme.md
./examples/jupyter
./examples/jupyter/install_example.ipynb
./examples/pipelines
./examples/pipelines/mock_pipeline_1.yaml
./examples/scripts
./examples/scripts/import_df.py
./LICENSE
./Makefile
./MANIFEST.in
./mkdocs.yml
./onboarding.folder_to_table.json
./pyproject.toml
./README.md
./scripts
./scripts/documentation
./scripts/documentation/gen_api_doc_pages.py
./scripts/documentation/gen_info_pages.py
./scripts/documentation/gen_schemas.py
./src
./src/kiara
./src/kiara/api.py
./src/kiara/context
./src/kiara/context/config.py
./src/kiara/context/orm.py
./src/kiara/context/runtime_config.py
./src/kiara/context/__init__.py
./src/kiara/data_types
./src/kiara/data_types/included_core_types
./src/kiara/data_types/included_core_types/filesystem.py
./src/kiara/data_types/included_core_types/internal
./src/kiara/data_types/included_core_types/internal/render_value.py
./src/kiara/data_types/included_core_types/internal/__init__.py
./src/kiara/data_types/included_core_types/metadata.py
./src/kiara/data_types/included_core_types/serialization.py
./src/kiara/data_types/included_core_types/__init__.py
./src/kiara/data_types/__init__.py
./src/kiara/defaults.py
./src/kiara/doc
./src/kiara/doc/generate_api_doc.py
./src/kiara/doc/gen_info_pages.py
./src/kiara/doc/mkdocstrings
./src/kiara/doc/mkdocstrings/collector.py
./src/kiara/doc/mkdocstrings/handler.py
./src/kiara/doc/mkdocstrings/renderer.py
./src/kiara/doc/mkdocstrings/__init__.py
./src/kiara/doc/mkdocs_macros_cli.py
./src/kiara/doc/mkdocs_macros_kiara.py
./src/kiara/doc/__init__.py
./src/kiara/exceptions.py
./src/kiara/interfaces
./src/kiara/interfaces/cli
./src/kiara/interfaces/cli/archive
./src/kiara/interfaces/cli/archive/commands.py
./src/kiara/interfaces/cli/archive/__init__.py
./src/kiara/interfaces/cli/context
./src/kiara/interfaces/cli/context/commands.py
./src/kiara/interfaces/cli/context/__init__.py
./src/kiara/interfaces/cli/data
./src/kiara/interfaces/cli/data/commands.py
./src/kiara/interfaces/cli/data/__init__.py
./src/kiara/interfaces/cli/info
./src/kiara/interfaces/cli/info/commands.py
./src/kiara/interfaces/cli/info/__init__.py
./src/kiara/interfaces/cli/module
./src/kiara/interfaces/cli/module/commands.py
./src/kiara/interfaces/cli/module/__init__.py
./src/kiara/interfaces/cli/operation
./src/kiara/interfaces/cli/operation/commands.py
./src/kiara/interfaces/cli/operation/__init__.py
./src/kiara/interfaces/cli/pipeline
./src/kiara/interfaces/cli/pipeline/commands.py
./src/kiara/interfaces/cli/pipeline/__init__.py
./src/kiara/interfaces/cli/proxy_cli.py
./src/kiara/interfaces/cli/render
./src/kiara/interfaces/cli/render/commands.py
./src/kiara/interfaces/cli/render/__init__.py
./src/kiara/interfaces/cli/run.py
./src/kiara/interfaces/cli/type
./src/kiara/interfaces/cli/type/commands.py
./src/kiara/interfaces/cli/type/__init__.py
./src/kiara/interfaces/cli/workflow
./src/kiara/interfaces/cli/workflow/commands.py
./src/kiara/interfaces/cli/workflow/__init__.py
./src/kiara/interfaces/cli/__init__.py
./src/kiara/interfaces/python_api
./src/kiara/interfaces/python_api/base_api.py
./src/kiara/interfaces/python_api/batch.py
./src/kiara/interfaces/python_api/kiara_api.py
./src/kiara/interfaces/python_api/models
./src/kiara/interfaces/python_api/models/archive.py
./src/kiara/interfaces/python_api/models/doc.py
./src/kiara/interfaces/python_api/models/info.py
./src/kiara/interfaces/python_api/models/job.py
./src/kiara/interfaces/python_api/models/workflow.py
./src/kiara/interfaces/python_api/models/__init__.py
./src/kiara/interfaces/python_api/operation.py
./src/kiara/interfaces/python_api/proxy.py
./src/kiara/interfaces/python_api/utils.py
./src/kiara/interfaces/python_api/value.py
./src/kiara/interfaces/python_api/workflow.py
./src/kiara/interfaces/python_api/__init__.py
./src/kiara/interfaces/__init__.py
./src/kiara/models
./src/kiara/models/aliases
./src/kiara/models/aliases/__init__.py
./src/kiara/models/archives.py
./src/kiara/models/context.py
./src/kiara/models/data_types.py
./src/kiara/models/documentation.py
./src/kiara/models/events
./src/kiara/models/events/alias_registry.py
./src/kiara/models/events/data_registry.py
./src/kiara/models/events/destiny_registry.py
./src/kiara/models/events/job_registry.py
./src/kiara/models/events/pipeline.py
./src/kiara/models/events/workflow_registry.py
./src/kiara/models/events/__init__.py
./src/kiara/models/filesystem.py
./src/kiara/models/metadata
./src/kiara/models/metadata/__init__.py
./src/kiara/models/module
./src/kiara/models/module/destiny.py
./src/kiara/models/module/jobs.py
./src/kiara/models/module/manifest.py
./src/kiara/models/module/operation.py
./src/kiara/models/module/persistence.py
./src/kiara/models/module/pipeline
./src/kiara/models/module/pipeline/controller.py
./src/kiara/models/module/pipeline/pipeline.py
./src/kiara/models/module/pipeline/stages.py
./src/kiara/models/module/pipeline/structure.py
./src/kiara/models/module/pipeline/value_refs.py
./src/kiara/models/module/pipeline/__init__.py
./src/kiara/models/module/__init__.py
./src/kiara/models/python_class.py
./src/kiara/models/rendering
./src/kiara/models/rendering/values.py
./src/kiara/models/rendering/__init__.py
./src/kiara/models/runtime_environment
./src/kiara/models/runtime_environment/kiara.py
./src/kiara/models/runtime_environment/operating_system.py
./src/kiara/models/runtime_environment/python.py
./src/kiara/models/runtime_environment/__init__.py
./src/kiara/models/values
./src/kiara/models/values/data_type.py
./src/kiara/models/values/lineage.py
./src/kiara/models/values/matchers.py
./src/kiara/models/values/value.py
./src/kiara/models/values/value_metadata
./src/kiara/models/values/value_metadata/included_metadata_types
./src/kiara/models/values/value_metadata/included_metadata_types/__init__.py
./src/kiara/models/values/value_metadata/__init__.py
./src/kiara/models/values/value_schema.py
./src/kiara/models/values/__init__.py
./src/kiara/models/workflow.py
./src/kiara/models/__init__.py
./src/kiara/modules
./src/kiara/modules/included_core_modules
./src/kiara/modules/included_core_modules/create_from.py
./src/kiara/modules/included_core_modules/export_as.py
./src/kiara/modules/included_core_modules/filesystem.py
./src/kiara/modules/included_core_modules/filter.py
./src/kiara/modules/included_core_modules/metadata.py
./src/kiara/modules/included_core_modules/mock.py
./src/kiara/modules/included_core_modules/pipeline.py
./src/kiara/modules/included_core_modules/pretty_print.py
./src/kiara/modules/included_core_modules/render_value.py
./src/kiara/modules/included_core_modules/serialization.py
./src/kiara/modules/included_core_modules/__init__.py
./src/kiara/modules/__init__.py
./src/kiara/operations
./src/kiara/operations/included_core_operations
./src/kiara/operations/included_core_operations/create_from.py
./src/kiara/operations/included_core_operations/export_as.py
./src/kiara/operations/included_core_operations/filter.py
./src/kiara/operations/included_core_operations/import_data.py
./src/kiara/operations/included_core_operations/metadata.py
./src/kiara/operations/included_core_operations/pipeline.py
./src/kiara/operations/included_core_operations/pretty_print.py
./src/kiara/operations/included_core_operations/render_data.py
./src/kiara/operations/included_core_operations/render_value.py
./src/kiara/operations/included_core_operations/serialize.py
./src/kiara/operations/included_core_operations/__init__.py
./src/kiara/operations/__init__.py
./src/kiara/processing
./src/kiara/processing/synchronous.py
./src/kiara/processing/__init__.py
./src/kiara/py.typed
./src/kiara/registries
./src/kiara/registries/aliases
./src/kiara/registries/aliases/archives.py
./src/kiara/registries/aliases/sqlite_store.py
./src/kiara/registries/aliases/__init__.py
./src/kiara/registries/data
./src/kiara/registries/data/data_store
./src/kiara/registries/data/data_store/filesystem_store.py
./src/kiara/registries/data/data_store/sqlite_store.py
./src/kiara/registries/data/data_store/__init__.py
./src/kiara/registries/data/__init__.py
./src/kiara/registries/environment
./src/kiara/registries/environment/__init__.py
./src/kiara/registries/events
./src/kiara/registries/events/metadata.py
./src/kiara/registries/events/registry.py
./src/kiara/registries/events/__init__.py
./src/kiara/registries/ids
./src/kiara/registries/ids/__init__.py
./src/kiara/registries/jobs
./src/kiara/registries/jobs/job_store
./src/kiara/registries/jobs/job_store/filesystem_store.py
./src/kiara/registries/jobs/job_store/sqlite_store.py
./src/kiara/registries/jobs/job_store/__init__.py
./src/kiara/registries/jobs/__init__.py
./src/kiara/registries/metadata
./src/kiara/registries/metadata/metadata_store
./src/kiara/registries/metadata/metadata_store/sqlite_store.py
./src/kiara/registries/metadata/metadata_store/__init__.py
./src/kiara/registries/metadata/__init__.py
./src/kiara/registries/models
./src/kiara/registries/models/__init__.py
./src/kiara/registries/modules
./src/kiara/registries/modules/__init__.py
./src/kiara/registries/operations
./src/kiara/registries/operations/__init__.py
./src/kiara/registries/rendering
./src/kiara/registries/rendering/__init__.py
./src/kiara/registries/templates
./src/kiara/registries/templates/__init__.py
./src/kiara/registries/types
./src/kiara/registries/types/__init__.py
./src/kiara/registries/workflows
./src/kiara/registries/workflows/archives.py
./src/kiara/registries/workflows/sqlite_store.py
./src/kiara/registries/workflows/__init__.py
./src/kiara/registries/__init__.py
./src/kiara/renderers
./src/kiara/renderers/included_renderers
./src/kiara/renderers/included_renderers/api
./src/kiara/renderers/included_renderers/api/base_api.py
./src/kiara/renderers/included_renderers/api/kiara_api.py
./src/kiara/renderers/included_renderers/api/__init__.py
./src/kiara/renderers/included_renderers/archive.py
./src/kiara/renderers/included_renderers/job.py
./src/kiara/renderers/included_renderers/pipeline.py
./src/kiara/renderers/included_renderers/value.py
./src/kiara/renderers/included_renderers/__init__.py
./src/kiara/renderers/jinja.py
./src/kiara/renderers/__init__.py
./src/kiara/resources
./src/kiara/resources/.gitkeep
./src/kiara/resources/templates
./src/kiara/resources/templates/doc_gen
./src/kiara/resources/templates/doc_gen/info_listing.j2
./src/kiara/resources/templates/render
./src/kiara/resources/templates/render/archive
./src/kiara/resources/templates/render/archive/static_page
./src/kiara/resources/templates/render/archive/static_page/page.html.j2
./src/kiara/resources/templates/render/kiara_api
./src/kiara/resources/templates/render/kiara_api/api_doc.md.j2
./src/kiara/resources/templates/render/kiara_api/kiara_api_endpoint.py.j2
./src/kiara/resources/templates/render/models
./src/kiara/resources/templates/render/models/generic_model_info.html
./src/kiara/resources/templates/render/models/info.operation.html
./src/kiara/resources/templates/render/models/info.value.html
./src/kiara/resources/templates/render/models/macros.html
./src/kiara/resources/templates/render/models/metadata.authors.html
./src/kiara/resources/templates/render/models/metadata.context.html
./src/kiara/resources/templates/render/models/metadata.documentation.html
./src/kiara/resources/templates/render/models/model_data.html
./src/kiara/resources/templates/render/pipeline
./src/kiara/resources/templates/render/pipeline/markdown
./src/kiara/resources/templates/render/pipeline/markdown/pipeline.md.j2
./src/kiara/resources/templates/render/pipeline/pipeline_info.md.j2
./src/kiara/resources/templates/render/pipeline/python_script.py.j2
./src/kiara/resources/templates/render/pipeline/static_page
./src/kiara/resources/templates/render/pipeline/static_page/page.html.j2
./src/kiara/resources/templates/render/pipeline/static_page/step.html.j2
./src/kiara/resources/tui
./src/kiara/resources/tui/pager_app.css
./src/kiara/utils
./src/kiara/utils/archives.py
./src/kiara/utils/class_loading.py
./src/kiara/utils/cli
./src/kiara/utils/cli/exceptions.py
./src/kiara/utils/cli/rich_click.py
./src/kiara/utils/cli/run.py
./src/kiara/utils/cli/__init__.py
./src/kiara/utils/concurrency.py
./src/kiara/utils/config.py
./src/kiara/utils/data.py
./src/kiara/utils/dates.py
./src/kiara/utils/db.py
./src/kiara/utils/debug.py
./src/kiara/utils/develop
./src/kiara/utils/develop/__init__.py
./src/kiara/utils/dicts.py
./src/kiara/utils/doc.py
./src/kiara/utils/downloads.py
./src/kiara/utils/files.py
./src/kiara/utils/global_metadata.py
./src/kiara/utils/graphs.py
./src/kiara/utils/hashfs
./src/kiara/utils/hashfs/__init__.py
./src/kiara/utils/hashing.py
./src/kiara/utils/html.py
./src/kiara/utils/introspection.py
./src/kiara/utils/json.py
./src/kiara/utils/metadata.py
./src/kiara/utils/models.py
./src/kiara/utils/modules.py
./src/kiara/utils/operations.py
./src/kiara/utils/output.py
./src/kiara/utils/pipelines.py
./src/kiara/utils/reflection.py
./src/kiara/utils/rendering.py
./src/kiara/utils/stores.py
./src/kiara/utils/string_vars.py
./src/kiara/utils/testing
./src/kiara/utils/testing/__init__.py
./src/kiara/utils/values.py
./src/kiara/utils/windows.py
./src/kiara/utils/yaml.py
./src/kiara/utils/__init__.py
./src/kiara/zmq
./src/kiara/zmq/client.py
./src/kiara/zmq/messages
./src/kiara/zmq/messages/__init__.py
./src/kiara/zmq/service
./src/kiara/zmq/service/__init__.py
./src/kiara/zmq/__init__.py
./src/kiara/__init__.py
./src/mkdocstrings_handlers
./src/mkdocstrings_handlers/kiara
./src/mkdocstrings_handlers/kiara/templates
./src/mkdocstrings_handlers/kiara/templates/material
./src/mkdocstrings_handlers/kiara/templates/material/.gitkeep
./src/mkdocstrings_handlers/kiara/__init__.py
./tests
./tests/conftest.py
./tests/resources
./tests/resources/archives
./tests/resources/archives/archive_create_jobs
./tests/resources/archives/archive_create_jobs/nand_true.yaml
./tests/resources/archives/nand_true.0.10.kiarchive
./tests/resources/archives/nand_true.0.10.kiarchive.json
./tests/resources/invalid_pipelines
./tests/resources/invalid_pipelines/logic_4.json
./tests/resources/kiara.config
./tests/resources/module_configs
./tests/resources/module_configs/and.json
./tests/resources/module_configs/and_wrapped.json
./tests/resources/module_configs/table_load.json
./tests/resources/pipelines
./tests/resources/pipelines/dummy
./tests/resources/pipelines/dummy/dummy_1.json
./tests/resources/pipelines/dummy/dummy_1_delay.json
./tests/resources/pipelines/logic
./tests/resources/pipelines/logic/logic_1.json
./tests/resources/pipelines/logic/logic_2.json
./tests/resources/pipelines/logic/logic_3.json
./tests/resources/pipelines/logic/logic_4.json
./tests/resources/pipelines/table_import.json
./tests/resources/pipelines/test_preseed_1.yaml
./tests/test_api
./tests/test_api/test_data_types.py
./tests/test_api/test_misc.py
./tests/test_api/test_module_types.py
./tests/test_api/test_operations.py
./tests/test_api/__init__.py
./tests/test_archives
./tests/test_archives/test_archive_export.py
./tests/test_archives/test_archive_import.py
./tests/test_cli
./tests/test_cli/test_context_subcommands.py
./tests/test_cli/test_data_subcommands.py
./tests/test_cli/test_metadata_subcommands.py
./tests/test_cli/test_misc_commands.py
./tests/test_cli/test_module_subcommands.py
./tests/test_cli/test_operation_subcommands.py
./tests/test_cli/test_pipeline_subcommands.py
./tests/test_cli/test_run_subcommand.py
./tests/test_cli/__init__.py
./tests/test_included_data_types
./tests/test_included_data_types/test_string.py
./tests/test_kiara_context.py
./tests/test_modules
./tests/test_modules/test_operations.py
./tests/test_modules/test_simple_module_exec.py
./tests/test_modules/__init__.py
./tests/test_module_instances.py
./tests/test_module_types.py
./tests/test_operations.py
./tests/test_operation_types
./tests/test_operation_types/test_extract_metadata.py
./tests/test_operation_types/test_persist_value.py
./tests/test_operation_types/test_render_value.py
./tests/test_operation_types/__init__.py
./tests/test_pipelines
./tests/test_pipelines/test_pipelines.py
./tests/test_pipelines/test_pipeline_configs.py
./tests/test_pipelines/__init__.py
./tests/test_pipeline_creation.py
./tests/test_rendering.py
./tests/test_values
./tests/test_values/test_values.py
./tests/test_values/__init__.py
./tests/utils.py
./tests/__init__.py


# kiara\kiara_plugin_network_analysis_files.txt
.
./.cruft.json
./.envrc.disabled
./.git
./.git/config
./.git/description
./.git/HEAD
./.git/hooks
./.git/hooks/applypatch-msg.sample
./.git/hooks/commit-msg.sample
./.git/hooks/fsmonitor-watchman.sample
./.git/hooks/post-update.sample
./.git/hooks/pre-applypatch.sample
./.git/hooks/pre-commit.sample
./.git/hooks/pre-merge-commit.sample
./.git/hooks/pre-push.sample
./.git/hooks/pre-rebase.sample
./.git/hooks/pre-receive.sample
./.git/hooks/prepare-commit-msg.sample
./.git/hooks/push-to-checkout.sample
./.git/hooks/update.sample
./.git/index
./.git/info
./.git/info/exclude
./.git/logs
./.git/logs/HEAD
./.git/logs/refs
./.git/logs/refs/heads
./.git/logs/refs/heads/develop
./.git/logs/refs/remotes
./.git/logs/refs/remotes/origin
./.git/logs/refs/remotes/origin/HEAD
./.git/objects
./.git/objects/info
./.git/objects/pack
./.git/objects/pack/pack-50d32aa2bfe040cdf261502e30541e3e3f19969a.idx
./.git/objects/pack/pack-50d32aa2bfe040cdf261502e30541e3e3f19969a.pack
./.git/packed-refs
./.git/refs
./.git/refs/heads
./.git/refs/heads/develop
./.git/refs/remotes
./.git/refs/remotes/origin
./.git/refs/remotes/origin/HEAD
./.git/refs/tags
./.gitattributes
./.github
./.github/ISSUE_TEMPLATE
./.github/ISSUE_TEMPLATE/bug_report.md
./.github/ISSUE_TEMPLATE/suggest-a-module.md
./.github/workflows
./.github/workflows/build-darwin.yaml
./.github/workflows/build-linux.yaml
./.github/workflows/build-windows.yaml
./.gitignore
./.git_archival.txt
./.pre-commit-config.yaml
./AUTHORS.md
./CHANGELOG.md
./ci
./ci/conda
./ci/conda/conda-pkg-patch.yaml
./commitlint.config.js
./docs
./docs/development.md
./docs/index.md
./docs/stylesheets
./docs/stylesheets/extra.css
./docs/SUMMARY.md
./docs/usage.md
./examples
./examples/data
./examples/data/gexf
./examples/data/gexf/lesmis.gexf
./examples/data/gexf/quakers.gexf
./examples/data/gexf/Readme.md
./examples/data/gml
./examples/data/gml/adjnoun.gml
./examples/data/gml/celegansneural.gml
./examples/data/gml/dolphins.gml
./examples/data/gml/football.gml
./examples/data/gml/karate.gml
./examples/data/gml/lesmis.gml
./examples/data/gml/polbooks.gml
./examples/data/gml/Readme.md
./examples/data/journals
./examples/data/journals/JournalEdges1902.csv
./examples/data/journals/JournalNodes1902.csv
./examples/data/journals/Readme.md
./examples/data/JSON
./examples/data/JSON/peacetreaties.json
./examples/data/JSON/Readme.md
./examples/data/quakers
./examples/data/quakers/quakers_edgelist.csv
./examples/data/quakers/quakers_nodelist.csv
./examples/data/quakers/Readme.md
./examples/data/Readme.md
./examples/data/simple_networks
./examples/data/simple_networks/connected
./examples/data/simple_networks/connected/SampleEdges.csv
./examples/data/simple_networks/connected/SampleNodes.csv
./examples/data/simple_networks/two_components
./examples/data/simple_networks/two_components/SampleEdges.csv
./examples/data/simple_networks/two_components/SampleNodes.csv
./examples/data/treaties
./examples/data/treaties/peace_treaties_rus.csv
./examples/data/treaties/Readme.md
./examples/jobs
./examples/jobs/create_journals_network.yaml
./examples/jobs/create_simple_disconnected_network.yaml
./examples/jobs/create_simple_network.yaml
./examples/jobs/notebooks
./examples/jobs/notebooks/create_journals_network.ipynb
./examples/jobs/notebooks/create_simple_network.ipynb
./examples/jobs/notebooks/utilities.ipynb
./examples/jobs/Readme.md
./examples/pipelines
./examples/pipelines/create_network_graph.yaml
./examples/pipelines/Readme.md
./examples/runs
./examples/runs/journal_components.run
./LICENSE
./Makefile
./MANIFEST.in
./mkdocs.yml
./pixi.toml
./pyproject.toml
./README.md
./scripts
./scripts/documentation
./scripts/documentation/gen_api_doc_pages.py
./scripts/documentation/gen_info_pages.py
./scripts/documentation/gen_module_doc.py
./src
./src/kiara_plugin
./src/kiara_plugin/network_analysis
./src/kiara_plugin/network_analysis/data_types
./src/kiara_plugin/network_analysis/data_types/__init__.py
./src/kiara_plugin/network_analysis/defaults.py
./src/kiara_plugin/network_analysis/models
./src/kiara_plugin/network_analysis/models/inputs.py
./src/kiara_plugin/network_analysis/models/metadata.py
./src/kiara_plugin/network_analysis/models/__init__.py
./src/kiara_plugin/network_analysis/modules
./src/kiara_plugin/network_analysis/modules/components.py
./src/kiara_plugin/network_analysis/modules/create.py
./src/kiara_plugin/network_analysis/modules/export.py
./src/kiara_plugin/network_analysis/modules/filters.py
./src/kiara_plugin/network_analysis/modules/rendering.py
./src/kiara_plugin/network_analysis/modules/__init__.py
./src/kiara_plugin/network_analysis/pipelines
./src/kiara_plugin/network_analysis/pipelines/.gitkeep
./src/kiara_plugin/network_analysis/pipelines/__init__.py
./src/kiara_plugin/network_analysis/py.typed
./src/kiara_plugin/network_analysis/resources
./src/kiara_plugin/network_analysis/resources/.gitkeep
./src/kiara_plugin/network_analysis/streamlit
./src/kiara_plugin/network_analysis/streamlit/components
./src/kiara_plugin/network_analysis/streamlit/components/data_import.py
./src/kiara_plugin/network_analysis/streamlit/components/__init__.py
./src/kiara_plugin/network_analysis/streamlit/__init__.py
./src/kiara_plugin/network_analysis/utils.py
./src/kiara_plugin/network_analysis/__init__.py
./tests
./tests/conftest.py
./tests/job_tests
./tests/job_tests/create_journals_network
./tests/job_tests/create_journals_network/outputs.py
./tests/job_tests/create_journals_network/outputs.yaml
./tests/resources
./tests/resources/.gitkeep
./tests/test_job_descs.py
./tests/test_kiara_modules_default.py


# kiara\NetworkAnalysis_files.txt
.
./.git
./.git/config
./.git/description
./.git/HEAD
./.git/hooks
./.git/hooks/applypatch-msg.sample
./.git/hooks/commit-msg.sample
./.git/hooks/fsmonitor-watchman.sample
./.git/hooks/post-update.sample
./.git/hooks/pre-applypatch.sample
./.git/hooks/pre-commit.sample
./.git/hooks/pre-merge-commit.sample
./.git/hooks/pre-push.sample
./.git/hooks/pre-rebase.sample
./.git/hooks/pre-receive.sample
./.git/hooks/prepare-commit-msg.sample
./.git/hooks/push-to-checkout.sample
./.git/hooks/update.sample
./.git/index
./.git/info
./.git/info/exclude
./.git/logs
./.git/logs/HEAD
./.git/logs/refs
./.git/logs/refs/heads
./.git/logs/refs/heads/main
./.git/logs/refs/remotes
./.git/logs/refs/remotes/origin
./.git/logs/refs/remotes/origin/HEAD
./.git/objects
./.git/objects/info
./.git/objects/pack
./.git/objects/pack/pack-c159c97bc25bb122279f93d0995cc25711431e4a.idx
./.git/objects/pack/pack-c159c97bc25bb122279f93d0995cc25711431e4a.pack
./.git/packed-refs
./.git/refs
./.git/refs/heads
./.git/refs/heads/main
./.git/refs/remotes
./.git/refs/remotes/origin
./.git/refs/remotes/origin/HEAD
./.git/refs/tags
./COPYING
./data
./data/JournalEdges1902.csv
./data/JournalNodes1902.csv
./OneModeNetwork.ipynb
./README.md


# kiara\TopicModelling_files.txt
.
./.git
./.git/config
./.git/description
./.git/HEAD
./.git/hooks
./.git/hooks/applypatch-msg.sample
./.git/hooks/commit-msg.sample
./.git/hooks/fsmonitor-watchman.sample
./.git/hooks/post-update.sample
./.git/hooks/pre-applypatch.sample
./.git/hooks/pre-commit.sample
./.git/hooks/pre-merge-commit.sample
./.git/hooks/pre-push.sample
./.git/hooks/pre-rebase.sample
./.git/hooks/pre-receive.sample
./.git/hooks/prepare-commit-msg.sample
./.git/hooks/push-to-checkout.sample
./.git/hooks/update.sample
./.git/index
./.git/info
./.git/info/exclude
./.git/logs
./.git/logs/HEAD
./.git/logs/refs
./.git/logs/refs/heads
./.git/logs/refs/heads/master
./.git/logs/refs/remotes
./.git/logs/refs/remotes/origin
./.git/logs/refs/remotes/origin/HEAD
./.git/objects
./.git/objects/info
./.git/objects/pack
./.git/objects/pack/pack-6093d41861fdde8fe22d0ba7c6b2616e854e3208.idx
./.git/objects/pack/pack-6093d41861fdde8fe22d0ba7c6b2616e854e3208.pack
./.git/packed-refs
./.git/refs
./.git/refs/heads
./.git/refs/heads/master
./.git/refs/remotes
./.git/refs/remotes/origin
./.git/refs/remotes/origin/HEAD
./.git/refs/tags
./archived_notebook_versions
./archived_notebook_versions/pyLDAvis_relevance.ipynb
./archived_notebook_versions/topic_modelling_bigrams.ipynb
./archived_notebook_versions/topic_modelling_bigrams_colab.ipynb
./archived_notebook_versions/topic_modelling_bigrams_colab_sample.ipynb
./archived_notebook_versions/topic_modelling_colab_mallet.ipynb
./archived_notebook_versions/topic_modelling_colab_tfidf_sample.ipynb
./archived_notebook_versions/topic_modelling_gensim.ipynb
./archived_notebook_versions/topic_modelling_gensim_colab.ipynb
./archived_notebook_versions/topic_modelling_gensim_colab_sample.ipynb
./archived_notebook_versions/topic_modelling_gensim_colab_sample_w_rollingmeanpub.ipynb
./archived_notebook_versions/topic_modelling_gensim_mallet.ipynb
./archived_notebook_versions/topic_modelling_gensim_real_data.ipynb
./archived_notebook_versions/topic_modelling_mallet.ipynb
./archived_notebook_versions/topic_modelling_mallet_colab.ipynb
./archived_notebook_versions/topic_modelling_tfidf.ipynb
./archived_notebook_versions/topic_modelling_tfidf_colab.ipynb
./COPYING
./img
./img/corpus_preview.png
./img/subset_check.png
./README.md
./requirements.txt
./stop_words.csv
./tests
./tests/coherence_tests.ipynb
./tests/mallet_gensim_conversion_test_colab.ipynb
./topic_modelling.ipynb
./vis-files
./vis-files/readme.md
./vis-files/tm_1
./vis-files/tm_1/2.64b0803b.chunk.js
./vis-files/tm_1/jscode-test-2.js
./vis-files/tm_1/jscode-test.js
./vis-files/tm_1/main.8a3a0dbd.chunk.css
./vis-files/tm_1/main.c8962a70.chunk.js
./vis-files/tm_1/tm1_data_prepare_gensim.py
./vis-files/tm_1/tm1_gensim.py
./vis-files/tm_1/tm1_mallet.py
./vis-files/tm_1/tm1_tfidf.py
./vis-files/tm_1/topic_info.csv
./vis-files/tm_1/topic_proportion.csv
./vis-files/tm_coherence_chart.js
./vis-files/tm_coherence_table.js
./vis-files/tm_distribution_year_rolling.js
./vis-files/tm_publication_dist.js
./vis-files/tm_topic_dist_year.js
./vis-files/tm_topic_dist_year_bar.js
./vis-files/tm_topic_proportion_chart.js
./vis-files/vis_tm1_notebook_integration_test.ipynb


# kiara\asciinet\README.md
asciinet
========

asciinet is a wrapper over the [ascii-graphs](https://github.com/cosminbasca/ascii-graphs) library for printing [networkx](https://networkx.github.io/) graphs as ASCII.

Important Notes
---------------
This software is the product of research carried out at the [University of Zurich](http://www.ifi.uzh.ch/ddis.html) and comes with no warranty whatsoever. Have fun!

TODO's
------
* The project is not documented (yet)

How to Install the Project
--------------------------
First make sure java is installed on your system. To install **asciinet** follow these instructions:

```sh
$ git clone https://github.com/cosminbasca/asciinet
$ cd asciinet/pyasciinet
$ python setup.py install
```

Also have a look at the build.sh script included in the codebase for a complete setup of the build process

Example
-------

```python
import networkx as nx
from asciinet import graph_to_ascii

#
# create a simple graph
#
G = nx.Graph()
G.add_node(1)
G.add_nodes_from([2, 3, 4])
G.add_edges_from([(1, 2), (1, 3), (3, 4), (1, 4), (2, 4)])

#
# should print
#  ┌───────┐
#  │   1   │
#  └┬────┬┬┘
#   │    ││
#   │    └┼───┐
#   v     v   │
# ┌───┐ ┌───┐ │
# │ 2 │ │ 3 │ │
# └──┬┘ └─┬─┘ │
#    │    │   │
#    │   ┌┼───┘
#    │   ││
#    v   vv
#  ┌───────┐
#  │   4   │
#  └───────┘
#
print graph_to_ascii(G)




```

Thanks a lot to
---------------
* [University of Zurich](http://www.ifi.uzh.ch/ddis.html) and the [Swiss National Science Foundation](http://www.snf.ch/en/Pages/default.aspx) for generously funding the research that led to this software.


# kiara\asciinet\.github\workflows\build-linux.yaml
name: "package build and deploy"
on: [push]

jobs:

  publish_python_package:
    name: publish python package
    runs-on: ubuntu-latest
    env:
        GEMFURY_PUSH_TOKEN: ${{ secrets.GEMFURY_PUSH_TOKEN }}
    steps:
      - name: Set up Python 3.10
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"
      - uses: actions/checkout@v3
        with:
          fetch-depth: 0
      - name: install pip
        run: pip install pip==21.2.4 setuptools==57.4.0 wheel
      - name: create packages
        run: cd pyasciinet && bash build.sh
      - name: upload source package
        if: ${{ ( github.ref == 'refs/heads/master') || startsWith(github.ref, 'refs/tags') }}
        run: curl -F package=@$(ls pyasciinet/dist/asciinet-*.tar.gz) https://${GEMFURY_PUSH_TOKEN}@dharpa.fury.land:443/pypi/
      - name: upload wheel
        if: ${{ (github.ref == 'refs/heads/master') || startsWith(github.ref, 'refs/tags') }}
        run: curl -F package=@$(ls pyasciinet/dist/asciinet-*.whl) https://${GEMFURY_PUSH_TOKEN}@dharpa.fury.land:443/pypi/
      - name: publish to PyPI
        if: startsWith(github.ref, 'refs/tags')
        uses: pypa/gh-action-pypi-publish@master
        with:
          user: __token__
          password: ${{ secrets.PYPI_API_TOKEN }}
          packages_dir: pyasciinet/dist

  conda_package_build:
    name: conda package build (and upload if release)
    runs-on: ubuntu-latest
    steps:
      - name: "Set up Python 3.9"
        uses: actions/setup-python@v4
        with:
          python-version: "3.9"
      - name: pip cache
        id: pip-cache
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/setup.*') }}
      - uses: actions/checkout@v3
        with:
          fetch-depth: 0
      - name: install kiara
        run: pip install -U --extra-index-url https://pypi.fury.io/dharpa/ .
      - name: install required plugin packages
        run: pip install -U --pre --extra-index-url https://pypi.fury.io/dharpa/ kiara_plugin.develop
      - name: build conda package
        if: ${{ ( github.ref == 'refs/heads/master') }}
        run: kiara conda build-package --patch-data ci/conda/conda-pkg-patch.yaml .
      - name: extract tag name
        run: echo "RELEASE_VERSION=${GITHUB_REF#refs/*/}" >> $GITHUB_ENV
      - name: build & publish conda package
        if: ${{ startsWith(github.ref, 'refs/tags/') }}
        run: kiara conda build-package --publish --user dharpa --token ${{ secrets.ANACONDA_PUSH_TOKEN }} --patch-data ci/conda/conda-pkg-patch.yaml .



# kiara\asciinet\ci\conda\conda-pkg-patch.yaml


# kiara\asciinet\pyasciinet\build.sh
#!/bin/bash
clear
echo "prepare ascii-graph jar ... "
ROOT_DIR="././../"
JAR_DIR=${ROOT_DIR}"target/scala-2.11/"
LIB_DIR=${ROOT_DIR}"pyasciigraph/asciinet/lib/"

CWD=`pwd`
cd ${ROOT_DIR}
sbt compile assembly
cd ${CWD}
cp `ls -t ${JAR_DIR}asciigraph-assembly-*.jar | head -1` ${LIB_DIR}

echo "install dependencies ... "
pip install -r "./dependencies.txt"
echo "testing asciinet ..."
nosetests --rednose -v -s ./asciinet/test/
echo "building module egg distribution ... "
python setup.py bdist_egg
echo "building source distribution ... "
python setup.py sdist --formats=gztar
echo "building module wheels distribution ... "
python setup.py bdist_wheel

echo "all done!"

# kiara\asciinet\pyasciinet\dependencies.txt
networkx>=1.9
natsort>=3.2.0
requests>=2.3.0
msgpack-python>=0.4.2

# kiara\asciinet\pyasciinet\ez_setup.py
#!python
"""Bootstrap setuptools installation

To use setuptools in your package's setup.py, include this
file in the same directory and add this to the top of your setup.py::

    from ez_setup import use_setuptools
    use_setuptools()

To require a specific version of setuptools, set a download
mirror, or use an alternate download directory, simply supply
the appropriate options to ``use_setuptools()``.

This file can also be run as a script to install or upgrade setuptools.
"""
import os
import shutil
import sys
import tempfile
import tarfile
import optparse
import subprocess
import platform
import textwrap

from distutils import log

try:
    from site import USER_SITE
except ImportError:
    USER_SITE = None

DEFAULT_VERSION = "2.0.1"
DEFAULT_URL = "https://pypi.python.org/packages/source/s/setuptools/"

def _python_cmd(*args):
    args = (sys.executable,) + args
    return subprocess.call(args) == 0

def _install(tarball, install_args=()):
    # extracting the tarball
    tmpdir = tempfile.mkdtemp()
    log.warn('Extracting in %s', tmpdir)
    old_wd = os.getcwd()
    try:
        os.chdir(tmpdir)
        tar = tarfile.open(tarball)
        _extractall(tar)
        tar.close()

        # going in the directory
        subdir = os.path.join(tmpdir, os.listdir(tmpdir)[0])
        os.chdir(subdir)
        log.warn('Now working in %s', subdir)

        # installing
        log.warn('Installing Setuptools')
        if not _python_cmd('setup.py', 'install', *install_args):
            log.warn('Something went wrong during the installation.')
            log.warn('See the error message above.')
            # exitcode will be 2
            return 2
    finally:
        os.chdir(old_wd)
        shutil.rmtree(tmpdir)


def _build_egg(egg, tarball, to_dir):
    # extracting the tarball
    tmpdir = tempfile.mkdtemp()
    log.warn('Extracting in %s', tmpdir)
    old_wd = os.getcwd()
    try:
        os.chdir(tmpdir)
        tar = tarfile.open(tarball)
        _extractall(tar)
        tar.close()

        # going in the directory
        subdir = os.path.join(tmpdir, os.listdir(tmpdir)[0])
        os.chdir(subdir)
        log.warn('Now working in %s', subdir)

        # building an egg
        log.warn('Building a Setuptools egg in %s', to_dir)
        _python_cmd('setup.py', '-q', 'bdist_egg', '--dist-dir', to_dir)

    finally:
        os.chdir(old_wd)
        shutil.rmtree(tmpdir)
    # returning the result
    log.warn(egg)
    if not os.path.exists(egg):
        raise IOError('Could not build the egg.')


def _do_download(version, download_base, to_dir, download_delay):
    egg = os.path.join(to_dir, 'setuptools-%s-py%d.%d.egg'
                       % (version, sys.version_info[0], sys.version_info[1]))
    if not os.path.exists(egg):
        tarball = download_setuptools(version, download_base,
                                      to_dir, download_delay)
        _build_egg(egg, tarball, to_dir)
    sys.path.insert(0, egg)

    # Remove previously-imported pkg_resources if present (see
    # https://bitbucket.org/pypa/setuptools/pull-request/7/ for details).
    if 'pkg_resources' in sys.modules:
        del sys.modules['pkg_resources']

    import setuptools
    setuptools.bootstrap_install_from = egg


def use_setuptools(version=DEFAULT_VERSION, download_base=DEFAULT_URL,
                   to_dir=os.curdir, download_delay=15):
    to_dir = os.path.abspath(to_dir)
    rep_modules = 'pkg_resources', 'setuptools'
    imported = set(sys.modules).intersection(rep_modules)
    try:
        import pkg_resources
    except ImportError:
        return _do_download(version, download_base, to_dir, download_delay)
    try:
        pkg_resources.require("setuptools>=" + version)
        return
    except pkg_resources.DistributionNotFound:
        return _do_download(version, download_base, to_dir, download_delay)
    except pkg_resources.VersionConflict as VC_err:
        if imported:
            msg = textwrap.dedent("""
                The required version of setuptools (>={version}) is not available,
                and can't be installed while this script is running. Please
                install a more recent version first, using
                'easy_install -U setuptools'.

                (Currently using {VC_err.args[0]!r})
                """).format(VC_err=VC_err, version=version)
            sys.stderr.write(msg)
            sys.exit(2)

        # otherwise, reload ok
        del pkg_resources, sys.modules['pkg_resources']
        return _do_download(version, download_base, to_dir, download_delay)

def _clean_check(cmd, target):
    """
    Run the command to download target. If the command fails, clean up before
    re-raising the error.
    """
    try:
        subprocess.check_call(cmd)
    except subprocess.CalledProcessError:
        if os.access(target, os.F_OK):
            os.unlink(target)
        raise

def download_file_powershell(url, target):
    """
    Download the file at url to target using Powershell (which will validate
    trust). Raise an exception if the command cannot complete.
    """
    target = os.path.abspath(target)
    cmd = [
        'powershell',
        '-Command',
        "(new-object System.Net.WebClient).DownloadFile(%(url)r, %(target)r)" % vars(),
    ]
    _clean_check(cmd, target)

def has_powershell():
    if platform.system() != 'Windows':
        return False
    cmd = ['powershell', '-Command', 'echo test']
    devnull = open(os.path.devnull, 'wb')
    try:
        try:
            subprocess.check_call(cmd, stdout=devnull, stderr=devnull)
        except:
            return False
    finally:
        devnull.close()
    return True

download_file_powershell.viable = has_powershell

def download_file_curl(url, target):
    cmd = ['curl', url, '--silent', '--output', target]
    _clean_check(cmd, target)

def has_curl():
    cmd = ['curl', '--version']
    devnull = open(os.path.devnull, 'wb')
    try:
        try:
            subprocess.check_call(cmd, stdout=devnull, stderr=devnull)
        except:
            return False
    finally:
        devnull.close()
    return True

download_file_curl.viable = has_curl

def download_file_wget(url, target):
    cmd = ['wget', url, '--quiet', '--output-document', target]
    _clean_check(cmd, target)

def has_wget():
    cmd = ['wget', '--version']
    devnull = open(os.path.devnull, 'wb')
    try:
        try:
            subprocess.check_call(cmd, stdout=devnull, stderr=devnull)
        except:
            return False
    finally:
        devnull.close()
    return True

download_file_wget.viable = has_wget

def download_file_insecure(url, target):
    """
    Use Python to download the file, even though it cannot authenticate the
    connection.
    """
    try:
        from urllib.request import urlopen
    except ImportError:
        from urllib2 import urlopen
    src = dst = None
    try:
        src = urlopen(url)
        # Read/write all in one block, so we don't create a corrupt file
        # if the download is interrupted.
        data = src.read()
        dst = open(target, "wb")
        dst.write(data)
    finally:
        if src:
            src.close()
        if dst:
            dst.close()

download_file_insecure.viable = lambda: True

def get_best_downloader():
    downloaders = [
        download_file_powershell,
        download_file_curl,
        download_file_wget,
        download_file_insecure,
    ]

    for dl in downloaders:
        if dl.viable():
            return dl

def download_setuptools(version=DEFAULT_VERSION, download_base=DEFAULT_URL,
                        to_dir=os.curdir, delay=15,
                        downloader_factory=get_best_downloader):
    """Download setuptools from a specified location and return its filename

    `version` should be a valid setuptools version number that is available
    as an egg for download under the `download_base` URL (which should end
    with a '/'). `to_dir` is the directory where the egg will be downloaded.
    `delay` is the number of seconds to pause before an actual download
    attempt.

    ``downloader_factory`` should be a function taking no arguments and
    returning a function for downloading a URL to a target.
    """
    # making sure we use the absolute path
    to_dir = os.path.abspath(to_dir)
    tgz_name = "setuptools-%s.tar.gz" % version
    url = download_base + tgz_name
    saveto = os.path.join(to_dir, tgz_name)
    if not os.path.exists(saveto):  # Avoid repeated downloads
        log.warn("Downloading %s", url)
        downloader = downloader_factory()
        downloader(url, saveto)
    return os.path.realpath(saveto)


def _extractall(self, path=".", members=None):
    """Extract all members from the archive to the current working
       directory and set owner, modification time and permissions on
       directories afterwards. `path' specifies a different directory
       to extract to. `members' is optional and must be a subset of the
       list returned by getmembers().
    """
    import copy
    import operator
    from tarfile import ExtractError
    directories = []

    if members is None:
        members = self

    for tarinfo in members:
        if tarinfo.isdir():
            # Extract directories with a safe mode.
            directories.append(tarinfo)
            tarinfo = copy.copy(tarinfo)
            tarinfo.mode = 448  # decimal for oct 0700
        self.extract(tarinfo, path)

    # Reverse sort directories.
    directories.sort(key=operator.attrgetter('name'), reverse=True)

    # Set correct owner, mtime and filemode on directories.
    for tarinfo in directories:
        dirpath = os.path.join(path, tarinfo.name)
        try:
            self.chown(tarinfo, dirpath)
            self.utime(tarinfo, dirpath)
            self.chmod(tarinfo, dirpath)
        except ExtractError as e:
            if self.errorlevel > 1:
                raise
            else:
                self._dbg(1, "tarfile: %s" % e)


def _build_install_args(options):
    """
    Build the arguments to 'python setup.py install' on the setuptools package
    """
    return ['--user'] if options.user_install else []

def _parse_args():
    """
    Parse the command line for options
    """
    parser = optparse.OptionParser()
    parser.add_option(
        '--user', dest='user_install', action='store_true', default=False,
        help='install in user site package (requires Python 2.6 or later)')
    parser.add_option(
        '--download-base', dest='download_base', metavar="URL",
        default=DEFAULT_URL,
        help='alternative URL from where to download the setuptools package')
    parser.add_option(
        '--insecure', dest='downloader_factory', action='store_const',
        const=lambda: download_file_insecure, default=get_best_downloader,
        help='Use internal, non-validating downloader'
    )
    options, args = parser.parse_args()
    # positional arguments are ignored
    return options

def main(version=DEFAULT_VERSION):
    """Install or upgrade setuptools and EasyInstall"""
    options = _parse_args()
    tarball = download_setuptools(download_base=options.download_base,
        downloader_factory=options.downloader_factory)
    return _install(tarball, _build_install_args(options))

if __name__ == '__main__':
    sys.exit(main())


# kiara\asciinet\pyasciinet\README.md
asciinet
========

*NOTE*: this is a fork of the original asciinet libary which seems to be abandoned. This fork is mainly intended to build artifacts to upload to pypi and conda.

asciinet is a wrapper over the [ascii-graphs](https://github.com/cosminbasca/ascii-graphs) library for printing [networkx](https://networkx.github.io/) graphs as ASCII.

Important Notes
---------------
This software is the product of research carried out at the [University of Zurich](http://www.ifi.uzh.ch/ddis.html) and comes with no warranty whatsoever. Have fun!

TODO's
------
* The project is not documented (yet)

How to Install the Project
--------------------------
To install **asciinet** you have two options: 1) manual installation (install requirements first) or 2) automatic with **pip**

Install the project manually from source (after downloading it locally):
```sh
$ cd pyasciinet
$ python setup.py install
```

Install the project with pip:
```sh
$ pip install https://github.com/cosminbasca/asciinet/pyasciinet
```

Also have a look at the build.sh script included in the codebase for a complete setup of the build process 

Thanks a lot to
---------------
* [University of Zurich](http://www.ifi.uzh.ch/ddis.html) and the [Swiss National Science Foundation](http://www.snf.ch/en/Pages/default.aspx) for generously funding the research that led to this software.


# kiara\asciinet\pyasciinet\requirements.sh
#!/bin/bash
clear
EXCLUDED="asciinet,cybloom,asyncrpc,cysparql,cyqplanner,avalanche_jvm,cytokyotygr,msgpackutil"
extract_requirements setup.py --excludes=${EXCLUDED} > requirements.txt

# kiara\asciinet\pyasciinet\requirements.txt
networkx>=1.9
natsort>=3.2.0
requests>=2.3.0
msgpack-python>=0.4.2


# kiara\asciinet\pyasciinet\setup.py
# !/usr/bin/env python
#!/usr/bin/env python
#
# author: Cosmin Basca
#
# Copyright 2010 University of Zurich
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#        http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
try:
    from setuptools import setup
except ImportError:
    from ez_setup import use_setuptools

    use_setuptools()
    from setuptools import setup

NAME = 'asciinet'

str_version = None
print('{0}/__version__.py'.format(NAME))
exec(open('{0}/__version__.py'.format(NAME)).read())
#execfile('{0}/__version__.py'.format(NAME))

# Load up the description from README
with open('README.md') as f:
    DESCRIPTION = f.read()

pip_deps = [
    'networkx>=1.9',
    'natsort>=3.2.0',
    'requests>=2.3.0',
    'msgpack-python>=0.4.2',
]

manual_deps = [
]

setup(
    name=NAME,
    version=str_version,
    author='Cosmin Basca',
    author_email='cosmin.basca@gmail.com; basca@ifi.uzh.ch',
    # url=None,
    description='A wrapper around the scala ascii-graphs library',
    long_description=DESCRIPTION,
    classifiers=[
        'Intended Audience :: Developers',
        # 'License :: OSI Approved :: BSD License',
        'Natural Language :: English',
        'Operating System :: OS Independent',
        'Programming Language :: Python',
        'Programming Language :: Java',
        'Programming Language :: Scala',
        'Topic :: Software Development'
    ],
    packages=[NAME, '{0}/test'.format(NAME)],
    package_data={
        NAME: ['lib/*.jar']
    },
    install_requires=manual_deps + pip_deps,
    entry_points={
        # 'console_scripts': ['asyncrpc = asyncrpc.cli:main']
    }
)


# kiara\asciinet\pyasciinet\asciinet\_libutil.py
#!/usr/bin/env python
#
# author: Cosmin Basca
#
# Copyright 2010 University of Zurich
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#        http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
import os
from subprocess import call
from natsort import natsorted

__author__ = 'basca'

__LIB__ = os.path.join(os.path.dirname(os.path.realpath(__file__)), "lib")
__JARS__ = natsorted([(jar.replace("asciigraph-assembly-", "").replace(".jar", ""),
                       os.path.join(__LIB__, jar))
                      for jar in os.listdir(__LIB__) if jar.startswith("asciigraph-assembly-")],
                     key=lambda x: x[0])


def latest_jar():
    global __JARS__
    return __JARS__[-1]


class JavaNotFoundException(Exception):
    pass

DEVNULL = open(os.devnull, 'w')

def check_java(message=""):
    if call(['java', '-version'], stderr=DEVNULL) != 0:
        raise JavaNotFoundException(
            'Java is not installed in the system path. {0}'.format(message))


# kiara\asciinet\pyasciinet\asciinet\__init__.py
#!/usr/bin/env python
#
# author: Cosmin Basca
#
# Copyright 2010 University of Zurich
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#        http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
from subprocess import Popen, PIPE, call
import uuid
from natsort import natsorted
import networkx as nx
import threading
import atexit
import os
import requests
from msgpack import dumps, loads
from requests.exceptions import ConnectionError, Timeout
from asciinet._libutil import latest_jar, check_java

__author__ = 'basca'

DEVNULL = open(os.devnull, 'w')

__all__ = ['graph_to_ascii', 'JavaNotFoundException', 'GraphConversionError']


class GraphConversionError(Exception):
    pass


class _AsciiGraphProxy(object):
    @staticmethod
    def instance():
        if not hasattr(_AsciiGraphProxy, "_instance"):
            _AsciiGraphProxy._instance = _AsciiGraphProxy()
        return _AsciiGraphProxy._instance

    def __init__(self, port=0):
        check_java("Java is needed to run graph_to_ascii")
        self._prefix = '{0}='.format(uuid.uuid1())
        ascii_opts = ['--port', str(port), '--die_on_broken_pipe', '--port_notification_prefix', self._prefix]
        latest_version, jar_path = latest_jar()
        self._command = ["java", "-classpath", jar_path] + ['.'.join(['com', 'ascii', 'Server'])] + ascii_opts
        self._proc = None
        self._port = None
        self._url = None
        self._start()

    def _start(self):
        self._proc = Popen(self._command, stdout=PIPE, stdin=PIPE)
        try:
            line = ''
            while not line.startswith(self._prefix):
                line = self._proc.stdout.readline().decode(encoding='UTF-8')
            self._port = int(line.replace(self._prefix, '').strip())
        except Exception as e:
            self._proc.kill()
            raise e
        self._url = 'http://127.0.0.1:{0}/asciiGraph'.format(self._port)

    def _restart(self):
        self._proc.kill()
        self._start()

    def graph_to_ascii(self, graph, timeout=10):
        try:
            graph_repr = dumps({
                'vertices': [str(v) for v in graph.nodes()],
                'edges': [[str(e[0]), str(e[1])] for e in graph.edges()],
            })
            response = requests.post(self._url, data=graph_repr, timeout=timeout)
            if response.status_code == 200:
                return loads(response.content)
            else:
                raise ValueError('internal error: \n{0}'.format(response.content))
        except (ConnectionError, Timeout):
            self._restart()
            raise GraphConversionError('could not convert graph {0} to ascii'.format(graph))


    def close(self):
        self._proc.kill()


_asciigraph = _AsciiGraphProxy.instance()


@atexit.register
def _cleanup():
    _asciigraph.close()


def graph_to_ascii(graph, timeout=10):
    if not isinstance(graph, nx.Graph):
        raise ValueError('graph must be a networkx.Graph')

    return _asciigraph.graph_to_ascii(graph, timeout=timeout).decode(encoding='UTF-8')


# kiara\asciinet\pyasciinet\asciinet\__version__.py
#!/usr/bin/env python
#
# author: Cosmin Basca
#
# Copyright 2010 University of Zurich
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#        http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
__author__ = 'basca'

version = (0, 3, 1)
str_version = '.'.join([str(v) for v in version])

# kiara\asciinet\pyasciinet\asciinet\test\base.py
# coding=utf-8
#!/usr/bin/env python
#
# author: Cosmin Basca
#
# Copyright 2010 University of Zurich
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#        http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
import traceback
from unittest import TestCase
import functools
import networkx as nx

__author__ = 'basca'


def catch_exception(func):
    @functools.wraps
    def wrapper(*args, **kwargs):
        try:
            func(*args, **kwargs)
        except Exception as e:
            print ('Have exception in [{0}]: {1}, \ntraceback = \n{2}'.format(func.__name__, e, traceback.format_exc()))

    return wrapper


class BaseTestCase(TestCase):
    @classmethod
    def setUpClass(cls):
        cls.graph = nx.Graph()
        cls.graph.add_node(1)
        cls.graph.add_nodes_from([2, 3, 4])
        cls.graph.add_edges_from([(1, 2), (1, 3), (3, 4), (1, 4), (2, 4)])

        cls.graph_repr = """
  ┌───────┐
  │   1   │
  └┬────┬┬┘
   │    ││
   │    └┼───┐
   v     v   │
 ┌───┐ ┌───┐ │
 │ 2 │ │ 3 │ │
 └──┬┘ └─┬─┘ │
    │    │   │
    │   ┌┼───┘
    │   ││
    v   vv
  ┌───────┐
  │   4   │
  └───────┘   """.replace("\n", "").replace("\t", "").replace(" ", "").strip()

    @classmethod
    def tearDownClass(cls):
        pass






# kiara\asciinet\pyasciinet\asciinet\test\test_ascii.py
#!/usr/bin/env python
#
# author: Cosmin Basca
#
# Copyright 2010 University of Zurich
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#        http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
from asciinet import graph_to_ascii
from asciinet.test.base import BaseTestCase


__author__ = 'basca'


class TestClient(BaseTestCase):
    def test_to_ascii(self):
        ascii = graph_to_ascii(self.graph)
        self.assertEqual(ascii.replace("\n", "").replace("\t", "").replace(" ", "").strip(), self.graph_repr)


# kiara\asciinet\pyasciinet\asciinet\test\__init__.py
#!/usr/bin/env python
#
# author: Cosmin Basca
#
# Copyright 2010 University of Zurich
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#        http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
__author__ = 'basca'


# kiara\asciinet\pyasciinet\examples\example.py
#!/usr/bin/env python
#
# author: Cosmin Basca
#
# Copyright 2010 University of Zurich
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#        http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
from time import time
import networkx as nx
from asciinet import graph_to_ascii

__author__ = 'basca'

if __name__ == '__main__':
    G = nx.Graph()
    G.add_node(1)
    G.add_nodes_from([2, 3, 4])
    # G.add_edges_from([(1, 2), (1, 3), (3, 4), (1, 4), (2, 4), (2, 3)])
    G.add_edges_from([(1, 2), (1, 3), (3, 4), (1, 4), (2, 4)])

    t0 = time()
    print graph_to_ascii(G)
    print 'took {0} seconds'.format(time() - t0)


# kiara\jupyterlab-extension-example\.eslintrc.js
module.exports = {
  parser: '@typescript-eslint/parser', // Specifies the ESLint parser
  parserOptions: {
    ecmaVersion: 2020, // Allows for the parsing of modern ECMAScript features
    sourceType: 'module', // Allows for the use of imports
    ecmaFeatures: {
      jsx: true // Allows for the parsing of JSX
    }
  },
  settings: {
    react: {
      version: 'detect' // Tells eslint-plugin-react to automatically detect the version of React to use
    }
  },
  extends: [
    'plugin:react/recommended', // Uses the recommended rules from @eslint-plugin-react
    'plugin:@typescript-eslint/recommended', // Uses the recommended rules from the @typescript-eslint/eslint-plugin
    'prettier/@typescript-eslint', // Uses eslint-config-prettier to disable ESLint rules from @typescript-eslint/eslint-plugin that would conflict with prettier
    'plugin:prettier/recommended' // Enables eslint-plugin-prettier and eslint-config-prettier. This will display prettier errors as ESLint errors. Make sure this is always the last configuration in the extends array.
  ],
  rules: {
    // Place to specify ESLint rules. Can be used to overwrite rules specified from the extended configs
    // e.g. "@typescript-eslint/explicit-function-return-type": "off",
    '@typescript-eslint/no-namespace': 'off'
  }
}


# kiara\jupyterlab-extension-example\.prettierrc.js
module.exports = {
  singleQuote: true,
  trailingComma: 'none',
  semi: false,
  tabWidth: 2,
  arrowParens: 'avoid',
  printWidth: 110
}


# kiara\jupyterlab-extension-example\babel.config.js
module.exports = {
  presets: ['@babel/preset-env', '@babel/preset-react', '@babel/preset-typescript'],
  plugins: [
    [
      '@babel/plugin-transform-runtime',
      {
        regenerator: true
      }
    ],
    '@babel/plugin-proposal-class-properties',
    ['@babel/plugin-transform-typescript', { allowNamespaces: true }]
  ]
}


# kiara\jupyterlab-extension-example\install.json
{
  "packageManager": "python",
  "packageName": "dharpa_jupyterlab_extension_example",
  "uninstallInstructions": "Use your Python package manager (pip, conda, etc.) to uninstall the package 'dharpa_jupyterlab_extension_example'"
}


# kiara\jupyterlab-extension-example\package.json
{
  "name": "@dharpa/jupyterlab-extension-ts-example",
  "version": "0.1.0",
  "description": "Basic JupyterLab 3.x extension. All development tools included.",
  "keywords": [
    "jupyter",
    "jupyterlab",
    "jupyterlab-extension"
  ],
  "homepage": "https://example.com",
  "main": "src/index.ts",
  "license": "TBD",
  "author": "Author",
  "scripts": {
    "develop": "jupyter labextension develop . --overwrite --debug",
    "watch": "jupyter labextension watch --development True .",
    "eslint": "eslint src --ext .ts,.tsx,.js,.jsx --fix",
    "eslint:check": "eslint src --ext .ts,.tsx,.js,.jsx",
    "build:prod": "jlpm run build:lib && jlpm run build:labextension",
    "build:labextension": "jupyter labextension build .",
    "build:lib": "tsc",
    "start": "webpack serve -c webpack.config.standalone.js"
  },
  "dependencies": {
    "@jupyterlab/application": "3",
    "@jupyterlab/launcher": "3",
    "@jupyterlab/mainmenu": "3",
    "@jupyterlab/nbformat": "3",
    "@jupyterlab/notebook": "3",
    "@jupyterlab/translation": "3",
    "@lumino/coreutils": "1",
    "@lumino/widgets": "1",
    "react": "17",
    "react-dom": "17",
    "tailwindcss": "2"
  },
  "devDependencies": {
    "@babel/core": "7",
    "@babel/plugin-proposal-class-properties": "7",
    "@babel/plugin-transform-runtime": "7",
    "@babel/preset-env": "7",
    "@babel/preset-react": "7",
    "@babel/preset-typescript": "7",
    "@jupyterlab/builder": "3",
    "@types/node": "14",
    "@types/react-dom": "17",
    "@typescript-eslint/eslint-plugin": "4",
    "@typescript-eslint/parser": "4",
    "babel-loader": "8",
    "eslint": "7",
    "eslint-config-prettier": "7",
    "eslint-plugin-prettier": "3",
    "eslint-plugin-react": "7",
    "html-loader": "1",
    "html-webpack-plugin": "5",
    "postcss": "8",
    "postcss-loader": "4",
    "prettier": "2",
    "rimraf": "3",
    "sass": "1",
    "sass-loader": "11",
    "source-map-loader": "2",
    "svg-url-loader": "7",
    "typescript": "4",
    "url-loader": "4",
    "webpack": "5",
    "webpack-dev-server": "3"
  },
  "sideEffects": [
    "*.css",
    "*.scss"
  ],
  "jupyterlab": {
    "extension": true,
    "outputDir": "dharpa_jupyterlab_extension_example/labextension",
    "webpackConfig": "./webpack.config.labext.js"
  }
}


# kiara\jupyterlab-extension-example\postcss.config.js
const tailwindcss = require('tailwindcss')
module.exports = {
  plugins: [tailwindcss]
}


# kiara\jupyterlab-extension-example\README.md
# JupyterLab extension example

An example app that can run as a Jupyter extension and as a standalone web app. It showcases how communication with the kernel is done in Jupyter.

When run as a standalone web app the communication with the kernel is mocked. Running the app standalone has the advantage of using the webpack development server which reloads the app automatically during development. When running in Jupyter the browser tab needs to be manually reloaded every time the extension is recompiled.

## Running in development mode

### Running in JupyterLab

Make sure you have `JupyterLab 3.x` installed.
In a terminal session start JupyterLab:

```shell
jupyter lab
```

In another terminal session:

```shell
# Install dependencies
yarn install

# Install python package (similar to pip install -e)
# Makes the extension available in JupyterLab in development mode.
yarn develop
```

Then to watch files for changes and recompile run:

```shell
yarn watch
```

### Running as a standalone app:

```shell
# Install dependencies
yarn install
```

Then start the development server that will watch files for changes, recompile and reload the browser tab:

```shell
yarn start
```

## Items that should be changed/renamed if this project is cloned

- In `package.json`:
  - `name` - the extension will live in JupyterLab extensions folder under this name.
  - `jupyterlab.outputDir` - should follow the pattern: `<python_project_name>/labextension` (see below).
- Python project folder (`dharpa_jupyterlab_extension_example`). To keep things consistent, use the `name` from the `package.json`, remove `@` and replace `/` with `_` and all `-` with `_`.
- Change python project name in `install.json`.


# kiara\jupyterlab-extension-example\setup.py
import json
import os

from jupyter_packaging import (
    create_cmdclass, install_npm, ensure_targets,
    combine_commands, skip_if_exists
)
import setuptools

CURRENT_DIR = os.path.abspath(os.path.dirname(__file__))

with open(os.path.join(CURRENT_DIR, 'package.json')) as fid:
    data = json.load(fid)

version = data['version']
lab_path = data['jupyterlab']['outputDir']
labext_name = data['name']

# # The name of the project
name = lab_path.split('/')[0]

# Representative files that should exist after a successful build
jstargets = [
    os.path.join(lab_path, "package.json"),
]

package_data_spec = {
    'name': [
        "*"
    ]
}

data_files_spec = [
    ("share/jupyter/labextensions/%s" % labext_name, lab_path, "**"),
    ("share/jupyter/labextensions/%s" % labext_name, CURRENT_DIR, "install.json"),
]

cmdclass = create_cmdclass("jsdeps",
    package_data_spec=package_data_spec,
    data_files_spec=data_files_spec
)

js_command = combine_commands(
    install_npm(CURRENT_DIR, build_cmd="build:prod", npm=["jlpm"]),
    ensure_targets(jstargets),
)

is_repo = os.path.exists(os.path.join(CURRENT_DIR, ".git"))
if is_repo:
    cmdclass["jsdeps"] = js_command
else:
    cmdclass["jsdeps"] = skip_if_exists(jstargets, js_command)

with open("README.md", "r") as fh:
    long_description = fh.read()

setup_args = dict(
    name=name,
    version=version,
    url=data.get('homepage'),
    author=data.get('author'),
    description=data['description'],
    long_description= long_description,
    long_description_content_type="text/markdown",
    cmdclass= cmdclass,
    packages=setuptools.find_packages(),
    install_requires=[
        "jupyterlab>=3.0.0rc15,==3.*",
    ],
    zip_safe=False,
    include_package_data=True,
    python_requires=">=3.6",
    license=data.get('license'),
    platforms="Linux, Mac OS X, Windows",
    keywords=["Jupyter", "JupyterLab", "JupyterLab3"],
    classifiers=[
        "License :: OSI Approved :: BSD License",
        "Programming Language :: Python",
        "Programming Language :: Python :: 3",
        "Programming Language :: Python :: 3.6",
        "Programming Language :: Python :: 3.7",
        "Programming Language :: Python :: 3.8",
        "Framework :: Jupyter",
    ],
)


if __name__ == "__main__":
    setuptools.setup(**setup_args)


# kiara\jupyterlab-extension-example\tailwind.config.js
module.exports = {
  purge: ['./**/*.html'],
  darkMode: false,
  theme: {
    extend: {}
  },
  variants: {
    extend: {}
  },
  plugins: []
}


# kiara\jupyterlab-extension-example\tsconfig.json
{
  "compilerOptions": {
    "allowSyntheticDefaultImports": true,
    "composite": true,
    "declaration": true,
    "esModuleInterop": true,
    "incremental": true,
    "jsx": "react",
    "module": "esnext",
    "moduleResolution": "node",
    "noEmitOnError": true,
    "noImplicitAny": true,
    "noUnusedLocals": true,
    "preserveWatchOutput": true,
    "resolveJsonModule": true,
    "outDir": "dist",
    "rootDir": ".",
    "strict": true,
    "strictNullChecks": false,
    "sourceMap": true,
    "inlineSources": true,
    "target": "es2017",
    "lib": ["es2015", "dom"],
    "types": ["node"],
    "allowJs": true
  },
  "include": [
    "src/**/*.ts",
    "src/**/*.tsx",
    "src/**/*.js",
    "src/**/*.jsx",
    "package.json"
  ]

}


# kiara\jupyterlab-extension-example\webpack.config.labext.js
const webpack = require('webpack')

/**
 * Overrides for JupyterLab supplied extension build config.
 * Make it pick the source maps generated by `tsc`.
 * Overrides https://github.com/jupyterlab/jupyterlab/blob/master/builder/src/webpack.config.base.ts
 */
module.exports = {
  devtool: 'source-map',
  stats: 'detailed',
  module: {
    rules: [
      {
        test: /\.(ts|js)x?$/,
        use: ['source-map-loader'],
        enforce: 'pre'
      },
      {
        test: /\.s[ac]ss$/i,
        use: ['style-loader', 'css-loader', 'postcss-loader', 'sass-loader']
      },
      {
        test: /\.(ts|js)x?$/,
        exclude: /node_modules/,
        use: ['babel-loader']
      }
    ]
  },
  resolve: {
    extensions: ['.js', '.jsx', '.ts', '.tsx']
  },
  plugins: [
    new webpack.DefinePlugin({
      'process.env.USE_JUPYTER_LAB': JSON.stringify(true)
    })
  ]
}


# kiara\jupyterlab-extension-example\webpack.config.standalone.js
const path = require('path')
const webpack = require('webpack')
const HTMLWebPackPlugin = require('html-webpack-plugin')

module.exports = {
  entry: { index: './src/standalone.tsx' },
  devtool: 'source-map',
  stats: 'detailed',
  mode: 'development',
  devServer: {
    historyApiFallback: true,
    hot: true,
    compress: true,
    disableHostCheck: true
  },
  module: {
    rules: [
      {
        test: /\.(ts|js)x?$/,
        use: ['source-map-loader'],
        enforce: 'pre'
      },
      {
        test: /\.s[ac]ss$/i,
        use: ['style-loader', 'css-loader', 'postcss-loader', 'sass-loader']
      },
      {
        test: /\.(ts|js)x?$/,
        exclude: /node_modules/,
        use: ['babel-loader']
      },
      {
        test: /\.html$/,
        use: [
          {
            loader: 'html-loader'
          }
        ]
      }
    ]
  },
  resolve: {
    extensions: ['.js', '.jsx', '.ts', '.tsx']
  },
  plugins: [
    new HTMLWebPackPlugin({
      filename: 'index.html',
      template: path.join(__dirname, 'src', 'index.html')
    }),
    new webpack.DefinePlugin({
      'process.env.USE_JUPYTER_LAB': JSON.stringify(false)
    })
  ]
}


# kiara\jupyterlab-extension-example\.vscode\settings.json
{
  "editor.formatOnSave": true,
  "[javascript]": {
    "editor.formatOnSave": true
  },
  "[typescript]": {
    "editor.formatOnSave": true
  }
}


# kiara\jupyterlab-extension-example\dharpa_jupyterlab_extension_example\__init__.py
import json
import os.path

CURRENT_DIR = os.path.abspath(os.path.dirname(__file__))

with open(os.path.join(CURRENT_DIR, 'labextension', 'package.json')) as fid:
    data = json.load(fid)

__version__ = data['version']

def _jupyter_labextension_paths():
    return [{
        'src': 'labextension',
        'dest': data['name']
    }]


# kiara\jupyterlab-extension-example\src\index.html
<!DOCTYPE html>
<html>
  <head>
    <title>DHARPA JupyterLab extension standalone app</title>
  </head>
  <body class="min-h-screen bg-gray-200 py-6 flex flex-col justify-start sm:py-12">
    <div id="root" class="relative max-w-xl mx-auto border-dashed border-2 border-gray-300" style="min-width: 80%;"></div>
  </body>
</html>


# kiara\jupyterlab-extension-example\src\index.ts
import extension from './jupyter'
export default extension


# kiara\jupyterlab-extension-example\src\standalone.tsx
import React from 'react'
import { render } from 'react-dom'
import { Signal } from '@lumino/signaling'
import { App } from './components/App'
import { ModelContextProvider, IModel, IMessage } from './common/ModelContext'

const createMockMessage = (text: string): IMessage => ({
  channel: 'shell',
  header: {
    msg_id: String(new Date().getTime()),
    msg_type: 'status',
    date: new Date().toISOString()
  },
  content: { data: { 'text/plain': text } }
})

class MockModel implements IModel {
  lastReceivedMessage = new Signal<MockModel, IMessage>(this)
  lastExecutionResult = new Signal<MockModel, IMessage>(this)

  execute(code: string): void {
    this.lastExecutionResult.emit(createMockMessage(`Result of: ${code}`))
  }
  sendMessage(target: string, msg: unknown): void {
    this.lastReceivedMessage.emit(
      createMockMessage(`Mock message: Sent message ${JSON.stringify(msg)} to target ${target}`)
    )
  }
}

const StandaloneApp = (): JSX.Element => {
  const model = React.useRef<IModel>(new MockModel())
  React.useEffect(() => {
    const signal = model.current.lastReceivedMessage as Signal<MockModel, IMessage>
    signal.emit(createMockMessage('NOTE: Running outside of Jupyter'))
  }, [])

  return (
    <ModelContextProvider value={model.current}>
      <App />
    </ModelContextProvider>
  )
}

render(<StandaloneApp />, document.getElementById('root'))


# kiara\jupyterlab-extension-example\src\common\ModelContext.tsx
import React from 'react'
import { KernelMessage } from '@jupyterlab/services'
import { ISignal } from '@lumino/signaling'

export interface IMessage {
  content: unknown
  channel: KernelMessage.Channel
  header: {
    msg_id: string
    msg_type: KernelMessage.MessageType
    date: string
  }
  metadata?: unknown
}

export interface IModel {
  lastReceivedMessage: ISignal<IModel, IMessage>
  lastExecutionResult: ISignal<IModel, IMessage>

  execute(code: string): void
  sendMessage(target: string, msg: unknown): void
}

const ModelContext = React.createContext<IModel>(null)
ModelContext.displayName = 'ModelContext'

export const ModelContextProvider = ModelContext.Provider

const useSignal = <T, P>(signal: ISignal<T, P>): P => {
  const [v, setV] = React.useState<P>()
  const slot = React.useRef((_: T, args: P) => setV(args))
  React.useEffect(() => {
    signal.connect(slot.current)
    return () => signal.disconnect(slot.current)
  }, [])

  return v
}

export const useLastReceivedMessage = (): IMessage | undefined => {
  return useSignal(React.useContext(ModelContext).lastReceivedMessage)
}

export const useExecute = (): [IMessage | undefined, (code: string) => void] => {
  const model = React.useContext(ModelContext)
  const result = useSignal(model.lastExecutionResult)

  return [result, (code: string) => model.execute(code)]
}

export const useSendCommMessage = (): [(target: string, msg: unknown) => void] => {
  const model = React.useContext(ModelContext)
  return [(target: string, msg: unknown) => model.sendMessage(target, msg)]
}


# kiara\jupyterlab-extension-example\src\components\App.tsx
import React from 'react'
import { useLastReceivedMessage, useExecute, IMessage, useSendCommMessage } from '../common/ModelContext'

import './app.scss'

const ExecuteResult = ({ result }: { result: Record<string, unknown> }) => {
  if (result?.['data'] == null) return <></>
  const data: Record<string, unknown> = (result?.['data'] as Record<string, unknown>) ?? {}

  if (data['text/html'] != null)
    return <div dangerouslySetInnerHTML={{ __html: data['text/html'] as string }}></div>

  return <pre className="whitespace-normal">{data['text/plain']}</pre>
}

const TableHeaderCell = ({ children }: { children: React.ReactNode }) => (
  <th scope="col" className="px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">
    {children}
  </th>
)

const TableCell = ({ children }: { children: React.ReactNode }) => (
  <td className="px-2 py-4 whitespace-nowrap">
    <div className="flex items-center">
      <div className="ml-4">
        <div className="text-sm font-medium text-gray-900">
          <pre className="whitespace-pre-wrap">{children}</pre>
        </div>
      </div>
    </div>
  </td>
)

const InputClasses =
  'focus:ring-indigo-500 focus:border-indigo-500 shadow-sm sm:text-sm border-gray-300 rounded-md p-2 w-full'
const ButtonClasses =
  'inline-flex justify-center py-2 px-4 border border-transparent shadow-sm text-sm font-medium rounded-md text-white bg-indigo-600 hover:bg-indigo-700 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-indigo-500 whitespace-nowrap'

export const App = (): JSX.Element => {
  const [executeString, setExecuteString] = React.useState('')
  const [commTarget, setCommTarget] = React.useState('')
  const [commMessage, setCommMessage] = React.useState('')

  const [executeResult, execute] = useExecute()
  const [sendMessage] = useSendCommMessage()

  const lastMessage = useLastReceivedMessage()
  const [messages, setMessages] = React.useState<IMessage[]>(lastMessage == null ? [] : [lastMessage])

  React.useEffect(() => {
    if (lastMessage == null) return
    setMessages([lastMessage].concat(messages))
  }, [lastMessage])

  const executeCode = () => execute(executeString)
  const sendCommMessage = () => sendMessage(commTarget, commMessage)

  return (
    <div className="flex flex-col">
      {/* execute code section */}
      <div className="flex flex-col py-3 px-2">
        <div className="flex flex-row space-x-3">
          <input
            type="test"
            placeholder="Python code"
            className={InputClasses}
            value={executeString}
            onChange={e => setExecuteString(e.target.value)}
          />
          <button className={ButtonClasses} onClick={executeCode}>
            Execute
          </button>
        </div>
        <div className="flex py-5 px-2">
          <ExecuteResult result={executeResult?.content as Record<string, unknown>} />
        </div>
      </div>

      {/* send comm message section */}
      <div className="flex flex-col py-3 px-2">
        <div className="flex flex-row space-x-3">
          <input
            type="text"
            placeholder="target"
            className={InputClasses}
            value={commTarget}
            onChange={e => setCommTarget(e.target.value)}
          />
          <input
            placeholder="Message"
            type="text"
            className={InputClasses}
            value={commMessage}
            onChange={e => setCommMessage(e.target.value)}
          />
          <button className={ButtonClasses} onClick={sendCommMessage}>
            Send Message
          </button>
        </div>
      </div>

      {/* kernel messages section */}
      <div className="flex flex-row">
        <table className="min-w-full divide-y divide-gray-200">
          <thead className="bg-gray-50">
            <tr>
              <TableHeaderCell>Status</TableHeaderCell>
              <TableHeaderCell>Date</TableHeaderCell>
              <TableHeaderCell>Channel</TableHeaderCell>
              <TableHeaderCell>Message</TableHeaderCell>
              <TableHeaderCell>Metadata</TableHeaderCell>
            </tr>
          </thead>
          <tbody className="bg-white divide-y divide-gray-200">
            {messages.map((message, index) => (
              <tr key={index}>
                <TableCell>{message.header.msg_type}</TableCell>
                <TableCell>{message.header.date}</TableCell>
                <TableCell>{message.channel}</TableCell>
                <TableCell>{JSON.stringify(message.content, null, 2)}</TableCell>
                <TableCell>{JSON.stringify(message.metadata, null, 2)}</TableCell>
              </tr>
            ))}
          </tbody>
        </table>
      </div>
    </div>
  )
}


# kiara\jupyterlab-extension-example\src\jupyter\index.ts
import { ILayoutRestorer, JupyterFrontEnd, JupyterFrontEndPlugin } from '@jupyterlab/application'
import { SessionContext, ICommandPalette, WidgetTracker } from '@jupyterlab/apputils'
import { ILauncher } from '@jupyterlab/launcher'
import { IMainMenu } from '@jupyterlab/mainmenu'
import { ITranslator } from '@jupyterlab/translation'
import { INotebookTracker } from '@jupyterlab/notebook'
import { Menu } from '@lumino/widgets'
import { WrapperPanel } from './panel'

const AppId = 'dharpa-extension-example'
const AppLabel = 'DHARPA JupyterLab extension example'
const AppCaption = 'DHARPA JupyterLab extension example'
const AppCategory = 'DHARPA Extension Examples'

/**
 * The command IDs used by the console plugin.
 */
namespace CommandIDs {
  export const create = `${AppId}:create`
}

/**
 * Initialization data for the extension.
 */
const extension: JupyterFrontEndPlugin<WidgetTracker<WrapperPanel>> = {
  id: AppId,
  autoStart: true,
  optional: [ILauncher, ILayoutRestorer],
  requires: [ICommandPalette, IMainMenu, ITranslator, INotebookTracker],
  activate: activate
}

/**
 * Activate the JupyterLab extension.
 */
function activate(
  app: JupyterFrontEnd,
  palette: ICommandPalette,
  mainMenu: IMainMenu,
  translator: ITranslator,
  notebooks: INotebookTracker,
  launcher: ILauncher | null,
  restorer: ILayoutRestorer | null
): WidgetTracker<WrapperPanel> {
  const manager = app.serviceManager
  const { commands, shell } = app
  const category = AppCategory
  const trans = translator.load('jupyterlab')

  const mainPanelTracker = new WidgetTracker<WrapperPanel>({
    namespace: AppId
  })

  // Add launcher
  if (launcher) {
    launcher.add({
      command: CommandIDs.create,
      category: category
    })
  }

  // restore open windows
  if (restorer) {
    void restorer.restore(mainPanelTracker, {
      command: CommandIDs.create,
      name: panel => panel.session.path,
      when: notebooks.restored.then(() => app.serviceManager.ready),
      args: panel => ({
        path: panel.session.path
      })
    })
  }

  // create new panel with a path to the notebook it tracks (if any)
  async function createPanel({ path }: { path?: string }): Promise<WrapperPanel> {
    const notebookWidget = notebooks.find(n => n.sessionContext.path === path) ?? notebooks?.currentWidget

    const existingContext = notebookWidget?.context?.sessionContext
    const sessionContext = existingContext
      ? existingContext
      : new SessionContext({
          sessionManager: manager.sessions,
          specsManager: manager.kernelspecs,
          name: AppLabel
        })

    const panel = new WrapperPanel(`${AppId}-panel`, AppLabel, sessionContext, translator)
    shell.add(panel, 'main')
    mainPanelTracker.add(panel)

    return panel
  }

  // add menu tab
  const exampleMenu = new Menu({ commands })
  exampleMenu.title.label = trans.__(AppLabel)
  mainMenu.addMenu(exampleMenu)

  // add commands to registry
  commands.addCommand(CommandIDs.create, {
    label: trans.__(AppLabel),
    caption: trans.__(AppCaption),
    execute: createPanel
  })

  // add items in command palette and menu
  palette.addItem({ command: CommandIDs.create, category })
  exampleMenu.addItem({ command: CommandIDs.create })

  return mainPanelTracker
}

export default extension


# kiara\jupyterlab-extension-example\src\jupyter\kernelModel.ts
import { ISessionContext } from '@jupyterlab/apputils'
import { Kernel, KernelMessage } from '@jupyterlab/services'
import { Signal, ISignal } from '@lumino/signaling'
import { IModel, IMessage } from '../common/ModelContext'

type KernelMessage = Readonly<KernelMessage.IMessage<KernelMessage.MessageType>>
type ExecuteFuture = Kernel.IFuture<KernelMessage.IExecuteRequestMsg, KernelMessage.IExecuteReplyMsg>

const ResultMessageTypes = ['execute_result', 'display_data', 'update_display_data']

export class KernelModel implements IModel {
  private _sessionContext: ISessionContext

  private _lastSentMessage = new Signal<KernelModel, IMessage>(this)
  private _lastReceivedMessage = new Signal<KernelModel, IMessage>(this)
  private _lastExecutionResult = new Signal<KernelModel, IMessage>(this)

  private _executeFuture: ExecuteFuture | null = null

  constructor(session: ISessionContext) {
    this._sessionContext = session

    this._sessionContext

    this._sessionContext.session?.kernel?.anyMessage?.connect(this._anyMessageHandler.bind(this))
  }

  _anyMessageHandler(sender: Kernel.IKernelConnection, args: Kernel.IAnyMessageArgs): void {
    if (args.direction === 'recv') {
      this._lastReceivedMessage.emit(args.msg)

      const parentMessageId = (args.msg.parent_header as KernelMessage.IHeader)?.msg_id
      if (parentMessageId === this.lastExecuteMessageId) {
        const { msg_type: messageType } = args.msg.header
        if (ResultMessageTypes.includes(messageType)) {
          this._lastExecutionResult.emit(args.msg)
        }
      }
    } else {
      this._lastSentMessage.emit(args.msg)
    }
  }

  execute(code: string): void {
    if (!this.isConnected) throw new Error('Not connected to kernel')
    this._executeFuture = this._sessionContext.session?.kernel?.requestExecute({
      code
    })
  }

  sendMessage(target: string, msg: unknown): void {
    const comm = this._sessionContext.session?.kernel?.createComm(target)
    comm
      .open()
      .done.then(() => comm.send({ body: msg as string }).done)
      .finally(() => comm.close().done.catch(() => undefined))
  }

  get isConnected(): boolean {
    return this?._sessionContext?.session?.kernel != null
  }

  get executeFuture(): ExecuteFuture {
    return this._executeFuture
  }

  get lastReceivedMessage(): ISignal<IModel, IMessage> {
    return this._lastReceivedMessage
  }

  get lastExecutionResult(): ISignal<IModel, IMessage> {
    return this._lastExecutionResult
  }

  get lastExecuteMessageId(): string {
    return this._executeFuture == null ? undefined : this._executeFuture.msg.header.msg_id
  }
}


# kiara\jupyterlab-extension-example\src\jupyter\panel.ts
import { ISessionContext, sessionContextDialogs } from '@jupyterlab/apputils'
import { ITranslator, nullTranslator, TranslationBundle } from '@jupyterlab/translation'
import { Message } from '@lumino/messaging'
import { StackedPanel } from '@lumino/widgets'
import { KernelView } from './widget'
import { KernelModel } from './kernelModel'

/**
 * Container panel
 */
export class WrapperPanel extends StackedPanel {
  constructor(id: string, label: string, sessionContext: ISessionContext, translator?: ITranslator) {
    super()

    const actualTranslator = translator || nullTranslator
    this._trans = actualTranslator.load('jupyterlab')

    this.id = id
    this.title.label = this._trans.__(label)
    this.title.closable = true
    this._sessionContext = sessionContext

    void this._sessionContext
      .initialize()
      .then(async shouldSelectKernel => {
        if (shouldSelectKernel) {
          await sessionContextDialogs.selectKernel(this._sessionContext)
        }
        this._model = new KernelModel(this._sessionContext)
        this._example = new KernelView(this._model)
        this.addWidget(this._example)
      })
      .catch(reason => {
        console.error(`Failed to initialize the session in ExamplePanel.\n${reason}`)
      })
  }

  get session(): ISessionContext {
    return this._sessionContext
  }

  dispose(): void {
    // uncomment to destroy kernel on exit
    // this._sessionContext.dispose()
    super.dispose()
  }

  protected onCloseRequest(msg: Message): void {
    super.onCloseRequest(msg)
    this.dispose()
  }

  private _model: KernelModel
  private _sessionContext: ISessionContext
  private _example: KernelView

  private _trans: TranslationBundle
}


# kiara\jupyterlab-extension-example\src\jupyter\widget.tsx
import React from 'react'
import { ReactWidget } from '@jupyterlab/apputils'
import { KernelModel } from './kernelModel'
import { App } from '../components/App'
import { ModelContextProvider } from '../common/ModelContext'

export class KernelView extends ReactWidget {
  private _model: KernelModel

  constructor(model: KernelModel) {
    super()
    this._model = model
    this.addClass('dharpa-scrollable-panel')
    this.addClass('bg-gray-200')
  }

  protected render(): React.ReactElement<unknown> {
    return (
      <ModelContextProvider value={this._model}>
        <App />
      </ModelContextProvider>
    )
  }
}


# kiara\kiara\.gitlab-ci.yml
---
variables:
  GIT_STRATEGY: fetch
  GIT_DEPTH: 0
  GIT_SUBMODULE_STRATEGY: recursive
  LM_PYTHON_VERSION: "2"
  DS_PYTHON_VERSION: "3"
  PIP_EXTRA_INDEX_URL: "https://pkgs.frkl.io/frkl/dev"

# include:
#  - template: Dependency-Scanning.gitlab-ci.yml
#  - template: License-Management.gitlab-ci.yml

image: python:3.8

stages:
  - test
  - build
  - build_windows
  - release

include:
  - local: '/ci/gitlab/test/tox.yml'
  - local: '/ci/gitlab/test/mypy.yml'
  - local: '/ci/gitlab/test/flake8.yml'
  - local: '/ci/gitlab/test/coverage.yml'
  - local: '/ci/gitlab/test/safety.yml'
  - local: '/ci/gitlab/test/commitlint.yml'
  - local: '/ci/gitlab/build/docs.yml'
  - local: '/ci/gitlab/build/pkg.yml'
  - local: '/ci/gitlab/build/binary_linux.yml'
  - local: '/ci/gitlab/build/binary_windows.yml'
#  - local: '/ci/gitlab/release/binaries.yml'
#  - local: '/ci/gitlab/release/container.yml'


pages:
  extends: .kiara_build_docs


# kiara\kiara\.git_archival.txt
ref-names: $Format:%D$


# kiara\kiara\.pre-commit-config.yaml
default_language_version:
    python: python3

repos:

- repo: https://github.com/alessandrojcm/commitlint-pre-commit-hook
  rev: 'v9.15.0'
  hooks:
    - id: commitlint
      stages: [commit-msg]
      additional_dependencies: ['@commitlint/config-conventional']

- repo: https://github.com/psf/black
  rev: 24.3.0
  hooks:
    - id: black

- repo: https://github.com/pre-commit/mirrors-mypy
  rev: 'v1.9.0'  # Use the sha / tag you want to point at
  hooks:
  - id: mypy
    files: "^src/kiara"
    pass_filenames: true
    args: ["--config-file", "pyproject.toml"]
    additional_dependencies: [pydantic>=2.0.0, pydantic_settings, rich>=10.0.0, ruamel.yaml, anyio>=3.0.0, pyzmq>=22.0.3, bidict, sqlalchemy-stubs, types-python-slugify, types-setuptools, types-python-dateutil, dag_cbor>=0.3.0, multiformats>=0.2.0, textual, regex, types-pytz, types-orjson, dag_cbor>=0.3.2]

- repo: https://github.com/astral-sh/ruff-pre-commit
  # Ruff version.
  rev: v0.4.1
  hooks:
    # Run the linter.
    - id: ruff
      args: [ --fix ]
    # Run the formatter.
#    - id: ruff-format

- repo: https://github.com/pre-commit/pre-commit-hooks
  rev: 'v4.5.0'
  hooks:
  - id: trailing-whitespace
    exclude: '(setup.cfg|src/kiara/utils/cli/__init__.py)'
  - id: check-added-large-files
  - id: check-ast
  - id: check-json
  - id: check-merge-conflict
  - id: check-xml
  - id: check-yaml
    exclude: '(tests/\*|ci/conda/kiara/meta.yaml)'
  - id: debug-statements
  - id: end-of-file-fixer
    exclude: '(.*.json|.*.j2)'
  - id: requirements-txt-fixer
  - id: fix-encoding-pragma
  - id: mixed-line-ending
    args: ['--fix=no']
  - id: no-commit-to-branch
    args: [--branch, main]

- repo: https://github.com/Kludex/no-optional
  rev: 0.4.0
  hooks:
    - id: no_optional


# kiara\kiara\CHANGELOG.md
# Changelog

## Version 0.5.10 (upcoming)

- archive export & import feature:
  - new cli subcommands:
    - `kiara archive import`
    - `kiara archive export`
    - `kiara archive explain`
    - `kiara data import`
    - `kiara data export`
  - new api endpoints:
    - `retrieve_archive_info`
    - `export_archive`
    - `import_archive`
    - `export_values`
    - `import_values`
- always store every job record and job result value(s)
- allow a 'comment' to be associated with a job:
  - require a 'comment' for every `run_job`/`queue_job` call
- new job record api endpoints:
  - `list_all_job_record_ids`
  - `list_job_record_ids`
  - `list_all_job_records`
  - `list_job_records`
  - `get_job_record`
  - `get_job_comment`
  - `set_job_comment`
- add convenience api endpoint `get_values`
- improved input options for 'store_values' API endpoint
- added 'value_created' property on 'Value' instances
- add '--runtime-info' cli flag to display runtime folders/files used by *kiara*
- fix: plugin info for plugins with '-' in name
- moved `KiaraAPI` class to `kiara.interfaces.python_api.kiara_api` module (the 'offical' import path `kiara.api.KiaraAPI` is still available, and should be used)
- have `KiaraAPI` proxy a `BaseAPI` class, to make it easier to extend the API and keep it stable

## Version 0.5.9

- mostly test coverage improvements
- fix: support alias prefixed strings as job inputs

## Version 0.5.8

- add 'mock' module type

## Version 0.5.5

- added pipeline related api endpoints:
  - `list_pipeline_ids`
  - `list_pipelines`
  - `get_pipeline`
  - `retrieve_pipeline_info`
  - 'retrieve_pipelines_info'

- added support for quering plugin information
  - cli: `kiara info plugin`
  - api endpoints:
    - `list_available_plugin_names`
    - `retrieve_plugin_info`
    - `retrieve_plugin_infos`

## Version 0.3.1 - 0.5.4

- changes not tracked

## Version 0.3.1

- allow 'dict' field name

## Version 0.3.0

- changed metadata format of stored value metadata: data store must be cleared when updating to this version
- refactoring of operation type input names -- this replaces most instances where the input name was 'value_item' (or similar). I've decided that using the value type as input name increases usability of those operations more than the costs associated with having different input names for operations of the same type, for example:
  - pretty_print: 'value_item' -> type name of the value to pretty print
  - extract_metadata: 'value_item' -> type name of the value to extract metadata from
  - import: 'source' input -> input file type, 'value_item' output -> target file type
  - save_value: 'value_item' input -> file type of value to save
  - convert (renamed to 'create'): value_item to source profile and target type

## Version 0.2.2

- fixes for cli pretty printing
- auto-publish kiara conda package

## Version 0.2.1

- removed 'save' and 'aliases' config/input options from import operations (it turns out its much better overall to do saving explicitely, not within modules)
- processing metrics and information will be added to value metadata
- rudimentary rendering template management
- 'kiara info' subcommand, displaying the current context (incl. all modules & operations)

## Version 0.2.0

- add conda packages
- support for extra pipeline folders via KiaraConfig & cli
- moved 'any' type into this package

## Version 0.1.3

- add sample operation type
- some minor bug fixes

## Version 0.1.2

- module development helper subcommand (very bare bones)
- option to skip errors due to misconfigured pipeline entrypoints

## Version 0.1.1 (Upcoming)

- bug-fix release: fix error when trying to print value that wasn't set

## Version 0.1.0

- first 'official' release on Pypi.org
- the code and its API in this release can by no means be considered stable, but the largest pieces of its architecture should be in place by now
- there are still some refactorings to be made, and lots of features to implement, but this version should be good enough to do some very basic processing and data management


## Version 0.0.15

- support for saving values of type string, integer, float, bool
- some CI improvements when a tag is pushed: release to PyPi, auto-create versioned documentation

## Version 0.0.14

- this version contains a rather large refactoring of the data registry and how values are handled, so expect some breakage. Please submit issues for anything that worked, but doesn't anymore. Also, you'll have to delete the kiara shared local data (`~/.local/share/kiara` on linux) when upgrading.
- renamed:
  - kiara.pipeline.config.PipelineModuleConfig -> kiara.pipeline.config.PipelineConfig
  - kiara.module_config.ModuleTypeConfig -> kiara.module_config.KiaraModuleConfig
  - kiara.module_config.ModuleConfig -> kiara.module_config.ModuleConfig
  - in kiara.pipeline.values:
    - StepInputField -> StepInputRef
    - StepOutputField -> StepOutputRef
    - PipelineInputField -> PipelineInputRef
    - PipelineOutputField -> PipelineOutputRef
  - kiara.pipeline.values -> kiara.data.registry:
    - ValueField -> ValueRef
    - PipelineValue -> removed
    - DataValue -> removed
    - LinkedValue -> removed
    - ValueUpdateHandler -> ValueUpdateHandler
  - kiara.data.values.ValueMap*-related -> kiara.data.values.value_set
- removed 'pipeline_id' attribute from 'PipelineStructure' class, but 'Pipeline" has 'id' and 'title' fields now instead
- refactored 'DataRegistry' and 'Value' object:
  - 'Value' objects are now immutable, and can only be created via a registry
  - all subclasses of 'Value' are removed, there is only one 'Value' type now, which is always connected to a data registry (which handles versioning and actual storage of the payload)
  - removed linked values, replaced by 'ValueSlot' class
  - 'ValueSlot' basically contains the history of all (immutable) Value objects on a specific spot (mostly within a pipeline, but can be used elsewhere)
  - 'set_value_data' on 'Value' class is removed (since values are no immutable)
  - the interface of 'ValueMap' however is mostly unchanged, and all 'set/get value_obj/value_data' methods should still work as before
- data store is now just a 'DataRegistry' subclass that persists to disk instead of memory, this means that getting data into the data store now uses the 'register_data' method, and getting it out uses 'get_value_obj'
- aliases can now only contain alphanumeric characters, '_' and '-"
- removed some data import modules/operations until I settled on a data onboarding strategy (current one was leaky). This is mostly relevant for the operation that imports a table from a (path) string -- use a mini-pipeline as replacement and save the table manually, something like: https://github.com/DHARPA-Project/kiara/blob/main/tests/resources/pipelines/table_import.json
- rudimentary data lineage support
- performance improvement for cli, because more stuff is now lazily loaded
- tests

### Version 0.0.13

- mostly tests

## Version 0.0.12

- renamed:
    - kiara.operations.OperationConfig -> kiara.operations.Operation
    - kiara.operations.Operations -> kiara.operations.OperationType
- re-organizing/re-naming of onboarding/import related module/operation names
- small change to how job control works in the pipeline-controller:
    - calling the `wait_for` job-ids method is now mandatory after calling the `process_step` method, even when using the synchronous non-parallel processor
    - because the `wait_for` method now comes with an argument `sync_outputs` (default: True) that allows not actually syncing the output of the processing step to the pipeline value (which gives the controller more control over when to do that)
    - if you were calling `wait_for` before, there is nothing more to do. If you used the synchronous (default) processor and omitted that step, you'll have to add a line below your `process_step` call, `wait_for`-ing for the job ids that method returned
- re-implementation/refactoring of operations (documentation still to be done)
- removed 'kiara.module.ModuleInfo' class (use 'kiara.metadata.module_models.ModuleTypeInfo' instead)
- refactorings:
    - kiara.pipeline.module.PipelineModuleInfo -> kiara.info.pipelines.PipelineModuleInfo
    - other renames/relocations of (hopefully) mostly internal classes -- if something is missing it should now be somewhere under 'kiara.info.*'
- renamed subcommand 'pipeline structure' -> 'pipeline explain'

## Version 0.0.11

- added 'BatchControllerManual' controller

## Version 0.0.10

- major refactoring:
  - renamed:
    - 'kiara.module_config.KiaraWorkflowConfig' -> 'kiara.module_config.ModuleConfig'
    - 'kiara.module_config.KiaraModuleConfig' -> 'kiara.module_config.KiaraModuleConfig'
  - moved classes/functions:
    - 'kiara.data.operations' -> 'kiara.operations.type_operations'
    - 'kiara.processing.ModuleProcessor' -> 'kiara.processing.processor.ModuleProcessor'
    - from 'kiara.module_config' -> kiara.pipeline.utils:
      - create_step_value_address
      - ensure_step_value_addresses
    - from 'kiara.module_config' -> kiara.pipeline.config:
      - PipelineStepConfig
      - PipelineStructureConfig
      - PipelineConfig
    - from 'kiara.data.values' -> 'kiara.pipeline.utils':
      - generate_step_alias
    - from 'kiara.data.values' -> 'kiara.pipeline'
      - PipelineValueInfo
      - PipelineValuesInfo
    - from 'kiara.data.values' -> 'kiara.pipeline.values'
      - ValueUpdateHandler
      - StepValueAddress
      - ValueRef
      - RegisteredValue
      - RegisteredValue
      - LinkedValue
      - StepInputRef
      - StepOutputRef
      - PipelineInputRef
      - PipelineOutputRef

## Version 0.0.9

- removed 'aliases' attribute from Value class, aliases are now specified when calling 'save' on the Value object
- ActiveJob details (incl. error messages -- check the kiara.processing.ActiveJob class) for the most recent or current module executions can be retrieved: `[controller_obj].get_job_details(step_id)```
- re-write of the DataStore class:
  - support for aliases, as well as alias versions & tags (still to be documented)
  - enables the option of having different data store types down the line
  - API and overall workings of this is still a draft, so expect to see some changes to how value ids and alias are handled and look like
  - internal organisation of existing data is different, so when updating to this version you'll have to re-import your data sets and ideally also delete the old folder (``DEVELOP=true kiara data clear-data-store``)
- '--save' option in the ``kiara run`` command does now take an alias as option (previously the '--alias` flag)

## Version 0.0.6

- housekeeping

## Version 0.0.5

- this is only a stop-gap release, to a) test the release pipeline, and b) prepare for a fairly major doc/testing effort in the next few weeks
- introduced data operations: operation do the same thing to values of different types (pretty printing, serializing, etc.)

## Version 0.0.4

- refactoring of most modules names, mostly centered around the name of a modules 'main' type now

## Version 0.0.3

- new way of discovering modules and pipelines -- https://dharpa.org/kiara/development/module_discovery/
- inital support for persisting values via the *kiara* data store
- module namespaces: modules type names are now namespaces strings

## Version 0.0.2

- metadata extraction method renamed to 'extract_type_metadata'; also, the type metadata format changed slightly: information extracted by type goes into 'type' subkey, python class information gets added under 'python' (automatically)
- type-hint signature of parameters in ``process`` method in a ``KiaraModule`` changed from ``StepInputs``/``StepOutputs`` to ``ValueMap``
- change all input and output data access within modules to use ``ValueMap.get_value_data()``  ``ValueMap.set_value(s)`` instead of direct attribute access -- for now, direct attribute access is removed because it's not clear whether the access should be for the value object, or the data itself
- 'dict' attribute in ValueData class renamed to 'get_all_value_data'
- added 'ModuleProcessor' class, to be able to implement different module execution strategies (multithreaded, multiprocess, ...)
- renamed ``kiara.config`` module to ``kiara.module_config``
- modules are now split up into several packages: ``kiara_modules.core`` being the most important one, others are topic specific (``language_processing``, ``network_analysis``, ...)

## Version 0.0.1

- first alpha release of *kiara*


# kiara\kiara\commitlint.config.js
module.exports = {extends: ['@commitlint/config-conventional']}


# kiara\kiara\mkdocs.yml
site_name: Kiara
site_description: Documentation for the Kiara project
site_url: https://dharpa.org/kiara
repo_url: https://github.com/DHARPA-Project/kiara
site_author: Markus Binsteiner
copyright: DHARPA project
docs_dir: docs
site_dir: build/site

theme:
  name: material
  features:
    - navigation.instant
    - navigation.tracking

extra_css:
  - stylesheets/extra.css

markdown_extensions:
- attr_list
- admonition
- def_list
- codehilite:
    guess_lang: false
- toc:
    permalink: true
    toc_depth: 3
- pymdownx.snippets:
    base_path: docs
- pymdownx.highlight
- pymdownx.superfences

extra:
  version:
    provider: mike

plugins:
- search
- autorefs
- mkdocstrings:
    default_handler: python
    handlers:
      python:
        path: [src]
        options:
          heading_level: 2
          show_category_heading: true
          members_order: source
          show_submodules: false
          docstring_style: google
          show_if_no_docstring: true
          show_signature_annotations: true
          separate_signature: false
          filters:
            - "!^_"  # exlude all members starting with _
            - "^_config_cls"

    watch:
      - "src"
    enable_inventory: true
- macros:
   modules:
     - kiara.doc.mkdocs_macros_cli
     - kiara.doc.mkdocs_macros_kiara
- gen-files:
    scripts:
      - scripts/documentation/gen_info_pages.py
      - scripts/documentation/gen_api_doc_pages.py
- literate-nav:
    nav_file: SUMMARY.md
- section-index


# kiara\kiara\onboarding.folder_to_table.json
{
  "table": {
    "id": [
      0,
      1
    ],
    "rel_path": [
      "csv_1.csv",
      "csv_2.csv"
    ],
    "file_name": [
      "csv_1.csv",
      "csv_2.csv"
    ],
    "content": [
      "a,b,c\nd,e,f\n",
      "a,b,c\nd,e,f\n"
    ]
  }
}

# kiara\kiara\README.md
[![PyPI status](https://img.shields.io/pypi/status/kiara.svg)](https://pypi.python.org/pypi/kiara/)
[![PyPI version](https://img.shields.io/pypi/v/kiara.svg)](https://pypi.python.org/pypi/kiara/)
[![PyPI pyversions](https://img.shields.io/pypi/pyversions/kiara.svg)](https://pypi.python.org/pypi/kiara/)
[![Build Status](https://img.shields.io/endpoint.svg?url=https%3A%2F%2Factions-badge.atrox.dev%2FDHARPA-Project%2Fkiara%2Fbadge%3Fref%3Ddevelop&style=flat)](https://actions-badge.atrox.dev/DHARPA-Project/kiara/goto?ref=develop)
[![Coverage Status](https://coveralls.io/repos/github/DHARPA-Project/kiara/badge.svg?branch=develop)](https://coveralls.io/github/DHARPA-Project/kiara?branch=develop)
[![Code style](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/ambv/black)

# kiara

*A data-centric workflow orchestration framework.*

 - *kiara* user documentation: [https://dharpa.org/kiara.documentation](https://dharpa.org/kiara.documentation/)
 - Code: [https://github.com/DHARPA-Project/kiara](https://github.com/DHARPA-Project/kiara)
 - Development documentation for this repo: [https://dharpa.org/kiara](https://dharpa.org/kiara)

## Description

*Kiara* is the data orchestration engine developed by the DHARPA project. It uses a modular approach
to let users re-use tried and tested data orchestration pipelines, as well as create new ones from existing building
blocks. It also helps you manage your research data, and augment it with automatically-, semi-automatically-, and manually-
created metadata. Most of this is not yet implemented.

## Development

### Requirements

- Python (version >=3.6 -- some make targets only work for Python >=3.7, but *kiara* itself should work on 3.6)
- pip, virtualenv
- git
- make
- [direnv](https://direnv.net/) (optional)


### Prepare development environment

```console
git clone https://github.com/DHARPA-Project/kiara.git
cd kiara
python3 -m venv .venv
source .venv/bin/activate
make init
```

If you use [direnv](https://direnv.net/), you can alternatively do:

``` console
git clone https://github.com/DHARPA-Project/kiara.git
cd kiara
cp .envrc.disabled .envrc
direnv allow
make init
```

*Note*: you might want to adjust the Python version in ``.envrc`` (should not be necessary in most cases though)

### ``make`` targets

- ``init``: init development project (install project & dev dependencies into virtualenv, as well as pre-commit git hook)
- ``update-modules``: update default kiara modules package from git
- ``flake``: run *flake8* tests
- ``mypy``: run *mypy* tests
- ``test``: run unit tests
- ``docs``: create static documentation pages (under ``build/site``)
- ``serve-docs``: serve documentation pages (incl. auto-reload) for getting direct feedback when working on documentation
- ``clean``: clean build directories

For details (and other, minor targets), check the ``Makefile``.


### Running tests

``` console
> make test
# or
> make coverage
```


## Copyright & license

This project is MPL v2.0 licensed, for the license text please check the [LICENSE](/LICENSE) file in this repository.

- Copyright (c) 2021, 2022 [DHARPA project](https://dharpa.org)
- Copyright (c) 2021, 2022 Markus Binsteiner


# kiara\kiara\.github\dependabot.yml
# To get started with Dependabot version updates, you'll need to specify which
# package ecosystems to update and where the package manifests are located.
# Please see the documentation for all configuration options:
# https://help.github.com/github/administering-a-repository/configuration-options-for-dependency-updates

version: 2
updates:
  - package-ecosystem: "pip" # See documentation for possible values
    directory: "/" # Location of package manifests
    schedule:
      interval: "daily"


# kiara\kiara\.github\ISSUE_TEMPLATE\bug_report.md
---
name: Bug report
about: Create a report to help us improve
title: ''
labels: ''
assignees: ''

---

**Describe the bug**
A clear and concise description of what the bug is.

**To Reproduce**
Please provide some minimal code to reproduce the issue. If at all possible, use a Python environment with only the `kiara` and `kiara_modules.core` packages installed, as this will make it easier to transform this issue into a unit test.

**Expected behavior**
A clear and concise description of what you expected to happen.

**Environment, versions (please complete the following information):**
 - OS: [e.g. iOS]
 - Python: [e.g. 3.9.1]
 - Versions of relevant packages (`kiara`, `kiara_modules.core`, etc.)
 - Python environment used (e.g. system, conda, ...)

**Additional context**
Add any other context about the problem here.


# kiara\kiara\.github\ISSUE_TEMPLATE\feature_request.md
---
name: Feature request
about: Suggest an idea for this project
title: ''
labels: ''
assignees: ''

---

**Is your feature request related to a problem? Please describe.**
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...]

**Describe the solution you'd like**
A clear and concise description of what you want to happen.

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.


# kiara\kiara\.github\workflows\build-darwin.yaml
name: "darwin tests for 'kiara'"
# This workflow is triggered on pushes to the repository.
on: [push]
env:
  DEVELOPER_DIR: /Applications/Xcode_12.4.app/Contents/Developer
  MACOSX_DEPLOYMENT_TARGET: 10.15


jobs:
  test-darwin:
    name: pytest on darwin
    runs-on: macos-11
    strategy:
      matrix:
        python_version: ["3.8", "3.9", "3.10", "3.11", "3.12"]
    steps:
      - name: "Set up Python ${{ matrix.python_version }}"
        uses: actions/setup-python@v5
        with:
          python-version: "${{ matrix.python_version }}"
      - name: pip cache
        id: pip-cache
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/setup.*') }}
      - uses: actions/checkout@v4
      - name: install core_types plugin
        run: pip install -U git+https://github.com/DHARPA-project/kiara_plugin.core_types@develop
      - name: install tabular types plugin
        run: pip install -U git+https://github.com/DHARPA-project/kiara_plugin.tabular@develop
      - name: install kiara
        run: pip install -U .[dev_testing]
      - name: display installed kiara and module package versions
        run: pip list | grep kiara
      - name: test with pytest
        run: make test


# kiara\kiara\.github\workflows\build-linux.yaml
name: "linux tests and documentation build for 'kiara'"
# This workflow is triggered on pushes to the repository.
on: [push]

jobs:

  commitlint:
    name: lint commit message
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - uses: wagoid/commitlint-github-action@v4

  test-linux:
    name: pytest on linux
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python_version: ["3.8", "3.9", "3.10", "3.11", "3.12"]
    steps:
      - name: "Set up Python ${{ matrix.python_version }}"
        uses: actions/setup-python@v5
        with:
          python-version: "${{ matrix.python_version }}"
      - name: pip cache
        id: pip-cache
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ matrix.python_version }}
      - uses: actions/checkout@v4
      - name: install core_types plugin
        run: pip install -U git+https://github.com/DHARPA-project/kiara_plugin.core_types@develop
      - name: install tabular plugin
        run: pip install -U git+https://github.com/DHARPA-project/kiara_plugin.tabular@develop
      - name: install kiara
        run: pip install -U  .[dev_testing]
      - name: display installed kiara and module package versions
        run: pip list | grep kiara
      - name: test with pytest
        run: pytest --cov-report=xml --cov=kiara tests
      - name: Coveralls
        uses: coverallsapp/github-action@v2
        with:
          parallel: true
          flag-name: run ${{ join(matrix.*, ' - ') }}
          format: cobertura
          file: coverage.xml

  coverage:
    name: test coverage
    runs-on: ubuntu-latest
    needs:
      - test-linux
    steps:
      - name: Coveralls Finished
        uses: coverallsapp/github-action@v2
        with:
          parallel-finished: true

  mypy-linux:
    name: mypy check on linux
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python_version: ["3.8", "3.9", "3.10", "3.11", "3.12"]
    steps:
      - name: "Set up Python ${{ matrix.python_version }}"
        uses: actions/setup-python@v5
        with:
          python-version: "${{ matrix.python_version }}"
      - name: pip cache
        id: pip-cache
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ matrix.python_version }}
      - uses: actions/checkout@v4
      - name: install kiara
        run: pip install -U  .[dev_testing]
      - name: Test with mypy
        run: make mypy

  linting-linux:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: lint sourcecode
        uses: chartboost/ruff-action@v1

#  build-docs:
#    name: build documentation
#    runs-on: ubuntu-latest
#    needs:
#      - test-linux
#      - mypy-linux
#      - linting-linux
#    steps:
#      - name: Set up Python 3.10
#        uses: actions/setup-python@v4
#        with:
#          python-version: "3.10"
#      - name: pip cache
#        id: pip-cache
#        uses: actions/cache@v3
#        with:
#          path: ~/.cache/pip
#          key: ${{ runner.os }}-pip-${{ hashFiles('**/setup.*') }}
#      - uses: actions/checkout@v3
#        with:
#          fetch-depth: 0
#      - name: install kiara
#        run: pip install -U .[dev_documentation,doc]
#      - run: git config --global user.email "markus@frkl.io"
#      - run: git config --global user.name "Markus Binsteiner"
#      - name: create latest documentation
#        if: ${{ ( github.ref == 'refs/heads/develop') }}
#        run: FAIL_DOC_BUILD_ON_ERROR=true mike deploy --push latest && mike set-default --push latest
#      - name: extract tag name
#        run: echo "RELEASE_VERSION=${GITHUB_REF#refs/*/}" >> $GITHUB_ENV
#      - name: create stable documentation
#        if: ${{ startsWith(github.ref, 'refs/tags/') }}
#        run: FAIL_DOC_BUILD_ON_ERROR=true mike deploy --push --update-alias --title "v ${RELEASE_VERSION}" "${RELEASE_VERSION}" stable

  build_python_package:
    name: build python package
    runs-on: ubuntu-latest
    steps:
      - name: Set up Python 3.12
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
      - name: pip cache
        id: pip-cache
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-3.12
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: install pip
        run: pip install -U pip setuptools setuptools_scm build wheel
      - name: create packages
        run: python -m build
      - name: upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: build-dists
          path: dist/

  publish_python_package:
    name: publish python package
    if: github.event_name == 'push' && startsWith(github.ref, 'refs/tags')
    runs-on: ubuntu-latest
    needs:
      - test-linux
      - mypy-linux
      - linting-linux
      - build_conda_package
      - build_python_package
    permissions:
      id-token: write  # IMPORTANT: this permission is mandatory for trusted publishing
    steps:
      - name: Retrieve build distributions
        uses: actions/download-artifact@v4
        with:
          name: build-dists
          path: dist/
      - name: Publish release distributions to PyPI
        uses: pypa/gh-action-pypi-publish@release/v1

  build_conda_package:
    name: conda package build
    runs-on: ubuntu-latest
    steps:
      - name: "Set up Python 3.12"
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
      - name: pip cache
        id: pip-cache
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-3.12
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: install kiara_plugin.develop
        run: pip install -U git+https://github.com/DHARPA-project/kiara_plugin.develop@develop
      - name: build conda package
        run: kiara build conda pkg --channel dharpa --channel conda-forge --patch-data ci/conda/conda-pkg-patch.yaml --output-folder build-dir .
      - name: upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: conda-pkgs
          path: build-dir/

  release_conda_package:
    name: publish python package to anaconda
    if: github.event_name == 'push' && startsWith(github.ref, 'refs/tags')
    runs-on: ubuntu-latest
    needs:
      - test-linux
      - mypy-linux
      - linting-linux
      - build_python_package
      - build_conda_package
    steps:
      - name: "Set up Python 3.12"
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
      - name: pip cache
        id: pip-cache
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-3.12
      - name: install kiara_plugin.develop
        run: pip install -U git+https://github.com/DHARPA-project/kiara_plugin.develop@develop
      - name: Retrieve build distributions
        uses: actions/download-artifact@v4
        with:
          name: conda-pkgs
          path: build-dir/
      - name: release conda package
        run: kiara build conda publish --user dharpa --channel dharpa --token ${{ secrets.ANACONDA_PUSH_TOKEN }} build-dir


  merge_tag_to_main:
    name: merge current tag to main branch
    runs-on: ubuntu-latest
    if: ${{ startsWith(github.ref, 'refs/tags') }}
    needs:
      - test-linux
      - mypy-linux
      - linting-linux
      - build_conda_package
      - build_python_package
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0
    - run: git config --global user.email "markus@frkl.io"
    - run: git config --global user.name "Markus Binsteiner"
    - name: extract tag name
      run: echo "RELEASE_VERSION=${GITHUB_REF#refs/*/}" >> $GITHUB_ENV
    - name: checkout main branch
      run: git checkout main
    - name: merge tag
      run: git merge "${RELEASE_VERSION}"
    - name: push updated main branch
      run: git push https://${{ secrets.GITHUB_TOKEN }}@github.com/DHARPA-Project/kiara.git


# kiara\kiara\.github\workflows\build-windows.yaml
name: "windows tests for 'kiara'"
# This workflow is triggered on pushes to the repository.
on: [push]


jobs:
  test-windows:
    name: pytest on windows
    runs-on: windows-latest
    strategy:
      matrix:
        python_version: ["3.8", "3.9", "3.10", "3.11", "3.12"]
    steps:
      - name: "Set up Python ${{ matrix.python_version }}"
        uses: actions/setup-python@v5
        with:
          python-version: "${{ matrix.python_version }}"
      - name: pip cache
        id: pip-cache
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/setup.*') }}
      - uses: actions/checkout@v4
      - name: install core_types plugin
        run: pip install -U git+https://github.com/DHARPA-project/kiara_plugin.core_types@develop
      - name: install tabular types plugin
        run: pip install -U git+https://github.com/DHARPA-project/kiara_plugin.tabular@develop
      - name: install kiara
        run: pip install  --prefer-binary -U  .[dev_testing]
      - name: test with pytest
        run: py.test


# kiara\kiara\ci\conda\conda-pkg-patch.yaml
requirements:
  mmhash3: "mmh3"
  email-validator: "email-validator>=1.0.0"
  docstring-parser: docstring_parser

channels:
  - dharpa
  - conda-forge

entry_points:
  kiara: kiara.interfaces.cli:cli

host_requirements:
  - pip
  - python
  - setuptools
  - setuptools_scm
  - setuptools_scm_git_archive

test:
  imports:
    - kiara
    - kiara.interfaces.python_api
  source_files:
    - tests
  commands:
    - kiara module list
#    - kiara data list#
    - kiara operation list
#    - pytest tests#
#  requires:#
#    - pytest#


# kiara\kiara\ci\conda\Readme.md
This folder contains the patch files needed to build conda packages for the 'kiara' package and its dependencies that are not already hosted on conda-forge.

The command to build and publish the packages is:

```
kiara conda build-package --user dharpa --publish --patch-data ci/conda/{{ package_name }}/conda-pkg-patch.yaml {{ package_name }}
```


# kiara\kiara\ci\conda\airium\conda-pkg-patch.yaml
host_requirements:
  - pip
  - python
  - poetry


# kiara\kiara\ci\conda\bases\conda-pkg-patch.yaml
channels:
  - conda-forge
  - dharpa


# kiara\kiara\ci\conda\dag-cbor\conda-pkg-patch.yaml
channels:
  - conda-forge
  - dharpa


# kiara\kiara\ci\conda\email-validator\conda-pkg-patch.yaml


# kiara\kiara\ci\conda\fasteners\conda-pkg-patch.yaml


# kiara\kiara\ci\conda\jupyter-client\conda-pkg-patch.yaml
channels:
  - conda-forge
  - dharpa

host_requirements:
  - pip
  - python
  - hatchling


# kiara\kiara\ci\conda\jupyter-core\conda-pkg-patch.yaml
channels:
  - conda-forge
host_requirements:
  - pip
  - python
  - hatchling


# kiara\kiara\ci\conda\makkus.bases\conda-pkg-patch.yaml
channels:
  - conda-forge
  - dharpa


# kiara\kiara\ci\conda\makkus.dag-cbor\conda-pkg-patch.yaml
channels:
  - conda-forge
  - dharpa


# kiara\kiara\ci\conda\makkus.multiformats\multiformats.yaml
channels:
  - conda-forge
  - dharpa
requirements:
  makkus.multiformats-config: ""


# kiara\kiara\ci\conda\makkus.multiformats-config\multiformats-config.yaml
channels:
  - conda-forge
  - dharpa


# kiara\kiara\ci\conda\mknotebooks\conda-pkg-patch.yaml
channels:
  - conda-forge
  - dharpa


# kiara\kiara\ci\conda\multiformats\multiformats.yaml
channels:
  - conda-forge
  - dharpa
#requirements:
#  multiformats-config: ""


# kiara\kiara\ci\conda\multiformats-config\multiformats-config.yaml
channels:
  - conda-forge
  - dharpa


# kiara\kiara\ci\conda\pp-ez\conda-pkg-patch.yaml


# kiara\kiara\ci\conda\puremagic\conda-pkg-patch.yaml


# kiara\kiara\ci\conda\pydantic-to-typescript\conda-pkg-patch.yaml


# kiara\kiara\ci\conda\typing-validation\conda-pkg-patch.yaml


# kiara\kiara\dev\script.py
# -*- coding: utf-8 -*-
# %%


from kiara.utils.cli import terminal_print

step_read_files_in_folder = Step(
    "onboarding.folder.import", step_id="read_files_in_folder"
)


step_create_table_from_csvs_config = {
    "columns": ["id", "rel_path", "file_name", "content"]
}
step_create_table_from_csvs = Step(
    "table.from_file_bundle",
    module_config=step_create_table_from_csvs_config,
    step_id="create_table_from_csvs",
)


step_extract_date_from_file_name_config = {"module_type": "date.extract_from_string"}
step_extract_date_from_file_name = Step(
    "array.map",
    module_config=step_extract_date_from_file_name_config,
    step_id="extract_date_from_file_name",
)


step_extract_ref_from_file_name_config = {
    "module_type": "string.match_regex",
    "module_config": {
        "regex": "(\\w+\\d+)_\\d{4}-\\d{2}-\\d{2}_",
        "only_first_match": True,
    },
}
step_extract_ref_from_file_name = Step(
    "array.map",
    module_config=step_extract_ref_from_file_name_config,
    step_id="extract_ref_from_file_name",
)


step_lookup_publication_name_config = {
    "module_type": "string.replace",
    "module_config": {
        "replacement_map": {
            "sn85066408": "L\\'Italia",
            "2012271201": "Cronaca Sovversiva",
        }
    },
}
step_lookup_publication_name = Step(
    "array.map",
    module_config=step_lookup_publication_name_config,
    step_id="lookup_publication_name",
)


step_create_date_range_filter_config = {
    "module_type": "date.range_check",
    "input_name": "date",
}
step_create_date_range_filter = Step(
    "array.map",
    module_config=step_create_date_range_filter_config,
    step_id="create_date_range_filter",
)


step_merged_table = Step("table.merge", step_id="merged_table")


step_filtered_table = Step("table.filter.with_mask", step_id="filtered_table")


step_tokenize_text_corpus_config = {
    "module_type": "language.tokens.tokenize_text",
    "input_name": "text",
}
step_tokenize_text_corpus = Step(
    "array.map",
    module_config=step_tokenize_text_corpus_config,
    step_id="tokenize_text_corpus",
)


step_remove_stopwords = Step(
    "language.tokens.remove_stopwords", step_id="remove_stopwords"
)


step_lemmatize_corpus = Step(
    "language.lemmatize.tokens_array", step_id="lemmatize_corpus"
)


step_generate_lda = Step("language.lda.LDA", step_id="generate_lda")

step_create_table_from_csvs.input.files = step_read_files_in_folder.output.file_bundle


step_extract_date_from_file_name.input.array = (
    step_create_table_from_csvs.output.table.file_name
)


step_extract_ref_from_file_name.input.array = (
    step_create_table_from_csvs.output.table.file_name
)


step_lookup_publication_name.input.array = step_extract_ref_from_file_name.output.array


step_create_date_range_filter.input.array = (
    step_extract_date_from_file_name.output.array
)


step_merged_table.input.sources = [
    step_create_table_from_csvs.output.table,
    step_extract_date_from_file_name.output.array,
    step_extract_ref_from_file_name.output.array,
    step_lookup_publication_name.output.array,
]


step_filtered_table.input.table = step_merged_table.output.table

step_filtered_table.input.mask = step_create_date_range_filter.output.array


step_tokenize_text_corpus.input.array = step_filtered_table.output.table.content


step_remove_stopwords.input.token_lists = step_tokenize_text_corpus.output.array


step_lemmatize_corpus.input.tokens_array = step_remove_stopwords.output.token_list


step_generate_lda.input.tokens_array = step_lemmatize_corpus.output.tokens_array


workflow = step_generate_lda.workflow

step_read_files_in_folder.input.path = (
    "/home/markus/projects/dharpa/notebooks/TopicModelling/data_tm_workflow"
)
step_create_date_range_filter.input.earliest = "1919-01-01"
step_create_date_range_filter.input.latest = "2000-01-01"
step_remove_stopwords.input.languages = ["italian", "german"]
step_generate_lda.input.compute_coherence = True
workflow.process()
terminal_print(workflow)


# kiara\kiara\docs\index.md
--8<-- "../README.md"


# kiara\kiara\docs\SUMMARY.md
* [Home](index.md)
* [Development](development/)
* [Included components](included_components/)
* [API reference](reference/)
* [Design docs](design_docs/)


# kiara\kiara\docs\design_docs\index.md
# Design documents

This section contains desig-related documents for the *Kiara* project.

Most of those are not up-to-date, but they can help to understand certain design decisions, and why they were made.


# kiara\kiara\docs\design_docs\SUMMARY.md
- *
- */


# kiara\kiara\docs\design_docs\architecture\assumptions.md
# Assumptions & considerations

## Core assumptions

I consider the following assumptions a given. They are not fuelled by user stories, but are the 'minimal' requirements
that emerged after initially presenting the 'open questions', and in other discussions with Sean and the team. If any
of those assumptions are wrong, some of the conclusions below will have to be adjusted.

- our (only) target audience (for now) are digital historians (and maybe also other digital humanity researchers) who can't code themselves
- the most important outcome of our project is for our target audience to be able to execute workflows in order to explore, explain, transform or augment their data
- we want the creation of workflows to be as easy and frictionless as possible, although not at the expense of end-user usability
- we want our product to be used by all DH researchers around the word, independent of their affiliation(s)
q- collaboration/sharing of data is not a priority, most of our target audience are either individuals, sometimes small teams (sharing of results and sharing of workflows are different issues, and not included in this assumption)

## Considerations around adoption

One way to look at how to prioritize and implement some of our user stories is through the lens of ease-of-adoption:
which characteristics make our application more likely to be adopted, by a larger group of researchers?

Those ones are obvious (at least to me) -- in no particular order:

 - ease of workflow use
 - ease of file-management use
 - ease of installation (if there is one involved)
 - whether there is a login/account creation requirement
 - how well it integrates and plays with tools researchers already use day to day
 - provides relevant (to them) workflows
 - the cheaper to use the better (free/monthly cost/pay-per-usage)
 - stability / reliability
 - performance (most importantly on the compute side, but also UI)
 - how easy it is to create workflows, and what skills are necessary to do that (easier creation -> more workflows)
 - whether and how easy it will be to share, re-use and adapt workflows (different to sharing data)


# kiara\kiara\docs\design_docs\architecture\decisions.md
# Decisions

This page lists a few of the main decisions that were taken, what the considerations around them were, their impact, as well as why they were made.

## Supporting two sorts of modules: 'core', and 'pipeline' modules

When starting to write code, we didn't have yet many examples of modules and how specific/broad they would be in their utility.
I think we should have done a better job gathering those and coming up with a set of modules that would be sufficient for
our first 10 or so workflows before writing any code, but alas, we didn't. One way I saw to lower the risk of us implementing
us ourselves into a corner was to make our modules as flexible, re-usable and 'combinable' as possible. And the best way
I could think of to do that was to have a very simple interface for each module (each module has a defined set of input/output fields, and one main function to transform inputs into outputs), and to allow several modules to be combined into a 'new'
module that has those same characteristics/interface.

Advantages:
- easy to declaratively create re-usable modules with just json/yaml
- easy to re-use modules/other pipelines for UI-specific subtasks (like data previews/querying)
- in most cases, the higher-level backend code does not know about core- and pipeline- modules, since they can be treated the same

Disadvantages:
- the lower-level backend code needs to implement two different ways to assemble/create modules, depending on whether it's a core-module, or a pipeline


## Use of subclassing in general

Across *kiara*, I'm using subclassing and inheritance in some instances, esp. important base classes are [KiaraModule][kiara.module.KiaraModule] and [PipelineController][kiara.pipeline.controller.PipelineController]. I'm aware that this is considered bad practice in a lot of cases, and I have [read](https://www.sicpers.info/2018/03/why-inheritance-never-made-any-sense/) [my](https://python-patterns.guide/gang-of-four/composition-over-inheritance/) [share](https://python-patterns.guide/gang-of-four/composition-over-inheritance/#problem-the-subclass-explosion) of opinions and thoughts about the matter. In principle I agree, and I'm not 100% happy with every decision I made (or thought I had to made) in this area for *kiara*, but overall I decided to allow for some inheritance and class-based code sharing in the code, partly to speed up my implementation work, partly because I thought some of the disadvantages (like having to search base classes for some function definitions) are not as bad in a certain context than in others. I can totally see how others would disagree here, though, and there are a few things I would like to change/improve later on, if I find the time.

One of the main advantages I get out of using inheritance is being able to automatically discover subclasses of a base class. This is done for multiple of those, like:

- [KiaraModule][kiara.module.KiaraModule]
- [ValueTypeOrm][kiara.data.type.ValueTypeOrm]
- [MetadataModel][kiara.metadata.MetadataModel]

Using auto-discovery in a Python virtualenv removes the need for workflow/module developers to understand Python packaging and entry_points. I've written a [project template](https://github.com/DHARPA-Project/kiara_modules.project_template) that sets up all the basics, and developers focus on creating new classes (basically plugins), with no extra registration work to be done. I hope this will aid adoption. And that I've managed to design those base classes well enough so that they are easy to use and understand, so that some of the main drawbacks of subclassing won't matter all that much.


## Requiring to subclass an abstract base class when creating a module

The main class that uses a subclassing-strategy is [KiaraModule][kiara.module.KiaraModule]. At it's heart, it's
basically just a wrapper around a pure function, with some utility methods describing it's input and output. One reason
I decided to not just create a decorator that wraps any function was the need to be able to describe the input the
function takes, and the output it produces in a stricter way than would have been possible with just type hints.
Another reason is that this way it is possible to add configuration to a module object, which should make module
code much more flexible and re-usable, and developers do not have to implement separate modules for just slightly different
use-cases.

This design decision does not prevent to allow for more 'loose' implementations of a module, like the above mentioned
function with a decorator. Those would be dynamically converted into a ``KiaraModule`` subclass/object, with potential
downsides of not being able to version control it properly (or as easliy). The point is, though, that the default
way of doing things will give us the best guarantees (and metadata).

Advantages:
- relatively easy to manage 'plugin-like' architecture, discovery of modules
- being able to describe module input/output fields in detail
- module versioning: by requiring the subclassing of a base class, and also having to add modules as entry_points, it will be possible describe exactly which version of the module was used in a workflow (as well as which version of the base class)

Disadvantages:
- more abstraction layers than strictly necessary
- other, usual disadvantages associated with subclassing/inheritance


## Separating data from the Python objects that describe them / Data registry

TBD

Advantages:
- efficiency, option to save on memory and IO
- (hopefully) decrease of complexity for non trivial scenarios like multi-process or remote job execution

Disadvantages:
- extra level of abstraction
- increase in complexity (at least for simple use-cases)


# kiara\kiara\docs\design_docs\architecture\index.md
# Architecture documents

Docs outlining some of the thinking behind the architecture of *kiara*.


# kiara\kiara\docs\design_docs\architecture\metadata.md
## Metadata

Metadata is more important in research than in other fields. Metadata can be used to, among other things, track provenance of data,
describe authorship, time of creation, location of creation, describing the 'shape' of data (schemas, etc.).

In some cases it's not easy to determine what's data and what's metadata. Sometimes metadata becomes data ("One persons metadata...").
Handling metadata is difficult, and it regularly gets lost somewhere in the process. Creating metadata in the first place can be
very time-consuming, I would wager that is more true in the digital humanities than in the harder sciences.

With the growing popularity of the open data movement, people are getting more aware of the importance of metadata, and
there is a growing infrastructure and services around all of this (DOIs, RDF, 'linked data', Dublin core, ...). None
of it is easy or intuitive to use, but I guess that's just the nature of the beast.

I think it is safe to say that whatever we come up with has to be able to create and handle metadata in some way or form,
and personally, I think we should 'bake' metadata handling in from the beginning. Looking at the user-stories it's quite
clear that this an important topic. How exactly that will look, I think there is some leeway, but all architecture proposals
should at least include some indication on how this would be handled.

### Schema information

One important piece of metadata is often schema information: what exactly is the shape of the data, how can I read it?
In some cases this can be inferred from the data easily, sometimes it's even obvious. But often that is not the case at all,
which makes things like creating generic data exploration tools very hard, if not impossible.
We would have, if we choose to create and attach it, all that information available, always, which would mean it would be easy
to create generic, peripheral tools like a generic data explorer. It will, of course, also make it easier to re-use such data in other workflows,
because users would not have to explicitly specify what their data is; we could infer that from the attached schema.

### Workflow metadata

One thing that is specific to our application is that we have full control over every part of the data-flow. So, we can
attach metadata of all inputs and previous steps to each result (or intermediate result) along the way. Which is quite an
unique opportunity; this is often not available at all, or has to be done manually by the researcher.

There is a lot that can be done with such annotated (result-)data. For example, each data set can include pointers to all
the original data that was involved in creating it (or it could even include that data itself), as well as a description
of all the transformation steps it went through. This means that one could potentially create a full visual representation of
what happened to the data since it was created, just by looking at the attached metadata. This is usually impossible, because
there is never a sort of 'unbroken cold-chain' of metadata available. Of course, this would also help with reproducability and
related issues.

This possibility is something I'm particularly excited about, even though it does not directly appear in any of our user
stories (so would not be a core requirement). But it's one of the things I would have liked to have available often in the past.


# kiara\kiara\docs\design_docs\architecture\SUMMARY.md
- *
- */


# kiara\kiara\docs\design_docs\architecture\data\index.md
From looking at the user stories, and after listening to the interviews Lorella conducted and also considering my own
personal experience in eResearch, I think its save to say that the central topic we are dealing with is data. Without data,
none of the other topics (workflows, visualisation, metadata...) would even exist. Because of its central nature I want to lay out the different forms it comes in, and which characteristics of it are
important in our context.

## What's data?

Data is created from sources. Sources come in different forms (analog, digital) and can be anything from handwritten
documents in an archive to a twitter feed. Photos, cave-paintings, what have you. I'm not websters dictionary, but I think
one usable working definition of data could be a 'materialized source', in our context 'materialized source in digital form'.
From here on out I'll assume we are talking about 'digital' data when I mention data.

One thing I'll leave out in this discussion is what is usually called 'dirty data' in data engineering, although it is
an important topic. Most of the issues there map fairly well to the structured/unstructured thing below. There are a few
differences, but in the interest of clarity let's ignore those for now...

## Structured data / Data transformations

Important for us is that data can come in two different formats: unstructured, and, who'd have guessed... structured. The same
piece of data can be theoretically expressed in structured as well as unstructured form: the meaning to a researcher would
be 100% the same, but the ways to handle, digest and operate with the data can differ, and in most scenarios adding structure
opens up possibilities to work with the data that weren't there before. In my head I call those two forms 'useless', and
'useful' data, but researcher usually get a bit agitated when I do, so I have learned to not do that in public anymore.

For researchers, the most (and arguably only) important feature of 'structure' is that it enables them to
do *more* with the data they already possess. By means of computation. I think it's fair to say that only structured data
can be used in a meaningful way in a computational context. With the exception that unstructured data is useful input to
create structured data.

One more thing to mention is that the line between structured and un-structured is sometimes hard to draw,
and can depend entirely on context. "One persons structured data is another persons unstructured data.", something like that.
In addition, in some instances unstructured data can be converted to structured data trivially, meaning without much effort
or any user-interaction. I'd argue we can consider those sorts of datasets basically 'structured'.

### Example

Lets use a simple example to illustrate all that: *a digital image of a document*.

Depending on what you are interested in, such an image might already be structured data. For example it could contain geo-tags, and a
timestamp, which are both digitally readable. If you want to visualize on a map where a document is from, you can do that instantly.
Structured data, yay!

Similarly, if you are interested in the color of the paper of the document (ok, I'm stretching my argument here as this seems fairly
unlikely, but this is really just to illustrate...), you might get the color histogram of the image (which is trivial to extract,
but needs some batch-computation), and for your purposes you would also consider the image file structured data.

Now, if you are interested in the text content of the document, things get more interesting. You will have to jump
through some hoops, and feed the image file to an OCR pipeline that will spit out a text file for example. The data
itself would still be the same, but now computers can access not only some probably irrelevant metadata, but also the text content,
which, in almost all cases, is where the 'soul' of the data is.

It could be argued that 'just' a text file is not actually structured. I'd say that groups of ascii-characters that
can be found in english-language dictionaries, separated by whitespaces and new-lines can be considered a structure,
even if only barely. The new format certainly allows the researcher to interact with the data in other ways (e.g. full-text search).

We can go further, and might be interested in characteristics of the text content (language, topics, etc.). This is where
the actual magic happens, everything before that is just rote data preparation: turning unstructured (or 'other-ly' structured)
data into (meaningful) structured data... On a technical level, those two parts (preparation/computation) of a research workflow might look (or be)
the same, but I think there is a difference worth keeping in mind. If I don't forget I'll elaborate on that later.

## 'Big-ish' data

I'm not talking about real 'Big data'-big data here, just largish files, or lots of them, or both. I don't think we'll encounter many use-cases where we have to move
or analyze terabytes of data, but I wouldn't be surprised if we come across a few gigabytes worth of it every now and then.

There are a few things we have to be prepared for, in those cases:

- transferring that sort of data is not trivial (esp. from home internet connections with limited upload bandwidth) -- and we will most likely have to be able to offer some sort of resumable-upload (and download) option (in case of a hosted solution)
- if we offer a hosted service, we will have to take into account and plan for this, so we don't run out of storage space (we might have to impose quotas, for example)
- computation-wise, we need to make sure we are prepared for large datasets and handle that in a smart way (if we load a huge dataset into memory, it can crash the machine where that is done)
- similarly, when we feed large datasets into a pipeline, we might not be able to just duplicate and edit the dataset like we could do for small amounts of data (too expensive, storage-wise) -- so we might need to have different strategies in place on how to execute a workflow, depending on file sizes (for example some sort of copy-on-write)


# kiara\kiara\docs\design_docs\architecture\data\persistence.md
# Data persistence

This is a document to describe my plans for storing data (and metadata) in *kiara*. (Almost) nothing I describe here is inmplemented yet, so it only reflects my current thinking. I think the overall strategy will hold, but there might be changes here and there.

## The problem

*kiara*s main functionality centers around transforming input data sets to output data sets. Those outputs need to be stored, to be of any use later on. Obviously. When deciding how to do this, we must take into account concerns about performance, disk- and memory-usage, data versioning, which metadata to attach, in what way, how to deal with metadata schemas (and versioning of both), etc.

## The solution

Well, solution. This is my current thinking of how to tackle the problem in a way that takes into account all of the aspects described above, while still being flexible enough to hopefully be able to incorporate solutions for future unforsseen issues.

I am having trouble coming up with a good structure for this document, so I think I'll just try to tell the story from the point of view of data. Starting from a state where data exists outside of *kiara*, to when it is in a state to be ready to be published. As with everything I'm writing here as an explanation of generic and abstract concepts, some of the technical details I'm describing might be simplified to the point of being incorrect...

## The 7 stages of data

One thing I'd like to say before I start to describe those stages: the transformation of a dataset, from one stage to the next, **always** **always** **always** happens by piping the dataset through a *kiara* module. At **absolutely** **no** point is this done without *kiara*s involvement and knowledge. The dataset is used as input for a module, and the result (technically a new dataset) is a representation of the dataset in its next stage. This is important to keep in mind, as it is crucial for us so we can track data lineage. I'll write more on the specifics of this below, where it makes more sense.

### 1) Unmanaged

At the beginning, there was csv. Whether I like it or not, csv is the most predominant form data comes in. Csv is bad in a lot of ways, but in my mind the worst thing about it is that it is schema-less. True, in some cases you have a header-line, which gives you column-names, but that's not a requirement. Also, in a lot of cases you can auto-determine the type of each column, and luckily libraries like Pandas or Apache Arrow solved that problem for us so we don't have to do it ourselves every time. But those auto-parsers are not fool-proof, and you end up with integers where you wanted floats (or doubles), or integers where you wanted strings, or vice versa.

In some cases we get data in a form that includes at least a semi-schema. Like a sqlite database file (which is more 'strongly' typed). But it's a lucky day when we get data that contains metadata about authorship, how and when it was created, from what sources, etc.

### 2) Onboarded

This is the first thing we need to do to unmanaged data: we need to 'onboard' it, so *kiara* knows the data exists, and what exact bytes it consists of. This last thing is very important: we have to be able to make sure the data we are talking about is not being changed externally, a lot of things in *kiara*s approach to data depend on this.

Practically, in most cases this means *kiara* will copy one or several files into a protected area that no other application can/should access. That way we always have a reference version of the dataset (the bytes) we are talking about.

One thing *kiara* does at this stage is give the dataset a uniuqe id, which can be used to reference it later (by users, or other objects/functions). Another thing is to collect some basic metadata: when the file/folder was imported, from what original path, what the filenames are, mime-type, size of files, original file attributes (creation data, permissions, etc.). This can all be captured automatically. We can also record who it was that imported the dataset, if we have some app-global configuration about the current user, like a full name and email-address. Note, that this might or might not be the person who created the dataset.

So, at this stage all we did was copy a file(set) into a protected area to sort of 'freeze' it, and augment it with very basic metadata. We don't know anything about the nature of the dataset yet, all we know is the bytes the datasets consists of. It is important to point out that we would not have to store those chunks of bytes as files again, using the same structure as the original set of files. The dataset ceased to be 'files' here for us, we are only interested in the chunks of bytes (and their meaning) from here on out. We could store the data in an object store, zipped, tarred and feathered (pun intended). Or as byte-stream directly on block storage, if we were crazy enough.

A side-note that makes things a bit more complicated, but it is probably necessary to address potential concerns: no, we don't actually need to copy the files, and can leave them in place and only generate the metadata and id for them. This might be necessary in cases where the source data is very big (photos, movies, audio-files, other large datasets). I don't think we need to figure out how exactly we deal with this scenario right now, but it basically comes down to making the user aware of what is happening, and what the implications are if the source data is changed externally (inconsistent metadata and potential incorrect result data-sets further down the line). There are strategies to help prevent some of those potential issues (checksums, for example), but overall we have to acknowledge that working with large-sized datasets is always a challenge, and in some cases we might just have to say: "sorry, this is too big for us right now".

### 3) Augmented with more (basic) metadata

To recapitulate: at this stage we have data (chunks of bytes -- not files!!! hit yourself over the head twice with something semi-heavy if you are still think in terms of files from here on out!) in a protected area, some very basic metadata, and an id for each dataset. We might or might not have authorship metadata (arguably one of the most important pieces of metadata), depending on whether who 'onboarded' the dataset actually created it.

So, as a first step and following good practice, at this stage we should try to get the user to tell us about authorship and other core metadata about our dataset (licensing, copyright, ...). I don't think we can make this step mandatory, in practice, but we should push fairly hard, even if that means a slight decrease in user experience. It is very important information to have...

So, one thing we could do was to have a checkbox that lets the user confirm: I created the data (in which case we can just copy the 'imported-by' field).



### 3) Typed

Chunks of bytes are not very useful by itself. They need to be interpreted to be of use. This means: determining in some way what the structure of the chunks of bytes is, and then applying common conventions for that particular structure (aka data type/format) when reading the chunks of bytes. Therefore 'interpreting' the chunks of bytes. This is a very obvious thing that happens all the time we use computers, but I think it makes sense to point it out here, because usually this is transparent to the user when they click an 'Open file' button in an application, and even some developers are ignorant to the underlying concept (and can afford to be, since they usually work several abstraction layers above where that is happening).

To encapsulate this concept, we will create a 'data type' for each important group of datasets that share some important characteristics. Examples for very simple data types are strings, integers, booleans. I'll ignore those, because those are trivial to use, and that triviality actually makes it harder to explain the concept I'm talking about. More relevant data types are: 'table', 'network graph', 'text corpus', 'photo collection'. Every data type inherently contains a description of, well, the 'type' of data represented by it, and, with that, information about how a user or code caqn access the actual data, and/or some of its properties.

From here on out, I'll focus on tabular data in some form or other, since I expect that this will be one of our most important (base-) data types. I expect the reader to 'translate' whatever I'm saying below to other types, and extrapolate the practical differences.

So, to explain this step I decided to look at three different use-cases (mostly because we use them in 2 of our example workflows, so people should be familiar with them):

- a csv file with tabular data
- an imported folder of text files (a corpus)
- two imported csv files containing edge and node information to form a network graph

#### Example: tabular data

This is the simplest case, and very common: we have a csv file, and need to have some sort of tabular data structure that we can use to query and analyze the data contained in it.

Let's assume we have onboarded a csv file using *kiara*, so we have a dataset id that we use to point to it. Technically, this dataset is already 'typed': it has the type 'file'. This is not a very useful type, all it allows can tell us is a file name (which in a way is metadata), and the file content. We can ask *kiara* to interpret the content of this file as table, though, because we know it must be one. This means we 'overlay' a different, more specific data type on top of the same data.

Under the hood, *kiara* will use the Apache Arrow [``read_csv``](TODO) helper method, which is very smart and fast, and it can create an Arrow Table object out of a csv file. It can figure out file encoding, column names (if present), column types, seperator characters. This detection is not fool-proof, but should work good enough in practice that we don't need to worry about it here. What really happens here is that the ``read_csv`` method is not just reading our data, but also, at the same time, is adding some important metadata to our dataset. Pandas can do the same with its csv import method. Even though this adding of metadata is more or less transparent to the user -- so they are not really aware of it -- it happens, and it is a very important thing that must happen to make our dataset useful. In our application, we might or might not want to ask users whether the extracted column names and types are correct, but this is a UI-specific implementation detail.

So, considering all this, the important point here is that at this stage we have actual 'table' data, there is no need for the original csv file anymore (except as a reference for data lineage purposes). Our dataset is now of the data type 'table'. Which means we have an assurance that we can query it for table metadata properties (number of rows, number and name of columns, column types, size in bytes, etc.). And we can apply functions against it that are commonly applied against tabular data (sql queries, filters, etc.). That means, a 'data type' is really just a convention, a marker, that tells us how the bytes are organized for a particular set of bytes, and how to interact with it. This is all users and 3rd party-code needs to worry about. Implementation details about how this data is stored or loaded are irrelevant on this level of abstraction. This reduces complexity for *kiara*s external facing API, while, of course, introducing some extra complexity internally.


#### Example: text corpus

The source data comes as a folder of files, each file contains (just) text (not structured like json, csv, etc.). When we do the 'onboarding' step for this data, all we do is copy the files verbatim into a new location. There could be some metadata implicit in the relative paths of each file (e.g. languages -- files for the same language live in a subfolder named after the language), and there can also be some metadata in the file names. We preserve all that metadata by copying the folder one-to-one, without changing anything. But it is important to note that this metadata, as of yet, is still uncaptured.

The 'soul' of this dataset (meaning: the properties of the dataset we are interested in and we want to use in our investigation, and which will hopefully answer our research question) is in the content of each text file (aka the unicode encoded chunks of bytes). It is important to say again: at this stage the dataset ceased to be a set of files! It is a dataset within *kiara* that has an id (a single one! not one for every text!), and it has a basic set of metadata fields (the ones we could collect automatically). Yes, the dataset is backed by a set of files in the *kiara* data store, but that is an implementation detail nobody needs to know about, and I think we should try hard to hide from users. If you haven't noticed so far: I strongly believe the file metaphor is a distraction, and not necessary for us, except when import/export is concerned.

Anyway, *kiara* does not know much about the dataset at this stage. To be of actual use, we need to interpret the data. In this case, we know we want to interpret the data as a text corpus.

The most basic shape we can imagine a text corpus to look like is a list of strings (an array, or a single-column table). For making it easier to work with the text corpus in the future, let's make up a convention to save in tabular form, and the column containing the text items is always named ``text_content``. If we use, for example Apache Arrow to store that table, it makes the stored chunks of data much smaller (in comparison to text files), and it also makes the whole thing easier (at least faster) to query. It also allows us to easily attach more (meta-)data to the dataset.

!!! Note
    The distinction between data and metadata becomes a bit blurry here. In a lot of cases, when I say metadata, it is metadata from the point of view of the research proess, not metadata for *kiara*. I don't know how to make it clear which I'm talking about in each case without making this whole thing even more unreadable as it already is, so I will just have to ask you to figure it out yourself, in each case :-)

Because we didn't lose any of the implied metadata when onboarding our folder of text files, it would be a shame if we wouldn't actually capture it. In this case, let's assume we didn't have any subfolders (so no metadata in their name), but our files are named in a special way:

```
[publication_id]_[date_of_publishing]_[other_stuff_we_are_not_interested_in]
```

!!! Note
     The information about the format is important (in a way it is also an input) and we need to retrieve it somehow. This is a real problem that doesn't look like a big problem. But it is, for us. I'll ignore this here, because it would complicate things too much and is only of tangential relevance.

This means, we can extract the publication id and the date of publishing with a simple regular expression, and we can add a column for each one to our table that so far only contains the actual text for each item. The publication id will be of type string (even though some of the ids might be integers -- we don't care), and the publication date will be a time format. Now we have a table with 3 columns, and we can already filter the texts by date easily, which is pretty useful! We wouldn't, strictly speaking those two additional columns to have a dataset of type 'text corpus' but it's much more useful that way. As a general rule: if we have metadata like that, it should be extracted and attached to the data in this stage. It's cheap to do in a lot of cases, and we never know when it will be needed later.

What we have at this stage is data that has the attributes of a table (columns with name and type info, as well as rows representing an item and it's metadata). This is basically the definition of our 'text corpus' data type: something that allows us to access text content items (the actual important data) using a mandatory column named ``text_content``, and that has zero to N metadata properties for each of those text items. In addition, we can access other metadata that is inherited from the base type (table): number of rows, size in bytes, etc, as well as its lineage (via a reference to the original onboarded dataset).
Internally, we'll store this data type as an Arrow table, but again, this is an implementation detail, and neither user nor frontend needs to know about this (exceptions apply, of course, but lets not get bogged down by those just now -- none of them are deal-breakers, as far as I can see).

#### Example: network graph data

Similar to the text corpus case above, let's think about what a basic definition of a network graph data type would look like. It would have to include a list of nodes, and a list of edges (that tell us how those nodes are connected). Actually, the list of nodes is implied in the list of edges, so we don't need to provide that if we don't feel like it (although, that doesn't apply if we have nodes that are not part of any edge). In addition, both nodes and edges can have attributes, but those are optional. So, our network graph data type would, at a minimum, need to be able to give us this information about all this via its interface. [networkx](TODO) is one of the most used Python libaries in this space, so let's decide that internally, for us, a network graph is represented as an object of the  [Graph](TODO) class, or one of its subclasses.

This class will give us a lot of useful methods and properties to access and query, one problem left is: how do we create an object of this class in a way that fits with our overall strategy? We can't save and load a networkx object directly ([pickling](TODO) would be a bad idea for several reasons), so we need to create (and re-create) it via some other way.

For this, lets look at the constructor arguments of this class, as well as what sort of data types we have available that we can use to feed those arguments. [One option](TODO) apparently is to use a list of edges contained in a Pandas dataframe as input, along with a name of columns representing the names of source and target column name, something like:

```
        graph: nx.DiGraph = nx.from_pandas_edgelist(
            pandas_table,
            source_column,
            target_column,
        )
```

This could work for us: as in the other example, we can use a table as the 'backing' data type for our graph object. Considering a graph without any node attributes, we can have a table with a minimum of two columns, and via a convention that we just made up, we say that the source column should be called ``edge_source``, and the target column ```edge_target```. We wrap all this in an Arrow table again, and save it as such. And later load it again, assuming the same convention (which, basically, saves us from asking for 2 column names every time). If our graph also includes node attributes, all we do is extend the implementation of our network graph data type to create a second table with a required column ``node_id``, and one or several more columns that hold node attributes, similar to the metadata in our 'text corpus' example from above.


### 4) Transformed

With all that out of the way, we can finally do something interesting with the data. Everything up to this point was more or less housekeeping: importing, tagging, marking, organizing datasets. We still are operating on the same actual data as was contained in the original files (whatever type they were). But we now know exactly what we can do with it without having to ask questions.

Using the 3 example from above, we now know we have 3 datasets: one table, one text corpus (which is also a table, but a more specific one), and a network graph. And each of those datasets also comes with metadata, and we know what metadata files are available for what data types, and what the metadata means in each context.

A first thing we can do is automatically matching datasets to available workflows: we know what input types a workflow takes (that is included in each workflow metadata). So all we need to do is check the input types of each available workflow against the type of a dataset. This works even with different specificity: give me all workflows that take as input a generic graph. Or: give me all workflows that take a directed graph as input (this is information that is included in the metadata of each network graph dataset).


# kiara\kiara\docs\design_docs\architecture\workflows\index.md
If we accept the premise that in the computational context we are really only interested in structured data, it follows
that there must be also 'things' that do stuff to our structured data. Let's call those things 'workflows'.

## Definition

I will concede that 'doing stuff to data' although entirely accurate is probably not the most useful of definitions.
Not Websters, all right? Well, how about:

    "A workflow is a tool to transform data into more structured data."

"more" can be read in one or all of those ways:

 - 'more data' -- we'll create what can be considered 'new' data out of the existing set
 - 'better structured' -- improve (and replace) the current structure (fix errors, etc.)
 - 'more structure' -- augment existing data with additional structure

In our context, workflows can also have secondary outcomes:

 - present data in different, more intuitive ways (e.g. visualisations), which researchers can use to get different ideas about the data, or new research questions
 - convert structured data into equivalent structured data, just a different format (e.g csv to Excel spreadsheet)
 - ... (I'm sure there's more, just can't think of anything important right now)

## Deconstructing a workflow

I've written more about it [here](https://github.com/DHARPA-Project/architecture-documents/blob/master/workflow-modularity/workflow-modularity.ipynb), but
conceptually, every data workflow is a collection of interconnected modules, where outputs of some modules are connected
to inputs of some other modules. The resulting network graph of modules defines a workflow. That's even the case for Jupyter
notebooks (which are really just fancy Python/R/Julia scripts); if you squint a bit you can see it:
the modules are functions that you call with some inputs, and you use the outputs of those functions (stored in variables) as inputs to
other functions. Move along, nothing to see here: this is really just how (most) programs work.

As I see it, there are three main differences to programs that are written in 'normal' computer engineering:

- the complexity of the resulting interconnected 'network graph' (the interconnection of functions) is usually lower
- it's a tad easier (or at least possible) to define, separate and re-use the building blocks needed in a majority of workflows (an example would be Numpy or the Pandas libraries, which are basically implementations of abstract problems that crop up often in this domain)
- it is possible to create workflows entirely out of modules that were previously created, with no or almost no other customization (normally, that customization would be very prominent in a program) -- often-times only some 'glue' code is needed

This means that data engineering workflows could be considered relatively simple script-like applications, where advanced
concepts like Object-Oriented-Design, Encapsulation, DRY, YAGNI, ... are not necessary or relevant (in most cases they wouldn't
hurt though).

## Data engineering

This way of looking at workflows is nothing new, there are quite a few tools and projects in the data engineering space
which deal with workflows in one level of abstraction or another.

As I'll point out below, the main difference to what we try to implement is that we'll add an element of 'interactivity'.
But I believe we can still learn a whole lot by looking at some aspects of those other tools.
I encourage everyone remotely interested to look up some of those projects, and maybe not read the whole documentation,
but at least the 'Why-we-created-yet-another-data-orchestrator', 'Why-we-are-better-than-comparable-projects' as well as
'What-we-learned'-type documentation pages you come across. 'I-tried-project-XXX-and-it-is-crap'-blog posts
as well as hackernews comment-threads related to those projects are usually also interesting. The '/r/dataengineering' and
'/r/datascience' sub-reddits are ok. But they are on Reddit, so, signal-to-noise is a bit, well..

Among others, interesting projects include:

- [dagster](https://github.com/dagster-io/dagster)
- [prefect](https://www.prefect.io/)
- [airflow](https://airflow.apache.org/)
- [luigi](https://github.com/spotify/luigi)

- also relevant, but less data-engineering-y: Node-RED, Apache NiFi, IFTTT, Zapier, Huginn, ...

## The 'workflow lifecycle'

One thing that I think is also worth considering is the different stages in the lifecycle of a workflow. For illustration,
I'll describe how each of those stages relates to the way data science is currently done with Jupyter, which is probably the most used tool
in this space at the moment.

### Workflow creation

This is the act of designing and implementing a new workflow transformed into one or a set of defined outcomes (which can be new data, or just a visualization, doesn't matter).
The actual creation of the workflow is similar to developing a script or application, and offers some freedom on how to implement it (e.g. which supporting
libraries to choose, whether and which defaults to set, ...).

In the Jupyter-case, this would be the iterative development of a Jupyter notebook, with one cell added after the other. One thing that is different for us
is that we will have a much stricter definition of the desired outcome of our workflow, whereas the creation of a Jupyter notebook is typically way more open-ended,
and a researcher would easily be able to 'follow some leads' they come across while working on the whole thing. This is a very important distinction that pays to
keep in mind, and I can't emphasize this enough: the workflows we are dealing with are a lot more 'static' than typical Jupyter notebooks, because we have decided in
advance which ones to implement, and how to implement them. There is not much we can do about this, and it's a trade-off with very little room to negotiate. This
has a few important implications on how our product is different from how data science is done by Jupyter users currently. I will probably mention this again
and again, because it is not intuitive at first, but has a big impact on how we view what we are building!

As per our core assumptions, end-users won't create new workflows, this is done by a group with a yet-to-be-determined 'special' skill set.

### Workflow execution

This is when a 'finished' workflow gets run, with a set of inputs that are chosen by the user. The schema/type of those inputs is a requirement
that has to be considered by the user. It's possible that a workflow allows for inputs to be in multiple formats, to make the users life easier (e.g. allow both '.csv' as well as '.json' formats),
but that also has to be documented and communicated to users. It is not possible to add elements to a workflow, and make it do different things
than it was designed to do. Our workflows are static, they never change (except in an 'iterative-development' sense where we release new versions)!

Compare that to a researcher who created their own Jupyter notebook: they will have run the workflow itself countless times by then, while developing it.
The execution phase is really only that last run that achieves the desired outcome, and which will 'fill' the notebook output cells with
the final results. That notebook state is likely to be attached to a publication. Often the data is 'hardcoded' into the notebook itself (for example
by adding the data itself in the git repo, and using a relative path to point to it in a notebook).
It is also possible, although not as common (as far as I've seen -- I might be wrong here) that researchers spend a bit more time on the notebook and
make the inputs easier to change, in order to be able to execute it with different parameters, quickly. This is more like what we will end up with,
although I'd argue that the underlying workflow is still much easier to change, fix, and adapt than will be the case with our application.

One difference between workflow creation and execution is that the creation part is more common for 'data scientists', and the execution part is a bigger
concern for 'data engineers' (both do both, of course). I think, our specific problem sits more in the data engineering than data science space (because
our main products are 'fixed'/'static' workflows), which is why I tend to look more for the tools used in that domain (data orchestrators, ...) than in the other
(Jupyter, ..) when I look for guidance.


### Workflow publication

Once a workflow is run with a set of inputs that yield a meaningful outcome for a researcher, it can be attached to a publication in some way.
This has one main purpose: to document and explain the research methodologies that were used, on a different level than 'just' plain language.

There is a long-term, idealistic goal of being able to replicate results, but the general sentiment is that it is unrealistic to attempt that at
this stage. It doesn't hurt to consider it a sort of 'guiding light', though.

It is getting more and more common for researchers to attach Jupyter notebooks to a publication. Jupyter notebooks are a decent fit for this
purpose, because the contain plain-text documentation, the actual code, as well as the output(s) of the code in a single file, that has a
predictable, well specified format (json, along with a required document schema). As our colleagues at the DHJ project have discovered, it's
not a perfect fit, but it can be bent to serve as the basis for a formal, digital publication.

In our case, it is my understanding that we would like to have an artefact like this too, and even though it's not one of the 'core' requirements,
it would be a very nice thing to have. One strong option is for us to re-use Jupyter notebooks for that. Depending on how we implement our
solution, we might already have one as our core element that 'holds' a workflow, in which case this is a non-issue.
Or, if that is not the case, we could 'render' a notebook from the metadata we have available, which should also not be too hard to do since the target
(the notebook) is well spec'ed. If that's the case, there is one thing I'd like to investigate before we commit though: what characteristics exactly are the
ones that make notebooks a good choice for that, and which one are detrimental? As I've mentioned, the DHJ project uses notebooks as the base
for creating article-(web)pages, and they came across some issues along the way. So I wonder: is there a better way to achieve the 'document and
explain research methodologies' goal than by using a Jupyter notebook? How would that look in a perfect world? How much effort would be involved?


## Interactivity / Long(-ish) running computations

One component that is different in our scenario to other implementations is the requirement for interactivity. In data-engineering,
this is never an issue, you describe your pipeline, then you or someone else uses that with a set of inputs, and off it goes,
without any further interaction. *Plomp*, notification, results, rinse, repeat.

For us that will be different, because we are creating a graphical user interface that reflects the workflow, and its state.
By definition, graphical user interfaces are interactive, and when a user triggers an action, they expect that to kick off
some instant response in the UI (maybe the change in a visualization, or a progress indicator, whatever).

### Computationally trivial/non-trivial

One main difficulty will be to find a good visual way to express what is happening to the user, ideally in the same way
for 2 different scenarios:

- computations that are computationally trivial, and will return a result back in a few seconds at most
- computations that take longer

In our workflows, I can see a few different ways those interactions can play out, depending on the characteristics of any particular workflow.

So, in the case where a user 'uploads' data or changes a setting:

   - *if the whole workflow is trivial, computationally*:
     - this triggers the whole workflow to execute and return with a new state/result immediately, and the output elements reflect the new state without any noticable delay

   - *if only some (or no) components of the workflows are trivial, computationally*:
     - this triggers the execution of only parts of the workflow immediately (from the point of user input to the next non-trivial step of the workflow).
     - all computationally non-trivial parts of the workflow will have some sort of "Process" button that users have to click manually to kick off those parts of the workflow. Otherwise the UI would be locked for an undefined amount of time after every user input -- which would result in a very bad UX).
     - alternatively, workflows with computationally non-trivial parts could have one 'global' "Process" button, which would trigger the execution of the whole workflow with all current inputs/settings.

There will be also inputs that don't directly kick off any processing (like for example control buttons in a visualisation). I
think we can ignore those for now, because this is what UIs usually do, and this does not present a difficulty in terms of
the overall UI/UX (just like the 'computationally trivial' workflow scenario).

### UI representations for the current workflow state

#### tldr;

In some cases it will be impossible for users to use a workflow fully interactively, because one or all workflow steps
will take too much time, which means the interactive session has to be interrupted. In those cases (depending on our setup
and other circumstances) we might need to include a 'job-management'/'queue' component to our application, which matches
running/completed jobs to users and 'sessions'/'experiments' (for lack of a better word).
We need to find a visual metaphors for workflows and workflow steps to make that intuitive, ideally in a way so that those scenarios are not
handled too differently in comparison to how our 100%-interactive workflows are used and executed.
In addition, we have to make sure our backend can deal with all the scenarios we want to support.

#### Details, skip if you want

I'll include some suggestions on how all this could look visually, but those are in no way to be taken as gospel. Just
the most obvious (to me) visual elements to use, which I hope will make it easier to get my point across.
It's probably obvious that the important cases we have to care about are the ones where there is non-trivial computation.
I think we can roughly divide them into 4 categories:

 - *execution time of a few seconds*:
     - in this case a 'spinning-wheel'-progress indidcator is probably enough
     - backend-wise, we (probably) don't have to worry (although, it's not a given this will not crash a hosted app if we have too many users and computations are executed 'in-line')
 - *execution time of a few minutes*:
     - not long enough so that for example a browser session would expire
     - in this case it would be good UX-wise to have a semi-exact progress indicator that either shows a 'done'-percentage, or remaining time
     - on the backend-side, we need to separate three scenarios:
        - local app:
            - the computation can happen locally, either in a new thread, or a different process (we can also make use of multiple cores if available)
        - hosted jupyter in some form or other:
            - the running Jupyter kernel can execute the computation, which is probably a good enough separation to not affect the hosted UI
        - hosted web app:
            - there needs to exist some infrastructure we can use to offload the computation, it can't run on the same host as our service (which means a lot of added complexity)
            - there is no need yet for authentication apart from that we need to be able to assign the result of the computation to individual sessions
 - *execution time of a few hours*:
     - long enough that a user will have left the computer in between, or closed a browser, etc.
     - now the separation of backend-scenarios kicks in earlier, and also affects the front-end:
         - local app:
            - as in the case before, the UI would display a progress-indicator of some sort
            - the computation would happen as a background process, and as long as the user does not shut-down or restart the
              computer there is no issue (the job should even survive a suspend/hibernate)
         - hosted jupyter:
            - difficult to say, the computation could either still happen in the running Jupyter kernel, or would have to be farmed out to an external service
            - one issue to be aware of is that, depending on how it is configured, Jupyter might or might not kill a notebook process (and underlying kernel)
              if there has been no activity in the browser for a while. We'd have to make sure this does not happen, or that we have some sort of user session
              management (which should be entirely possible -- but of course increases complexity by quite a bit). The latter will also be necessary if a user
              comes back to their session after having been disconnected in some way, because otherwise they'd loose their result.
            - ui-wise there needs to be session and compute-job management, and a list of currently running and past jobs and links to the experiments that produced them
         - hosted web app:
            - as with the jupyter case, we'll need session as well as job management
 - *execution time of more than a few hours (days, weeks)*:
     - in all cases the computation now needs to be outsourced, and submitted to a compute service (cloud, HPC, local dask-cluster, whatever...)
     - all cases need to implement some sort of session authentication and job management (which would probably be a bit more transparent to the user in the local case, but overall it would be implemented in a similar way in each scenario)


### Externally running computations

One thing to stress is that 'outsourcing' computationally intensive tasks comes with a considerable amount of complexity.
Nothing that can't be implemented, and there are several ways I can think of to do this. I'd still advise to be very aware of the
cost and complexity this incurs. I do believe we will have to add that in some form at some stage though, if we are in
any way successful and have people adopting our solution. Which means we have to include the issue in our architecture
design, even if we only plan to implement it later.


# kiara\kiara\docs\design_docs\architecture\workflows\modularity\workflows\corpus_processing.yaml
---
_doc: |
  Receives a dict with the id of a text as key, and the text as value, tokenizes the text(s) and then processes the tokenized values according to the provided settings.

  Currently, lowercasing and the removal of stopwords is supported.

modules:

- type: tokenize_corpus
  input_map:
    text_map: __workflow_input__.text_map

- type: lowercase_corpus
  input_map:
    tokenized_text: tokenize_corpus.tokenized_text
    enabled: __workflow_input__.make_lowercase

- type: remove_stopwords_from_corpus
  input_map:
    tokenized_text: lowercase_corpus.tokenized_text
    enabled: __workflow_input__.remove_stopwords
    stopwords_list: __workflow_input__.stopwords
  workflow_outputs:
    tokenized_text: processed_text_corpus


# kiara\kiara\docs\design_docs\architecture\workflows\modularity\workflows\corpus_processing_simple.yaml
---
modules:

- type: tokenize_corpus

- type: lowercase_corpus
  input_map:
    tokenized_text: tokenize_corpus.tokenized_text

- type: remove_stopwords_from_corpus
  input_map:
    tokenized_text: lowercase_corpus.tokenized_text
  workflow_outputs:
    tokenized_text: processed_text_corpus


# kiara\kiara\docs\design_docs\architecture\workflows\modularity\workflows\input_files_processing.yaml
---
_doc: |
  Reads one or several (text)-files, tokenzies the content, then processes the tokenized content according to the provided settings.

  Currently, lowercasing and the removal of stopwords is supported.

modules:

- type: file_reader
  input_map:
    files: __workflow_input__.files

- type: corpus_processing
  input_map:
    text_map: file_reader.content_map
    make_lowercase: __workflow_input__.make_lowercase
    remove_stopwords: __workflow_input__.remove_stopwords
    stopwords: __workflow_input__.stopwords
  workflow_outputs:
    processed_text_corpus: processed_text_corpus


# kiara\kiara\docs\development\index.md
This page is a work in progress, its purpose is to provide an overview of the relevant parts of *kiara*, as well as
links to further information.

## Basics

- [Getting started](https://dharpa.org/kiara.documentation/latest/usage/getting_started/): A quick (end-user) introduction to *kiara*.

### Install a development environment

- [install dev env](install.md)

### Create a *kiara* plugin package

- [`kiara_plugin.develop repo`](https://github.com/DHARPA-Project/kiara_plugin.develop)

### Create a *kiara* module

- [create module](https://dharpa.org/kiara.documentation/latest/extending_kiara/creating_modules/the_basics/)

### Creeae a *kiara* pipeline

- [create pipeline](https://dharpa.org/kiara.documentation/latest/extending_kiara/pipelines/assemble_pipelines/)


# kiara\kiara\docs\development\install.md
# Install development environment

## Python package

``` console
> python3 -m venv ~/.venvs/kiara
> source ~/.venvs/kiara/bin/activate
> pip install 'kiara[dev_all]'
...
...
...
Successfully installed ... ... ...
> kiara --help
Usage: kiara [OPTIONS] COMMAND [ARGS]...
   ...
   ...
```

In addition to the ``kiara`` package, you'll need to install plugin packages, for example:

``` console
> pip install kiara_plugin.core_types kiara_plugin.tabular
```


## Conda

```console
> conda create -n kiara python=3.10
> conda activate kiara
> conda install -c conda-forge -c dharpa kiara
```

And also plugin packages, like:

```console
> conda install -c conda-forge -c dharpa kiara_plugin.core_types kiara_plugin.tabular
```

Note, the conda install does not include development dependencies, so you'll need to either install those manually, or use pip:

```console
> pip install 'kiara[dev_all]'
```


# kiara\kiara\docs\development\rendering.md
# Render an internal type

*kiara* has a sort of loose framework that helps when there is a requirement to render an internal kiara
model instance into some external format. This has developed in a sort of ad-hoc manner, because requirements where this is the solution to where not very clearly (or at all) defined initially, the continued to surface until at some stage I got tired of implementing the same thing over and over again. At the same token, none of this seemed to ever be exposed to higher levels, which is why I never bothered to fix its API etc.

## cli usage

To see what sort of functionality is possible with this, explore the `kiara render` command. Which, up to a point should be self-exploratory.


# kiara\kiara\docs\development\stores.md
# *kiara* stores

This page contains some information about how *kiara* stores work.

Practically, there are two types of stores in *kiara*:

- *archives*: stores that can only be read from, but not written to
- *stores*: atual 'stores', those are read as well as write

*kiara* has different store types, depending on what exactly is stored:

- *data stores*: stores that store actual data, those are the most important ones
- *alias stores*: stores that keep human readable references (aliases), and link them to actual data (using their value_id)
- *job stores*: stores details and records about past jobs that were run in a *kiara* instance

## Base store

All archives & stores inherit from the base class 'kiara.registries.BaseArchive', which manages basic attributes like thie stores id, it's configuration, and it holds a reference to the current kiara context.

As a developer, you probably won't be using this directly, but you will inherit from either a higher level abstract base class, in case of data-stores that would be:

- `kiara.registries.data.DataArchive`
- `kiara.registries.data.DataStore`

Depending on what you want to store, it's a good idea to check out the source code of those base classes, and look up which methods you need to implement.
Also, you can check out the default implementation of such a store/archive ('filesystem'-based in all cases), to get an idea what needs to happen in each of those methods.


# kiara\kiara\docs\development\SUMMARY.md
- *
- */


# kiara\kiara\docs\development\modules\render_value.md
## Create a module that renders a value of a custom data type

When you create a new data type, by default *kiara* does not know how to render it for specific target(s) (html, terminal, ...). Which means you'll have to create a module for each of the targets you want to support. *kiara* uses the custom `render_value` operation type for this.

There are multiple ways to implement support for rendering a new data type, the easiest one is to add a method with the following signature to the data type class:

```python
    def render_as__<target_type>(
        self, value: "Value", render_config: Mapping[str, Any], manifest: "Manifest"
    ) -> <target_type_cls>:
        ...
        ...
```

So, to implement terminal rendering, that would be:

```python
    def render_as__terminal_renderable(
        self, value: "Value", render_config: Mapping[str, Any], manifest: "Manifest"
    ) -> RenderableType:
        ...
        ...
```

As an example, here's how to implement basic rendering of a 'dict' value:


# kiara\kiara\docs\development\modules\SUMMARY.md
- *
- */


# kiara\kiara\docs\included_components\index.md
## Package content

The *kiara* main package also contains basic, low-level data-types, modules and operations that are necessary
for its core functionality. This page lists all of them.

{% for item_type, item_group in get_context_info().get_all_info().items() %}

### {{ item_type }}
{% for item, details in item_group.item_infos.items() %}
- [`{{ item }}`][kiara_info.{{ item_type }}.{{ item }}]: {{ details.documentation.description }}
{% endfor %}
{% endfor %}


# kiara\kiara\docs\stylesheets\extra.css
div.doc-contents:not(.first) {
  padding-left: 25px;
  border-left: .05rem solid var(--md-default-fg-color--lightest);
  margin-bottom: 80px;
}


# kiara\kiara\examples\data\journals\Readme.md
Data created by [Lena Jaskov](https://github.com/yaslena)


# kiara\kiara\examples\data\text_corpus\Readme.md
Example text corpus.

Source: https://zenodo.org/record/4596345/files/ChroniclItaly_3.0_original.zip?download


# kiara\kiara\examples\data\text_corpus\La_Ragione\sn84037024_1917-04-25_ed-1_seq-1_ocr.txt
LA RAGIONE
ORGANO DI DIFESA DELLA ITALIANITÀ'
contro 1 vili, i camorristi, i sicari, i falsari e gli austriacanti, nemici della patria di origine e di quella d'adozione.
F. SILVAGNI, Direttore, 1010 Christian Street, Philadelphia, Pa.
Tanto per incominciare
La colonia vera, quella sana e composta della grande massi
che lavora e produce, è stanca di assistere a certi spettacoli, ne
quali prendono parte pochissimi protagonisti: spioni austrìaci, si
carii, ricattatori, satiri, banchisti di pessimo carato et similia.
Convinti che un giornale fustigatore, ma veritiero sopratutto
potrà spazzare tutto il luridume che si annida in certi lupanari
diamo vita alla Ragione, che attingerà i suoi fondi dalle nostre ta
sche pulite e non dai depositi che i "calandrielli" affidano alle fauc
di volgari speculatoli. E la dilezione l'affidiamo ad un operaio corm
noi, al Signor Francesco Silvagni, anche perchè siamo sicuri che e
gli, se necessità lo richiedesse, con la punta dei suoi stivali saprì
mettere a posto mandanti e mandatario
A tutti i nostri fratelli dell'Ordine Figli d'ltalia, siano essi del
le autorità o semplici gregarii, una parola
r nostri nemici hanno assunto al loro servizio, un degenerato
un sicario, nella vana speranza di distruggere la nostra Istituzione
le iniziative che ad essa iaranno capo. Hanno fornito il sicario d
denaio peichè meglio possa riuscire alla vergognosa impresa. Ess
sono banchisti prossimi al fallimento; venditori di cerotti; consu
lenti legali di fallimenti; art...isti dall'anima prava; satiri, farabutt
insomma.
Tutti i veri Figli d'ltalia sanno il loro dovere. Se volete ricono
sceiii li scorgerete dando uno sguardo alla Fogna; se non li sapete
venite da noi e vi forniremo i nomi.
Raffaele Settanni, Presidente; Nicoli
Rivano, Segretario; Antonino Viglio
ne. Tesoriere; Vito Gallo, Francese*
Tropea, Carmine Del Giorno, Aristo
demo Palladino, Gaetano Gangemi, G
Citi varese-
V Accettando l'incaricò
< N °n sono un ladro ragioniere di Ranche, nè un sicario, nè ur
Jj&tiro.
/• 1 incarico che operai come me mi affidano e promette
W iente di fai e scomparire da questo mondo coloniale, e pei
\ b tutti gl'insetti malefici.
Ir*
Francesco Silvagni
3 ■
ire NOSTRO PROGRAMMA
Sono trascorsi diciotto mesi da quando un gruppo di operai
-non volendo oltre tollerare un libellista degenerato, sfamato dal
, *»ro tedesco per la sua propaganda antipatriottica, lo schiacciava
dal seno del Comitato della Mobilitazione Civi
le, peichè egli aveva osato insultare una massa che rappresentava
il sentimento spontaneo della Colonia. Da quel tempo egli scoili
parve dal consorzio degli uomini. Oggi ricomparisce sulla scena
con la leva di persone degne dei più luridi bassifondi sodali-artisti
ci da strapazzo, banchisti sull'orlo del fallimento e simili, e noi
pure scendiamo nuovamente nell'agone, ma non per combattere
lui, che non merita neppure il nostro disprezzo, sibbene quelli che
si nascondono alle sue spalle.
Che questi siano dei banchisti privati, lo si rileva dal fatte
che molto tempo prima di venir fuori il foglio libello, si diceva
pubblicamente in Colonia che il suo programma sarebbe state
quello di combattere la Banca Statale Figli d'ltalia. Non solo, ma
il degenerato è andato da per ogni dove, mostrando i CHECK*?
di coloro che s'erano sottoscritti per dar vita al suo giornale. Co
storo, contro questi privati banchisti, di cui in mille occasioni il
degenerato ha detto di conoscere la situazione finanziaria e di sa
pere che si mantengono sui trampoli, noi affileremo le nostre armi,
ingaggiando una guerra spietata, inesorabile, e quando non sani
sufficiente il giornale, andremo pei- le vie e le piazze, predicando
alle masse, esortando i depositanti a ritirare i loro depositi che
corrono pericolo di andarsene in fumo.
Non tutti i banchisti privati di Filadelfia hanno sentito il biso
gno di fornire fondi segreti al degenerato, per comprarne il silenzi®
e per accaparrarsene il patrocinio. Elenchiamo, a titolo d'onore, al
cuni banchieri che non si son fatti prendere all'amo.
Francesco Cerceo, azionista della Sons of Italy State Rank.
Gennaro Di Genova, che sollecitato dal mestierante volgare a
dargli denaro, si rifiutava energicamente e si aveva, in cambio del
rifiuto, la minaccila di prossimi attacchi; Michele Rerardini; Glor
iando Tu moli Ilo; la Ditta Raldi, Pasquale Teti del 1022 Catharine
Street e qualche altro.
A questi la nostra ammirazione, agli altri il nostro disprezzo.
Noi siamo gli stessi operai dello scorso anno, che, per epura
re una volta per sempre la colonia dai rettili che Li infestano, ci
presentiamo al pubblico con questo giornale LA RAGIONE, che
viene fatto a spesse del popolo, con la contribuzione volontaria di
umili lavoratori.
LA DIREZIONE
PHILADELPHIA, PA.. 25 APRILE, 1917
A la gogna!
Una cricca di emeriti affaristi,
composta esclusivamente di colo
ro che gettano il tozzo alla cagna
randagia, nel timore di essere es
si stessi addentati, ha rinnovato
il miracolo della risurrezione di
Lazzaro, riesumando dal fondo
di una misera tomba abbandona
ta, una putrida carogna che a
veva appestato il mondo con le
sue ribalderie.
E così dopo un lungo letargo
ed un'ignominiosa accidia, sbuca
fuori nuovamente dalla sentina
ove vivacchiò, rosicando come un
ratto delle chiaviche, questo de
linquente abbrutito dalla mise
ria, dal sudiciume e dalla crapu
la.
Continuando la sua vecchia
carriera di soldataccio di ventu
ra, al soldo di figure bieche e si
nistre, egli ha preso nuovamen
te ad attaccare, per una vilissi
ma mercede, le migliori istituzio
ni e gli uomini migliori della Co
lonia, perchè egli per la sua indo
le prava è nemico dell'onesto e
del buono.
E' vergognoso per il nome ita
liano che una figura tanto moral
mente sinistra e bacata fino al
midollo, trovi ancora in colonia
della gente che. sebbene cento
volte turlupinata, sia sempre di
sposta a saziare gli immondi ap
petiti di una tale rapacissima ar
pia- Questi ganerGßi
che si tengono celati nell'ombra
in omaggio ai precetti del Van
gelo, sono veramente mossi da
un sentimento di pietà verso i fi
gli e la moglie di questo degene
rato, come essi affermano ge
suiticamente? Ed allora perchè
affidano il danaro agli artigli ra
paci di Jui che lo sciupa nelle bet
tole e nei postriboli mentre nel
la casa regna la miseria più
squallida; nella casa ove sovente
egli bastona la infelice compa
gna cui oltre ai maltrattamenti
quotidiani, lece un regalo degno
di lui, satiro abietto e feroce, nel
periodo del puerperio?
E questa laida figura proteg
gono i prominenti umanitari?
Approvano forse il di lui opera
to? Ipocrita in tal caso, la scusa
che li impietosisca la miseria
dei figliuoli. Per quegli innocenti
siamo pronti anche noi ad apri
re una sottoscrizione per strap
parli all'abbiezione e alla mise
ria. Ma non è la pietà che muo
ve i provvedimenti dei fondi se
greti, essi sono sospinti da un
sentimento meno nobile, da! sen
timento della paura perchè non
ignorano che il degenerato pos
siede molti dei loro segreti che
vogliono nascondere al pubblico,
e perciò aizzano la cagna affama
ta contro i galantuomini, nel ti
more che questa non si avventi
alle loro calcagna e metta a nudo
le loro vergogne ed il loro passa
to. Questa la ragione che li ob
bliga a mordere il freno e a su
bire continuamente i ricatti; il
famigerato redattore del nuovo
organo della camorra organizza
ta, che è libellista per mestiere
ed ha l'ingegno rivolto al male,
sa bene di potere attingere dana
ro in un pozzo senza fondo, dove
è il denaro dei clienti minghioni.
Ma la corda troppo tesa si
spezza e domani, messi con le
spalle al muro i ricattati potreb
bero ricorrere ad estremi rime
di per sbarazzarsi una volta pei
sempre della sanguisuga insazia
bile. Sarebbe umano che un si
cario della penna vedesse
drizzarsi dinanzi minaccioso lo
spettro di un altro sicario certo
non più ignobile di lui.
frattanto al presente il dege
nerato fila di nuovo l'idillio'amo
roso e gode tutto il favore di
luelli che lo sostengono perchè
ne hanno paura. Quanto durerà
questa nuova fase della luna di
miele ? Oh ! non molto certamen
te e quest'altro tentativo giorna
listico del più ignorante e del più
iisonesto tra tytti coloro che ab
biano mai vilipeso una penna,
Ì condannato come i precedenti
»d una vergognosa debacle.
Un mandato molto arduo ed
antipatico egli si è assunto que
sta volta, quello di combattere
jn'lstituzione che èla più bella
conquista della massa operaia di
Philadelphia. Ha tentato bensì di
?irare la quistione attaccando le
persone cui spetta il merito del
'iniziativa della Banca Statale
l'igli d'ltalia, ma il grossolano
stratagemma non ha ingannato
nessuno e tanto meno noi che
siamo le sentinelle vigili del no
stro Istituto. 11 libellista, istiga
lo dalla cricca, ci lancia il guan
to di sfida: noi lo raccogliamo
per gettarci a capofitto in una
otta ad oltranza.
Egli, austriacante incorregibi
le che per la sua fellonia fu ver
gognosamente scacciato dal seno
iel Comitato per la mobilitazio
ne civile, oggi che è entrata in
guerra anche l'America contro
l'Austria e là Germania vigliac
camente si rintana nei meandri
iella sua nera coscienza ed osten
ta sentimenti che non ebbe mai.
perchè già da tempo venduto al
■Vrrinà desco* Ma'noi gii .strappa
rei J- la maschera.
Egli si accanisce oggi contro la
Banca Statale Figli d'ltalia e
pretende metterla in cattiva lu
ce, stabilendo un confronto tra
luesta e la South Philadelphia
■>tate Bank. Veramente se que
sta Istituzione non ha altri moc
"oli da accendere, può bene anda
e a letto all'oscuro. Tutti in co
onia ricordano gli attacchi fero
•i che il degenerato lanciò con
io questa Banca un paio di anni
a. Un bel giorno però gli attac
hi cessarono improvvisamente
>d il giornalista onesto con quel
•ibuttante cinismo che tutti' gli
sconoscono potè vantarsi di ave
e venduto il suo silenzio per
juattrocento dollaracci sonanti.
Più tardi, non molti mesi fa,
juando aspettava di essere im
)iegato nella South Philadelphia
state Bank, e rimase deluso per
'atto energico del suo nuovo Pre
sidente, scrisse una lettera apo
ogetica al consulente legale dei
àllimenti e di atroci insulti con
io il capo di quella azienda bau
:aria, conchiudendo che non
>uò dirsi buona una istituzione j
Il degenerato, spione austriaco
da' la sua solidarietà' agli Indipendenti
di Philadelphia
Le cronache quotidiane, forse per un senso di pudore, non lo
riferiscono, anzi vi furono alcuni che se ne scandalizzarono, ma sia
mo stati informati che al Ballo della Loggia Tripoli e Cirene deK
l'Ordine Indipendente, i cui profitti sono destinati al soddisfaci
mento di un note di Mr. Curiangiolo, a richiesta di alcuni capi
fu data la parola al degenerato spione austriaco per fargli dire
che egli combatterà il Grande Venerabile di questo Stato dell'Ordi
ne Figli d'ltalia.
Sforzi inani quelli di certi Indipendenti, se credono che essi si in
nalzeranno mercè l'opera denigratoria e mercenaria di colui che,
per loro volere più che di altri, fu messo all'indice dal Comitato ita
liano prò patria; lotta di mulini a vento quella che il degenerato
spione austriaco ha intrapreso contro i nostri maggiori!
Che fra certi Indipendenti vi sia corrispondenza di amorosi sensi
con gli spioni austriaci, niente di straordinario, e noi da
oggi in poi sapremo come classificarli ; ma che essi credano
di abbattere la nostra colossale Organizzazione con gli strali di un
sifcario, è una delle loro solite illusioni- Noi, anzi, siamo arciconten
ti di questo patriottico connubio che ci darà l'opportunità di strin
gerci maggiormente e più fortemente attorno al nostro conduttore.
Filippo il Corso Male
se a capo di essa vi è un disone
sto il quale, secondo ciò che an
java sbraitando in colonia, si e
'a arricchito ricettando merce
rubata.
Oggi poi addita quella Bancs
come una Banca modello e la no
stra, che non teme confronti, 1*
chiama un'Agenzia di speculazio
le.
Quando è il degenerato a trin
ciare di simili giudizi, poco gio
cano le laudi alla South Philadel
ahia State Bank «d a noi gli at
tacchi e le insinuazioni non nuoc
ciono affatto, anzi, ci accaparra
lo ogni giorno nuove simpatie.
Raffaele Settanni.
Don Percuocolo
c'o pizzo!!!
Degli amici ci domandano se
Don Percuocolo se e vvero che a
atte pezze per pubblicare il sue
aglio, loglio (pardon) foglio.
lezze non sono sue ma di pochi
scoscienti e vili che non hanno i
coraggio di farsi conoscere per
chè sanno che la loro coscienze
non è tanto pulita pei- affronta
e dei galantuomini. Essi si so
no comprato don Percuocolo i
cui mestiere è stato e sarà sem
pre quello di vendersi al migliore
afferente.
Per costui non vi sono scrupo
li, basta che gli si diano sold
per dedicarsi <r/f aicoffttsnforitf l'i
bertinaggio, tradirebbe famiglia
amici e Patria. Patria? Ma 110
1011 crediamo che egli abbia uni
latria.
Chi non ricorda il suo attacca
nento per la esecrata Au
stria? E chi potrebbe oggi
lubitare tenendo presente con
.'hi si ha a fare e del modo come
spende che non sia una spia Te
losca? Che iddio ce ne liberi da
luesti avanzi della Società, diso
ìore dell'umanità-
Ridete amici miei, dice che
mole ragionare della Ragione! e
pianto mai hai tu ragionato!
Questa volta caro il mio don Per
icolo l'hai detta grossa... Un
ietto napolitano dice; Ornine è
antimi ciente pe carline...
Noi ti consigliamo di ecclissar
i come facesti in quell'altra cir
costanza se non vuoi che i buoni
lella nostra colonia ti fafccia
-10 un brutto servizio. Dì ai tuoi
ladroni che si mostrino essi, e se
lon lo fanno spontaneamente,
lei prossimi numeri li smasche
eremo noi.
Ciao.
Fra Maniaco
Anno I No. 1 Soldi Li Copia
Punte di spillo
CASSIERE LIBERTINO
E' cassiere ed oggi ha avuto
una velleità nuova; ha voluto di
ventare anche cassiere del gior
nale La Fogna.
Non è proprio un giornali*'
ma any-how —si può <•&
! aria di esserlo, pur non ess* 0
da tanto di (are un O col bic«. >.
re. Ma che vi pare? Cassiere d.
banca ! Cassiere anche di un giov
ile, sia pure la Fogna ! Non 0
che dire: il Cassiere aveva 1
gno di un tantino di noto fl "
Concediamogliela. Jant,
* * * ; mai
'N<
\ unuto da Scranton ann.-f a
in condizioni deplorevolissime
con le scarpe rotte e coi
brandelli, il sempre gener s :
Giuseppe Di Silvestro lo fece «m
piegare presso una Cooperati' ( ,
ed egli oggi, da quel serpent ut p
che è, degno consocio del de,'
rato, lo ricambia con la cai ania
e con la diffamazione. Più
elevato alla onorifica carie, dì
cassiere, iniziò le sue impres *
lanti e gettò sul lastrico ui- Hll j z _*
gazza, dopo averla disonorai m) j
quale oggi tace nell'abbando,..,
per non esporre al pubblico là
propria vergogna.
Intanto l'avventuriero ai jp** '
soffocato_ogni rimor!
ge airimpaTniare'Tina fafcSutìa
appartenente ad una famiglia
spettabile, la quale certo ignora
il turpe retroscena.
Un individuo così fatto non
poteva non simpatizzare col de
generato. Il degenerato è cono
sciuto da tutti per tale, da amici
e nemici tutti lo disprezzano,
fili uomini di coraggio gli scara
ventano il loro disprezzo sul gr,i.
gno; i vili questo disprezzo li
sentono nell'intimo dell'animo,
ma atteggiano il viso ipocri
ta ad un sorriso di compiacenza.
Il degenerato! Oli, come appa
re oggi giustificato il violento lin
guaggio di Lucifero, a suo riguar
do. quel linguaggio che, all'epoca
delle pubblicazioni del Ribelle,
non andava a sangue ad alcuni
soverchiamente puritani. Scac
ciato dal natio paesello, dove, in
qualità di usciere di ccfticiliazio
ne, commise atti poco puliti, ven
ne in America e s'impiegò in una
banca, donde fu messo fuori per
continue sottrazioni di franco
bolli.
Come il suo socio dell'ultima
ora, ingrato verso il benefattore,
lo attaccò vigliaccamente per fa
re le difese di un altro banchie
re con cui lavorava.
Nella campagna contro il con
sole Naselli, appoggiò i Di Silve
stro e Tresca ed i suoi articoli
sono sempre visibili ; ma poi, per
chè impostogli dal suo padrone,
per ragione di pagnotta, come e
gli diceva, smise la pubblicazione
del giornale.
Falsa quindi la sua afferma
zione ultima che egli abbia sem
pre combattuto i fratelli Di Sil
vestro.
Parassita, ricattatore di ban
che, martirizzatore della propria
moglie; tale la figura morale di
colui che oggi il Cassiere liberti
no proclama un galantuomo.
Well! de gustibus con quel
che segue. Ognuno può avere
quei gusti confacenti alla propria
natura, ed il sig. Cassiere perciò
ha anche il diritto di avere i suoi.
Così può farsi una cornice dei
gusti del degenerato e appender
sela a capo al letto, ora di scapo
lo, domani di ammogliato; e può
farsi di lui il modello al quale i-


# kiara\kiara\examples\data\text_corpus\La_Ragione\sn84037024_1917-04-25_ed-2_seq-1_ocr.txt
LA RAG ONE
contro i vili, i camorristi, i sicari, i falsari e gli austriacanti, nemici della patria di origine e di quella d'adozione.
F. SILVAGNI, Direttore, 1010 Christian Street, Philadelphia, Pa
Tanto per incominciare
La colonia vera, quella sana e composta della grande massa
che lavora e produce, è stanca di assistei* a certi spettacoli, nei
quali prendono parte pochissimi protagonisti: spioni austriaci, si
earii, ricattatori, satiri, banchisti di pessimo carato et simili^.
Convinti che un giornale fustigatore, ma veritiero sopratutto,
potrà spazzare tutto il luridume che si annida in certi lupanari,
diamo vita alla Ragione, che attingerà i suoi fondi dalle nostre ta
sche pulite e non dai depositi che i "calandrielli" affidano alle fauci
di volgari speculatori. E la direzione l'affidiamo ad un operaio come
noi, al Signor Francesco Silvagni, anche perchè siamo sicuri che e
gli, se necessità lo richiedesse, con la punta dei suoi stivali saprà
mettere a posto mandanti e mandatario
A tutti i nostri fratelli dell'Ordine Figli d'ltalia, siano essi del
ie autorità o semplici gregarii, una parola-
I nostri nemici hanno assunto al loro servizio, un degenerato,
un sicario, nella vana speranza di distruggere la nostra Istituzione,
le iniziative che ad essa faranno capo. Hanno fornito il sicario di
denaro perchè meglio possa riuscire alla vergognosa impresa. Essi
sono banchisti prossimi al fallimento; venditori di cerotti; consu
lenti legali di fallimenti ; art...isti dall'anima prava; satiri, farabutti
insomma.
Tutti i veri Figli d'ltalia sanno il toro dovere. Se volete ricono
scerli li scorgerete dando uno sguardo alla Fogna ; se non li sapete,
venite da noi e vi forniremo i nomi.
Raffaele Settanni, Presidente; Nicola
Rivano, Segretario; Antonino Viglio
ne, Tesoriere; Vito Callo, Francesco
Tropea, Carmine Del Giorno. Aristo
demo Palladino, Gaetano Gangemi, G.
Calvaret*?.
Accettando l'incarico
Non sono un ladro ragioniere di Banche, nè un sicario, nè un
satiro.
Accetto l'incarico che operai come me mi affidano e prometto
solennemente di fare scomparire da questo mondo coloniale, e per
sempre, tutti gl'insetti malefici.
Francesco Silvagni
IL NOSTRO PROGRAMMA
Sono trascorsi diciotto mesi da quando un gruppo di operai,
non volendo oltre tollerare un libellista degenerato, sfamato dal
l'oro tedesco per la sua propaganda antipatriottica, lo schiacciava
ignominiosamente dal seno del Comitato della Mobilitazione Civi
le, perchè egli aveva osato insultare una massa che rappresentava
il sentimento spontaneo della Colonia. Da quel tempo egli scom
parve dal consorzio degli uomini. Oggi ricomparisce sulla scena,
con la leva di persone degne dei più luridi bassifondi sodali-artisti
ci da strapazzo, banchisti sull'orlo del fallimento e simili, e noi
pure scendiamo nuovamente nell'agone, ma non per combattere
lui, che non merita neppure il nostro disprezzo, sibbene quelli che
si nascondono alle sue spalle.
Che questi siano dei banchisti privati, lo si rileva dal fatto
che molto tempo prima di venir fuori il foglio libello, si diceva
pubblicamente in Colonia che il suo programma sarebbe stato
quello di combattere la Banca Statale Figli d'ltalia. Non solo, ma
il degenerato è andato da per ogni dove, mostrando i CHECK£>
di coloro che s'erano sottoscritti per dar vita al suo giornale. Co
storo, contro questi privati banchisti, di cui in mille occasioni il
degenerato ha detto di conoscere la situazione finanziaria e di sa
pere che si mantengono sui trampoli, noi affileremo le nostre armi,
ingaggiando una guerra spietata, inesorabile, e quando non sarà
sufficiente il giornale, andremo per le vie e le piazze, predicando
alle masse, esortando i depositanti a ritirare i loro depositi che
corrono pericolo di andarsene in fumo.
Non tutti i banchisti privati di Filadelfia hanno sentito il biso
gno di fornire fondi segreti al degenerato, per comprarne il silenzio
e per accaparrarsene il patrocinio. Elenchiamo, a titolo d'onore, al
cuni banchieri che non si son fatti prendere all'amo.
Francesco Cerceo, azionista della Sons of Italy State Bank.
Gennaro Di Genova, che sollecitato dal mestierante volgare a
dargli denaro, si rifiutava energicamente e si aveva, in cambio del
rifiuto, la minaccia di prossimi attacchi; Michele Berardini; Glor
iando Tumolillo: la Ditta Baldi, Pasquale Teti del 1022 Catharine
Street e qualche altro.
A questi la nostra ammirazione, agli altri il nostro disprezzo.
Noi siamo gli stessi operai dello scorso anno, che, per epura
re una volta per sempre la colonia dai rettili che la infestano, ci
presentiamo al pubblico con questo giornale LA RAGIONE, che
viene fatto a spese del popolo, con la contribuzione volontaria di
umili lavoratori.
LA DIREZIONE
ORGANO DI DIFESA DELLA ITALIANITÀ
A la gogna!
Una cricca di emeriti affaristi,
composta esclusivamente di colo
ro che gettano il tozzo alla cagna
randagia, nel timore di essere es
si stessi addentati, ha rinnovato
il miracolo della risurrezione di
Lazzaro, riesumando dal fondo
di una misera tomba abbandona
ta, una putrida carogna che a
veva appestato il mondo con le
sue ribalderie.
E così dopo un lungo letargo
ed un'ignominiosa accidia, sbuca
fuori nuovamente dalla sentina
ove vivacchiò, rosicando come un
ratto delle chiaviche, questo de
linquente abbrutito dalla mise
ria, dal sudiciume e dalla crapu
la.
Continuando la sua vecchia
carriera di soldataccio di ventu
ra, al soldo di figure bieche e si
nistre, egli ha preso nuovamen
te ad attaccare, per una vilissi
ma mercede, le migliori istituzio
ni e gli uomini migliori della Co
lonia, perchè egli per la sua indo
le prava è nemico dell'onesto e
del buono.
E' vergognoso pei' il nome ita
liano che una figura tanto moral
mente sinistra e bacata fino al
midollo, trovi ancora in colonia
della gente che, sebbene cento
volte turlupinata, sia sempre di
sposta a saziare gli immondi ap
petiti di una tale rapacissima ar
pia- Questi generosi fihtntioy
che si tengono celati nell'ombra
in omaggio ai precetti del Van
gelo, sono veramente mossi da
un sentimento di pietà verso i fi
gli e la moglie di questo degene
rato, come essi affermano ge
suiticamente? Ed allora perchè
danaro agli artigli ra
paci di lui che lo sciupa nelle bet
tole e nei postriboli mentre nel
la casa regna la miseria più
squallida; nella casa ove sovènte
egli bastona la infelice compa
gna cui oltre ai maltrattamenti
quotidiani, fece un regalo degno
di lui, satiro abietto e feroce, nel
periodo del puerperio?
E questa laida figura proteg
gono i prominenti umanitari?
Approvano forse il di lui opera
to? Ipocrita in tal caso, la scusa
che li impietosisca la miseria
dei figliuoli. Per quegli innocenti
siamo pronti anche noi ad apri
re una sottoscrizione per strap
parli all'abbiezione e alla mise
ria. Ma non è la pietà che muo
ve i provvedimenti dei fondi se
greti, essi sono sospinti da un
sentimento meno nobile, dal sen
timento della paura perchè non
ignorano che il degenerato pos
siede molti dei loro segreti che
vogliono nascondere al pubblico,
e perciò aizzano la cagna affama
ta contro i galantuomini, nel ti
more che questa non si avventi
alle loro calcagna e metta a nudo
le loro vergogne ed il loro passa
to. Questa la ragione che li ob
bliga a mordere il freno e a su
bire continuamente i ricatti; il
famigerato redattore del nuovo
organo della camorra organizza
ta, che è libellista per mestiere
ed ha l'ingegno rivolto al male,
sa bene di potere attingere dana
ro in un pozzo senza fondo, dove
è il denaro dei clienti minghioni-
Ma la corda troppo tesa si
spezza e dbmani, messi con le
spalle al muro i ricattati potreb
bero ricorrere ad estremi rime
di per sbarazzarsi una volta per
sempre della sanguisuga insazia
bile. Sarebbe umano che un si
cario della penna vedesse
drizzarsi dinanzi minaccioso lo
spettro di un altro sicario certo
non più ignobile di lui.
PHILADELPHIA, PA., 25 APRILE, 1917
Frattanto al presente il dege
nerato fila di nuovo l'idillio amo
roso e gode tutto il favore di
quelli che lo sostengono perchè
ne hanno paura. Quanto durerà
! questa nuova fase della luna di
! miele ? Oh ! non molto certamen
te e quest'altro tentativo giorna
! listico del più ignorante e del più
disonesto tra tutti coloro che ab
! biano mai vilipeso una penna,
j è condannato come i precedenti
ad una vergognosa debacle.
Un mandato molto arduo ed
antipatico egli si è assunto que
sta volta, quello di combattere
un'lstituzione che è la più bella
conquista della massa operaia di
i Philadelphia. Ha tentato bensì di
girare la quistione attaccando le
persone cui spetta il merito del
l'iniziativa della Banca Statale
Figli d'ltalia, ma il grossolano
stratagemma non ha ingannato
nessuno e tanto meno noi che
: siamo le sentinelle vigili del no
stro Istituto. Il libellista, istiga
| lo dalla cricca, ci lancia il guan
■to di sfida : noi lo raccogliamo
per gettarci a capofitto in una
lotta ad oltranza.
Egli, austriacante incorregibi
le che per la sua fellonia fu ver
gognosamente scacciato dal seno
del Comitato per la mobilitazio
ne civile, oggi che è entrata in
guerra anche l'America contro
l'Austria e la Germania vigliac
i camente si rintana nei meandri
; della sua nera coscienza ed osten
ta sentimenti che non ebbe mai,
erchè già da tempo venduto pl
' ì oYÌ? Ceuesco. iùà rtOi gli Strappe
! remo la maschera.
Egli si accanisce oggi conti o la
Banca Statale Figli d'ltalia e
pretende metterla in cattiva lu
ce, stabilendo un confronto tra
questa e la South Philadelphia
State Bank. Veramente se que
sta Istituzione non ha altri moc
coli da accendere, può bene anda
re a letto all'oscuro. Tutti in co
lonia ricordano gli attacchi fero
ci che il degenerato lanciò con
tro questa Banca un paio di anni
fa. Un bel giorno però gli attac
chi cessarono improvvisamente
ed il giornalista onesto con quel
ributtante cinismo che tutti gli
riconoscono potè vantarsi di ave
re venduto il suo silenzio per
quattrocento dollaracci sonanti.
Più tardi, non molti mesi fa,
quando aspettava di essere im
piegato nella South Philadelphia
State Bank, e rimase deluso per
l'atto energico del suo nuovo Pre
sidente, scrisse una lettera apo
logetica al consulente legale dei
fallimenti e di atroci insulti con
tro il capo di quella azienda ban
caria, conchiudendo che non
può dirsi buona una istituzione
Il degenerato, spione austriaco
da' la sua solidarietà' agli Indipendenti
di Philadelphia
Le cronache quotidiane, forse per un senso di pudore, non lo
riferiscono, anzi vi furono alcuni che se ne scandalizzarono, ma sia
mo stati informati che al Ballo della Loggia Tripoli e Cirene del
l'Ordine Indipendente, i cui profitti sono destinati al soddisfaci
mento di un note di Mr. Curiangiolo, a richiesta di alcuni capi
fu data la parola al degenerato spione austriaco per fargli dire
che egli combatterà il Grande Venerabile di questo Stato dell'Ordi
ne Figli d'ltalia.
Sforzi inani quelli di certi Indipendenti, se credono che essi si in
nalzeranno mercè l'opera denigratoria e mercenaria di colui che,
per loro volere più che di altri, fu messo all'indice dal Comitato ita
liano prò patria; lotta di mulini a vento quella che il degenerato
spùone austriaco ha intrapreso contro i nostri maggiori !
Che fra certi Indipendenti vi sia corrispondenza di amorosi sensi
con gli spioni austriaci, niente di straordinario, e noi da
oggi in poi sapremo come classificarli ; ma che essi credano
di abbattere la nostra colossale Organizzazione con gli strali di un
sicario, è una delle loro solite illusioni- Noi, anzi, siamo arciconten
ti di questo patriottico connubio che ci darà l'opportunità di strin
gerci maggioi*mente e più fortemente attorno al nostro conduttore.
Filippo il Corso Male
j se a capo di essa vi è un disone
j sto il quale, secondo ciò che an- j
dava sbraitando in colonia, si e
ra arricchito ricettando merce
; rubata.
Oggi poi addita quella Banca
| come una Banca modello e la no
stra. che non teme confronti, la
chiama un'Agenzia di speculazio
ne.
Quando è il degenerato a trin
! dare di simili giudizi, poco gio
-1 vano le laudi alla South Philadel
; phia State Bank ed a noi gli at
! tacchi e le insinuazioni non nuoc
| ciono affatto, anzi, ci accaparra
| no ogni giorno nuove simpatie.
Raffaele Settanni.
Don Percuocolo
c'o pizzo!!!
Degli amici ci domandano se
i Don Percuocolo se e wero che a
j fatte pezze per pubblicale il suo
faglio, feglio (pardon) foglio. Le
; pezze non sono sue ma di pochi
incoscienti e vili che non hanno il
i coraggio di farsi conoscere per
! chè sanno che la loro coscienza
| non è tanto pulita per afl'ronta
! re dei galantuomini. Essi si so
jno comprato don Percuocolo il
cui mestiere è stato e sarà sem
pre quello di vendersi al migliore
offerente.
Per costui non vi sono scrupo
li, basta che gli si diano soldi
per' u'euit;trsi afTEictìiJirsriio, a/Mi
j bertinaggio, tradirebbe famiglia,
'amici e Patria. Patria? Ma noi
non crediamo che egli abbia una
! patria.
Chi non ricorda il suo attacca
mento per la esecrata Au
stria? E chi potrebbe oggi
' dubitare tenendo presente con
chi si ha a fare e del modo come
spende che non sia una spia Te
desca? Che iddio ce ne liberi da
questi avanzi della Società, diso
! noie dell'umanità-
Ridete amici miei, dice che
vuole ragionare della Ragione! e
quanto mai hai tu ragionato!
Questa volta caro il mio don Per
cuocolo l'hai detta grossa... Un
detto napolitano dice; Omme è
cantina dente pe carline...
Noi ti consigliamo di ecclissar
ti come facesti in quell'altra cir
' costanza se non vuoi che i buoni
| della nostra colonia ti faccia
! no un brutto servizio. Dì ai tuoi
; padroni che si mostrino essi, e se
non lo fanno spontaneamente,
j nei prossimi numeri li smasche
j i - eremo noi.
Ciao.
Fra Manisco
Anno I No. 1 5 Soldi la Copia
Punte di spillo
CASSIERE LIBERTINO
E' cassiere ed oggi ha avuto
una velleità nuova; ha voluto di
ventale anche cassiere del gior
nale La Fogna.
Non è proprio un giornalista,
ma any-how si può dare
l'aria di esserlo, pur non essendo
da tanto di fare un 0 col bicchie
re. Ma che vi pare? Cassiere di
banca ! Cassiere anche di un gior
ne, sia pure la Fogna! Non c'è
che dire: il Cassiere aveva biso
gno di un tantino di notorietà-
Concediamogliela.
* * «
Venuto da Scranton anni fa,
in condizioni deplorevolissime,
con le scarpe rotte e coi vestiti a
brandelli, il sempre generoso
Giuseppe Di Silvestro lo fece im
piegare presso una Cooperativa,
ed egli oggi, da quel serpentello
che è, degno consocio del degene
rato, lo ricambia con la calunnia
e con la diffamazione. Più tardi,
elevato alla onorifica carica di
cassiere, iniziò le sue imprese ga
lanti e gettò sul lastrico una ra
gazza, dopo averla disonorata, la
quale oggi tace nell'abbandono,
per non esporre al pubblico la
propria vergogna.
i+ìtauto
■
ge jjd impalmare una "nciuìla
appartenente ad una famiglia ri
spettabile, la quale certo ignora
il turpe retroscena.
Un individuo così fatto non
poteva non simpatizzare col de
generato. Il degenerato è cono
sciuto da tutti per tale, da amici
e nemici tutti lo disprezzano.
Gli uomini di coraggio gli scara
ventano il loro disprezzo sul gru
gno; i vili questo disprezzo lo
sentono nell'intimo dell'animo,
ma atteggiano il viso ipocri
ta ad un sorriso di compiacenza.
11 degenerato ! Oh, come appa
re oggi giustificato il violento lin
guaggio di Lucifero, a suo rigu al
do, quel linguaggio che, all'epoca
delle pubblicazioni del Ribelle,
non andava a sangue ad alcuni
soverchiamente puritani. Scac
ciato dal natio paesello, dove, in
qualità di usciere di conciliazio
ne, commise atti poco puliti, ven
ne in America e s'impiegò in una
banca, donde fu messo fuori per
continue sottrazioni di franco
bolli.
Come il suo socio dell'ultima
ora, ingrato verso il benefattore,
lo attaccò vigliaccamente per fa
re le difese di un altfo banchie
re con cui lavorava.
Nella campagna contro il con
sole Naselli, appoggiò i Di Silve
stro e Tresca ed i suoi articoli
sono sempre visibili; ma poi, per
chè impostogli dal suo padrone,
per ragione di pagnotta, come e
gli diceva, smise la pubblicazione
del giornale.
Falsa quindi la sua afferma
zione ultima che egli abbia sem
pre combattuto 1 fratelli Di Sil
vestro.
Parassita, ricattatore di ban
che, martirizzatore della propria
moglie ; tale la figura morale di
colui che oggi il Cassiere liberti
no proclama un galantuomo-
Well ! de gustibus con quel
che segue. Ognuno può avere
quei gusti confacenti alla propria
natura, ed il sig. Cassiere perciò
ha anche il diritto di avei*e i suoi.
Così può farsi una cornice dei
gusti del degenerato e appender
sela a capo al letto, ora di scapo
lo, domani di ammogliato; e può
farsi di lui il modello al quale i-


# kiara\kiara\examples\data\text_corpus\La_Ragione\sn84037024_1917-04-25_ed-3_seq-1_ocr.txt
LA RAGIONE
ORGANO DI DIFESA DELLA ITALIANITÀ
contro i vili, i camorristi, i sicari, i falsari e gli austriacanti, nemici della patria di origine e di quella d'adozione.
F. SILVAGNI, Direttore, 1010 Christian Street, Philadelphia, Pa
Tanto per incominciare
La colonia vera, quella sana e composta della grande massa
che lavora e produce, è stanca di assistere a certi spettacoli, nei
quali prendono parte pochissimi protagonisti: spioni austriaci, si
carii, ricattatori, satin, banchisti di pessimo carato et similia.
Convinti che un giornale fustigatore, ma veritiero sopratutto,
potrà spazzare tutto il luridume che si annida in certi lupanari,
diamo vita alla Ragione, che attingerà i suoi fondi dalle nostre ta
sche pulite e non dai depositi che i "calandrielli" affidano alle fauci
di volgari speculatori. E la dilezione l'affidiamo ad un operaio come
iioi, al Signor Francesco Silvagni, anche perchè siamo sicuri che e
gli, se necessità lo richiedesse, con la punta dei suoi stivali saprà
mettere a posto mandanti e mandatario
A tutti i nostri fratelli dell'Ordine Figli d'ltalia, siano essi del
le autorità o semplici gregarii, una parola-
I nostri nemici hanno assunto al loro servizio, un degenerato,
un sicario, nella vana speranza di distruggere la nostra Istituzione,
le iniziative che ad essa faranno capo. Hanno fornito il sicario di
denaro perchè meglio possa riuscire alla vergognosa impresa. Essi
sono banchisti prossimi al fallimento ; venditori di cerotti ; consu
lenti legali di fallimenti; art..-isti dall'anima prava; satiri, farabutti
insomma.
Tutti i veri Figli d'ltalia sanno il loro dovere. Se volete ricono
scerli li scorgerete dando uno sguardo alla Fogna; se non li sapete,
venite da noi e vi forniremo i nomi.
Raffaele Settanni, Presidente; Nicola
Rivano, Segretario; Antonino Viglio
ne, Tesoriere; Vito Callo, Francesco
Tropea, Carmine Del Giorno, Aristo
demo Palladino, Gaetano Gangemi, G.
Calvarese-
Accettando l'incarico
... '
'
Non sono un ladro ragioniere di Banche, nè un sicario, nè un
- satiro.
Accetto l'incarico che operai come me mi affidano e prometto
solennemente di fare scomparire'da questo mondo coloniale, e per
sempre, tutti gl'insetti malefici.
Francesco Silvagni
IL NOSTRO PROGRAMMA
Sono trascorsi diciotto mesi da quando un gruppo di operai,
non volendo oltre tollerare un libellista degenerato, sfamato dal
l'oro tedesco per la sua propaganda antipatriottica, lo schiacciava
ignominiosamente dal seno del Comitato della Mobilitazione Civi
le, perchè egli aveva osato insultine una massa che rappresentava
il sentimento spontaneo della Colonia. I)a quel tempo egli scom
parve dal consorzio degli uomini. Oggi ricomparisce sulla scena,
con ia leva di persone dei più luridi bassifondi sociali-artisti
ci da strapazzo, banchisti sull'orlo del fallimento e simili, e noi
pure scendiamo nuovamente nell'agone, ma non per combattere
lui, che non merita neppure il nostro disprezzo, sibbene quelli che
si nascondono alle sue spaile.
Che questi siano dei banchisti privati, lo si rileva dal fatto
che molto tempo prima di venir fuori il foglio libello, si diceva
pubblicamente in Colonia che il suo programma sarebbe stato
quello di combattere la Banca Statale Figli d'ltalia. Non solo, ma
il degenerato è andato da per ogni dove, mostrando i CHECKfrv
di coloro che s'erano sottoscritti per dar vita al suo giornale. Co
storo, contro questi privati banchisti, di cui in mille occasioni il
degenerato ha detto di conoscere Li situazione finanziaria e di sa
pere che si mantengono sui trampoli, noi affileremo le nostre armi,
ingaggiando una guerra spietata, inesorabile, e quando non sarà
sufficiente il giornale, andremo per le vie e le piazze, predicando
alle masse, esortando i depositanti a ritirare i loro depositi che
corrono pericolo di andarsene in fumo.
Non tutti i banchisti privati di Filadelfia hanno sentito il biso
gno di fornire fondi segreti al degenerato, per comprarne il silenzio
e per accaparrarsene il patrocinio. Elenchiamo, a titolo d'onore, al
cuni banchieri che non si son fatti prendere all'amo.
Francesco Cerceo, azionista della Sons of Italy State Bank.
Gennaro Di Genova, che sollecitato dal mestierante volgare a
dargli denaro, si rifiutava energicamente e si aveva, in cambio del
rifiuto, la minaccia di prossimi attacchi; Michele Berardini; Glor
iando Tumolillo; la Ditta Baldi, Pasquale 'Feti del 1022 Catharine
Street e qualche altro.
A questi la nostra ammirazione, agli altri il nostro disprezzo.
Noi siamo gli stessi operai dello scorso anno, che, per spaia
re una volta per sempre la colonia dai rettili che la infestano, ci
'presentiamo al pubblico con questo giornale LA RAGIONE, che
"Mene fatto a spese del popolo, con la contribuzione volontaria di
, fin ili lavoratori.
LA DIREZIONE
PHILADELPHIA, PA., 25 APRILE, 1917
A la gogna!
Una cricca di emeriti affaristi,
composta esclusivamente di colo
ro che gettano il tozzo alla cagna
randagia, nel timore di essere es
si stessi addentati, ha rinnovato
il miracolo della risurrezione di
Lazzaro, riesumando dal fondo
di una misera tomba abbandona
| ta, una putrida carogna che a
veva appestato il mondo con le
Ì sue ribalderie.
E così dopo un lungo letargo
ed un'ignominiosa accidia, sbuca
fuori nuovamente dalla sentina
ove vivacchiò, rosicando come un
ratto delle chiaviche, questo de
linquente abbrutito dalla mise
ria, dal sudiciume e dalla crapu
la.
Continuando la sua vecchia
carriera di soldataccio di ventu
ra, al soldo di figure bieche e si
nistre, egli ha preso nuovamen
te ad attaccare, per una vilissi
ma mercede, le migliori istituzio
ni e gli uomini migliori della Co
lonia, perchè egli per la sua indo
le prava è nemico dell'onesto e
del buono.
E' vergognoso per il nome ita
liano che una figura tanto moral
mente sinistra e bacata fino al
midollo, trovi ancora in colonia
della gente che, sebbene cento
volte turlupinata, sia sempre di
sposta a saziare gli immondi al
lietiti di una tale rapacissima ar
pia- Questi fìhmhr. *
che si tengono celati nell'ombra
in omaggio ai precetti del Van
gelo, sono veramente mossi da
| un sentimento di pietà verso i fi
gli e la moglie di questo degene
rato, come essi affermano ge
suiticamente? Ed allora perchè
I affidano il danaro agli artigli ra
paci di lui che lo sciupa nelle bet
tole e nei postriboli mentie nel
|la casa regna la miseria più
j squallida; nella casa ove sovente
egli bastona la infelice compa
; glia cui oltre ai maltrattamenti
j quotidiani, fece un regalo degno
| di lui, satiro abietto e feroce, nel
! periodo del puerperio ?
E questa laida figura proteg
gono i prominenti umanitari?
! Approvano forse il di lui opera
to? Ipocrita in tal caso, la scusa
! che li impietosisca la miseria
j dei figliuoli. Per quegli innocenti
! siamo pronti anche noi ad apri
re una sottoscrizione per strap
parli all'abbiezione e alla mise
ria. Ma non è la pietà che muo
ve i provvedimenti dei fondi se
greti, essi sono sospinti da un
sentimento meno nobile, dal sen
! timento della paura perchè non
ignorano che il degenerato pos
! siede molti dei loro segreti che
vogliono nascondere al pubblico,
e perciò aizzano la cagna affama
ta contro i galantuomini, nel ti
more che questa non si avventi
alle loro calcagna e metta a nudo
le loro vergogne ed il loro passa
to. Questa la ragione che li ob
bliga a mordere il freno e a su
bire continuamente i ricatti; il
i famigerato redattore del nuovo
organo della camorra organizza
j ta, che è libellista per mestiere
ed ha l'ingegno rivolto al male,
sa bene di potere attingere dana
' ro in un pozzo senza fondo, dove
è il denaro dei clienti minghioni-
Ma la corda troppo tesa si
spezza e domani, messi con le
spalle al muro i ricattati potreb
bero ricorrere ad estremi rime
di per sbarazzarsi una volta per
sempre della sanguisuga insazia
bile. Sarebbe umano che un si
cario della penna vedesse
drizzarsi dinanzi minaccioso lo
spettro di un altro sicario certo
non più ignobile di lui.
| Frattanto al presente il dege-j
| nerato fila di nuovo l'idillio amo
j roso e gode tutto il favore di
i quelli che lo sostengono perchè
; ne hanno paura. Quanto durerà
j questa nuova fase della luna di
; miele ? Oh ! non molto certamen
| te e quest'altro tentativo giorna- j
listico del più ignorante e del piìi
j disonesto tra tutti coloro che ab
j biano mai vilipeso una penna, |
! è condannato come i precedenti
! ad una vergognosa debacle.
Un mandato molto arduo ed
antipatico egli si è assunto que- !
sta volta, quelle combattere
un'lstituzione che èla più bella !
conquista della massa operaia di
| Philadelphia. Ha tentato bensì di
girare la quistione attaccando lo
persone cui spetta il merito del ;
l'iniziativa della Banca Statale ;
• Figli d'ltalia, ma il grossolano
stratagemma non ha ingannato ;
nessuno e tanto meno noi che :
j siamo le sentinelle vigili del no-
I stro Istituto. Il libellista, istiga- j
i to dalla cricca, ci lancia il guan
to di sfida: noi lo raccogliamo
{ per gettarci a capofitto in una
lotta ad oltranza.
Egli, austriacante incorregibi
ì le che per la sua fellonia fu ver
! gognosamente scacciato dal seno
del Comitato per la mobilitazio
|ne civile, oggi che è entrata in
I guerra anche l'America contro
! l'Austria e la Germania vigliac
| camente si rintana nei meandri
1 della sua nera coscienza ed osten-
I ta sentimenti che non ebbe mai,
oerchè già da tempo venduto al
r}' !!; r.c: gli sti^w
frèmo la maschera.
Egli si accanisce oggi contro la
Banca Statale Figli d'ltalia e
j pretende metterla in cattiva lu
i ce, stabilendo un confronto tra
questa e la South Philadelphia
State Bank. Veramente se que
| sta Istituzione non ha altri moc
: coli da accendere, può bene anda
re a letto all'oscuro. Tutti in co
lonia ricordano gli attacchi fero
ci che il degenerato lanciò con
, tro questa Banca un paio di anni
l'a. Un liei giorno però gli attac
chi cessarono improvvisamente
;ed il giornalista onesto con quel
ributtante cinismo che tutti gli
riconoscono potè vantarsi di ave
re venduto il suo silenzio per
| quattrocento dollaracci sonanti.
Più tardi, non molti mesi fa,
i quando aspettava di essere im
j piegato nella South Philadelphia
State Bank, e rimase deluso per
l'atto energico del suo nuovo Pre
sidente, scrisse una lettera apo
logetica al consulente legale dei
fallimenti e di atroci insulti con
tro il capo di quella azienda ban
j caria, conchiudendo che non
l)uò dirsi buona una istituzione
il degenerato, spione austriaco
da' la sua solidarietà' agli Indipendenti
di Philadelphia
Le cronache quotidiane, forse per un senso di pudore, non lo
riferiscono, anzi vi furono alcuni che se ne scandalizzarono, ma sia
mo stati informati che al Ballo della Loggia Tripoli e Cirene del
l'Ordine Indipendente, i cui profitti sono destinati al soddisfaci
mento di un note di Mr. Curiangiolo, a richiesta di alcuni capi
fu data la parola al degenerato spione austriaco per fargli dire
che egli combatterà il Grande Venerabile di questo Stato dell'Ordi
ne Figli d'ltalia.
Sforzi inani quelli di certi Indipendenti, se credono che essi si in
nalzeranno mercè l'opera denigratoria e mercenaria di colui che,
per loro volere più che di altri, fu messo all'indice dal Comitato ita
liano pio patria; lotta di mulini a vento quella che il degenerato
spione austriaco ha intrapreso contro i nostri maggiori!
Che fra certi Indipendenti vi sia corrispondenza di amorosi sensi
con gli spioni austriaci, niente di straordinario, e noi da
oggi in poi sapremo come classificarli ; ma che essi credano
di abbattere la nostra colossale Organizzazione con gli strali di un
sicario, è una delle loro solite illusioni- Noi, anzi, siamo arciconten
ti di questo patriottico connubio che ci darà l'opportunità di strin
gerci maggiormente e più fortemente attorno al nostro conduttore.
Filippo il Corso Male
se a capo di essa vi è un disone
sto il quale, secondo ciò che an
dava sbraitando in colonia, si e
ra arricchito ricettando merce
rubata.
Oggi poi addita quella Banca
come una Banca modello e la no
stra, che non teme confronti, la
chiama un'Agenzia di speculazio
ne.
Quando è il degenerato a trin
ciare di simili giudizi, poco gio
vano le laudi alla South Philadel
phia State Bank ed a noi gli at
tacchi e le insinuazioni non nuoc
ciono affatto, anzi, ci accaparra
no ogni giorno nuove simpatie.
Settanni.
Don Percuocolo
c'o pizzo!!!
Degli amici ci domandano se
Don Percuocolo se e vvero che a
fatte pezze per pubblicale il suo
faglio, feglio (pardon) foglio. Le
. pezze non sono sue ma di pochi
incoscienti e vili che non hanno il
coraggio di farsi conoscere per
chè sanno che la loro coscienza
j non è tanto pulita per affronta
! re dei galantuomini. Essi si so-
Ino comprato don Percuocolo il
| cui mestiere è stato e sarà sem
pre quello di vendersi al migliore
! offerente.
Per costui non vi sono scrupo
li, basta che gli si
vìvAliuavy? àii'àWòoii.-sfilii, ai li
bertinaggio, tradirebbe famiglia,
amici e Patria. Patria? Ma noi
non crediamo che egli abbia una
patria.
Chi non ricorda il suo attacca
mento pei- la esecrata Au
stria? E chi potrebbe oggi
! dubitare tenendo presente con
chi si ha a fare e del modo come
ì spende che non sia una spia Te
desca? Che iddio ce ne liberi da
questi avanzi della Società, diso
nore dell'umanità-
Ridete amici miei, dice che
vuole ragionare della Ragione ! e
quanto mai hai tu ragionato!
Questa volta caro il mio don Per
cuocolo l'hai detta grossa... Un
detto napolitano dice; Ornine è
cantina ciente pe carline...
Noi ti consigliamo di ecclissar
! ti come facesti in quell'altra cir
costanza se non vuoi che i buoni
della nostra colonia ti faccia
no un brutto servizio. Dì ai tuoi
padroni che si mostrino essi, e se
non lo fanno spontaneamente,
nei prossimi numeri li smasche
reremo noi.
Ciao.
Fra Manisco
Anno I No. 1 5 Soldi la Copia
Punte di spillo
CASSIERE LIBERTINO
E' cassiere ed oggi ha avuto
una velleità nuova; ha voluto di
ventare anche cassiere del gior
nale La Fogna.
Non è proprio un giornalista,
ma any-how —si può dare
l'aria di esserlo, pur non essendo
da tanto di fare un O col bicchie
re. Ma che vi pare? Cassiere di
banca ! Cassiere anche di un gior
ne, sia pure la Fogna! Non c'è
che dire: il Cassiere aveva biso
gno di un tantino di notorietà-
Concediamogliela.
# * $
\ enuto da Scranton anni fa,
in condizioni deplorevolissime,
con le scai-pe rotte e coi vestiti a
brandelli, il sempre generoso
Giuseppe Di Silvestro lo lece im
piegare presso una Cooperativa,
ed egli oggi, da quel serpentello
che è, degno consocio del degene
rato, lo ricambia con la calunnia
e con la diffamazione. Più tardi,
elevato alla onorifica carica di
cassiere, iniziò le sue imprese ga
lanti e gettò sul lastrico una ra
gazza, dopo averla disonorata, la
quale oggi tace nell'abbandono,
per non esporre al pubblico la
propria vergogna.
Intanto l'avventuriero audace,
locato ogni rimorso, si
ire a'óf impalmare una fanciulla"''
1 appartenente ad una famiglia ri
| spettabile, la quale certo ignora
! il turpe retroscena.
Un individuo così fatto non
poteva non simpatizzare col de
generato. Il degenerato è corio
; scinto da tutti per tale, da amici
ré nemici tutti lo disprezzano.
| (ili uomini di coraggio gli scara
ventano il loro disprezzo sul gru-
I gno ; i vili questo disprezzo lo
I sentono nell'intimo dell'animo,
ma atteggiano il viso ipocri
ta ad un sorriso di compiacenza.
11 degenerato! Oh, come appa
re oggi giustificato il violento lin
guaggio di Lucifero, a suo riguar
do, quel linguaggio che, all'epoca
delle pubblicazioni del Ribelle,
non andava a sangue ad alcuni
soverchiamente puritani. Scac
ciato dal natio paesello, dove, in
qualità di usciere di conciliazio
ne, commise atti poco puliti, ven
ne in America e s'impiegò in una
banca, donde fu messo fuori per
continue sottrazioni di franco
bolli.
Come il suo socio dell'ultima
ora, ingrato verso il benefattore,
lo attaccò vigliaccamente per fa
re le difese di un altro banchie
re con cui lavorava.
Nella campagna contro il con
sole Naselli, appoggiò i Di Silve
stro e Tresca ed i suoi articoli
sono sempre visibili; ma poi, per
chè impostogli dal suo padrone,
per ragione di pagnotta, come e
gli diceva, smise la pubblicazione
del giornale.
Falsa quindi la sua afferma
zione ultima che egli abbia sem
pre combattuto i fratelli Di Sil
vestro.
Parassita, ricattatore di ban
che, martirizzatole della propria
moglie; tale la figura morale di
colui che oggi il Cassiere liberti
no proclama un galantuomo-
Well !de gustibus.._ con quel
che segue. Ognuno può avere
quei gusti confacenti alla propria
natura, ed il sig. Cassiere perciò
ha anche il diritto di avere i suoi.
Così può farsi una cornice dei
gusti del degenerato e appender
sela a capo al letto, ora di scapo
lo, domani di ammogliato; e può
farsi di lui il modello al quale i-


# kiara\kiara\examples\data\text_corpus\La_Ragione\sn84037024_1917-04-25_ed-4_seq-1_ocr.txt
contro i vili, i camorristi, i sicari, i falsari e gli austriacanti, nemici della patria di e di quella d adozione.
F. SILVAGNI, Direttore, 1010 Christian Street, Philadelphia, Pa
Tanto per incominciare
La colonia vera, quella sana e composta della grande massa
che lavora e produce, è stanca di assistere a certi spettacoli, nei
quali prendono parte pochissimi protagonisti : spioni austriaci, si
carii, ricattatori, satiri, banchisti di pessimo carato et similia.
Convinti che un giornale fustigatore, ma veritiero sopratutto,
potrà spazzare tutto il luridume che si annida in certi lupanari,
diamo vita alla Ragione, che attingerà i suoi fondi dalle nostre ta
sche pulite e non dai depositi che i "calandrielli" affidano alle fauci
di volgari speculatoli. E la direzione l'affidiamo ad un operaio come
noi, al Signor Francesco Silvagni, anche perchè siamo sicuri che e
gii, se necessità lo richiedesse, con la punta dei suoi stivali saprà
mettere a posto mandanti e mandatario
A tutti i nostri fratelli dell'Ordine Figli d'ltalia, siano essi del
le autorità o semplici gregarii, una parola-
T nostri nemici hanno assunto al loro servizio, un degenerato,
un sicario, nella vana speranza di distruggere la nostra Istituzione,
le iniziative che ad essa faranno capo. Hanno fornito il sicario di
denaro perchè meglio possa riuscire alla vergognosa impresa. Essi
sono banchisti prossimi al fallimento; venditori di cerotti; consu
lenti legali di fallimenti; art...isti dall'anima prava; satiri, farabutti
insomma.
Tutti i veri Figli d'ltalia sanno il loro dovere. Se volete ricono
scerli li scorgerete dando uno sguardo alla Fogna; se non li sapete,
venite da noi e vi forniremo 1 nomi.
Raffaele Set tanni, Presidente; Nicola
Ri vano. Segretario; Antonino Viglio
ne. Tesoriere; Vito («allo, Francesco
Tropea, Carmine Del Giorno, Aristo
demo Palladino, Gaetano Gangenii, G.
Cai vare»e.
. Accettando, l'incarico
Non sono un ladro ragioniere di Banche, nè un sicario, nè un
satiro.
Accetto l'incarico che operai come me mi affidano e prometto
solennemente di fare scomparire da questo mondo coloniale, e per
sempre, tutti gl'insetti malefici.
Francesco Silvagni
IL NOSTRO PROGRAMMA
Sono trascorsi diciotto mesi da quando un gruppo di operai,
non volendo oltre tollerare un libellista degenerato, sfamato dal
l'oro tedesco per la sua propaganda antipatriottica, lo schiacciava
ignominiosamente dal seno del Comitato della Mobilitazione Civi
le, perchè egli aveva osato insultare una massa che rappresentava
il sentimento spontaneo della Colonia. Da quel tempo egli scom
parve dal consorzio degli uomini. Oggi ricomparisce sulla scena,
con la leva di persone degne dei più luridi bassifondi sodali-artisti
ci da strapazzo, banchisti sull'orlo del fallimento e simili, e noi
pure scendiamo nuovamente nell'agone, ma non per combattere
lui, che non merita neppure il nostro disprezzo, sibbene quelli che
si nascondono alle sue spalle.
Che questi siano dei baMchisti privati, lo si rileva dal fatto
che molto tempo prima di venir fuori il foglio libello, si diceva
pubblicamente in Colonia che il suo programma sarebbe stato
quello di combattere la Banca Statale Figli d'ltalia. Non solo, ma
il degenerato è andato da per ogni dove, mostrando i CHECKfrv
di coloro che s'erano sottoscritti per dar vita al suo giornale. Co
storo, contro questi privati banchisti, di cui in mille occasioni il
degenerato ha detto di conoscere la situazione finanziaria e di sa
pere '"he si mantengono sui trampoli, noi affileremo le nostre armi,
ingaggiando una guerra spietata, inesorabile, e quando non sarà
sufficiente il giornale, andremo per le vie e le piazze, predicando
alle masse, esortando i depositanti a ritirare i loro depositi che
corrono pericolo di andarsene in fumo.
Non tutti i banchisti privati di Filadelfia hanno sentito il biso
gno di fornire fondi segreti al degenerato, per comprarne il silenzio
e per accaparrarsene il patrocinio. Elenchiamo, a titolo d'onore, al
cuni banchieri che non ai son fatti prendere all'amo.
Francesco Cerceo. azionista della Sons of Italy State Bank.
Gennaio Di Genova, che sollecitato dal mestierante volgare a
dargli denaro, si rifiutava energicamente e si aveva, in cambio del
rifiuto, la minaccia di prossimi attacchi; Michele Berardini; Glor
iando Tumolillo; la Ditta Baldi, Pasquale Teti del 1022 Catharine
Street e qualche altro.
A questi la nostra ammirazione, agli altri il nostro disprezzo.
Noi siamo gli stessi operai dello scorso anno, che, per epura
re una volta per sempre la colonia dai rettili che la infestano, ci
presentiamo al pubblico con questo giornale LA RAGIONE, che
▼iene fatto a spese del popolo, con la contribuzione volontaria di
amili lavoratori.
LA DIREZIONE
ORGANO DI DIFESA DELLA ITALIANITÀ
A la gogna!
Una cricca di emeriti affaristi, j
composta esclusivamente di colo--,
10 che gettano il tozzo alla cagna
randagia, nel timore di essere es- j
si stessi addentati, ha rinnovato
11 miracolo della risurrezione di !
Lazzaro, riesumando dal fondo
di una misera tomba abbandona
ta, una putrida carogna che a-'
veva appestato il mondo con le
sue ribalderie.
E così dopo un lungo letargo
ed un'ignominiosa accidia, sbucai
fuori nuovamente dalla sentina
ove vivacchiò, rosicando come un ;
ratto delle chiaviche, questo de- 1
linquente abbrutito dalla mise- ì
ria, dal sudiciume e dalla crapu- !
la.
Continuando la sua vecchia
carriera di soldataccio di ventu-j
ra, al soldo di figure bieche e si-!
nistre, egli ha preso nuovarrien-j
te ad attaccare, per una vilissi
ma mercede, le migliori istituzio- ;
ni e gli uomini migliori della Co- ì
lonia, perchè egli per la sua indo- j
le prava è nemico dell'onesto e
del buono.
E' vergognoso per il nome ita- :
liano che una figura tanto moraj- ;
mente sinistra e bacata fino al |
midollo, trovi ancora in colonia;
della gente che, sebbene cento >
volte turlupinata, sia semprtf di
sposta a saziare gli immondi? au
"petili <fi lina lale~ràpacissim<y
pia- Questi generosi filantropi
che si tengono celati nell'ombra)
in omaggio ai precetti del Van
gelo, sono veramente mossi daj
un sentimento di pietà verso i fi- !
gli e la moglie di questo degene
rato, come essi affermano ge- !
suficamente ? Ed allora perchè j
affidano il danaro agli artigli ra- j
paci di lui che lo sciupa nelle bet- !
tole e nei postriboli mentre nel
la casa regna la miseria più
squallida ; nella casa ove sovente
egli bastona la infelice compa-1
gna cui oltre ai maltrattamenti
quotidiani, fece un regalo degno i
di lui, satiro abietto e feroce, nel |
periodo del puerperio?
E questa laida figura proteg- ;
gono i prominenti umanitari?;
Approvano forse il di lui opera-1
to? Ipocrita in tal caso, la scusa'
che li impietosisca la miseria j
dei figliuoli. Per quegli innocenti j
siamo pronti anche noi ad apri- 1
re una sottoscrizione per strap-j
parli all'abbiezione e alia mise-]
ria. Ma non è la pietà che muo
ve i provvedimenti dei fondi se
greti, essi sono sospinti da un
sentimento meno nobile, dal sen
timento della paura perchè non
ignorano che il degenerato pos
siede molti dei loro segreti che
vogliono nascondere al pubblico,
e perciò aizzano la cagna affama
ta contro i galantuomini, nel ti
more che questa non si avventi
alle loro calcagna e metta a nudo
le loro vergogne ed il loro passa
to. Questa la ragione che li ob
bliga a mordere il fieno e a su
bire continuamente i ricatti; il
famigerato redattore del nuovo
organo della camorra organizza
ta, che è libellista per mestiere
ed ha l'ingegno rivolto al male,
sa bene di potere attingere dana
ro in un pozzo senza fondo, dove
è il denaro dei clienti minghioni-
Ma la corda troppo tesa si
spezza e domani, messi con le
spalle al muro i ricattati potreb
bero ricorrere ad estremi rime
di per sbarazzarsi una volta per
sempre della sanguisuga insazia
bile. Sarebbe umano che un si
cario della penna vedesse
drizzarsi dinanzi minaccioso lo
spettro di un altro sicario eerto
non più ignobile di lui.
PHILADELPHIA, PA., 25 APRILE, 1917
Frattanto al presente il dege
nerato fila di nuovo l'idillio amo
roso e gode tutto il favore di
quelli che lo sostengono perchè
ne hanno paura. Quanto durerà
questa nuova fase della luna di
miele ? Oh ! non molto certamen
te e quest'altro tentativo giorna
listico del più ignorante e del più
disonesto tra tutti coloro che ab
biano mai vilipeso una penna,
è condannato come i precedenti
ad una vergognosa debacle.
Un mandato molto arduo ed
antipatico egli si è assunto que
sta volta, quello di combattere
un'lstituzione che è la più bella
conquista della massa operaia di
Philadelphia. Ha tentato bensì di
girare la quistione attaccando le
persone cui spetta il merito del
l'iniziativa della Banca Statale
Figli d'ltalia, ma il grossolano
stratagemma non ha ingannato
nessuno e tanto meno noi che
siamo le sentinelle vigili del no
stro Istituto. Il libellista, istiga
to dalla cricca, ci lancia il guan
to di sfida: noi lo raccogliamo
per gettarci a capofitto in una
lotta ad oltranza.
Egli, austriacante incorregibi
le che per la sua fellonia fu ver
gognosamente scacciato dal s'eno
del Comitato per la mobilitazio
ne civile, oggi che è entrata in
guerra anche l'America contro
l'Austria e la Germania vigliac
camente si rintana nei meandri
della sua nera coscienza ed osten
ta sentimenti che non ebbe mai.
perche già "da tempo' Venduto' al
l'oro tedesco. Ma noi gli strappe
remo la maschera.
Egli si accanisce oggi contro la
Banca Statale Figli d'ltalia e
pretende metterla in cattiva lu
ce, stabilendo un confronto tra
questa e la South Philadelphia
State Bank. Veramente se que
sta Istituzione non ha altri moc
coli da accendere, può bene anda
re a letto all'oscuro. Tutti in co
lonia ricordano gli attacchi fero
ci che il degenerato lanciò con
tro questa Banca un paio di anni
fa. Un lx>l giorno però gli attac
chi cessarono improvvisamente
ed il giornalista onesto con quel
ributtante cinismo che tutti gli
riconoscono potè vantarsi di ave
re venduto il suo silenzio " per
quattrocento dollaracci sonanti.
Più tardi, non molti mesi fa,
quando aspettava di essere im
piegato nella South Philadelphia
State Bank, e rimase deluso per
Patto energico del suo nuovo Pre
sidente, scrisse una lettera apo- !
logetica al consulente legale dei
fallimenti e di atroci insulti con- \
tro il capo di quella azienda ban
caria, conchiudendo che non ;
può dirsi buona una istituzione !
Il degenerato, spione austriaco
da' la sua solidarietà' agli Indipendenti
di Philadelphia
J-' 6 Clon &che quotidiane, forse per un senso di pudore, non lo
" e ' l ls cono anzi vi furono alcuni che se ne scandalizzarono, ma sia-
T in Ì^ rm j . a ' Ballo della Loggia Tripoli e Cirene del-
Indipendente, i cui profitti sono destinati al soddisfaci
fn rint i Un "? di Mr. Curiangiolo, a richiesta di alcuni capi
lap f'°. laa ! degenerato spione austriaco per fargli dire
neVi Jli dTUdiaGrande Venerabile di questo Stato dell'Ordi
ni, q " e i l . h di certi Indipendenti, se credono che essi si in
denigratoria e mercenaria di colui che,
liano r>m m t.'if U | C ** e < i l - a messo all'indice dal Comitato ita
sEirfS'J dl muhni a vento c l ue,la che il degenerato
ChP fìfr+i r S- 1 ? 1 !? 0 contro i nostri maggiori !
con o-iì «ninni n " v f s ' a corrispondenza di amorosi sensi
olii S ' T?™T tnaCh nie ì lte - di straordinario, e noi da
di abbattei In nLt™ come classificarli ; ma che essi credano
Sirio èÙL Hpit S foloss a le Organizzazione cu,, gli strali di un
ti di questo mtrinHiV° illusioni- Noi, anzi, siamo arciconten
gerriSriSipnte9?nnub!° che ci darà l'opportunità di strin
° piu fortemente attorno al nostro conduttore.
Filippo i! Corso Male
: se a capo di essa vi è un disone- •
sto il quale, secondo ciò che an
dava sbraitando in colonia, si e
ra arricchito ricettando merce
i-übata.
Oggi poi addita quella Banca
come una Banca modello e la no-
I stra, che non teme confronti, la
i chiama un'Agenzia di speculazio
ne.
Quando è il degenerato a trin
i ciare di simili giudizi, poco gio
| vano le laudi alla South Philadel
! phia State Bank ed a noi gli at
| tacchi e le insinuazioni non nuoc
j ciono affatto, anzi, ci accaparra
! no ogni giorno nuove simpatie.
Raffaele Settanni.
Don Percuocolo
c'o pizzo!!!
Degli amici ci domandano se
j Don Percuocolo se e vvero che a
ì fatte pezze per pubblicale il suo
! faglio, feglio (pardon) foglio. Le
] pezze non sono sue ma di pochi
j incoscienti e vili che non hanno il
1 coraggio di farsi conoscere per
i ! chè sanno che la loro coscienza
I non è tanto pulita per affronta
! re dei galantuomini. Essi si so
no comprato don Percuocolo il'
! cui mestiere è stato e sarà sem
pre quello di vendersi al migliore
offerente.
Per costui non vi sono scrupo
li, liasta 'eli' 1 iTiTtWi —ztfTt*
per dedicarsi alF alcoolismo, al li
bertinaggio, tradirebbe famiglia,!
amici e Patria. Patria? Ma noi
non crediamo che egli abbia una
! patria.
Chi non ricorda il suo attacca
mento per la esecrata Au
stria? E chi potrebbe oggi
| dubitare tenendo presente con
chi si ha a fare e del modo come
| spende che non sia una spia Te
desca? Che iddio ce ne liberi da!
questi avanzi della Società, diso
nore dell'umanità-
Ridete amici miei, dice che
i vuole ragionare della Ragione ! e
! quanto mai hai tu ragionato !
i Questa volta caro il mio don Per
cuocolo l'hai detta grossa... Un
detto napolitano dice; Omnie è
cantina ciente pe carline...
Noi ti consigliamo di ecclissar
! ti come facesti in quell'altra cir
| costanza se non vuoi che i buoni
| della nostra colonia ti faccia
no un brutto servizio. Dì ai tuoi
padroni che si mostrino essi, e se
i non lo fanno spontaneamente,
! nei prossimi numeri li smasche
j re remo noi.
Ciao.
Fra Manisco
Anno I No. 1 5 Soldi la Co:
Punte di spilli
CASSIERE ÜBERTINO 4
\
E' cassiere ed oggi ha
una velleità nuova; ha voluto
ventare anche cassiere del
naie IJH Fogna.
Non è proprio 1111
ma any-how si
l'aria di esserlo, pur non
da tanto di fare un O col
re. Ma che vi pare? Casini
banca ! Cassiere
ne, sia pure la
che dire: il Cassiere
gno di un tantino
Concediamogliela.
Venuto da
in condizioni
con le scarpe rotte e
brandelli, il sempre
Giuseppe Di Silvestro lo
piegare presso una Coopem
ed egli oggi, da quel vjÉM
che è, degno
rato, lo ricambia
e con la
elevato alla onorifica
cassiere, iniziò le sue impi X
lanti e gettò sul lastrico fl
gazza, dojK) avel la disonoJM
quale oggi tace
per non esporre aV'i
propria vergogna^
Intanto l'avVeìltu^B
V ; n„
Jge ad impalmare una faraPM
! appartenente ad una famiglia
spettabile, la quale certo ignori
il turpe retroscena. fl
lin indivìmio così fatto
ixiteva non simpatizzare coljlH
generato. Il degenerato àgiÉH
sciuto da tutti per tale,
•e nemici tutti lo
Oli uomini di coraggio
ventano il loro disprezzo
Igno; i vili questo disnrezznj
I sentono nell'intimo dell'adi
ma atteggiano il viso
ta ad un sorriso di compi^B
Il degenerato! Oh,
re oggi giustificato il
guaggio di Lucifero, a suo
do, quel linguaggio che,
delle pubblicazioni del IflJ
non andava a sangue ad
soverchiamente puritani
ciato dal natio paesello,
qualità di usciere di
ne, commise atti poco puliti, vet«
ne in America c s'impiegò in uiw
banca, donde fu messo
continue sottrazioni di fia|
Ixilli.
Come il suo socio dell'ultimS
ora, ingrato verso il benefattoràl
lo attaccò vigliaccamente per 1»(
re le difese di un altro banchie
re con cui lavorava.
Nella campagna contro il con- ,
sole Naselli, appoggiò i Di Silve-1
stro e Tresca ed i suoi articoli
sono sempre visibili; ma poi, per
chè impostogli dal suo padrone,
per ragione di pagnotta, come e
gli diceva, smise la pubblicazione
del giornale.
Falsa quindi la sua afferma
zione ultima che egli abbia semi|
pre combattuto i fratelli Di SiH
vestro.
Parassita, ricattatore di ban
che, martirizzatore della propria
moglie ; tale la figura morale di
colui che oggi il Cassiere liberti
no proclama un galantuomo-
VVell! de gustibus... con quel
, che segue. Ognuno può avere
quei gusti confacenti alla propria
natura, ed il sig. Cassiere perciò
ha anche jl diritto di avere i suoij
Così può farsi una cornice de|
gusti del degenerato e appender
sela a capo al letto, ora di scapo
lo, domani di ammogliato; e può
farsi di lui il modello al quale i-


# kiara\kiara\examples\data\text_corpus\La_Ragione\sn84037024_1917-05-05_ed-1_seq-1_ocr.txt
contro i vili, i camorristi, i sicari, i falsari e gli austriacanti, nemici della patria di origine e di quella d' adozione
F. SI LV AGNI, Direttole, 911 Christian Street, Philadelphia, Pa-
LA RAGIONE SI AFFERMA
i
Il primo numero de La Ragione non poteva non incontrare il fa-1
vore del pubblico che l'aspettava ansiosamente.
Un giornale come il nostro, fatto da operai con denaro suda
lo: un giornale che ha intrapreso una campagna di epurazione,
scoprendo le piaghe purulenti di cui sono infette le poche putride
carogne che hanno lanciato il degenerato all'assalto contro le no
stre maggiori istitu/.ionii : che ha richiamata l'attenzione della no
stra colletflività sui reati nascosti nei bassamente che ha fatto
schioccare la frusta fustigatrice sui grugni incalliti di uomini
senza coscienza, preparatori e consulenti legali di fallimenti; che
ha dato il salutare allarme contro i banchisti pronti a fuggire al Ca
nada: che ha promesso di fare la storia vergognosa di tutti i Cu
riangiolo e loro luogotenenti e sgombrare la colonia di una com
briccola di malviventi: un giornale cosi fatto, dicevamo, non pote
va non incontrare le simpatie della nostra massa che aspira ad una
colonia italiana rigenerata.
E la gara dei lettori r.-in ha avuto riscontro nella storia degli
annali del giornalismo coloniale. Ia 1 copie sono state pagate lino ad
un dollaro ciascuna. Il primato nella vendita di esse lo ha avuto il
nostro TURIDDU il quale, con le sue grida: BANCHISTI CHE
SCAPPANO Al. CANADA : CONIATORI DI MONETE FALSE;
CONSULENTI LEGALI DI FALLIMENTI, la sera in cui vide la lu
ce La Ragione richiamava attorno a sè, lungo Christian St., una fol
la di passanti che aveva con ansia indicibile attesa l'uscita di questo
giornale.
Intanto, coloro che armarono la penna del sicario, ne han
no risentito tutto l'effetto, perchè si sono subito scatenate minacce
di arresti in mas-a: arresti isolati; insulti contro i nostri compila
tori lungo le strade, fuori le farmacie e nelle alcove delle carogne;
tanto fiele e tanta bile si son viste spruzzare dai musi degli ORAN
GOTANGHI, dalle facce ingiallite e dalle occhiaie come caverne,
indizio questo della vita di vizio e di degenerazione nella quale essi
vivono.
Agli attacchi del degenerato, fatti al nostro Ordine ed ai
suoi capi, noi che pure avremmo avuto ragione, perchè provocati, di
assestargli un MAN ROVESCIO sulla sua facciaccia abbrutita, sia
mo rimasti calmi. Perchè dunque, tanto clamore; perchè i mandata
rii del sicario si sono risentiti delle nostre difese e dei nostri giusti
con t ro-a t tacchi ?
I>e coscienze tranquille restano calme, le coscienze traviate si
agitano!
Noi abbiamo ragione di credere che i mandatarii, affidando Li
loro causa al degenerato spione austriaco, hanno commesso un gran
dissimo errore. Essi, specialmente i banchisti prossimi al fallimen
to, dei quali i nosfei amici e nostri fratelli debbono guardarsi, me
glio avrebbero fatto che quelle poche centinaia di dollari, consegna
te al sicario, le avessero portate seco loro insieme al bottino pronto
a valicare l'Oceano. Ma che volete? Anche Pati, il banchista milio
nario di New York, prima di prendere il volo si accaparrò qualche
giornale della metropoli per farsi cantar le laudi. Niente di sor
prendente, perciò, se anche i furfanti, di questa città hanno voluto
imitare il loro collega newyorkese.
La Ragione è uscita e rimarrà sulla breccia, sempre che i man
tanti continueranno ad armare la mano del loro sicario. Vi è anzi Li
probabilità che questo giornale si trasformi in meglio e resti peren
ne minaccia contro gli sfruttatori delle nostre colonie.
Il nostro atteggiamento dipenderà dunque dalle mosse della
piccola combriccola di furfanti dalle unghie adunche. Ad essi la !
scelta, a noi i mezzi per annientarli, per distruggerli.
NOI.
AVVISÒ AI LETTORI
Nei primi due numeri della Ragione abbiamo prospettato al
pubblico le losche figure di cui alcuni prominenti si servono per ten
tare di abbattere i buoni uomini e le buone Istituzioni. Per l'avve
nire passeremo in rassegna, uno per uno, tutti coloro che han prov
veduto i fondi segreti, illustrandone le gesta compiute in America
did giorno del loro arrivo e rievocando anche la loro vita nella pa
tria di origine.
Sarà umi rubrica interessantissima. Molti scritti già pronti
dobbiamo rimandare per esuberanza di materia. Fra essi vi è una
lettera aperta a Fmak L. Garbarono, detective federale, al quale si
indica uno spione austriaco che vive nella nostra colonia.
LA REDAZIONE
Circolare del Grande Concilio
DI PENNSYLVANIA DELL'ORDINE F. d'l. in AMERICA
Questo Ufficio ha deciso di fare appello alle Logge per potei'
assolvere a un duplice dovere: quello di gratitudine per l'atteggia
mento della Nazione che ci ospita, e l'altro di riconoscenza verso il
condottiero del nostro esercito, che deve riconquistare all'ltalia le
terre che ci appartengono.
Pertanto è stato stabilito di offrire una medaglia d'oro al Ge
nerale Cadorna, a nome dei Figli d'ltalia della Penssylvnia; ed una
bandiera alla Divisione del Colonnello Roosevelt o a quella Divisione
dell'esercito americano che prima salperà alla volta dell'Europa.
A tale scopo sollecitiamo le Logge a far pervenire subito le loro
offerte a questo Uffizio, iniziando anche delle sottoscrizioni tra i
fratelli ; con avvertenza che la medaglia al Generale Cadorna saia
mandata per mezzo della Commissione Italiana che si reca in mis
sione presso il Governo degli Stati Uniti.
Si raccomanda poi alle Logge di Philadelphia e dintorni di te
nersi pronte per aderire a quella iniziativa che dal Regio Console
Cav. Uff. G. Poccardi sarà per prendersi in onore della Commissio
ne suddetta, se e quando sarà a Philadelphia, cosa che per motivi di
facile intuizione non si può anticipatamente stabilire. A tal uopo,
appena i Venerabile riceveranno nuovo avviso, chiameranno imme
diatamente tutti i fratelli con invito d'urgenza.
Questo Ufficio aveva anche fatto le pratihee per poter concor
rere ai ricevimenti ai rappresentanti delle nazioni alleate: francese
ed inglese. Essendoci stato risposto che parate non ve ne saranno
ed essendo incerti del giorno ed ora di arrivo non si sono potuti fa
re dei preparativi. Però tutti i Figli d'ltalia si riuniran
no nel luogo che sarà indicato dalla stampa americana
ed italiana. Il Grande Concilio intanto e la Commissione F. U. M.
contribuiscono alla sottoscrizione del giornale North American per
offrire una spada d'onore al Generale Joffre.
ORGANO DI DIFESA DELLA ITALIANITÀ
E' notte alta e profonda; la piog- ■
già viene giù impetuosa dal cielo 1
foschissimo e il vento pare che I
mormori voci di minaccia e di mi- I
stero. Le vie della città sono in- '
teramente deserte, poiché la not
te incute spavunto ed il freddo è '
intensissimo. ' '
All'improvviso, come se . sbu
casse dal sottosuolo, appare in
fondo ad una strada un solitario :
nottambulo. Gesticola, procode !
a ZIG-ZAG, poiché ha non meno
di un palmo di vino nello stomaco
e le gambe mal sostengono il i>e
so soverchio del co-po.
Non ha alcun riparo contro la
pioggia che continua a scro sciare
abbondante, inzuppandolo fiìio
alle ossa, ma egli procede i:n
perterrito sotto la bufera, e con !
la lingua grossa e con la voce
rauca degli übbriachi fradici cin
ta una oscena canzone.
Al lume di un fanale si scoile
per un momento la sua faceta;
lo sguardo semispento in un vlt'o
livido, incute semplicemente ri
brezzo.
E' il degenerato,
celebre nell'elemento losco co;c>-
niale, che, dopo una giornata -di
crapule nei saloons e nei ris fo
ranti e mezza nottata d'i orgia,
nel lupanare di Fiorimi, stai
ma non sazio, si riporta come pùo
nella meschina dimora, ove la po
vera moglie e i figli derelitti lo
attendono, morenti di freddo e j
di fame, perchè privi di carbone
e di cibo.
Giunto dinanzi alla porta, do
po aver tentato inutilmente di a
prilla, batte vigorosamente, ac
compagnando i ripetuti colpi con
le più triviali ingiurie all'indiriz
! zo della moglie.
La porta si apre e compare sul
la soglia una figura gentile di ;
donna, mite e rassegnata come u-1
na martire. Non versava lagrime ;
ma portava il segno di averne j
spar e tante e l'ubriaco, dandole
un sonoro ceffone, le grida 1 ab- j
biosamente : Dove ti eri cacciata,
strega del diavolo, che e. mi hai j
fatto attendere due ore con que- j
sto tempaccio di inferno? Ti ho
ripetuto le mille volte che devi
attendermi alla porta, perchè tu
sei la mia schiava e devi obbedir
mi ciecamente
Taci ; non voglio che mi ri- 1
sponda, perchè la tua voce mi !
suona all'orecchio come un catti-1
vo augurio. Preparami un fiasco i
sul tavolo, perchè ho una sete da i
tedesco e poi conduci a letto la !
canaglia e liberami anche della j
tua odiosa presenza.
Oh a proposito, tieni a
niente che, rimangono nel celiar
ancora otto fiaschi. March !
« * *
Ed ecco il mostro, sconcia
mente sdraiato su di una vecchia :
poltrona, tutto assorto nella con
templazione del fiasco che ha già
vuotato a metà e del bicchiere ri
colmo.
Maledetta esistenza! egli dice
a se stesso. Non è a credere
quanto soffro tutte le volte che
debbo venirmi a rinchiudere in
questa topaia che mi sembra una !
cella di morte. La vista di questa
strega e di questi marmocchi, che
sono per me una croce pesantis
sima, mi indispettisce a quel Dio
Come è dura la pelle di questa
donna; sembra che abbia, come i
gatti, sette anime ed un'animel
la. La meschina dote che mi por
tò, quando commisi l'enormità di
sposarla, non durò più di un me
PHILADELPHIA, PA., MAGGIO, 1917.
Le visioni fosche e terrorizzanti
DEL DEGENERATO
so; non so comprendere quindi
perchè debba essere tanto lunga
la sua esistenza. Non potrebbe
finalmente decidersi a fare un
viaggio senza ritorno?
Bah ! in attesa che il fausto av
venimento si compia, beviamoci
sopia
Questo vino non è cattivo, e di
venta squisito se si pensa che
non costa un centesimo. Quel
mascalzone del venditore potrà
fare a meno di mandarmi il suo
8i11....
Bravo amico, quel cassiere; a
micone sincero ed effezionato; io
e lui siamo compagni indissolu
bili di erotiche gesta : ejjli ha gli
stessi miei gusti; è l'unica cosa
sennata e giusta che abbiano
. stampato quegli imbecilli della
Ragione. Mangiamo allo stesso
piatto; beviamo allo stesso bic
chiere e Fiorina può farne fede.
I.'amico cassiere è molto fur
bo; egli sta per fare una magnifi
ca speculazione matrimoniale; se
il colpo riesce, ci sarà da sciala
re a lungo anche per me. Co
si fossi scapolo io pure! Chi sa
però se mi sarebbe ancora possi
bile di trovare un'altra moglie
con un buon gruzzolo! Bah! per
chè no? Dopo tutto ci sarebbe
Fiorimi, la quale forse non rifiu
terebbe là'mia nian'o; essa rìt've
aver pezze a bizzeffe e son sicu
ro che saprebbe guadagnarne an
j cora dopo il matrimonio. Ma è i
nutile pascersi di illusioni; la for
1 tuna non mai mi è stata secon
da ed anche al presente mi nega
i suoi favori, allungando la vita
di questa megera, che mi ama
reggia e mi indispettisce con la
sua rassegnazione ipocrita.
11 manigoldo, così monologan
do, aveva quasi vjsto il fondo del
! fiasco, ma il soverchio vino lo a-
J veva ridotto una massa insensi-
I bile ed inerte. Piegò il capo sul
I petto e chiuse gli occhi.
I fumi dell'alcool gli gorgoglia
vano vorticosamente nel cranio e
j la camera, tutta all'intorno, ap
| pai'iva alla mente ottenebrata
dell'ebbro ripiena di bianchi fan
tasmi. 1 quali, dapprima indistin
ti, assunte a poco a poco figure
nette e decise, incominciarono a
danzare una macabra danza.
Uno di essi, uscendo dalla
schiera, gli si avvicina e lo guar
da a lungo con uno sguardo tra
! la compassione ed il disprezzo. Iti
[quel fantasma il degenerato rico
nosce la moglie morta e sulla
! massa inerte passa un lungo bri
vido.
Dopo morta ancora mi per
seguiti ? parea volesse gridar
le l'ubriaco, atterrito dalla im
provvisa apparizione. Tu non
hai nulla a rimproverarmi, giac
ché quando avesti la felice ispi
razione di andartene in un mon
do migliore, io non mancai di far
ti il funerale che poi mi feci un
dovere di non pagare come non
_ y:>i' ;i ■ 11 medico che mi portò
in Corte e ti accompagnai
perfino al camposanto. Levamiti
ì quindi dinanzi e fammi gustare
in santa pace quest'altro bicchie-
I re.
* * ♦
Dileguossi lo spettro della
donna, forse atterrito da tanto
cinismo, ma in quel posto mede
simo sorse il fantasma di un te
nero giovanetto, che levò il pic
colo dito minaccioso quasi a sfio
rare la testa scarniigliata, gron
dante il freddo sudore.
La tenibile visione scosse quel
corpo arrove ciato, come se lo a
vesse attraversato una corrente
elettrica, e parve che su quella
fronte violacea passasse un lam
po di rimorso; ma fu solo un i
s tante.
Osi, o sciagurato, minaccia
re tuo padre? Fu colpa mia forse
se tu peristi tra le fiamme? E'
vero che la Compagnia del Gas
aveva in quell'epoca chili o la
corrente, perchè io avevo trascu
rato di pagare il HILL, ma io a
vevo comperato le candele, ed
anche le candele fanno luce, né
potevo prevedere che la fiamma
avrebbe appiccato l'incendio alla
casa
* * *
Padre tu ? Questo nome scin
to e dolce, nella tua bocca, suo
na profanazione e bestemmia.
No, tu non hai visceri paterne, tu
che il denaro disonestamente lu
crato, sciupi in bagordi, e l'ai
morire di fame la povera lami
glia. Tu non sei padre, non sei
marito; tu fosti il carnefice di
mia madie, ed oggi sei l'aguzzi
no dei miei fratelli e della pove
ra donna che, per sua eterna di
sgrazia, ha occupato il posto la
sciato vuoto dall'altra, che oggi
finalmente, libera dei tuoi ceppi,
gode un po' di pace e di tranquil
lità.
Tu non hai diritto di dirti
padre ; i padri lavorano pei loro
figli, ed in casa li educano con la
virtù e con l'esempio. Tu non hai
mai goduto la sublime soddisfa
.ÌÌVM. 'I -*»> /'
proprio dovere; tu non hai senti
to mai affetto pei figli.
Quando perii tra le fiamme,
io povera vittima della tua depra
vazione, tu non volesti neppure
accompagnarmi alla mia piccola
fossa, forse per dare ad intende
re a qualche imbecille che non sa
0 finge d'ignorare di (piai fango
sei impastato, che ti tratteneva
in casa la profondità del dolore.
1 Ma io che tutto vidi, io che nel
' l'altro mondo mi sentii dotalo di
iun discernimento superiore ai
I miei teneri anni, compresi tutto
'il marcio della tua ipocrisia.
Quell'ombra di rimorso che si ce
la nel fondo dell'anima dei mag
giori delinquenti, ti impedì per il
momento di mostrarti alla luce
del sole, ma alla notte, protetto
dalle tenebre amiche, non sape
sti rinunziare alla consueta visi
ta a Fiorina. E l'orgia del satiro
immondo fu l'unico tributo di
dolore che sapesti dare alla mia
morte immatura; ed allora io ti
maledissi, come ti maledico ora,
o carnefice di mia madre e mio
assassino, o aguzzino dei miei po
veri fratelli.
Dopo questo anatema tremen
do, anche il secondo fantasma
scomparve, ma il corpo inerte eb
be un lungo fremito. Aperti gli oc
chi a fatica, stese la mano al bic
chiere, elo tracannò di un sol
fiato, per affogare meglio nel vi
no i ricordi terrorizzanti.
* # #
Ricaduto poscia nella primiti
va immobilità, altri fantasmi gli
danzavano attorno la ridda. Que
sta volta erano innumerevoli, ed
egli, abbracciandoli con un lungo
sguardo, avendo riconosciuto in
essi l'esercito dei suoi creditori,
scoppiò in una sonora sgignazza
ta.
Anche qui pareva gri
dasse la voce interna anche
qui venite a rompermi le scatole,
per poche migliaia di miserabili
dollari che ebbi la bontà di pren
dere in prestito da voi. (io to
hell ! disgraziati, e ficcatevi in te
sta, per vostra norma, che il fi
glio di mio padre non ha mai ne
gato nulla a nessuno. E quando
Anno I No. 2 •"> Soldi la Copia
tutti sarete stati soddi tatti, a
vete altro a pretendere? No. Le
vatevi dunque dai miei piedi, s<>
non volete che vi scacci ignomi
niosamente, o mercanti profana
tori.
E la fitta schiera, o impaurita
dalla minaccia o soddisfatta del
la promessa, ritirossi, lasciando
lo solo, coi residui della sua orgia
solitaria.
Ed allora il sinistro eroe levi is
si dalla sedia ove pareva inchio
dato, ed allungando le braccia, fi
mise un lungo, sonoro sbadiglio.
Sono diventato (orse un ra
dazzo, che mi metto paura delle Jp
ombre? La mia coscienza è l>en
nascosta sotto un duro, fittissi
mo pelame ed i rimorsi non vi al
lignano. Sciocco è veramente chi
si illude che nell'animo mio po ;-
sa albergare affetto o ricono
scenza. Questa canaglia che mi
paga per t'armisi cantare le laudi
false e bugiarde, questi briganti
che mi aizzano alle calcagna dei
loro nemici e credono disobbli
garsi con una manata di dollari,
10 li odio di un odio profondo ed
inestinguibile.
Per adesso son costretto a
lai' tacere i ruggiti della mia a
nima, perchè essi pagano profu
matamente le mio orgie ed ali
mentano i miei bisogni. Ma guai
ad essi se riuscirò ad emancipar
mi! So tante delle loro porcherie
da poterli ridurre un mucchio di
cenci. E ve li ridurrò pdtarla
M . perchè troppo mi umilia
no colla loro t'orzata, millantata
' :'V ■' "-T
--rà terribile, inesorabile !
♦ ♦ ♦
11 degenerato arrotava i denti
in preda ad una rabbia bestiale.
Il vino ingoiato fuori misura, do
po lunga fermentazione veniva
fuori a fiotti insieme col cibo, ed
egli, colla faccia riversa, cad
de sui rifiuti del suo stomaco,
rotolandovisi sconciamente come
11 porco nel braco.
Fuori la pioggia continuava a
scrosciare impetuosa, ed il vento
fischiava sinistramente, mentre
nel cielo plumbeo apparivano i
primi albori indistinti di una
giornata nera come l'anima del
l'alcoolizzato
Il novelliere.
Afferratelo
per la coda...
In Colonia si vocifera che il
degenerato si stia preparando a
fuggire da l'hiladelphia portando
seco i pochi ehecks rimastigli dei
banchisti prossimi al fallimento
e degli altri bacati pari suoi. La
voce pare che sia fondata, sebbe
ne lanciata a nio' di sospetto dal
cavadenti delle nove strade che
piange i suoi cento dollari, e
corroborata da un altro colono
molto addentro nelle sporche se
grete cose.
Gli interessati restano avvisa
ti.
Trasferimento
La barberia del nostro diretto
le fratello Francesco Sxlvagni r
coi relativi uffici de La Ragione
si sono trasferiti al No. 911 Chri
stian St.
Gli amici, i fratelli, si ricordi
no che il nostro direttore è l'uni
co barbiere che taglia i capelli
artisticamente ed è specialista
per il taglio delle unghie adunche
' dei carbonari che non hanno ri
tegno di approfittarsi da due so
| di in su.


# kiara\kiara\examples\data\text_corpus\La_Ragione\sn84037024_1917-05-05_ed-2_seq-1_ocr.txt
LA RAGIONA
ORGANO DI DIFESA DELLA ITALIANITÀ
contro i vili, i camorristi, i sicari, i falsari e gli austriacanti, nemici della patria di origine e di quella d'adozione.
F. SII A'AGNI, Direttore, 911 Christian Street, Philadelphia, I'a-
LA RAGIONE SI AFFERMA
Il primo numero de La Ragione non poteva non incontrare il fa-1
vore del pubblico che l'aspettava ansiosamente.
Un giornale come il nostro, fatto da operai con denaro suda- j
to; un giornale che ha intrapreso una campagna di epura/ione, !
scoprendo le piaghe purulenti di cui sono infette le poche putride j
carogne che hanno lanciato il degenerato all'assalto contro le no
stre maggiori istituzioni; che ha richiamata l'attenzione della no
stra colletflività sui reati nascosti nei bassamente; che ha fatto
schioccare la frusta fustigatrice sui grugni incalliti di uomini
senza coscienza, preparatori e consulenti legali di fallimenti; che
ha dato il salutare allarme contro i banchisti pronti a fuggire al Ca
nada; che ha promesso di fare la storia vergognosa di tutti i Cu
ciangiolo e loro luogotenenti e sgombrare la colonia di una com
briccola di malviventi; un giornale così fatto, dicevamo, non pote
va non incontrare le simpatie della nostra massa che aspira ad una
colonia italiana rigenerata.
E la gara dei lettori non ha avuto riscontro nella storia degli
annali del giornalismo coloniale. Le copie sono state pagate fino ad
un dollaro ciascuna. Il primato nella vendita di esse lo ha avuto il
nostro TI RIDDI' il quale, con le sue grida: BANCHISTI CUF
SCAPPANO AL CANADA'; CONIATORI DI MONETE FALSE;
CONSULENTI LEGALI DI FALLIMENTI, la sera in «àvide la lu
ce La Ragione richiamava attorno a sè, lungo Christian St., una fol
la di passanti che aveva con ansia indicibile attesa l'uscito di questo
giornale.
Intanto, coloro che armarono la penna del sicario, ne han
no risentito tutto l'effetto, perchè si sono subito scatenate minacce
di arresti in inasta; arresti isolati; insulti contro i nostri compila
toli lungo le strade, fuori le farmacie e nelle alcove delle carogne;
tanto liele e tanta bile si son viste spruzzare dai musi degli ORAN
GOTANGHI, dalle facce ingiallite e dalle occhiaie come caverne,
indizio questo della vita di vi. '<> e Ji degenerazione nella quale essi
vivono.
Agli attacchi del degenei i, latti al nostro Ordine ed ai
suoi capi, noi che pure avremi o avuto ragione, perchè provocati, di
assestargli un MAN ROVESCIO sulla sua facciaccia abbrutita, sia
mo rimasti calmi. Perchè dunque, tanto clamore; perchè i mandata
li i del sicario si sono risentiti delle nostre difese e dei nostri giusti
contro-attacchi?
Le coscienze tranquille restano calme, le coscienze traviate si j
agitano!
Noi abbiamo ragione di credere che i mandatarii, affidando la
loro causa al degenerato spione austriaco, hanno commesso un gran
dissimo errore. Essi, specialmente i banchisti prossimi al fallimen
to, dei quali i nostri amici e nostri fratelli debbono guardarsi, me
glio avrebbero fatto che quelle poche centinaia di dollari, consegna
te al sicario, ie avessero portato seco loro insieme al bottino pio,«to'
a valicare l'Oceano. Ma che volete? Anche Pati, il banchista milio
nario di New York, prima di prendere il volo si accaparrò qualche
giornale della metropoli pei- farsi cantar le laudi. Niente eli sor
prendente, perciò, se anche i furfanti di questa città hanno voluto
imitare il loro collega newyorkese.
La Ragione è uscita e rimarrà sulla breccia, sempre che i man
tanti continueranno ad armare la mano del loro sicario. Vi è anzi la
probabilità che questo giornale si trasformi in meglio e resti peren
ne minaccia contro gli sfruttatori delle nostre colonie.
Il nostro atteggiamento dipenderà dunque dalle mosse della
piccola combriccola di furfanti dalle unghie adunche. Ad essi la 1
scelta, a noi i mezzi per annientarli, per distruggerli.
NOI.
AVVISO AI LETTORI
Nei primi due numeri della Ragione abbiamo prospettato al
pubblico le losche ligure di cui alcuni prominenti si servono per ten
tare di abbattere i buoni uomini e le buone Istituzioni. Per l'avve
nire passeremo in rassegna, uno per uno, tutti coloro che han prov
veduto i fondi segreti, illustrandone le gesta compiute in America
dal giorno del loro arrivo e rievocando anche la loro vita nella pa
tria di origine.
Sarà una rubrica interessantissima. Molti scritti già pronti
dobbiamo rimandare per esuberanza di materia. Fra essi vi è una
lettera aperta a Frnak L. Garbarino, detective federale, al quale si
indica uno spione austriaco che vive nella nostra colonia,
jr - LA REDAZIONE
Circolare del Grande Concilio
DI PENNSYLVANIA DELL'ORDINE F. d'l. in AMERICA
Questo Ufficio ha deciso di fare appello alle Logge per poter
assolvere a un duplice dovere: quello di gratitudine per l'atteggia-*
mento della Nazione che ci ospita, e l'altro di riconoscenza verso il
condottiero del nostro esercito, che deve riconquistare all'ltalia le
terre che ci appartengono.
Pertanto è stato stabilito di offrire una medaglia d'oro al Ge
nerale Cadorna, a nome dei Figli d'ltalia della Penssylvnia; ed una
bandiera alla Divisione del Colonnello Roosevelt o a quella Divisione
dell'esercito americano che prima salperà alla volta dell'Europa.
A tale scopo sollecitiamo le Logge a far pervenire subito le loro
offerte a questo Uffizio, iniziando anche delle sottoscrizioni tra i
fratelli ; con avvertenza che la medaglia al Generale Cadorna sarà
mandata per mezzo della Commissione Italiana che si reca in mis
sione presso il Governo degli Stati Uniti.
Si raccomanda poi alle Logge di Philadelphia e dintorni di te
nersi pronte per aderire a quella iniziativa che dal Regio Console
Cav. Uff. G. Poccardi sarà per prendersi in onore della Commissio
ne suddetta, se e quando sarà a Philadelphia, cosa che per motivi di
facile intuizione non si può anticipatamente stabilire. A tal uopo,
appena i Venerabile riceveranno nuovo avviso, chiameranno imme
diatamente tutti i fratelli con invito d'urgenza.
Questo Ufficio aveva anche fatto ie pratihee per poter concor
rere ai ricevimenti ai rappresentanti delle nazioni alleate: francese
ed inglese. Essendoci stato risposto che parate non ve ne saranno
ed essendo incerti del giorno ed ora di arrivo non si sono potuti fa
re dei preparativi. Però tutti i Figli d'ltalia si riuniran
no nel luogo che sarà indicato dalla stampa americana
ed italiana. Il Grande Concilio intanto e la Commissione F. U. M.
contribuiscono alla sottoscrizione del giornale North American per
offrire una spada d'onore al Generale Joffre.
PHILADELPHIA, PA„ 5 MAGGIO, 1917.
Le visioni fosche e terrorizzanti
DEL DEGENERATO
E' notte alta e profonda; la piog- j
già viene giù impetuosa dal cielo
foschissimo-e il vento pare che
mormori voci di minaccia e di mi
stero. Le vie della città sono in
teramente deserte, poiché la not
te incute spavento ed il freddo è
intensissimo.
All'improvviso, come se sbu
casse dal sottosuolo, appare in
fondo ad una strada un solitario
nottambulo. Gesticola, procede
a ZIG-ZAG, poiché ha non meno
di un palmo di vino nello stomaco
e le gambe mal sostengono il pe
so soverchio del corpo.
Non ha alcun riparo contro la
pioggia che continua a scrosciare
abbondante, inzuppandolo fino
alle ossa, ma egli procede im
perterrito sotto la bufera, e con
la lingua grossa e con la voce
rauca degli übbriachi fradici can
ta una oscena canzone.
Al lume di un fanale si scorge
per un momento la sua faccia;
lo sguardo semispento in un viso
livido, incute semplicemente ri
brezzo.
E' il degenerato, sinistramente
celebre nell'elemento losco colo
niale, che, dopo una giornata di
crapule nei saloons e nei risto
ranti e mezza nottata d'i orgia,
nel lupanare di Fiorimi, stanco
ma non sazio, si riporta come può
nella meschina dimora, ove i-_ in
vera moglie e i 'u f
attendono, morenti di freddo e
di fame, perchè privi di carbone
e di cibo.
Giunto dinanzi alla porta, do
po aver tentato inutilmente di a
prilla, batte vigorosamente, ac
compaginando i ripetuti colpi con ;
le più triviali ingiurie all'indiriz
zo della moglie.
La porta si apre e compare sul-1
la soglia una figura gentile di
donna, mite e rassegnata come li
na martire. Non versava lagrime
ma portava il segno di averne
spar e tante e l'ubriaco, dandole
ttn sonoro ceffone, le grida rab
biosamente: Dove ti eri cacciata,
strega del diavolo, che mi hai
fatto attendere due ore con que
sto tempaccio di inferno? Ti ho
ripetuto le mille volte che devi
attendermi alla porta, perchè tu
sei la mia schiava e devi obbedir
mi ciecamente
Taci; non voglio che mi ri
sponda, perchè la tua voce mi
suona all'orecchio come un catti
vo augurio. Preparami un fiasco |
sul tavolo, perchè ho una sete da
tedesco e poi conduci a letto la !
canaglia e liberami anche della
tua odiosa presenza.
Oh a proposito, tieni a
mente che, rimangono nel celiar
ancora otto fiaschi. March !
* * *
Ed ecco il mostro, sconcia
mente sdraiato su di una vecchia
poltrona, tutto assorto nella con
templazione del fiasco che ha già
vuotato a metà e del bicchiere ri
colmo.
Maledetta esistenza! egli dice
ase stesso. Non è a credere
quanto soffro tutte le volte che
debbo venirmi a rinchiudere in
questa topaia che mi sembra una
cella di morte. La vista di questa
strega e di questi marmocchi, che
sono per me una croce pesantis
sima, mi indispettisce a quel Dio
Come è dura la pelle di questa
donna ; sembra che abbia, come i
gatti, sette anime ed un'animel
la. La meschina dote che mi por
tò, quando commisi l'enoiTnità di
sposarla, non durò più di un me-
ise ; non so comprendere quindi
perchè debba essere tanto lunga
la sua esistenza. Non potrebbe
finalmente decidersi a fare un
viaggio senza ritorno?
Bah ! in attesa che il fausto av
venimento si compia, beviamoci
sopra
Questo vino non è cattivo, e di
venta squisito se si pensa che
non costa un centesimo. Quel
mascalzone del venditore potrà
fare a meno di mandarmi il suo
8i1i....
Bravo amico, quel cassiere; a
micone sincero ed effezionato; io
e lui siamo compagni indissolu
bili di erotiche gesta; egli ha gli
stessi miei gusti; è l'unica cosa
sennata e giusta che abbiano
stampato quegli imbecilli della
Ragione. Mangiamo allo stesso
piatto; beviamo allo stesso bic
chiere e Fiorimi può farne fede.
L'amico cassiere è molto fur
bo; egli sta per fare una magnifi
ca speculazione matrimoniale; se
il colpo riesce, ci saia da sciala
le a lungo anche per me. Co
uà fossi scapolo io pure! Chi sa
però se mi sarebbe ancora possi
bile di trovare un'altra moglie
con un buon gruzzolo ! Bah ! per
chè no? Dopo tutto ci sarebbe
Fiorina, la quale forse non rifiu
i terebbe la mia mano ; essa deve
avpi; De;:ze a bizzeffe e son sicu
;- e « COI'!
j cora dopo il matrimonio. Ma è i
nutile pascersi di illusioni ; la for
; tuna non mai mi è stata secon-
* .fiche al presente mi nega
j i iiioi favori, allungando la vita
di questa megera, che mi ama
reggia emi indispettisce con ia
' sua rassegnazione ipocrita.
# * *
11 manigoldo, così monologali
! do, aveva quasi visto il fondo del
I fiasco, ma il soverchio vino lo a-
I veva ridotto una massa insensi
j bile ed inerte. Piegò il capo sul
J petto e chiuse gli occhi.
I rumi dell'alcool gli gorgoglia
i vano vortico-amente nel cranio e
la camera, tutta all'intorno, ap
pariva alla mente ottenebrata
dell'ebbro ripiena di bianchi fan
tasmi. I quali, dapprima indistin
ti, assunte a poco a poco figure
nette e decise, incominciarono a
danzare una macabra danza.
Uno di essi, uscendo dalla
schiera, gli si avvicina e lo guar
da a lungo con uno sguardo tra
| la compassione ed il disprezzo. In
| quel fantasma il degenerato rico
nosce la moglie morta e sulla
massa inerte passa 1111 lungo bri
! vido.
Dopo morta ancora mi per
seguiti ? parea volesse gridar
|le l'ubriaco, atterrito dalla im
! provvisa apparizione. Tu non
hai nulla a rimproverarmi, giac
che quando avesti la felice ispi
razione di andartene in un mon
do migliore, io non mancai di far
ti il funerale che poi mi feci un
dovere di non pagare come 11011
pagai il medico che mi porte
in Coite e ti accompagnai
perfino al camposanto. Levamiti
quindi dinanzi e fammi gustare
in santa pace quest'altro bicchie
-1 re.
!* * *
Dileguossi lo spettro dell*
1 ' donna, forse atterrito da tante
1 cinismo, ma in quel posto mede
' simo sorse il fantasma di un te
■ nero giovanetto, che levò il pie
• colo dito minaccioso quasi a sfio
i rare la testa scarmigliata, gron
• ! dante il freddo sudore.
La terribile visione scosse quel
conio arrovesciato, come se lo a
vésse attraversato una corrente
elettrica, e parve che su quella
fronte violacea passasse un lam
po di rimorso; ma fu solo 1111 i
stante.
Osi,wo sciagurato, minaccia
re tuo padre? Fu colpa mia forse
se tu peristi tra le fiamme? E'
vero che la Compagnia del C.as
aveva in quell'epoca chili o la
corrente, perchè io avevo trascu
rato di pagare il BILL, ma io a
vevo comperato le candele, ed
anche le candele fanno luce, né
potevo prevedere che la fiamma
avrebbe appiccato l'incendio alla
casa
•+ * *
Padre tu ? Questo nome san
to e dolce, nella tua bocca, suo
na profanazione e bestemmia.
No, tu non hai visceri paterne, tu
che il denaro disonestamente lu
crato, sciupi in bagordi, e fai
morire di fame la povera fami
glia. Tu non sei padre, non sei
marito; tu fosti il carnefice di
mia madre, ed oggi sei l'aguzzi
no dei miei fratelli e della pove
ra donna che, per sua eterna di
sgrazia, ha occupato il posto la- j
sciato vuoto dall'altra, che oggi
finalmente, liliera dei tuoi ceppi,
gode 1111 po' di pace e di tranquil
lità.
Tu non hai diritto di dirti
padre; i padri lavorano pei loro
figli, ed in casa li educano con la
virtù e con l'esempio. Tu non hai
mai goduto la sublime soddisfa
zione che dà il compimento del
proprio dovere; tu non hai senti
lo mai affetto pei figli.
l'/ftàììftTrjVi 11 ira ier u^mnte,
10 povera vittima della tua depra
vazione, tu non volesti neppure
accompagnarmi alla mia piccola
fossa, forse per dare ad intende
re a qualche imbecille che non sa
0 finge d'ignorare di qual fango
: sei impastato, che ti tratteneva
in casa la profondità del dolore.
Ma io che tutto vidi, io che nel
l'altro mondo mi sentii dotato di
|un discernimento superiore ai
j miei teneri anni, compresi tutto
11 marcio della tua ipocrisia.
Quell'ombra di rimorso che si ce
-1 la nel fondo dell'anima dei mag-
I gioi i delinquenti, ti impedì per il
momento di mostrarti alla luce
del sole, ma alla notte, protetto
dalle tenebre amiche, non sape
sti rinunziare alla consueta visi
ta a Fiorina. E l'orgia del satiro
immondo fu l'unico tributo di
dolore che sapesti dare alla mia
morte immatura; ed allora io ti
maledissi, come ti maledico ora,
o carnefice di mia madre e mio
assassino, o aguzzino dei miei po
veri fratelli.
Dopo questo anatema tremen
do, anche il secondo fantasma
scomparve, ma il corpo inerte eb
be un lungo fremito. Aperti gli oc
chi a fatica, stese la mano al bic
chiere, elo tracannò di un sol
fiato, per affogare meglio nel vi
no i ricordi terrorizzanti.
* * «
Ricaduto poscia nella primiti
va immobilità, altri fantasmi gli
danzavano attorno la ridda. Que
sta volta erano innumerevoli, ed
egli, abbracciandoli con un lungo
sguardo, avenelo riconosciuto in
essi l'esercito dei suoi creditori,
scoppiò in una sonora sgignazza
ta.
Anche qui pareva gri
dasse la voce interna anche
qui venite a rompermi le scatole
per poche migliaia di miserabili
dollari che ebfti la bontà di pren
dere in prestito da voi. . (io te
hell! disgraziati, e ficcatevi in te
sta, per vostra norma, che il fi
glio di mio padre non h» mai ne
gato nulla a nessuno. E quandi
Anno I No. 2 5 Soldi la Copia
tutti sarete stati soddi fatti, a
vete altro a pretendere? No. Le
vatevi dunque dai miei piedi, se
non volete che vi scacci ignofiii
ìliosamente, o mercanti profana
tori.
E la fitta schiera, o impaurita
dalla minaccia o soddisfatta del
la promessa, riti rossi, lasciando
lo solo, coi residui della sua orgia
solitaria.
Ed allora il sinistro eroe levos
si dalla sedia ove pareva inchio
dato, ed allungando le braccia, e
mise un lungo, sonoro sbadiglio.
Sono diventato forse un ra
gazzo, che mi metto paura delle
ombre? La mia coscienza è ben
nascosta sotto un duro, fittissi
mo pelame ed i rimorsi non vi al
lignano. Sciocco è veramente chi
si illude che nell'animo mio pov
sa albergare affetto o ricono
scenza. Questa canaglia che mi
paga per l'armisi cantare le laudi
false e bugiarde, questi briganti
che mi aizzano siile calcagna dei
loro nemici e credono Hjsohbli
garsi con una manata di dollari,
10 li odio di un odio profondo ed
inestinguibile.
Per adesso son costretto a
| far tacere i ruggiti della misi a
| nima, perchè essi pagano profu
matamente le mie orgie ed ali
mentano i miei bisogni. Ma guai
ad essi se riuscirò ad emancipar
mi ! So tante delle loro porcherie
da poterli ridurre un mucchio di
cenci. E ve li ridurrò per la
M . perchè troppo mi umilia
no colla loro forzata, millantata
i generosità, e la mia vendetta sa
rà terribile, inesorabile !
* * *
ii degenerato arrota v**-*-fW*» i
in preda ad una rabbiay^'^"*.
' 11 vino ingoiato fuori misura, 00-1
1 ]io lunga fermentazione veniva
■ fuori a fiotti insieme col cibo, ed
1 egli, colla faccia riversa, cad
' de sui rifiuti del suo stomaco,
1 rotolandovisi sconciamente come
11 porco nel braco.
; Fuori la pioggia continuava a
1 scrosciale impetuosa, ed il vento
' fischiava sinistramente, mentre
• nel cielo plumbeo apparivano i
" primi albori indistinti di una
giornata nera come l'anima del
' l'alcoolizzato
) 11 novelliere.
' Afferratelo
i
per la coda...
, In Colonia si vocifera che il
- degenerato si stia preparando a
fuggire da Philadelphia portando
. seco i pochi checks rimastigli dei
, banchisti prossimi al fallimento
.je degli altri bacati pari suoi. La
c voce pare che sia fondata, sebbe
. ne lanciata a mo' di sospetto dal
,1 cavadenti delle nove strade che
_ ; piange i suoi cento dollari, e
corrol>orata da un altro colono
molto addentro nelle sporche se
grete cose.
Gli interessati restano avvisa
li ti.
d '*"*"*"* *"*"*"*' * *
Trasferimento
i, La barberia del nostro diretto
i- re fratello Francesco Silvagni,
coi relativi uffici de La Ragione,
i- si sono trasferiti al No. 911 Chri
e stian St.
J, Gli amici, i fratelli, si ricordi
li no che il nostro direttore è l'uni
>- co barbiere che taglia i capelli
o artisticamente ed è specialista
3- per il taglio delle unghie adunche
i- ! dei carbonari che non hanno ri
-- legno di approfittarsi da due sol
lo di in su.


# kiara\kiara\examples\data\text_corpus\La_Ragione\sn84037024_1917-05-05_ed-3_seq-1_ocr.txt
LA RAGIONE
ORGANO DI DIFESA DELLA ITALIANITÀ'
contro i vili, i camorristi, i sicari, i falsari e gli austriacanti, nemici della patria di origine e di quella d'adozione.
F. SILVAGNI, Direttore, 911 Christian Street, Philadelphia, ['a.
LA RAGIONE SI AFFERMA
Il primo numero de La Ragione non poteva non incontrare il fa
vore del pubblico che l'aspettava ansiosamente.
Un giornale come il nostro, fatto da operai con denaro suda
to; un giornale che ha intrapreso una campagna di epurazione,
scoprendo le piaghe purulenti di cui sono infette le poche putride
carogne che hanno lanciato il degenerato all'assalto contro le no
stre maggiori istituzioni; che ha richiamata l'attenzione della no
stra colletCivita sui reati nascosti nei bassamenti; che ha fatto
schioccare la frusta fustigatrice sui grugni incalliti di uomini
senza coscienza, preparatori e consulenti legali di fallimenti; che
ha dato il salutare allarme contro i banchisti pronti a fuggire al Ca
nada; che ha promesso di fare la storia vergognosa di tutti i Cu
riangiolo e loro luogotenenti e sgombrare la colonia di una com
briccola di malviventi; un giornale cosi fatto, dicevamo, non pote
va non incontrare le simpatie della nostra massa che aspira ad una
colonia italiana rigenerata.
E la gara dei lettori non ha avuto riscontro nella storia degli
annali del giornalismo coloniale. Le copie sono state pagate fino ad
un dollaro ciascuna. Il primato nella vendita di esse lo ha avuto il
nostro TURIDDU il quale, con le sue grida : BANCHISTI CHE
SCAPPANO AL CANADA*; CONIATORI DI MONETE FALSE;
CONSULENTI LEGALI 1)1 FALLIMENTI, la sera in cui vide la lu
ce La Ragione richiamava attorno a sè, lungo Christian St., una fol
la di passanti che aveva con ansia indicibile attesa l'uscita di questo
giornale.
Intanto, coloro che armarono la-penna del sicario, ne han
no risentito tutto l'effetto, perchè si sono subito scatenate minacce
di arresti in massa; arresti isolati; insulti contro i nostri compila
tori lungo le strade, fuori le farmacie e nelle alcove delle carogne;
tanto fiele e tanta bile si son viste spruzzare dai musi degli ORAN
GOTANGHI, dalle facce ingiallite e dalle occhiaie come caverne,
indizio questo della vita di vizio e di degenerazione nella quale essi
vivono.
Agli attacchi del degenerato, fatti al nostro Ordine ed ai
suoi capi, noi che pure avremmo avuto ragione, perchè provocati, di
assestargli un MAN ROVESCIO sulla sua facciaccia abbrutita, sia
mo rimasti calmi. Peichè dunque, tanto clamore; perchè i mandata
ri i del sicario si sono risentiti delle nostre difese e dei nastri giusti
t »nt ro-attacchi?
Le coscienze tranquille restano calme, le coscienze traviate si
agitano!
Noi abbiamo ragione di credere che i mandatarii, affidando la
loro causa al degenerato spione austriaco, hanno commesso un gni li
di ssi ino errore. Essi, specialmente i banchisti prossimi al fallimen
to, dei quali i nostri'aMci e nostri fratelli debbono guardarsi, me
glio avrebbero fatto che quelle poche centinaia di dollari, consegna
te al sicario, le avessero portate seco loro insieme al bottino pronto
a valicare l'Oceano. Ma che volete? Anche Pati, il banchista milio
nario di New York, prima di prendere il volo si accaparrò qualche
giornale della metropoli per farsi cantar le laudi. Niente di sor
prendente, perciò, se anche i furfanti di questa città hanno voluto
imitare il loro collega newyorkese.
La Ragione è uscita e rimarrà sulla breccia, sempre che i man
tanti continueranno ad armare la mano del loro sicario. Vi è anzi la
probabilità che questo giornale si trasformi in meglio e resti peren
ne minaccia contro gli sfruttatori delle nostre colonie.
Il nostro atteggiamento dipenderà dunque dalle mosse della
piccola combriccola di furfanti dalle unghfe adunche. Ad essi la
scelta, a noi i mezzi per annientarli, per distruggerli.
NOI.
AVVISO AI LETTORI
Nei primi due numeri della Ragione abbiamo prospettato al
pubblico le losche figure di cui alcuni prominenti si servono per ten
tare di abbattere i buoni uomini e le buone Istituzioni. Per l'avve
nire passeremo in rassegna, uno per uno, tutti coloro che han prov
veduto i fondi segreti, illustrandone le gesta compiute in America
dal giorno del loro arrivo e rievocando anche la loro vita nella pa
tria di origine.
Sarà una rubrica interessantissima. Molti scritti già pronti
dobbiamo rimandare per esuberanza di materia. Fra essi vi è una
ietterà aperta a Frnak L. Garbarino, detective federale, al quale si
indica uno spione austriaco che vive nella nostra colonia.
LA REDAZIONE
Circolare del Gr&nde Concilio
DI PENNSYLVANIA DELL'ORDINE F. d'l. in AMERICA
Questo Ufficio ha deciso di fare appello alle Logge per poter
assolvere a un duplice dovere: quello di gratitudine per l'atteggia
mento della Nazione che ci ospita, e l'altro di riconoscenza verso il
condottiero del nostro esercito, che deve riconquistare all'ltalia le
terre che ci appartengono.
Pertanto è stato stabilito di offrire una medaglia d'oro al Ge
nerale Cadorna, a nome dei Figli d'ltalia della Penssylvnia; ed una
bandiera alla Divisione del Colonnello Roosevelt o a quella Divisione
dell'esercito americano che prima salperà alla volta dell'Europa.
A tale scopo sollecitiamo le Logge a far pervenire subito le loro
offerte a questo Uffìzio, iniziando anche delle sottoscrizioni tra i
fratelli ; con avvertenza che la medaglia al Generale Cadorna sarà
mandata per mezzo della Commissione Italiana che si reca in mis
sione piesso il Governo degli Stati Uniti.
Si raccomanda poi alle Logge di Philadelphia e dintorni di te
nersi pronte per aderire a quella iniziativa che dal Regio Console
Cav. Uff. G. Poccardi sarà per prendersi in onore della Commissio
ne suddetta, se e quando sarà a Philadelphia, cosa che per motivi di
facile intuizione non si può anticipatamente stabilire. A tal uopo,
appena i Venerabile riceveranno nuovo avviso, chiameranno imme
diatamente tutti i fratelli con invito d'urgenza.
Questo Ufficio aveva anche fatto le pratihee per poter concor
rere ai ricevimenti ai rappresentanti delle nazioni alleate : francese
ed inglese. Essendoci stato risposto che parate non ve ne saranno
ed essendo incerti del giorno td ora di arrivo non si sono potuti fa
re dei preparativi. Però tutti i Figli d'ltalia si riuniran
no nel luogo che sarà indicato dalla stampa americana
ed italiana. Il Grande Concilio intanto e la Commissione F. U. M.
contribuiscono alla sottoscrizione del giornale North American per
offrire una spada d'onore al Generale Joffie.
PHILADELPHIA, PA., 5 MAGGIO, 1917.
Le visioni fosche e terrorizzanti
DEL DEGENERATO
E' notte alta e profonda; la piog
gia viene giù impetuosa dal cielo
foschissimo e il vento pare che
mormori voci di minaccia e di mi
stero. Le vie della città sono in
teramente deserte, poiché la not
te incute spavento ed il freddo è
intensissimo.
All'improvviso, come se sbu
casse dal sottosuolo, appare in
fondo ad una strada un solitario
nottambulo. Gesticola, procede
a ZIG ZAG, poiché ha non meno
di un palmo di vino nello stomaco
e le gambe mal sostengono il pe
so soverchio del corpo.
Non ha alcun riparo contro la
pioggia che continua a scrosciare
abbondante, inzuppandolo fino
alle ossa, ma egli procede im
perterrito sotto la bufera» e con
la lingua grossa e con *la voce
rauca degli übbriachi fradici can
ta una oscena canzone.
Al lume di un fanale si scorge
per un momento la sua faccia;
lo sguardo semispento in un Viso
livido, incute semplicemente ri
brezzo.
E' il degenerato, sinistramente
celebre nell'elemento losco colo
niale, che, dopo una giornata di
crapule nei saloons e nei risto
ranti e mezza nottata d'i orgia,
nel lupanare di Fiorimi, ;
ma non sazio, si riportiTcofiU; pùfT
nella meschina dimora, ove la po
vera moglie e i figli derelitti lo
attendono, morenti di freddo e
di fame, perchè privi di carbone i
e di cibo.
Giunto dinanzi alla porta, do
po aver tentato inutilmente di a
prirla, batte vigorosamente, ac
compagnando i ripetuti colpi con
le più triviali ingiurie all'indiriz
zo della moglie.
La porta-si apre e compare sul
la soglia una figura gentile di
donna, mite e rassegnata come li
na martire. Non versava lagrime
ma portava il segno di averne
spar e tante e l'ubriaco, dandole
un sonoro ceffone, le grida rab
biosamente : Dove ti eri cacciata,
strega del diavolo, che mi hai
fatto attendere due ore con que
sto tempaccio di inferno? Ti ho
ripetuto le mille volte che devi
attendermi alla porta, perchè lu
sei la mia schiava e devi obbedir
mi ciecamente
Taci; non voglio che mi ri
sponda, perchè la tua voce mi
suona all'orecchio come un catti
vo augurio. Preparami un fiasco i
sul tavolo, perchè ho una sete da
tedesco e poi conduci a letto la
canaglia e liljerami anche della
tua odiosa presenza.
Oh a proposito, tieni a
mente che, rimangono nel celiai
ancora otto fiaschi. March !
* * *
Ed ecco il mostro, sconcia
mente sdraiato su di una vecchia
poltrona, tutto assorto nella con
templazione del fiasco che ha già
vuotato a metà e del bicchiere ri
colmo.
Maledetta esistenza ! egli dice
ase stesso. Non è a credere
quanto soffro tutte le volte che
debbo venirmi a rinchiudere in
questa topaia che mi sembra una
cella di morte. La vista di questa
strega e di questi marmocchi, che
sono per me una croce pesantis
sima, mi indispettisce a quel Dio
Come è dura la pelle di questa
donna; sembra che abbia, come i
gatti, sette anime ed un'animel
la. La meschina dote che mi "tor
to, quando commisi l'enormi , di
sposarla, non durò più di un ne
se; non so comprendere quindi
perchè debba essere tanto lunga
la sua esistenza. Non potrebbe
finalmente decidersi a fare un
viaggio senza ritorno?
Bah ! in attesa che il fausto av
venimento si compia, beviamoci
sopra
Questo vino non è cattivo, e di
venta squisito se si pensa che
non costa un centesimo. Quel
mascalzone del venditore potrà
fare a meno di mandarmi il suo
Hi 11....
Bravo amico, quel cassiere; a
micone sincero ed effezionato; io
e lui siamo compagni indissolu
bili di erotiche gesta; egli ha gli
stessi miei gusti; è l'unica cosa
sennata e giusta che abbiano
stampato quegli imbecilli della
Ragione. Mangiamo allo stesso
piatto; beviamo allo stesso bic
chiere e Fiorimi può farne fede.
L'amico cassiere è molto fur
bo ; egli sta per fare una magnifi
ca speculazione matrimoniale; se
il colpo riesce, ci sarà da sciala
re a lungo anche per me. Co
sì fossi scapolo io pure! Chi sa
però se mi sarebbe ancora possi
bile di trovare un'altra moglie
con un buon gruzzolo ! Bah ! per
chè no? Dopo tutto ci sarebbe
Fiorimi, la quale forse non rifiu-
TéreßKe ia mia mano; essa dyve
aver pezze a bizzeffe e son sicu
ro che saprebbe guadagnarne an
cora dopo il matrimonio. Ma è i
nutile pascersi di illusioni ; la for
tuna non mai mi è stata secon
da ed anche al presente mi nega
i suoi favori, allungando la vita
di questa megera, che mi ama
reggia e mi indispettisce con la
sua rassegnazione ipocrita.
* ■* »
Il manigoldo, così monologan
do, aveva quasi visto il fondo del
fiasco, ma il soverchio vino lo a
veva ridotto una massa insensi
bile ed inerte. Piegò il capo sul
petto e chiuse gli occhi.
I fumi dell'alcool gli gorgoglia
vano vortico aniente nel cranio e
la camera, tutta all'intorno, ap
pariva alla niente ottenebrata
dell'ebbro ripiena di bianchi fan
tasmi. I quali, dapprima indistin
ti, assunte a poco a poco figure
nette e decise, incominciarono a
danzare una macabra danza.
Uno di essi, uscendo dalla
schiera, gli si avvicina e lo guar
da a lungo con uno sguardo tra
la compassione ed il disprezzo. In
quel fantasma il degenerato rico
nosce la moglie morta e sulla
massa inerte passa un lungo bri
vido.
Dopo morta ancora mi per
seguiti ? parea volesse gridar
le l'ubriaco, atterrito dalla im
provvisa apparizione. Tu non
hai nulla a rimproverarmi, giac
ché quando avesti la felice ispi
razione di andartene in un mon
do migliore, io non mancai di far
ti il funerale che poi mi feci un
dovere di non pagare come non
pagai il medico che mi portò
in Corte e ti accompagnai
perfino al camposanto. Levamiti
quindi dinanzi e fammi gustare
in santa pace quest'altro bicchie
re.
* * *
Dileguossi lo spettro della
donna, forse atterrito da tanto
cinismo, ma in quel posto mede
simo sorse il fantasma di un te
nero giovanetto, che levò il pic
colo dito minaccioso quasi a sfio
rare la testa scarmigliata, gron
dante il freddo sudore.
La terribile visione scosse quel
corpo arrovesciato, come se lo a
vesse attraversato una corrente
elettrica, e parve che su quella
fronte violacea passasse un lam
po di rimorso; ma fu solo un i
stante.
Osi, o sciagurato, minaccia
re tuo padre? Fu colpa mia forse
se tu j>eristi tra le fiamme? E'
vero che la Compagnia del Gas
aveva in quell'epoca chiuso la
corrente, perchè io avevo trascu
rato di pagare il BILL, ma io a
vevo comperato le candele, ed
anche le candele fanno luce, nè
potevo prevedere che la fiamma
avrebbe appiccato l'incendio alla
casa
4 »«
Padre tu? Questo nome san
to e dolce, nella tua bocca, suo
na profanazione e bestemmia.
No, tu non hai visceri paterne, tu
che il denaro disonestamente lu
crato, sciupi in bagordi, e fai
morire di fame la povera fami
glia. Tu non sei padre, non sei
marito ; tu fosti il carnefice di
mia madre, ed oggi sei l'aguzzi
no dei miei fratelli e della pove
ra donna che, per sua eterna di
sgrazia, ha occupato il posto la- j
.- ciato vuoto dall'altra, che ogg.
finalmente, libera dei tuoi ceppi,
gode un po' di pace e di tranquil
lità.
Tu non hai diritto di dirti
padre ; i padri lavorano pei loro
figli, ed in casa li educano con la
virtù e con l'esempio. Tu non hai
mai goduto la sublime soddisfa
:/i.or>" I-IIP dà il compimento del
proprio dovei e, tu non bai ..OÌ» ta
to mai affetto liei figli.
Quando perii tra le fiamme,
10 povera vittima della tua depra
| vazione, tu non volesti neppure
accompagnarmi alla mia piccola
fossa, forse per dai e ad intende
re a qualche imbecille che non sa
i o finge d'ignorare di qual fango
| sei impastato, che ti tratteneva
i in casa la profondità del dolore.
Ma io che tutto vidi, io che nel
; l'altro mondo mi sentii dotato di
un discernimento superiore ai
miei teneri anni, compresi tutto
11 marcio della tua ipocrisia.
Quell'ombra di rimorso che si ce
la nel fondo dell'anima dei mag
giori delinquenti, ti impedì per il
momento di mostrarti alla luce
del sole, ma alla notte, protetto
dalle tenebre amiche, non sape
sti rinunziare alla consueta visi
ta a Fiorimi. E l'orgia del satiro
immondo fu l'unico tributo di
dolore che sapesti dare alla mia
morte immatura; ed allora io ti
maledissi, come ti maledico ora,
o carnefice di mia madre e mio
assassino, o aguzzino dei miei po
veri fratelli.
Dopo questo anatema tremen
do, anche il secondo fantasma
scomparve, ma il corpo inerte eb
be un lungo fremito. Aperti gli oc
chi a fatica, stese la mano al bic
chiere, elo tracannò di un sol
fiato, per affogare meglio nel vi
no i ricordi terrorizzanti.
* « #
Ricaduto poscia nella primiti
va immobilità, altri fantasmi gli
danzavano attorno la ridda. Que
sta volta erano innumerevoli, ed
egli, abbracciandoli con un lungo
sguardo, avendo riconosciuto in
essi l'esercito dei suoi creditori,
scoppiò in una sonora sgignazza
ta.
Anche qui pareva gri
dasse la voce interna anche
qui venite a rompermi le scatole,
per poche migliaia di miserabili
dollari che ebbi la bontà di pren
dere in prestito da voi. Go to
hell! disgraziati, e ficcatevi in te
sta, per vostra norma, che il fi
glio di mio padre non ha mai ne
gato nulla a nessuno. E quando
Anno I No. 2 5 Soldi la Copia
tutti .sarete stati soddi l'atti, a
vete altro a pretendere ? No. Le
vatevi dunque dai miei piedi, se
non volete che vi scacci ignomi
niosamente, o mercanti profana
tori.
E la fitta schiera, o impaurita
dalla minaccia o soddisfatta del
la promessa, riti rossi, lasciando
lo solo, coi residui della sua orgia
solitaria.
Ed allora il sinistro eroe levos
si dalla sedia ove pareva inchio
dato, ed allungando le braccia, e
mise un lungo, s'onoro sbadiglio.
Sono diventato forse un ra
gazzo, che mi metto paura delle
ombre? La mia coscienza è ben
nascosta sotto un duro, fittissi
mo pelame ed i rimorsi non vi al
lignano. Sciocco è veramente chi
si illude che nell'animo mio po;-
sa albergare affetto o ricono
scenza. Questa canaglia che mi
paga per l'armisi cantare le laudi
false e bugiarde, questi briganti
che mi aizzano alle calcagna dei
loro nemici e credono disobbli
garsi con una manata di dollari,
10 li odio di un odio profondo ed
inestinguibile.
Per adesso son costretto a
far tacere i ruggiti della mia a
nima, perché essi pagano profu
matamente le mie orgie ed ali
mentano i miei bisogni. Ma guai
ad essi se riuscirò ad emancipar
mi! So tante delle loro porcherie
da poterli ridurre un mucchio di
cenci. E ve li ridurrò per la
M . perchè troppo mi umilia
no colla loro forzata, millantata
generosità, *» la niù> wn<»et* -
ià ttynbiìe, inesorabile!
Il degenerato arrotava i denti
in preda ad una rabbia l>estiale.
11 vino ingoiato fuori misura, do
lio lunga fermentazione veniva
fuori a fiotti insieme col cibo, ed
egli, colla faccia riversa, cad
de sui rifiuti del suo stomaco,
rotolandovisi sconciamente come
il porco nel braco.
Fuori la pioggia continuava a
scrosciare impetuosa, ed il vento
fischiava sinistramente, mentre
nel cielo plumbeo apparivano i
primi albori indistinti di una
giornata nera come l'anima del
l'alcoolizzato
I! novelliere.
Afferratelo
per la coda...
In Colonia si vocifera che il
degenerato si stia preparando a
fuggile da Philadelphia portando
seco i pochi checks rimastigli dei
banchisti prossimi al fallimento
e degli altri bacati pari suoi. La
voce pare che sia l'ondata, sebbe
i ne lanciata a mo' di sospetto dal
cavadenti delle nove strade che
piange i suoi cento dollari, e
corroborata da un altro colono
molto addentro nelle sporche se
grete cane.
Gli interessati restano avvisa
ti.
Trasferimento
La barberia del nostro diretto
re fratello Francesco Silvagni,
coi relativi uffici de La Ragione,
si sono trasferiti al No. 911 Chri
stian St.
Gli amici, i fratelli, si ricordi
no che il nostro direttore è l'uni
co barbiere che taglia i capelli
artisticamente ed è specialista
, per il taglio delle unghie adunche
dei carbonari che non hanno ri
tegno di approfittarsi da due sol
j di in su.


# kiara\kiara\examples\data\text_corpus\La_Ragione\sn84037024_1917-05-05_ed-4_seq-1_ocr.txt
LA RAGIONE
contro i vili, 1 camorristi, i sicari, i falsari e gli austriacanti, nemici della patria di origine e di quella d'adozione
|F. SI LV AGNI, Direttore, 911 Christian Street, Philadelphia, l'a.
LA RAGIONE SI AFFERMA
■ 4 .
Ì; Il primo numero de La Ragione non poteva non incontrare il fa
kore del pubblico che l'aspettava ansiosamente.
Un giornale come il nostro, fatto da operai con denaro suda-
É - Ito; un giornale che ha intrapreso una campagna di epurazione,
le piaghe purulenti di cui sono infette le poche putride
■rarogne che hanno lanciato il degenerato all'assalto contro le no
||<tre maggiori istituzioni; che ha richiamata l'attenzione della no
stra colletti vi tà sui reati nascosti nei bassamente che ha fatto
(chioccare la frusta fustigatrice sui grugni incalliti di uomini
enza coscienza, preparatori e consulenti legali di fallimenti; che
a dato il salutare allarme contro i banchisti pronti a fuggire al Ca
adà; che ha promesso di fare la storia vergognosa di tutti i Cu
iangiolo e loro luogotenenti e sgombrare la colonia di una com
riccola di malviventi; un giornale così fatto, dicevamo, non pote
a non incontrare le simpatie della nostra massa che aspira ad una
ulonia italiana rigenerata.
E la gara dei lettori non ha avuto riscontro nella storia degli
nnali del giornalismo coloniale. Le copie sono state pagate fino ad
Sin dollaro ciascuna. Il primato nella vendita di esse lo ha avuto il
Hiostro TURIDDU il quale, con le sue gi'ida ; BANCHISTI CHE
SCAPPANO AL CANADA*; CONIATORI DI MONETE FALSE;
■TONSLLENTI LEGALI DI FALLIMENTI, la sera in cui vide la lu
flte I.a Ragione richiamava attorno a sè, lungo Christian St., una fo!-
pi di passanti che aveva con ansia indicibile attesa l'uscita di questo
viornalc.
Intanto, coloro che armarono la penna del sicario, ne han
* ho risentito tutto l'eitetto, perchè si sono subito scatenate minacce
tfi arresti in nuissa; arresti isolati; insulti contro i nostri compila
tori lungo le strade, fuori le farmacie e nelle alcove delle carogne;
■unto fiele e tanta bile si son viste spruzzare dai musi degli ORAN
■.OI ANGUI, dalle facce ingiallite e dalle occhiaie come caverne,
■ndi/.io questo della vita di vizio e di degenerazione nella quale essi
Byivono.
Agli attacchi del degenerato, fatti al nostro Ordine ed ai
Buoi capi, noi che pure avremmo avuto ragione, perchè provocati, di
Assestargli un MAN ROVESCIO sulla sua facciaccia abbrutita, sià
jhio rimasti calmi. Perchè dunque, tanto clamore; perchè i mandata
|ii del sicario si sono risentiti delle nostre difese e dei nostri giusti
Contro-attacchi ?
Le coscienze tranquille restano calme, le coscienze traviate si
■gitano !
■ Noi abbiamo ragione di credere che i mandatarii, affidando la
H>ro causa al degenerato spione austriaco, hanno commesso un gran
assimo errore. Essi, specialmente i banchisti prossimi al fallimen
■o, dei quali i nostri amici e nostri fratelli debbono guardarsi, me
glio avrebbero fatto che quelle poche centinaia di dollari, consegna
■* al sicario, le avessero portate seco loro insieme al bottino pronto
H valicare l'Oceano. Ma che volete? Anche Pati, il banchista milio
nario di New York, prima di prendere il volo si accaparrò qualche
giornale della metropoli per farsi cantar le laudi. Niente di sor
prendente, perciò, se anche i furfanti di questa città hanno voluto
Jlmitare il loro collega newyorkese.
.La Ragione è uscita e rimarrà s illa breccia, sempre che i man-
Kinti continueranno ad armare la mano del loro sicario. Vi è anzi la
-irobabilita che questo giornale si trasformi in meglio e resti peren
9jle minaccia contro gli sfruttatori delle nostre colonie.
■ Il nostro atteggiamento dipenderà dunque dalle mosse della
piccola combriccola di furfanti dalle unghie adunche. Ad essi la
•celta, a noi i mezzi per annientarli, per distruggerli.
NOI. I
AVVISO AI LETTORI
Nei primi due numeri della Ragione abbiamo prospettato al
pubblico le losche figure di cui alcuni prominenti si servono per ten
tare di abbattere i buoni uomini e le buone Istituzioni. Per l'avve
nire passeremo in rassegna, uno per uno, tutti coloro che han prov
veduto i fondi segreti, illustrandone le gesta compiute in America
lai giorno del loro arrivo e rievocando anche la loro vita nella pa
tria di origine.
Saia una lubrica interessanti sminuì. Gioiti scritti già pronti
dobbiamo rimandare per esuberanza di materia. Fra essi vi è una
ettera aperta a Frnak L. (ìarbarino, detective federale, al quale si
ndica uno spione austriaco che vive nella nostra colonia.
LA REDAZIONE
Circolare del Grande Concilio
DI PENNSYLVANIA DELL'ORDINE F. d'l. in AMERICA
Questo l ificio ha deciso di fare appello alle Logge per potei'
issolvere a un duplice dovere: quello di gratitudine per l'atteggia
xiento della Nazione che ci ospita, e l'altro di riconoscenza verso il
:ondottiero del nostro esercito, che deve riconquistare all'ltalia le
erre che ci appartengono.
Pertanto è stato stabilito di offrire una medaglia d'oro al Ge
lerale Cadorna, a nome dei Figli d'ltalia della Penssylvnia; ed una
bandiera alla Divisione del Colonnello Roosevelt o a quella Divisione
lei l'esercito americano che prima salperà alla volta dell'Europa
A tale scopo sollecitiamo le Logge a far pervenire subito le lóro
)fferte a questo Uffizio, iniziando anche delle sottoscrizioni tra i
rateili ; con avvertenza che la medaglia al Generale Cadorna sarà
nandata per mezzo della Commissione Italiana che si reca in mis
none presso il Governo degli Stati Uniti.
Si raccomanda poi alle Logge di Philadelphia e dintorni di te
mersi pronte per aderire a quella iniziativa che dal Regio Console
av. Uff. G. Poccardi sarà per prendersi in onore della Commissio
e suddetta, se e quando sarà a Philadelphia, cosa che per motivi di
icile intuizione non si può anticipatamente stabilire. A tal uopo,
jpena i Venerabile riceveranno nuovo avviso, chiameranno imme
■atamente tutti i fratelli con invito d'urgenza.
Questo Ufficio aveva anche fatto le pratihce per poter concor
rere ai ricevimenti ai rappresentanti delle nazioni alleate: francese
■d inglese. Essendoci stato risposto che parate non ve ne saranno
mi essendo incerti del giorno ed ora di arrivo non si sono potuti fa
\ _-dei preparativi. Però tutti i Figli d'ltalia si riuniran
kr<3??- °&° che sarà . | nd .icato dalla stampa americana
m j^' ia p a - Il Grande Concilio intanto e la Commissione F. U. M.
f° leniscono alla sottoscrizione del giornale North American per
una spada d'onore al Generale Joffre.
ORGANO DI DIFESA DELLA ITALIANITÀ -
Le visioni fosche e terrorizzanti
DEL DEGENERATO
ì ____
E' notte alta e profonda ; la piog
gia viene giù impetuosa dal cielo
foschissimo e il vento pare che
mormori voci di minaccia e di mi
stero. I,e vie della città sono in
teramente deserte, poiché la not
te incute spavento ed il freddo è
intensissimo.
All'improvviso, come se sbu
casse dal sottosuolo, appare in
fondo ad una strada un solitario
nottambulo. Gesticola, procede
a ZIG-ZAG, poiché ha non meno
di un palmo di vino nello stomaco
e le gambe mal sostengono il pe
so soverchio del corpo.
Non ha alcun riparo contro la
pioggia che continua a scrosciare
abbondante, inzuppandolo fino
: alle ossa, ma egli procede im
| perterrito sotto la bufera, e con
Ila lingua grossa e con la voce
j rauca degli übbriachi fradici can
ta una oscena canzone.
Al lume di un fanale si scorge
per un momento la sua faccia ;,
lo sguardo semispento in un viso
livido, incute semplicemente ri
brezzo.
E' il degenerato, sinistramente :
i celebre nell'elemento losco colo
i niale, che, dopo una giornata di
crapule nei saloons e nei risto
! ranti e mezza nottata d'i orgia,
nel lupanare di Fiorina, stanco
ma non savio rjpoili» rrw .pvìit
nella meschina dimora, ove la po
vera moglie e i figli derelitti lo j
attendono, morenti di freddo e
di fame, perchè privi di carbone >
e di cibo.
Giunto dinanzi alla porta, do
po aver tentato inutilmente di a
prilla, batte vigorosamente, ac
compagnando i ripetuti colpi con
le più triviali ingiurie all'indiriz-i
( zo della moglie.
La porta si apre e compare sul- j
la, soglia una figura 'gentile di j
donna, mite e rassegnata come ti-1
na martire. Non versava lagrima#
ma portava il segno di averne j
spar e tante e l'ubriaco, dandole |
un sonoro ceffone, le grida lab- j
biosamente: Dove ti eri cacciata,!
strega del diavolo, che mi hai i
fatto attendere due ore con que
sto tempaccio di inferno? Ti ho
ripetuto le mille volte che devi
attendermi alla porta, perchè tu
sei la mia schiava e devi obbedir
mi ciecamente
Taci ; non voglio che mi ri
sponda, perchè la tua voce mi
suona all'orecchio come un catti
vo augurio. Preparami un fiasco)
sul tavolo, jierchè ho una sete da
tedesco e poi conduci a letto la '
canaglia e liberami anche della
tua odiosa presenza.
Oh a proposito, tieni a
mente che, rimangono nel celiar
ancora otto fiaschi. March !
* * *
Ed ecco il mostro, sconcia
mente sdraiato su di una vecchia
poltrona, tutto assorto nella con- ;
templazione del fiasco che ha già
vuotato a metà e del bicchiere ri
colmo.
Maledetta esistenza ! egli dice j
ase stesso. Non è a credere
quanto soffro tutte le volte che
debbo venirmi a rinchiudere in ,
questa topaia che mi sembra una
cella di morte. La vista di questa
strega e di questi marmocchi, che
sono pei - me una croce pesantis
sima, mi indispettisce a quel Dio ,
Come è dura la pelle di questa j (
donna ; sembra che abbia, come i I,
gatti, sette anime ed un'animel- ,
la. La meschina dote che mi por- ,
tò, quando commisi l'enormità di ;
sposarla, non durò più di un me- ,
PHILADELPHIA, PA.. 5 MAGGIO, 1917.
se; non so comprendere quindi
perchè debba essere tanto lunga
la sua esistenza. Non potrebbe
finalmente decidersi a fare un
viaggio senza ritorno ?..
Bah ! in attesa che il fausto av
venimento si compia, beviamoci
sopra
Questo vino non è cattivo, e di
venta squisito se si pensa che
non costa un centesimo. Quel
mascalzone del venditore potrà
fare a meno di mandarmi il suo
8i11....
Bravo amico, quel cassiere ; a
micone sincero ed affezionato; io
e lui siamo compagni indissolu
bili di erotiche gesta; egli ha gli
stessi miei gusti; è l'unica cosa
sennata e giusta che abbiano
stampato quegli imbecilli della
Ragione. Mangiamo allo stesso
piatto; beviamo allo stesso bic
chier ! e Fiorimi può farne fede.
L'i mico cassiere è molto fur
bo; f\<li sta per fare una magnifi
ca speculazione matrimoniale; se
il colpo riesce, ci sarà da sciala
re a lungo anche per me. Co
sì fossi scapolo io pure ! Chi sa
però se mi sarebbe ancora possi
bile di trovare un'altra moglie
c<»n un buon gruzzolo! Bah! per
c\larà"V) ? Dopo tutto ci sarebbe
I' on<i<i> la quale forse non riliu
vn- no ; . «Jpyp
a'veryezi*! a bizzeffe e son sicu
ro eh® saprebbe guadagnarne an
cora dopo il matrimonio. Ma è i
nutile pascersi di illusioni; la for
tuna non mai mi è stata secon
da ecì anche al presente mi nega
i suoi favori, allungando la vita
di questa megera, che mi ama
reggia e mi indispettisce con la
sua rassegnazione ipocrita.
* * #
li manigoldo, così monologali
do, aveva quasi visto il fondo del
fiasco, ma il soverchio vino lo a
■ffla ridotto una massa insensi
bile ed inerte. Piegò il capo sul
petto e chiuse gli occhi.
I fumi dell'alcool gli gorgoglia
vano vorticosamente nel cranio e
la camera, tutta all'intorno, ap
pariva alla mente ottenebrata
dell'ebbro ripiena di bianchi fan
tasmi. I quali, dapprima indistin
ti, assunte a poco a poco figure
nette e decise, incominciarono a
danzare una macabra danza.
Uno di essi, uscendo dalla
schiera, gli si avvicina e lo guar
da a lungo con uno sguardo tra
la compassione ed il disprezzo. In
quel fantasma il degenerato rico
nosce la moglie morta e sulla
massa inerte passa un lungo bri
vido.
Dopo morta ancora mi per
seguiti? parea volesse gridar
le l'ubriaco, atterrito dalla im
provvisa apparizione. Tu non
hai nulla a rimproverarmi, giac
ché quando avesti la felice ispi
razione di andartene in un mon
do migliore, io non mancai di far
ti il funerale che poi mi feci un
dovere di non pagare come non
pagai il medico che mi portò
in Corte e ti accompagnai
perfino al camposanto. Levamiti
quindi dinanzi e fammi gustare
in santa pace quest'altro bicchie
re.
* * *
Dileguossi lo spettro della
donna, forse atterrito da tanto
cinismo, ma in quel posto mede
simo sorse il fantasma di un te
nero giovanetto, che levò il pic
colo dito minaccioso quasi a sfio
rare la testa scarmigliata, gron
dante il freddo sudore.
i-a tenibile visione scosse quel
corpo arrovesciato, come se lo a
vesse attraversato una corrente
elettrica, e parve che su quella
fronte violacea passasse un lam
po di rimorso; ma fu solo un i
stante.
i Osi, o sciagurato, minaccia
t re tuo padre? Fu colpa mia l'orse
, se tu peristi tra le fiamme? E'
, vero che la Compagnia del Gas
aveva in quell'epoca chiù o la
corrente, perchè io avevo trascu
j rato di pagare il BILL, ma io a
vevo comperato le candele, ed
anche le candele fanno luce, nè
potevo prevedere che la fiamma
I avrebbe appiccato l'incendio alla
CUScI
♦ * *
—* Padre tu ? Questo nome san
to e dolce, nella tua bocca, suo
, ua profanazione e bestemmia.
No, tu non hai visceri paterne, tu
che il denaro disonestamente lu
crato, sciupi in bagordi, e l'ai
morire di fame la povera fami
glia. Tu non sei padre, non sei
marito; tu l'osti il carnefice di
mia madre, ed oggi sei l'aguzzi
no dei miei fratelli e della
ra donna che, per sua eterna di
sgrazia, ha occupato il posto la
sciato vuoto dall'altra, che oggi
finalmente, libera dei tuoi ceppi,
gode un po' di pace e di trariquil
, lità.
Tu non hai diritto di dirti
padre ; i padri lavorano pei loro
figli, ed in casa li educano con la
virtù e con l'esempio. Tu non hai
mai g< •jtìto la sublime soddisfa
zione uili dà il compimento del
! proprio dovere ; tu non hai senti
to mai affetto pei figli.
Quando perii tra le fiamme,
10 povera vittima della tua depra
vazione, tu non volesti neppure
accompagnarmi alla mia piccola
fossa, forse per dare ad intende
re a qualche imbecille che non sa
o finge d'ignorare di qual fango
sei impastato, che ti tratteneva
in casa la profondità del dolore.
Ma io che tutto vidi, io che nel
l'altro mondo mi sentii dotato di
un discernimento superiore ai
miei teneri anni, compresi tutto
11 marcio della tua ipocrisia.
Quell'ombra di rimorso che si ce
la nel fondo dell'anima dei mag
giori delinquenti, ti impedì per il
momento di mostrarti alla luce
del sole, ma alla notte, protetto
dalle tenebre amiche, non sape
sti rinunziare alla consueta visi
ta a Fiorina. E l'orgia del satiro
immondo fu l'unico tributo di
dolore che sapesti dare alla mia
morte immatura; ed allora io ti
maledissi, come ti maledico ora,
o carnefice di mia madre e mio
assassino, o aguzzino dei miei po
veri fratelli.
Dopo questo anatema tremen
do, anche il secondo fantasma
scomparve) ma il corpo inerte eb
l>e un lungo fremito. Aperti gli oc
chi a fatica, stese la mano al bic
chiere, e lo tracannò di un sol
fiato, per affogare meglio nel vi
no i ricordi terrorizzanti.
* # #
Ricaduto poscia nella primiti
va immobilità, altri fantasmi gli
danzavano attorno la ridda. Que
sta volta erano innumerevoli, ed
egli, abbracciandoli con un lungo
sguardo, avendo riconosciuto in
essi l'esercito dei suoi creditori,
scoppiò in una sonora sgignazza
ta. .
Anche qui pareva gri
dasse la voce interna anche
qui venite a'rompermi le scatole,
per poche migliaia di miserabili
dollari fhe ebbi la bontà di pren
dere in prestito da voi. (io to
hell! disgraziati, e ficcatevi in te
: sta, per vostra norma, che il fi
gliò di mio padre non ha mai ne
! gato nulla a nessuno. E quando
Anno I No. 2 ."> Soldi la Copia
1 tutti sarete stati sockii fatti, a
- vote altro a pretendere? No. Le
■ vatevi dunque dai miei piedi, -e
i 11011 volete che vi scacci ignomi
- niosamente, o mercanti profana
• tori.
E la fitta schiera, o impaurita
dalla minaccia o soddisfatta del
• la promessa, ritirossi, lasciando
lo solo, coi residui della sua orgia
solitaria.
i Ed allora il sinistro eroe levos
• si dalla sedia ove pareva inchio
■ dato, ed allungando le braccia, e-
I mise 1111 lungo, sonoro sbadiglio.
Sono diventato forse un ra
i gazzo, che mi metto paura delle
ombre? La mia coscienza è "ben
nascosta sotto un duro, fittissi
mo pelame ed i rimorsi non vi al
lignano. Sciocco è veramente chi
si illude che nell'animo mio pos
sa albergare affetto o ricono
scenza. Questa canaglia che mi
paga per l'armisi cantare le laudi
false e bugiarde, questi briganti
che mi aizzano alle calcagna dei
loro nemici e credono disobbli
garsi con una manata di dollari,
10 li odio di un odio profondo ed
inestinguibile.
Per adesso son costretto a
j far tacere i ruggiti della mia a
nima, perchè essi pagano profu
matamente le mie orgie ed ali
mentano i miei bisogni. Ma guai
ad essi se riuscirò ad emancipar
mi ! So tante delle loro porcherie
da poterli ridurre un mucchio di
cenci. E ve li ridurrò per la
M . perchè troppo mi umilia
no colla loro forzata, millantata
' « la ivi a vMidettvhe: 1 -
rà tenibile, inesorabile!
* * *
11 degenerato arrotava i denti
; in preda ad una rabbia bestiale.
' Il vino ingoiato fuori misura, do
po lunga fermentazione veniva
fuori a fiotti insieme col cibo, ed
egli, colla faccia riversa, cad
de sui rifiuti del suo stomaco,
rotolandovisi sconciamente come
11 porco nel braco.
Fuori la pioggia continuava a
scrosciare impetuosa, ed il vento
1 fischiava sinistramente, mentre
nel cielo plumbeo apparivano i
primi albori indistinti di una
giornata nera come l'anima del
l'alcool i zzato
, 11 novelliere.
Afferratelo
i |
per la coda...
, In Colonia si vocifera che il
degenerato si stia preparando a
fuggile da Philadelphia portando
seco i pochi checks rimastigli dei
banchisti prossimi al fallimento
e degli altri bacati pari suoi. La
voce pare che sia fondata, sebbe
ne lanciata a mo' di sospetto dal
cavadenti delle nove strade che
piange i suoi cento dollari, e
corroborata da un altro colono
molto addentro nelle sporche se
grete cose.
j Gli interessati restano avvisa
ti.
Trasferimento
La barberia del nostro diretto
re fratello Francesco Silvagni,
coi relativi uffici de La Ragione,
si sono trasferiti al No. 911 Chri
stian St.
Gli amici, i fratelli, si ricordi
no che il nostro direttore è l'uni
co barbiere che taglia i capelli
artisticamente ed è specialista
: per il tjmlio delle unghie adunche
1 dei carbonari che non hanno ri
tegno di approfittarsi da due sol
di in su.


# kiara\kiara\examples\data\text_corpus\La_Ragione\sn84037024_1917-05-16_ed-1_seq-1_ocr.txt
contro i vili, i camorristi, i sicari, i falsari e gli austriacanti, nemici della patria di origine e di quella d'adozione
F. SILVAGNI, Direttore, 911 Christian Street, Philadelphia, Pa-
LA MONTAGNA HA PARTORITO
ed ha fatto un topolino
LADRO DI FRANCOBOLLI: TRUFFATORE; ASSASSINO
DELLA PRIMA E MARTIRIZZATOR E DELLA SECONDA
' MOGLIE; RICATTATORE; SICARIO; UOMO DI
FANGO, CRIMINALE, ASCOLTATECI:
La montagna ha partorito ed ha fatto un topolino!
I>a minacciata lettera ricatto, pardon, aperta, è venuta fuo
ri, dalla melma che ti ricopre, per eternare la tua impossibilità di
colpire i migliori uomini delle nostre colonie: per convincere il pub
blico, ancora una volta, se ve ne fosse il bisogno, che la figura di
Giuseppe Di Silvestro si erge troppo austera e dignitosa su una base
granitica che non si sgretolìi, neanche contro i colpi di vigliacchi co
spiratori che hanno armata la mano del sicario e lo hanno lanciato
e continuano a lanciarlo contro i forti.
Signori bnch'isti; signori coniatori di monete false; signori in
cendiarii; signori curatoli di fallimenti; signori ipocriti dalle parole
dolci e dal cuore di fanno, il denaro gitlato ai vostro sicario non sor
tirà l'effetto desiderato. Voi vi siete scavata la fossa e noi vi ci sot
terreremo!
i't Jjt *
La montagna, dicevamo, ha partorito ed ha fatto un topolino!
Siamo costretti a riportare, ancora oggi, perchè il pubblico dei
lettori non dimentichi da un numero all'altro, l'ultimo spunto scrit
to e pubblicato dal degenerato sull'Opinione (lei Popolo del .1 Aprile
1915, appena due anni fa, spunto che è un contributo di omaggio ai
sacrifìci fatti, alle energie onestamente spese, come il sicario stesso
ammette, da Giuseppe Di Silvestro per aver dato alle colonie italia
ne d'America una fiaccola dalla luce limpida e pura, una Voce del
Popolo, che i pwtcri ricorderanno, un giornale è il degenerato ad
affermarlo CHE RISPONDEVA A PREFERENZA 1)1 QUA
LUNQUE ALTRO ALLE ESIGENZE DELLA ITALIANITÀ' IM
MIGRATA, GIACCHE' NELLE NON POCHE LOTTE SOSTE
NUTE DETTE MOLTE PROVE LUMINOSE DI QUELLO SPI
RITO D'LMPARZIALITA' E DI INDIPENDENZA CHE QUASI
MAI SI EBBE RAGIONE 1)1 LODARE IN ALTRI.
E, sempre il degenerato a parlare, aggiungeva. NON STA A
NOI INDAGARE E DISCUTERE LE RAGIONI CHE AVRANNO
0 STARANNO PER DETERMINARE IL PASSO DEL VOCIFE
RATO TRASFERIMENTO; CERTAMENTE NON VI SARANNO
ESTRANEE QUELLE DI INDOLE AMMINISTRATIVA, GIAC
CHE', COME OGNUNO SA, SE VI E' STATO UN GIORNALE
QUOTIDIANO CHE HA SEMPRE PER TRE QUARTI DOVUTO
DIPENDERE DALLE ENERGIE E DAI SACRIFICI DI POCHI
CHE LO REDIGEVANO E DIRIGEVANO, QUESTO QUOTIDIA
NO E' STATO PRECISAMENTE LA VOCE DEL POPOLO, SEN
ZA CHE L V COLONIA AVESSE MA! POTUTO, DAL LATO
DELLA PUBBLICITÀ' INCORAGGIARLO COME AVREBBE
DOVUTO.
Noi invitiamo il degenerato a smentire quanto sopra; noi do
mandiamo al sicario se diceva la verità quando egli pubblicava il
suddetto spunto; noi siamo ansiosi di sapere fino a qua! punto arri
va l'incoscienza di un'anima venduta, di un'anima prava: di un'ani
ma di fango.
Ma noi conosciamo il degenerato. Egli ci risponderà che quan
do scrisse quello spunto era infradicito dalla bevanda e perciò non
ricorda nulla.
* * *
Giuseppe Di Silvestro, è vero, ha parecchi difetti dipendenti
dal suo temperamento caldo; ma Giuseppe Di Silvestro amici
e nemici glie lo riconoscono è l'uomo che tutto dimentica; è il
connazionale dal carattere aperto e leale; pieno di sincerità; l'ita
liano che per la sua scrupolosa onestà non ha confronti; egli è, so
pratutto, generoso. Non è da sorprendersi perciò se lo si vede sem
pre circondato e sostenuto dalla ma-sa, come non deve sembrar
strano se oggi noi, che in lui apprezziamo le sue virtù personali e
quelle di condottiere, temuto e rispettato, dei Figli d'ltalia in Penn
sylvania, gli siamo e gli saremo sempre al fianco e con noi le die
cine di migliaia di sudi estimatori per difenderlo dalle zanne av
velenate di un mercenario e di anime vili che questo mercenario so
spingono.
Ladro di francobolli, apri le orecchie: Giuseppe Di Silvestro in
verità non ha bisogno di difensori. I suoi difensori sono i 20 anni
di vita d'America, vita di sacrifìci, vita di lotte contro i disonesti
come te; vita intemerata. I difensori di Giuseppe Di Silvestro sono
1 suoi compaesani residenti qui che in Italia lo ricordano il beniami
no di tutti i ceti nella natia Bussi.
Tu capirai, degenerato, che un galantuomo non può e non deve
ripresentare al pubblico le sue credenziali ogni qualvolta un cane
affamato lo afferri per i pantaloni. Noi, per esempio, avevamo con
sigliato Giuseppe Di Silvestro di non rispondere affatto e con noi ,
Io avevano consigliato tanti altri. Ma Giuseppe Di Silvestro ha deci-1
so di dire nei giornali coloniali la sua parola non a te. crimi
nale, bensì jfl pubblico, per dimostrare come anche in commercio,
sebbene non ne avesse avuto il dovere, non ha pari che possano
uguagliarlo nell'onestà la più rigida.
Truffatore, senti: devi dirci ora se sei proprio tu che puoi par
lare di moralità; tu che quando facevi l'assistente usciere di conci
liazione nel paesello che non era tuo. ti scacciarono perchè truffavi
perfino i 6 soldi che ti si consegnavano per le citazioni; tu che in
America rubasti anche i francobolli; tu che hai ripetutamente ricat
tato i banchisti coloniali, minacciandoli di esporre le loro piaghe al
pubblico; tu che pure oggi, con minaccia di scoprire le sue gesta
boccaccesche, hai truffato 50 dollari al cavadenti delle nove stra
de; tu che torturavi la prima moglie perchè la credevi disonesta; tu
che la opprimevi con due dozzine di bordanti pei- poter meglio goz
zovigliare con il frutto del suo lavoro impostole; tu che dopo aver
salassato di duemila dollari quel buon'uomo di Pasquale lo minac
ciasti poi d'arresto; tu che torturi, martirizzi la povera donna che ti
ha raccolto dal fango; tu. tu, ricattatore, sicario, uomo di fango,
criminale.
Arrivederci al prossimo numero. LA RAGIONE.
DA NON CONFONDERSI
Perchè il pubblico non abbia a confondere Angelo Curi con il
Dr. Curiangiolo, ci teniamo a dire che il primo è un onestissimo
connazionale, giornalista nato, collaboratole ambito de "La Voce
della Colonia" ; il secondo è un disturbatore di società ; fomentatore
di discordie coloniali ; dal cai-attere elasticissimo, come lo definisce '
Daniele Cubicciotti ; fegatoso, vendicatore fino al punto da lanciare I
i suoi giannizzeri all'assalto. Quando vuole incitare qualcuno, egli .
esclama: voi siete buoni a fare i popolani con le parole; però fatti !
ci vogliono, fatti.
ORGANO DI DIFESA DELLA ITALIANITÀ"
Le Rocambolesche gesta
di "Gnore Cocuccio"
L'Abruzzo è una vasta regione
dell'ltalia, sita quasi nel centro
di quella penisola suggestiva ed
incantevole che quasi enorme si
rena, si addormenta su un tri
plice mare. Una buona nsetà di
essa regione è lambita dalle
glauche acque dell'Adria sonan
te, l'altra metà è carezzata dalla
brezza degli zeffiri nelle afose
giornate estive, e tormentata dal
le tempeste del pigro gelo nella
stagion brumale.
E forse per questo il poeta
chiamò l'Abruzzo "forte e genti
le" intendendo, col primo qualifi
cativo, riferirsi a quella parte
della regione che si arrampica
sulle montagne aspre e scoscese
e la cui popolazione è dedita alla
pastorizia; col secondo, l'altra
parte che, baciata dal mare, e
colle vie del commercio dischiuse,
divide l'esistenza sua operosa tra
l'agricoltura e l'industria.
In un ridente lembo di questa
terra, sotto un cielo in
cantevole ed azzurro, sorge un
paesello pittoresco di circa tre
mila abitanti, ai ci piedi si di
stende una pianura übertosa e
feconda, ricca a preferenza di vi
gneti, che producono, ogni anno
in bella foggia e nuova, il frut
to ambrato dai cui succhi si spre
me il vino generoso.
Questa tranquilla dimoraci
fortunati mortali giace a ciréa
un miglio di distanza dalla più
bella e più incantevole spiaggia
del mondo e ad un miglio dalla
stazione ferroviaria, mentre una
j distanza quasi doppia la separa
Ida Giulianova, superba sede bal
neare.
ì Oh !la vita gioconda tra il si
! lenzio verde ! Nel crepuscolo, tra
; gli alberi folti, si radunano a
cianciare in coro le passere e dal
campanile della chiesa bianca, al
la domenica, slanciasi, acuta e
sottile, la voce della squilla! *
Se è vero che i nomi sono la
conseguenza dei fatti, questo
villaggio che ci siamo ingegnati
di descrivere alla meglio, dovet
te avere a fondatore, in epoca
non precisata, un notaio, ma le
cronache nulla dicono in proposi
to.
Molti anni addietro viveva nel
paese una famiglia distinta per
nascita, ma sfornita di mezzi di
fortuna, la quale, come appariva
anche dal nome, doveva essere
discesa dalla parte montagnosa
della regione.
Il padre, solerte e valoroso in
segnante, che coi frutti di un la
voro onorato, sostentava la fami
glia, aveva tre figli maschi, dei
quali, il terzogenito è il protago
nista di questa storica novella.
Fin dai più teneri anni il fan
ciullo rivelava le sue tendenze, ed
ognuno poteva indovinare che co
sa sarebbe diventato l'uomo a
dulto.
E le previsioni si avverarono;
fatto grande, egli diventò quel
che si aspettava: fannullone,
maldicente, prepotente, gesuita,
ignorante.
I compaesani, buoni villici, dai
costumi semplici, ma dalla mente
fertile ed immaginosa, volendo
compendiale in due sole parole
il meritato rispetto verso la fa
miglia ed il legittimo disprezzo
verso l'individuo indegno di ap
partenervi lo chiamarono: Ono
re Cocuccio (Signore Cocuccio),
e quell'appellativo gli rimase, fi
no a quando un bel giorno egli
non si decise a cambiar aria.
Giovanetto, frequentava le
PHILADELPHIA, PA., 16 MAGGIO, 1917.
scuole del villaggio, ma con mol
' to scarso successo, ed il genitore
jche gli faceva scuola non riuscì,
; malgrado tutti i suoi sforzi, a
fargli superare gli esami di pro
scioglimento. Negato allo studio,
i ribelle a qualsiasi disciplina, ven
ne su con perfide tendenze, col
l'animo pieno di odio verso di
tutti, da tutti cordialmente ri
cambiato, specialmente poi dal
maestro "Arrotino" del paese,
che non sapeva perdonargli la
lingua maledica.
Intanto il fratello primogenito,
ottimo giovane, per le sue buo
nissime doti di niente e di cuo
: re, aveva avuto la fortuna di
fare un buonissimo matrimonio.
(ìnore Cocuccio, nemico giura
to del lavoro ed avido di diverti
menti e di vagabondaggio, aveva
sperato di potei - attingere alla
dote della cognata e siccome tan
to questa che il marito il più
delle volte si opponevano alle im
moderate pretese di (inoro Co
cuccio, questi sperò di potere ot
tenere ogni cosa colle minacce e
colla violenza. Ed allora la'fan
tasia popolare, sempre disposta
all'esagerazione, si sbizzarrì co
ime un cavallo indomito e nel
villaggio si disse, ad una voce,
che il discolo aveva spianato il
l ucile contro suo padre.
Maja notizia era falsa, perchè
(inore t'ócuccio aveva compiute
i l'atto brigantesco solo contro il
; proprio fratello.
11 rimorso della colpa commes
| sa, l'indignazione sollevata ir
paese, sgomentarono (ìnore Co
cuccio che si sentì tutto invase
! dal desiderio di redimersi e di la
; vorare e si impiegò a piantar vi
! ti pei- prevenire la filossera, alle
I stipendio di 15 lire mensili. Ma
| buoni propositi, non avevano, ne
suo animo, una lunga durata, ec
egli si stancò subito di questa vi
| ta di sacrificio, affatto corrispon
dente ai suoi desideri.
Ritornò alle sue inveterate a
| bitudini ; all'ozio, al vagabon
! daggio, alla prepotenza, ed allo
I ra la famiglia di lui, pensò di di
! sfarsene, una volta per sempre
| inviandolo in un grande paese
j d'oltremare, ove un suo cugine
fioriva nel commercio, ed un al
: tro suo fratello, il secondogenito
mieteva allori ben meritati ne
campo libero della professione
A questi due si rivolse la fami
glia, implorando, ed essi aderen
do alla preghiera, inviarono, a
(ìnore Cocuccio, il biglietto di
passaggio.
■* * *
11 paesello pittoresco di circa
tremila anime, ai cui piedi si di
stende una pianura übertosa e fe
conda, è in festa. Vi si nota pei
le vie e nella piazza un movimeli
to insolito, la chiesa madre è af
follata di fedeli che ascoltano il
Te I)euni e le campane suonano
a festa giocondamente.
Qual'è la causa di tanta gioia?
La tranquilla popolazione con ai
la testa la famiglia di (ìnore
Cocuccio, si abbandona alla più
rumorosa allegria perchè costui
s'è deciso finalmente a stendere
lo sterminato Oceano tra lui ed il
villaggio natio.
E mentre lo squillo delle cam
pane arrivava fino al cielo, e la
nave lotta colle onde che si rin
corrono spaventosamente, Gnore
Cocuccio, dritto sulla tolda del
transoceanico, lo sguardo verso
la spiaggia che va mano mano
scomparendo, sentì la nostalgia
: dei luoghi che furono testimoni
delle rocambolesche sue gesta.
Dopo lunga e fortunosa navi
gazione, il piroscafo approdò fi
nalmente in un porto immenso di
una stenninata metropoli orien
tale esi incominciarono le ope
razioni di sbarco.
La figura secca ed allampata
di Gnore Cocuccio, in meschini:--
1 simo arnese, presso a poco equi
paggiato come il Sig. Cassiere,
quando giunse da Scranton, il
suo copricapo dal colore di cane
in fuga, con dodici buchi appari
scenti, il vestito lacero, richiama
rono subito su di lui la diffiden
te attenzione delle Autorità di
Emigrazione. Fu quindi sottopo
sto ad un accurato esame e ad
un minuzioso interrogatorio, ma
alla prova di lettura e alla prova
grafica se la cavò alla men trista.
Il guaio fu, quando, domandato
se avesse i mezzi necessari per
la continuazione del viaggio, po
tè, vuotando le tasche, raggra
nellare appena la meschina som
ma di lire 2.57. Rimase pertanto
detenuto in batteria, e per met
terlo fuori dovettero intervenire
il germano ed il cugino.
Quest'ultimo specialmente, fa
i cile di lingua, ma di animo otti
mo, lo accolse con grandi di
; mostrazioni di affetto, lo ammi
se nella sua azienda e, come pri
; mo attestato, gli comprò un cap
pello di due dollari.
* * *
Qualche tempo dopo l'arrivt
dal natio paesello, Gnore Cocuc
sunto anche l'aria di persona t
modo. Qualcuno ricorre a lui pei
pareri ed egli si rivela ad ur
tratto espertissimo nel consiglia
re fallimenti dolosi, ricomperali
do a metà prezzo la merce da lu
fatta nascondere. Dà persine
prove del suo valore letterario
ricopiando da un vecchio statuti
il regolamento da servire pei
una nuova Società provinciale
che poi non sorse, alla stessj
guisa che più tardi aborti mise
ramente un altro tentativo d
Gnore Cocuccio di fondare unì
loggia di indipendenti.
* Ma il nuovo ambiente, la posi
zione di comproprietario di une
azienda (poiché il buon cugino si
l'era associato fin dal primo gior
no dell'arrivo) non valsero a ri
l'ormare la sua indole ed a rifar
ne il carattere.
I cattivi istinti, per qualche
tempo sopiti, si ridestarono, ed il
serpentello, riscaldato, tentò di
mordere il suo benefattore.
Cercò, senza riuscirvi, di sba
razzarsi del cugino e, finalmente,
decise di separarsene, dopo aver
gli tolte parecchie rappresentan
ze.
E lo si vide a capo di una nuo
va Ditta, sempre uguale a se
stesso; ipocrita, gesuita, che
mentre ti strisciava di fronte, ti
colpiva alle spalle, colla sua mal
dicenza.
Da allora rifulse tutta la sua
capacità a delinquere ; i generi
domestici li smerciava per gene
ri importati ; vendeva, a danaro
contante, bottigline-campioni che
egli aveva gratis dalle Case, per
distribuirle ai medici e farmaci
sti a titolo di reclame.
Un bel giorno decise di ammo
gliarsi ; ma egli che mai, in vita
sua, aveva sentito un affetto,
tentò di fare, del matrimonio, u
na ignobile speculazione.
E si mise alla caccia di una do
te, e quando gli sembrò di averla
trovata, strinse subito il contrat
to. Ma, accortosi che la realtà
non rispondeva all'ardente sua
cupidigia, imprecò contro i ma
nipolatori che lo avevano indotto
Anno I No. ."> Snidi la Copia
al gran passo, prospettandogli un
falso miraggio.
Tale il moralista, l'onesto, l'in
telligente, il ricco signore che si
atteggia, in pubblico, anche a fi
lantropo, a protettore di pupilli.
Ma allorché si richiude in sè
stesso, ia sua mente rivola al pae
sello sito in prossimità della
spiaggia ridente, ove potè vive
re a lungo, scroccando ed ozian
do, senza che non gli turbasse i
sonni e la digestione.
Il novelliere.
Punte di spillo
FILIPPO CORRE AL PRO
PRIO SALVATAGGIO
La scorsa settimana, non ap
pena Filippo lesse sulle colonne
della Ragione i piccanti aneddoti
illustranti la sua vita educata al
la scuola dell'onestà e dei dovere»
ebbe un scossa di nervi che fece
temere della -uà preziosa esisten
za.
Rimessoci poco dopo, si armò...
di coraggio e di pazienza, infilò
l'uscio di casa e giii a rompicollo,
per le vie della colonia.
Dopo una lunga ed affannosa
corsa, andò a battere di muso
contro una campana che era in
sieme ad un campani. ..010, e gri
dò, con tutto il (iato dei suoi pol
moni :
Tu, mio amico, tu che da
tanti anni mi conosci, puoi cal
mare la mia coscienza che quai
che malvagio vorrebbe avvelena
re. inoculandovi il dubbio.
Credi tu che io sia un uomo one
sto? Favella, esprimi il tuo con
vincimento e riabilita un galan
tuomo! Ma l'altro, con una
calma che avrebbe fatto perdere
1 la pazienza anche ad un morto.
E che ne so io, o Filippo,
dei fatti tuoi ? Cioè, no ; a vo
ler essere sincero, qualche cosa
posso dirti, che, se non riesce a
calmar i tuoi nervi e la tua co
scienza, non deve ascriversi a
mia colpa.
Lo afferma 1111 tuo collega iti
giornalismo che porta un nome
che incute spavento e posso ri
peterlo anch'io. Secondo questo
tuo collega mazza scarica, tu a
vresti imbrogliato dollari 400 ad
un figlio d'ltalia
Pallido come un cadavere, Fi
lippo continua la sua corsa sfre
nata e pensa tra sè :
E' mai possibile che io sia
un disonesto e finora non me ne
era accorto? Sarebbe orribile,
specie dopo aver apposto la fir
ma a quella lettera che è causa
di tutti i miei mali? Un partico
lare mi viene in mente che avva
lora le basse calunnie delle Ra
gione. Quando mi diedi il ban
chetto, che è rimasto famoso in
colonia, il rappresentante della
Keyston Coal Co. non interven
ne, sebbene cento volte invitato.
Che anch'egli mi abbia preso per
un disonesto?
Per la verità, i conti non erano
in regola
E corri, corri, corri il po
vero Filippo, trafelato ed ansan
te, piombò nel mio store, come
un bolide. A tutta prima pensai
| ad un'aggressione. Il furore, si sa
bene, è cattivo consigliere e, ad
ogni buon fine, aprii il cassetto,
per dar di piglio alla rivoltella,
ma apparve tanto ridicola e tan
to innocua la faccia di Filippo,
che scoppiai in una sonora risa
ta.
Anche tu, compare Turid
du, attacchi un vecchio amico,
che ebbe sempre per te la massi
ma stima? Sappi che hai affer-


# kiara\kiara\examples\data\text_corpus\La_Ragione\sn84037024_1917-05-16_ed-2_seq-1_ocr.txt
LA RAG ONE
ORGANO DI DIFESA DELLA ITALIANITÀ"
contro i vili, i camorristi, i sicari, i falsari e gli austriacanti, nemici della patria di origine e di quella d'adozione.
F. SILVAGNI, Direttore, 911 Christian Street, Philadelphia, Pa-
LA MONTAGNA HA PARTORITO
ed ha fatto un topolino
LADRO DI FRANCOBOLLI; TRUFFATORE; ASSASSINO
DELLA PRIMA E MARTIRIZZATORE DELLA SECONDA
MOGLIE; RICATTATORE; SICARIO; UOMO DI
FANGO, CRIMINALE, ASCOLTATECI:
La montagna ha partorito ed ha l'atto un topolino!
La minacciata lettera ricatto, pardon, aperta, è venuta fuo
ri, dalla melma che ti ricopre, per eternare la tua impossibilità di
colpire i migliori uomini delle nostre colonie; per convincere il pub
blico, ancora una volta, se ve ne fosse il bisogno, che la figura di
Giuseppe Di Silvestro si erge troppo austera e dignitosa su una base
granitica che non si sgretola, neanche contro i colpi di vigliacchi co
spiratori che hanno armata la mano del sicario e Io hanno lanciato
e continuano a lanciarlo contro i forti.
Signori bnchisti; signori coniatori di monete false; signori in
cendiar»; signori curatori di fallimenti; signori ipocriti dalle parole
dolci e dal cuore di fango, il denaro gittato al vostro sicario non sor
tirà l'effetto desiderato. Voi vi siete scavata la fossa e noi vi ci sot
terreremo!
« * *
L;i montagna, dicevamo, ha partorito ed ha fatto un topolino!
Siamo costretti a riportare, ancora oggi, perchè il pubblico dei
lettori non dimentichi da un numero all'altro, l'ultimo spunto scrit
to e pubblicato dal degenerato sull'Opinione del Popolo del ."I Aprile
1915, appena due anni fa, spunto che è un contributo di omaggio ai
sacrifici fatti, alle energie onestamente spese, come il sicario stesso
ammette, da Giuseppe Di Silvestro per aver dato alle colonie italia
ne d'America una fiaccola dalla luce limpida e pura, una Voce del
Popolo, che i posteri ricorderanno, un giornale èil degenerato ad
affermarlo CHE RISPONDEVA A PREFERENZA DI QUA
LUNQUE ALTRO ALLE ESIGENZE DELLA ITALIANITÀ' E
MIGRATA, GIACCHE' NELLE NON POCHE LOTTE SOSTE
NUTE DETTE MOLTE PROVE LUMINOSE 1)1 QUELLO SPI
RITO D'IMPARZIALITA' E 1)1 INDIPENDENZA CHE QUASI
MAI SI EBBE RAGIONE DI LODARE IN ALTRI.
E, sempre il degenerato a parlare, aggiungeva. NON STA A
NOI INDAGARE E DISCUTERE LE RAGIONI CHE AVRANNO
0 STARANNO PER DETERMINARE IL PASSO DEL VOCIFE
RATO TRASFERIMENTO; CERTAMENTE NON VI SARANNO
ESTRANEE QUELLE DI INDOLE AMMINISTRATIVA, (iIAC
CHE', COME OGNUNO SA, SE VI E' STATO UN GIORNALE
QUOTIDIANO CHE HA SEMPRE PER TRE QUARTI DOVUTO
DIPENDERE DALLE ENERGIE E DAI SACRIFICI DI POCHI
CHE LO REDIGEVANO E DIRIGEVANO, QUESTO QUOTIDIA
NO E' STATO PRECISAMENTE LA VOCE DEL POPOLO, SEN
ZA CHE LA COLONIA AVESSE MAI POTUTO, DAL LATO
DELLA PUBBLICITÀ' INCORAGGIARLO COME AVREBBE
DOVUTO.
Noi invitiamo il degenerato a smentire quanto sopra; noi do
mandiamo al sicario se diceva la verità quando egli pubblicava il
suddetto spunto; noi siamo ansiosi di sapere fino a qual punto arri
va l'incoscienza di un'anima venduta, di un'anima prava: 'li un'ani
ma di fango.
Ma noi conosciamo il degenerato. Egli ci risponderà che quan
do scrisse quello spunto era infradicito dalla bevanda e perciò non
ricorda nulla.
* # #
Giuseppe Di Silvestro, è vero, ha parecchi difetti dipendenti
dal suo temperamento caldo; ma Giuseppe Di Silvestro amici
e nemici glie lo riconoscono è l'uomo che tutto dimentica; è il
connazionale dal carattere aperto e leale; pieno di sincerità; l'ita
liano che per la sua scrupolosa onestà non ha confronti: egli è, so
pratatto, generoso. Non è da sorprendersi perciò se lo si vede sem
pre circondato e sostenuto dalla come non deve sembrar
strano se oggi noi, che in lui apprezziamo le sue virtù personali e
quelle di condottiere, temuto e rispettato, dei Figli d'ltalia in Penn
sylvania, gli siamo e gli saremo sempre al fianco e con noi le die
cine di migliaia di suoi estimatori per difenderlo dalle zanne av
velenate di un mercenario e di anime vili che questo mercenario so
spingono.
Ladro di francobolli, apri le orecchie: Giuseppe Di Silvestro in
verità non ha bisogno di difensori. I suoi difensori sono i 20 anni
di vita d'America, vita di sacrifici, vita di lotte contro i disonesti
come te; vita intemerata. I difensori di (ìiuseppe Di Silvestro sono
1 suoi compaesani residenti qui che in Italia lo ricordano il beniami
no di tutti i ceti nella natia Bussi.
Tu capirai, degenerato, che un galantuomo non può e non deve
ripresentare al pubblico le sue credenziali ogni qualvolta un cane
affamato lo afferri pei- i pantaloni. Noi. per esempio, avevamo con
sigliato (iiuseppe Di Silvestro di non rispondere affatto e con noi
lo avevano consigliato tanti altri. Ma (iiuseppe Di Silvestro ha deci
so di dire nei giornali coloniali la sua parola non a te, crimi
nale, bensì al pubblico, per dimostrare come anche in commercio,
sebbene non ne avesse avuto il dovere, non ha pari che possano
uguagliarlo nell'onestà la più rigida.
Truffatore, senti: devi dirci ora se sei proprio tu che puoi par
lare di moralità; tu che quando facevi l'assistente usciere di conci
liazione nel paesello che non era tuo. ti scacciarono perchè truffavi
perfino i 6 soldi che ti si consegnavano per le citazioni; tu che in
America rubasti anche i francobolli; tu che hai ripetutamente ricat
tato i banchisti coloniali, minacciandoli di esporre le loro piaghe al
pubblico; tu che pure oggi, con minaccia di scoprire le sue gesta
boccaccesche, hai truffato 50 dollari al cavadenti delle nove stra
de; tu che torturavi la prima moglie perchè la credevi disonesta; tu
che la opprimevi con due dozzine di bordanti per poter meglio goz
zovigliare con il frutto del suo lavoro impostole; tu che dopo aver
salassato di duemila dollari quel buon'uomo di Pasquale lo minac
ciasti poi d'arresto; tu che torturi, martirizzi la povera donna che ti
ha raccolto dal fango: tu, tu, ricattatore, sicario, uomo di fango,
criminale.
Arrivederci al prossimo numero. LA RAGIONE.
1
DA NON CONFONDERSI
Perchè il pubblico non abbia a confondere Angelo Curi con il
)r. Curiangiolo, ci teniamo a dire che il primo è un onestissimo
onnazionale, giornalista nato, collaboratore ambito de "La Voce
lella Colonia" ; il secondo è un disturbatore di società ; fomentatore
li discordie coloniali ; dal carattere elasticissimo, come lo definisce
)aniele Cubicciotti ; fegatoso, vendicatore fino al punto da lanciale
suoi giannizzeri all'assalto. Quando vuole incitale qualcuno, egli
isclama: voi siete buoni a fare i popolani con le parole; però fatti
ti vogliono, fatti.
PHILADELPHIA, PA., 16 MAGGIO, 1917.
Le Rocambolesche gesta
di "Gnore Cocuccio"
L'Abruzzo è una vasta regione
dell'ltalia, sita quasi nel centro
di quella penisola suggestiva ed
incantevole che quasi enorme si
rena, si addormenta su un tri
plice mare. Una buona metà di
essa regione è lambita dalle
glauche acque dell'Adria sonan
te, l'altra metà è carezzata dalla
brezza degli zeffiri nelle afose !
giornate estive, e tormentata dal
le tempeste del pigro gelo nella
stagion brumale.
E forse per questo il poeta
chiamò l'Abruzzo "forte e genti
le" intendendo, col primo qualifi
cativo, riferirsi a quella parte
della regione che si arrampica
sulle montagne aspre e scoscese
e la cui popolazione è dedita alla
pastorizia ; col secondo, l'altra
parte che, baciata dal mare, e
colle vie del commercio dischiuse,
divide l'esistenza sua operosa tra
l'agricoltura e l'industria.
In un ridente lembo di questa
terra, sotto un cielo in- i
cantevole ed azzurro, sorge un
paesello pittoresco di circa tre
mila abitanti, ai cui piedi si di
stende una pianura übertosa e
feconda, ricca a preferenza di vi
gneti, che producono, ogni anno
in liella foggia e nuova, il frut
to ambrato dai cui succhi si spre
me il vino generoso.
Ciuesty tranquilla. jtioiw' < t,; ~
fortunati mortali giace a circa
im miglio di distanza dalla più
liella e più incantevole spiaggia
del mondo e ad un miglio dalla :
stazione ferroviaria, mentre una i
I distanza quasi doppia la separa j
'da Giulianova, superba sede bal
neare.
i Oh! la vita gioconda tra il si
lenzio verde ! Nel crepuscolo, tra
! gli silberi folti, si radunano a
; cianciare in coro le passere e dal -
campanile della chiesa bianca, al
;la domenica, slanciasi, acuta e
Isottile, la voce della squilla!
| So è vero c|ie i nomi sono la
i con ;eguenza dei fatti, questo
villaggio che ci siamo ingegnati
di descrivere alla meglio, dovet-
Ite avere a fondatore, in epoca
j non precisata, un notaio, ma le
cronache nulla dicono in proposi
to.
Molti anni addietro viveva nel
paese una famiglia distinta per
nascita, ma sfornita di mezzi di
fortuna, la quale, come appariva j
anche dal nome, doveva essere
discesa dalla parte montagnosa,
della regione.
Il padre, solerte e valoroso in
gegnante, che coi frutti di un la
| voro onorato, sostentava la 1 ani i -
; glia, aveva tre figli maschi, dei
quali, il terzogenito è il protago
nista di questa storica novella.
Fin dai più teneri anni il fan
| ciullo rivelava le sue tendenze, ed
ognuno poteva indovinare che co
sa sarebbe diventato l'uomo a
dulto.
E le previsioni si avverarono;!
fatto grande, egli diventò quel
jche si aspettava: fannullone,
! maldicente, prepotente, gesuita,
I ignorante.
I compaesani, buoni villici, dai
costumi semplici, ma dalla mente i
t'ertile ed immaginosa, volendo j
compendiare in due sole parole
il meritato rispetto verso la fa
miglia ed il legittimo disprezzo
verso l'individuo indegno di ap
partenervi lo chiamarono: Gno
|re Cocuccio (Signore Cocuccio),
|e quell'appellativo gli rimase, fi
;no a quando un bel giorno egli
i non si decise a cambiar aria.
Giovanetto, frequentava le
scuole del villaggio, ma con mol
lo scarso successo, ed il genitore
| che gli faceva scuola non riuscì,
malgrado tutti i suoi sforzi, a
fargli superare gli esami di pro
scioglimento. Negato allo studio,
ribelle a qualsiasi disciplina, ven
ne su con perfide tendenze, col
| l'animo pieno di odio verso di
; tutti, da tutti cordialmente ri
cambiato, specialmente poi dal
maestro "Arrotino" del pause,
che non sapeva perdonargli la
lingua maledica.
Intanto il fratello primogenito,
; ottimo giovane, per le sue bup-,
nissime doti di mente e di cuo
re, aveva avuto la fortuna di
fare un buonissimo matrimonio.
(ìnore Cocuccio, nemico giura- ì
to del lavoro od avido di diverti
menti e di vagabondaggio, aveva
sperato di poter attingere alla
dote della cognata e siccome tan
to questa che il marito il più
delle volte si opponevano alle im- :
moderate pretese di Gnore Co
cuccio, questi sperò di potere ot-1
tenere ogni cosa colle minacce e
fcolla violenza. Ed allora la fan
tasia popolare, sempre disposta
all'esagerazione, si sbizzarrì co
me un cavallo indomito e nel
villaggio si disse, ad una voce,
che il discolo aveva spianato il
fucile contro suo padre.
Gnore Cocuccio aveva compiuto
l'atto brigantesco solo contro il
proprio fratello.
: 11 rimorso della colpa commes
i sa, l'indignazione sollevata in
paese, sgomentarono Gnore Co
cuccio che si sentì tutto invaso
dal desiderio di redimersi e di la
vorare e si impiegò a piantar vi
ti per prevenire la filossera, allo !
stipendio di 15 lire mensili. Ma i
; buoni propositi, non avevano, nel
suo animo, una lunga durata, ed
egli si stancò subito di questa vi
ta di sacrificio, affatto corrispon
dente ai suoi desideri.
Ritornò alle sue inveterate a
bitudini; all'ozio, al vagabon
| daggio, alla prepotenza, ed allo
ra la famiglia di lui, pensò di di
sfarsene, una volta per sempre,
inviandolo in un grande paese
d'oltremare, ove un suo cugino
fioriva nel commercio, ed un al
tro suo fratello, il secondogenito,
mieteva allori ben meritati nel
| campo libero della professione.
; A questi due si rivolse la fami
glia, implorando, ed essi aderen
do alla preghiera, inviarono, a
Gnore Cocuccio, il biglietto di
passaggio.
* * ♦
il paesello pittoresco di circa
tremila anime, ai cui piedi si di
stende una pianura übertosa e fe
! conda, è in festa. Vi si nota per
le vie e nella piazza un movimen
to insolito, la chiesa madre è af
follata di fedeli che ascoltano il
Te Deum e le campane suonano
a festa giocondamente.
Qual'è la causa di tanta gioia ?
La tranquilla popolazione con al
la testa la famiglia di Gnore
Cocuccio, si abbandona alla piti
| rumorosa allegria perchè costui
| s'è deciso finalmente a stendere
lo sterminato Oceano tra lui ed il
villaggio natio.
E mentre lo squillo delle cam
pane arrivava fino al cielo, e la
nave lotta colle onde che si rin
corrono spaventosamente, Gnore
Cocuccio, dritto sulla tolda del I
transoceanico, lo sguardo verso |
la spiaggia che va mano mano
scomparendo, sentì la nostalgia ;
dei luoghi che furono testimoni
delle rocambolesche sue gesta.
Dopo lunga e fortunosa navi
gazione, il piroscafo approdò fi
nalmente in un porto immenso di
una sterminata metropoli orien
tale e si incominciarono le ope
razioni di sbarco.
La figura secca ed allampata
di (ìnore Cocuccio, in meschinis
simo arnese, presso a poco equi
paggiato come il Sig. Cassiere.
: quando giunse da Scranton, il
suo copricapo dal colore di cane
in fuga, con dodici buchi appari
scenti, il vestito lacero, richiama
! rono subito su di lui la diffiden
te attenzione delle Autorità di
Emigrazione. Fu quindi sottopo
sto ad un accurato esame e ad
un minuzioso interrogatorio, ma
alla prova di lettura e alla prova
grafica se la cavò alla meii trista.
Il guaio fu, quando, domandato
se avesse i mezzi necessari per
la continuazione del viaggio, po
tè, vuotando le tasche, ra"gra
nellare appena la meschina som
ina di lire 2.57. Rimase pertanto
detenuto in batteria, e per met
terlo fuori dovettero intervenire
; il germano ed il cugino.
Quest'ultimo specialmente, fa
i die di lingua, ma di animo otti
i ino, lo accolse con grandi di
mostrazioni di affetto, lo animi
se nella sua azienda e, come pri
mo attestato, gli comprò un cap
pello di due dollari.
* * *
Qualche tempo dopo l'arrivo
; dpt .fi»*"" """i'- 'J*? if' ""X ''
ciò si è molto ripulito ed ha as
sunto anche l'aria di persona a
modo. Qualcuno ricorre a lui per
pareri ed egli si rivela ad un |
tratto espertissimo nel consiglia
re fallimenti dolosi, ricomperan
do a metà prezzo la merce da lui
fatta nascondere. Dà persino
prove del suo valore letterario,
i ricopiando da un vecchio statuto
il regolamento da servire per
una nuova Società provinciale
che poi non sorse, alla stessa
guisa che più tardi aboru mise
ramente un altro tentativo di
(ìnore Cocuccio di fondare una
loggia di indipendenti.
Ma il nuovo ambiente, la posi
zione di comproprietario di una
azienda (poiché il buon cugino se
l'era associato fin dal primo gior
no dell'arrivo) non valsero a ri
formare la sua indole ed a rifar
ne il carattere.
I cattivi istinti, per qualche
tempo sopiti, si ridestarono, ed il
serpentello, riscaldato, tentò di
mordere il suo benefattore.
Cercò, senza riuscirvi, di sba
razzarsi del cugino e, finalmente,
decise di separarsene, dopo aver
gli tolte parecchie rappresentan
ze.
E lo si vide a capo di una nuo
va Ditta, sempre uguale a se
stesso; ipocrita, gesuita, che
mentre ti strisciava di fronte, ti
colpiva alle spalle, colla sua mal
dicenza.
Da allora rifulse tutta la sua
capacità a delinquere; i generi
domestici li smerciava per gene
ri importati; vendeva, a danaro
contante, bottigline-campioni che
egli aveva gratis dalle Case, per
distribuirle ai medici e farmaci
sti a titolo di reclame.
Un liei giorno decise di ammo
i gliarsi ; ma egli che mai, in vita
sua, aveva sentito un affetto,
tentò di fare, del matrimonio, u
na ignobile speculazione.
E si mise alla caccia di una do
te, e quando gli sembrò di averla
trovata, strinse subito il contrat
, to. Ma, accortosi che la realtà
| non rispondeva all'ardente sua
I cupidigia, imprecò contro i ma
j nipolatori che lo avevano indotto
Anno I No. 5 Soldi la Copia
al gran passo, prospettandogli un
falso miraggio.
Tale il moralista, l'onesto, l'in
telligente, il ricco signore che st
atteggia, in pubblico, anche a fi
lantropo. a protettore di pupilli-
Ma allorché si richiude in sè
stesso, la sua niente rivola al pae
sello sito in prossimità della
spiaggia ridente, ove potè vive
re a lungo, scroccando ed ozian
do, senza che non gli turbasse i
sonni e la digestione.
Il novelliere.
Punte di spillo
FILIPPO CORK IO AL PRO
PRIO SALVATAGGIO
La scorsa settimana, non ap
pena Filippo lesse sulle colonne
della Ragione i piccanti aneddoti
illustranti la sua vita educata al
la scuola dell'onestà e del dovere,
ebbe un scossa di nervi che fece
temere della sua preziosa esisten
za.
Rimessosi poco dopo, si armò...
di coraggio e di pazienza, infilò
l'uscio di casa e giù a rompicollo,
per le vie della colonia.
Dopo una lunga ed affannosa
corsa, andò a battere di muso
contro una campana che era in
sieme ad un campani. ..010, e gri
dò, con tutto il fiato dei suoi pol
moni :
Tu, mio amico, tu che da
tanti anni mi conosci, puoi
iiiuiv ,< ini.» cK.scifcilza cne ijiiai
che malvagio vorrebbe avvelena
re, inoculandovi il dubbio,
Credi tu che io sia un uomo one
sto? Favella, esprimi il tuo con
vincimento e riabilita un galan
tuomo! —Ma l'altro, con una
calma che avreblie fatto perdere
la pazienza anche ad un morto.
E che ne so io, o Filippo,
dei fatti tuoi? Cioè, no; a vo
ler essere sincero, qualche cosa
posso dirti, che, se non riesce a
calmar i tuoi nervi e la tua co
scienza, non deve ascriversi a
mia colpa.
Lo afferma un tuo collega iti
giornalismo che porta un nome
che incute pavento e posso ri
peterlo anch'io. Secondo questo
tuo collega mazza scarica, tu a
vresti imbrogliato dollari 400 ad
un figlio d'ltalia
Pallido come un cadavere, Fi
lippo continua la sua corsa sfre
nata e pensa tra sè :
E' mai possibile che io >ia
un disonesto e finora non me ne
era accorto? Sarebbe orribile,
specie dopo aver apposto la fir
ma a quella lettera che è causa
di tutti i miei mali? Un partico
lare mi viene in mente che avva
lora le basse calunnie delle Ra
j gione. Quando mi diedi il ban
| chetto, che è rimasto famoso in
i colonia, il rappresentante della
! Keyston Goal Co. non interven
j ne, sebbene cento volte invitato,
j Che anch'egli mi abbia preso pei
un disonesto ?
Per la verità, i conti non erano
in tegola
E corri, corri, corri il po
vero Filippo, trafelato ed ansan
te, piombò nel mio store, come
un bolide. A tutta prima pensai
I ad un'aggressione. Il furore, si sa
bene, è cattivo consigliere e, ad
ogni buon fine, aprii il cassetto,
per dar di piglio alla rivoltella
ma apparve tanto ridicola e tan
to innocua la faccia di Filippo
che scoppiai in una sonora risa
ta.
Anche tu, compare Turid
du, attacchi un vecchio amico,
che ebbe sempre per te la massi
ma stima? Sappi che hai affer-


# kiara\kiara\examples\data\text_corpus\La_Ragione\sn84037024_1917-05-16_ed-3_seq-1_ocr.txt
contro 1 vili, i camorristi, i sicari, i falsari e gli austriacanti, nemici della patria di origine e di quella d'adozione.
F. SILVAGNI, Direttore, 911 Christian Street, Philadelphia, Pa.
LA MONTAGNA HA PARTORITO
ed ha fatto un topolino
LADRO DI FRANCOBOLLI; TRUFFATORE; ASSASSINO
DELLA PRIMA E MARTI RIZZATORE DELLA SECONDA
MOGLIE; RICATTATORE; SICARIO; UOMO 1)1
FANGO, CRIMINALE, ASCOLTATECI:
La montagna ha partorito ed ha fatto un topolino!
La minacciata lettera ricatto, pardon, aperta, è venuta fuo
ri, dalla melma che ti ricopre, per eternare Li tua impossibilità di
colpirei migliori uomini delle nostre colonie; per convincere il pub
blico, ancora una volta, se ve ne fosse il bisogno, che la figura di
Giuseppe Di Silvestro si erge troppo austera e dignitosa su una base
granitica che non si sgretola, neanche contro i colpi di vigliacchi co
spiratori che hanno armata la mano del sicario e lo hanno lanciate
e continuano a lanciarlo contro i forti.
Signori bnchisti; signori coniatori di monete false; signori in
cendiarti; signori curatori di fallimenti; signori ipocriti dalle parok
dolci e dal cuore di fiingo, il denaro gittato ili vostro sicario non sor
tira l'effetto desiderato. Voi vi siete scavata la fossa e noi vi ci sot
ferreremo!
* * *
La montagna, dicevamo, ha partorito ed ha fatto un topolino!
Siamo costretti a riportare, ancora oggi, perchè il pubblico dei
lettori non dimentichi da un numero all'altro, l'ultimo spunto scrit
to e pubblicato dal degenerato sull'Opinione del Popolo del 3 Aprile
1915, appena due anni fa, spunto che è un contributo di omaggio ai
sacrifici fatti, alle energie onestamente spese, come il sicario stesse
ammette, da Giuseppe Di Silvestro per aver dato alle colonie Italia
ne d'America una fiaccola dalla luce limpida e pura, una Voce del
Popolo, che i posteri ricorderanno, un giornale èil degenerato ac
affermarlo CHE RISPONDEVA A PREFERENZA DI QUA
LUNQUE ALTRO ALLE ESIGENZE DELLA ITALIANITÀ' E
MIGRATA, GIACCHE' NELLE NON POCHE LOTTE SOSTE
NUTE DETTE MOLTE PROVE LUMINOSE I» QUELLO SPI
RITO D'IMPARZIALITA' E 1)1 INDIPENDENZA CHE QUASI
M AI SI EBBE RAGIONE 1)1 LODARE IN ALTRI.
E, sempre i! degenerato*a parlare, aggiungeva. NON STA \
NOI INDAGARE E DISCUTERE LE RAGIONI CHE AVRANNO
O STARANNO PER DETERMINARE IL PASSO DEL VOCIFE
RATO TRASFERIMENTO: CERTAMENTE NON VI SARANNO
ESTRANEE QUELLE DI INDOLE AMMINISTRATIVA, GIAC
CHE', COME OGNUNO SA, SE VI E' STATO UN GIORNALE
QUOTIDIANO CHE HA SEMPRE PER TRE QUARTI DOVUTO
DIPENDERE DALLE ENERGIE E DAI SACRIFICI DI POCHI
CHE LO REDIGEVANO E DIRIGEVANO, QUESTO QUOTIDIA
NO E' STATO PRECISAMENTE LA VOCE DEL POPOLO. SEN
ZA CHE LA COLONIA AVESSE MAI POTUTO, DAL LVTO
DELLA PUBBLICITÀ' INCORAGGIARLO COME AVREBIiF
DOVUTO.
Noi invitiamo il degenerato a smentire quanto sopra; noi do
mandiamo al sicario se diceva la verità quando egli pubblicava il
suddetto spunto: noi siamo ansiosi di sapere lino a qual punto arri
va l'incoscienza di un'anima venduta, di un'anima prava; di un'ani
ma di fango.
Ma noi conosciamo il degenerato. Egli ci risponderà che quan
do scrisse quello spunto era infradicito dalla bevanda e perciò nor
ricorda nulla.
« # *
Giuseppe Di Silvestro, è vero, ha parecchi difetti dipendenti
dal suo temperamento caldo; ma Giuseppe Di Silvestro amici
e nemici glie lo riconoscono è l'uomo che tutto dimentica; è il
connazionale dal carattere aperto e leale; pieno di sincerità: l'ita
liano che per la sua scrupolosa onestà non ha confronti; egli è, so
pra tutto, generoso. Non è da sorprendersi perciò se lo si vede seni
pre circondato e sostenuto dalla ma ■•sa, come non deve sembrai
strano se oggi noi, che in lui apprezziamo le sue virtù personali e
quelle di condottiere. temuto e rispettato, dei Figli d'ltalia in Penn
sylvania, gli siamo e gli sa remore ni pre al fianco e con noi le die
cine di migliaia di suoi estimatori ;>er difenderlo dalle zanne av
velenate di un mercenario e di anime vili che questo mercenario so
spingono.
Iridio di francobolli, apri le orecchie: Giuseppe Di Silvestro ir
verità non ha bisogno di difensori. I suoi difensori sono i 20 anni
di vita d'America, vita di sacrifici, vita di lotte contro i disonesti
come te; vita intemerata. I difensori di Giuseppe Di Silvestro sono
i suoi compaesani residenti qui che in Italia lo ricordano il beniami
no di tutti i ceti nella natia Bussi.
Tu capirai, degenerato, che un galantuomo non può e non deve
ripresentare al pubblico le sue credenziali ogni qualvolta un cane
affamato lo afferri pei i pantaloni. Noi, per esempio, avevamo con
sigliato Giuseppe Di Silvestro di non rispondere affatto e con noi
lo avevano consigliato tanti altri. Ma Giuseppe Di Silvestro ha deci
so di dire nei giornali coloniali la sua parola non a te. crimi
nale. bensi al pubblico, per dimostrare come anche in commercio,
■ sebbene non ne avesse avuto il dovere, non ha pari che possano
uguagliarlo nell'onestà la più rigida.
Truffatore, senti: devi dirci ora se sei proprio tu che puoi par
lare di moralità; tu che quando facevi l'assistente usciere di conci
liazione nel paesello che non era tuo. ti scacciarono perchè truffavi
perfino i 6 soldi che ti si consegnavano per le citazioni; tu che in
America rubasti anche i francobolli: tu che hai ripetutamente ricat
tato i banchisti coloniali, minacciandoli di esporre le loro piaghe al
pubblico; tu che pure oggi, con minaccia di scoprire le sue gesta
boccaccesche, hai truffato ">0 dollari al cavadenti delle nove stra
de; tu che torturavi la prima moglie perchè la credevi disonesta; tu
che la opprimevi con due dozzine di bordanti per poter meglio goz
zovigliare con il frutto del suo lavoro impostole; tu che dopo aver
■salassato di duemila dollari quel buon'uomo di Pasquale Io minac
ciasti poi d'arresto; tu che torturi, martirizzi la povera donna che ti
ha raccolto dal fango; tu, tu, ricattatore, sicario, uomo di faniro
criminale.
Arrivederci al prossimo numero. LA RAGIONE.
DA NON CONFONDERSI
I Perchè il pubblico non abbia a confondere Angelo Curi con il
Dr. Curiangiolo, ci teniamo a dire che il primo è un onestissimo
(connazionale, giornalista nato, collaboratore ambito de "La Voce
[ella Colonia"; il secondo è un disturbatore di società; fomentatore
i discordie coloniali ; dal carattere elasticissimo, come lo definisce
>aniele Cubicciotti ; fegatoso, vendicatore fino al punto da lanciale
suoi giannizzeri all'assalto. Quando vuole incitare qualcuno, egli
sclama: voi siete buoni a fare i popolani con le parole; però fatti
( i vogliono, fatti.
ORGANO DI DIFESA DELLA ITALIANITÀ'
Le Rocambolesche gesta
di "Gnore Cocuccio"
L'Abruzzo è una vasta regione
dell'ltalia, sita quasi nel centro
di quella penisola suggestiva ed
incantevole che quasi enorme si
rena, si addormenta su un tri
plice mare. Una buona metà di
sssa regione è lambita dalle
glauche acque dell'Adria sonan
te, l'altra metà è carezzata dalla
brezza degli zeffiri nelle afose
giornate estive, e tormentata dal
le tempeste del pigro gelo nella
stagion brumale.
E forse pei - questo il poeta
chiamò l'Abruzzo "forte e genti
le" intendendo, col primo qualifi
cativo, riferirsi a quella parte
della legione che si arrampica
sulle montagne aspre e scoscese
e la cui popolazione è dedita alla
pastorizia; col secondo, l'altra
parte che, baciata dal mare, e
colle vie del commercio dischiuse,
divide l'esistenza sua operosa tra
l'agricoltura e l'industria.
In un ridente lembo di questa
terra, sotto un cielo in
cantevole ed azzurro, sorge un
paesello pittoresco di circa tre
mila abitanti, ili cui piedi si di
stende una pianura übertosa e
feconda, ricca a preferenza di vi
gneti, che producono, ogni anno
in bella foggia e nuova, il frut
to ambrato dai cui succhi si spre
me il vino generoso.
Questa tranquilla dimora di
fortunati mortali giace a circa
un miglio di distanza dalla più
bella e più incantevole spiaggia
del mondo e ad un miglio dalla
stazione ferroviaria, mentre una
distanza (piasi doppia la separa
da Giulianova, superba sede bal
neare.
Oh ! la vita gioconda tra il si
lenzio verde ! Nel crepuscolo, tra
gli altieri folti, si radunano a
cianciare in coro le passere e dal
campanile della chiesa bianca, al
la domenica, slanciasi, acuta e
sottile, la voce della squilla!
Se è vero che i nomi sono la
conseguenza dei fatti, questo
villaggio che ci siamo ingegnati
di descrivere alla meglio, dovet
te avere a fondatore, in epoca
non precisata, un notaio, ma le
cronache nulla dicono in proposi
to.
Molti anni addietro viveva nel
paese una famiglia distinta per
nascita, ma sfornita di mezzi di
fortuna, la quale, come appariva
anche dal nome, doveva essere
discesa dalla parte montagnosa
della regione.
Il padre, solerte e valoroso in
segnante, che coi frutti di un la
voro onorato, sostentava la fami
glia, aveva tre figli maschi, dei
quali, il terzogenito è il protago
nista di questa storica novella.
Fin dai più teneri anni il fan
ciullo rivelava le sue tendenze, ed
ognuno poteva indovinare che co
sa sarebbe diventato l'uomo a
dulto.
E le previsioni si avverarono;!
fatto grande, egli diventò quel
che si aspettava: fannullone,
maldicente, prepotente, gesuita,
ignorante.
I compaesani, buoni villici, dai
costumi semplici, ma dalla mente
fertile ed immaginosa, volendo
compendiare in due sole parole
il meritato rispetto verso la fa
miglia ed il legittimo disprezzo
verso l'individuo indegno di ap
partenervi lo chiamarono: (ìno
re Codicelo (Signore Cocuccio),
e quell'appellativo gli rimase, fi
no a quando un bel giorno egli
non si decise a cambiar aria.
Giovanetto, frequentava le
PHILADELPHIA, PA., ltt MAGGIO, 1917.
scuole del villaggio, ma con mol
to scarso successo, ed il genitore
che gli faceva scuola non riuscì,
malgrado tutti i suoi sforzi, a
fargli superare gli esami di pro
scioglimento. Negato allo studio,
ribelle a qualsiasi disciplina, ven
ne su con perfide tendenze, col
l'animo pieno di odio verso di
tutti, da tutti cordialmente ri
cambiato, specialmente poi dal
maestro "Arrotino" del paese,
che non sapeva perdonargli la
lingua maledica.
Intanto il fratello primogenito,
ottimo giovane, per le sue buo
nissime doti di mente e di cuo
re, aveva avuto la fortuna di
fare un buonissimo matrimonio.
(inore Cocuccio, nemico giura
to del lavoro ed avido di diverti
menti e di vagabondaggio, aveva
sperato di poter attingere alla
dote della cognata e siccome tan
to questa che il marito il più
delle volte si opponevano alle im
moderate pretese di (inore Co
cuccio, questi spelò di potere ot
tenere ogni cosa colle minacce c
colla violenza. Ed allora la fan
tasia popolare, sempre disposta
all'esagerazione, si sbizzarrì co
me un cavallo indomito e nel
villaggio si disse, ad una voce,
che il discolo aveva spianato il
fucile contro suo padre.
onore cocuci'io aveva compiute
l'atto brigantesco solo contro i
proprio fratello.
Il rimorso della colpa commes
sa, l'indignazione sollevata ir
paese, sgomentarono (inore Co
cuccio che si sentì tutto invase
dal desiderio di redimersi e di la
vorare e si impiegò a piantar vi
ti per prevenire la filossera, alle
stipendio di 15 lire mensili. Ala
buoni propositi, non avevano, ne
suo animo, una lunga durata, ec
egli si stancò subito di questa vi
ta di sacrificio, affatto corrispon
dente ai suoi desideri.
Ritornò alle sue inveterate a
bitudini ; all'ozio, al vagabon
daggio, alla prepotenza, ed allo
ra la famiglia di lui, pensò di di
siarsene, una volta per sempre
inviandolo in un grande paese
d'oltremare, ove un suo cugine
fioriva nel commercio, ed un al
tro suo fratello, il secondogenito
mieteva allori ben meritati nei
campo libero della professione
A questi due si rivolse la fami
glia, implorando, ed essi aderen
do alla preghiera, inviarono, a
(ìnore Cocuccio, il biglietto di
passaggio.
♦ * *
Il paesello pittoresco di circa
tremila anime, ai cui piedi si di
stende una pianura übertosa e fe
conda, è in festa. Vi si nota per
le vie e nella piazza un movimen
to insolito, la chiesa madre è af
follata di fedeli che ascoltano il
Te Deum e le campane suonano
a festa giocondamente.
Qual'è la causa di tanta gioia?
La tranquilla popolazione con al
la testa la famiglia di (inore
Cocuccio, si abbandona alla più
rumorosa allegria perchè costui
s'è deciso finalmente a stendere
lo sterminato Oceano tra lui ed il
villaggio natio.
E mentre lo squillo delle cam
pane arrivava fino al cielo, e la
nave lotta colle onde che si rin
corrono spaventosamente, (inore
Cocuccio, dritto sulla tolda del
transoceanico, lo sguardo verso
la spiaggia che va mano mano
scomparendo, sentì la nostalgia
dei luoghi che furono testimoni
delle rocambolesche sue gesta.
Dopo lunga e fortunosa navi
gazione, il piroscafo approdò fi
nalmente in un porto immenso di
una sterminata metropoli orien
tale esi incominciarono le ope
razioni di sbarco.
La figura secca ed allampata
di Gnore Cocuccio, in meschinis
simo arnese, presso a poco equi
paggiato come il Sig. Cassiere,
quando giunse da Scranton, il
suo copricapo dal colore di cane
in tuga, con dodici buchi appari
scenti, il vestito lacero, richiama
rono subito su di lui la diffiden
te attenzione delle Autorità di
Emigrazione. Fu quindi sottopo
sto ad un accurato esimie e ad
uii minuzioso interrogatorio, ma
alla prova di lettura e alla prova
grafica se la cavò alla men trista.
Il guaio fu, quando, domandato
se avesse i mezzi necessari per
la continuazione del viaggio, po
tò, vuotando le tasche, raggra
nellare appena la meschina som
ma di lire 2.57. Rimase pertanto
detenuto in batteria, e per met
terlo fuori dovettero intervenire
il germano ed il cugino.
Quest'ultimo specialmente, fa
cile di lingua, ma di animo otti
mo, lo accolse con grandi di
rci di affetto, lo ammi
se nella sua azienda e, come pri
mo attestato, gli comprò un cap
pello di due dollari.
* « *
Qualche tempo dopo l'arrivo
da' vatio paesello, (ìnore Cocuc
s& vriiwiir e« ritTas*
sunto anche l'aria di persona n
modo. Qualcuno ricorre a lui pei
pareri ed egli si rivela ad un
tratto espertissimo nel consiglia
re fallimenti dolosi, ricomperan
do a metà prezzo la merce da lui
fatta nascondere. Dà persino
prove del suo valore letterario,
ricopiando da un vecchio statuto
il regolamento da servire pei
una nuova Società provinciale
che poi non sorse, alla stessa
guisa che più tardi aborti mise
ramente un altro tentativo di
(inoro Cocuccio di fondare una
loggia di indipendenti.
Ma il nuovo ambiente, la posi
zione di comproprietario di una
azienda (poiché il buon cugino se
l'era associato fin dal primo gior
no dell'arrivo) non valsero a ri
formare la sua indole ed a ri far
ne .il carattere.
I cattivi istinti, per qualche
tempo sopiti, si ridestarono, ed il
serpentello, riscaldato, tentò di
mordere il suo l>enefattore.
Cercò, senza riuscirvi, di sba
razzarsi del cugino e, finalmente,
decise di separarsene, dopo aver
gli tolte parecchie rappresentan
ze.
E lo si vide a capo di una nuo
va Ditta, sempre uguale ase
stesso; ipocrita, gesuita, che
mentre ti strisciava di fronte, ti
colpiva alle spalle, colla sua mal
dicenza.
Da allora rifulse tutta la sua
capacità a delinquere; i generi
domestici li smerciava per gene
ri importati; vendeva, a danaro
contante, bottigline-campioni che
egli aveva gratis dalle Case, per
distribuirle ai medici e farmaci
sti a titolo di reclame.
Un bel giorno decise di ammo
gliarsi ; ma egli che mai, in vita
sua, aveva sentito un affetto,
tentò di fare, del matrimonio, u
na ignobile speculazione.
E si mise alla caccia di una do
te, e quando gli sembrò di averla
trovata, strinse subito il contrat
to. Ma, accortosi che la realtà
non rispondeva all'ardente sua
cupidigia, imprecò contro i ma
nipolatori che lo avevano indotto
Anno I No. ."5 ."> Soldi la Copia
al gran passo, prospettandogli un
falso miraggio.
Tale il moralista, l'onesto, l'in
telligente, il ricco signore che si
atteggia, in pubblico, anche a fi
lantropo, a protettore di pupilli.
Ma allorché si richiude in sè
stesso, la sua mente rivola al pae
sello sito in prossimità della
spiaggia ridente, ove potè vive
re a lungo, scroccando ed ozian
do, senza che non gli turbasse i
sonni e la digestione.
Il novelliere.
Punte di spillo
FILIPPO CORRE AL PRO
PRIO SALVATAGGIO
La scorsa settimana, non ap
pena Filippo lesse sulle colonne
della Ragione i piccanti aneddoti
illustranti la ua vita educata al
la scuola dell'onestà e del dovere,
ebbe un scossa di nervi che fece
temere della sua preziosa esisten
za.
Rimessi, i poco dopo, si armò...
di coraggio e di pazienza, infilò
l'uscio di casa e giù a rompicollo,
per le vie della colonia.
Dopo una lunga ed all'annosa
corsa, andò a battere di muso
contro una campana che era in
sieme ad un campani...o)o, e gri
dò, con tutto il (iato dei suoi pol
moni :
Tu, mio amico, tu che da
tanti anni mi conosci, puoi caI
.V2MS Itì-'ÌÙ I ' coscienza, che qual
che malvagio vorrebbe avvelena
re, inoculandovi il dubbio.
Credi tu che io sia un uomo one
sto? Favella, esprimi il tuo con
vincimento e riabilita un galan
tuomo! Ma l'altro, con una
calma che avrebbe l'atto perdere
la pazienza anche ad un morto.
E che ne so io, o Filippo,
dei fatti tuoi ? Cioè, 110 ; a vo
ler essere sincero, qualche cosa
posso dirti, che, se non riesce a
calmar i tuoi nervi e la tua co
scienza, non deve ascriversi a
mia colpa.
Lo afferma un tuo collega in
giornalismo che porta un nome
che incute spavento e posso ri
peterlo anch'io. Secondo questo
tuo collega mazza scarica, tu a
vresti imbrogliato dollari 400 ad
un figlio d'ltalia
Pallido come 1111 cadavere, Fi
lippo continua la sua corsa sfre
nata e pensa tra sè:
E mai possibile che 10 sia
un disonesto e finora r.Oll me ne
era accorto? Sarebbe orribile,
specie dopo aver apposto la fu
ma a quella lettera che è causa
di tutti i miei mali? Un partico
lare mi viene in mente che avva
lora le basse calunnie delle Ra
gione. Quando mi diedi il ban
chetto, che è rimasto famoso in
colonia, il rappresentante della
Keyston Coal Co. non interven
ne, sebbene cento volte invitato.
Che aneli'egli mi abbia preso per
un disonesto?
Per la verità, i conti non erano
in regola
E corri, corri, corri il po
vero Filippo, trafelato ed ansan
te, piombò nel mio store, come
un bolide. A tutta prima pensai
ad un'aggressione. Il furore, si sa
bene, è cattivo consigliere e, ad
ogni buon fine, aprii il cassetto,
per dar di piglio alla rivoltella,
ma apparve tanto ridicola e tan
to innocua la faccia di Filippo,
che scoppiai in una sonora risa
ta.
Anche tu, compare Turid
du, attacchi un vecchio amico,
che ebbe sempre per te la massi
ma stima? Sappi che hai affer


# kiara\kiara\examples\data\text_corpus\La_Rassegna\sn84037025_1917-04-07_ed-1_seq-1_ocr.txt
■■■
La Rassegna
_ I
Both Phones
ANNO L No. 1
Il perche' de "La Rassegna"
In Colonia s'è sempre risentita la necessità di un giornale che,
attraverso l'indipendenza di idee, avesse anche potuto dire di sta
le alquanto bene in gamba dal lato finanziario. Parecchi giornali
sti, intelligenti ed onestissimi per giunta, dalle idee focose sprigio
nanti raggi inconfutabili di luce bagliora, se ne sono sempre avuti.
Mancò ad essi pero sempre quel puntello che disse e dice sempre
di "madonna finanza"; per cui si ha sempre ragione di cadere, in
fallibilmente cadere.
Sorge "La Rassegna" per hi volontà e per i mezzi finanziari
di pochi cui stanno a cuore sul serio le sorti della nostra Colonia.
Questa è almeno l'intenzione; il nostro pubblico coloniale dovrà fa
ìe il resto.
Noi vogliamo assolutamente giovare alla collettività; faccia la
collettività il suo adeguato dovere.
Col fermo proponimento che il compito che ci assumiamo di
dare una zaffata di aria vivificatrice al nostro ambiente troppo cor
rotto, ed al contempo, apata e sonnolente, affidiamo la direzione del
giornale al Signor Silvio Liberatore, troppo noto nel campo del
giornalismo coloniale cui seppe sempre dare, coikenuincipazione di
idee, con indipendenza di principii tutto l'ardore di una coscienza
non mutabile e nè vendibile per aver bisogno di più pomposa pre
srntazioite.
LA RASSEGNA PUB. CO.
i ANGELO CUSANO, Presidente
G. TREVISANI, Tesoriere
ALFONSO Rag. CARUSO, Seg.
RITORNANDO NELL'AGONE GIORNALISTICO
Un gruppo di stimabilissimi connazionali, in mezzo al quale
vado superbo di annoverare amici ed estimatori fidatissimi, mi
commette l'incarico di assumere la direzione di questo giornale. Ac
cettando l'incarico, prometto a me stesso ed agli altri di essere e di
l'i memore »cmpr« «lut fui JMtr il lymaafA nùl canina «risii*, j
nalismo coloniale. Ho un pubblico che mi conosce, mi stima e mi
combatte all'istesso tempo dacché io volli darmi alla pubblicazione
di un giornale. Non sento allora il ben che menomo bisogno di una
auto-presentazione, o quanto meno della esposizione di un program
ma qualsiasi. Chi mi conosce, mi conosce; chi non, si lasci servire.
Nemico sempre dei tratti disonesti rimarrò sempre sulla breccia per
combattere i disonesti. Ove si possa andare a finire con un pro
gramma di tal genere poco importa. Io parto sempre dal principio
che, a questo mondo, finiscono sempre prima, e male, i disonesti
anziché i galantuomini. Poco importi! se i primi sieno sempre ver
gognosamente ricchi a soldi e gli altri non abbiami di che onore
volmente sfamare le proprie creature. L'ora del "redde rationem
dovrà* fatalmente suonare per le coscienze elastiche e per gli animi
gretti. Io saprò sempre rimanere in vigile attesa per fare il mio do
vere, nient'altro che il mio dovere.
SILVIO LIBERATORE
VIVANO GLI STATI UNITI
Gli Stati Uniti, patria predi
letta di Washington e di Lin
coln, i due presidenti più po
polari che si sieno qui avuti,
hanno alfine saputo imporre
"l'alto là" alle azioni guerresche
degl'imperi centrali, potenti vio
latori di ogni diritto umano, di o
gni prerogativa internazionale,
di ogni ragione acquisita delle
genti.
Appena la Germania annun
ziò l'intrapresa spietata di rap
presaglie dei sottomarini, gli Sta
ti Uniti non potettero a meno, a
salvaguardia ed a tutela del pre
stigio cui si sente di aver diritto
una grande nazione della loro
specie, con gesto energico, ruppe
ro ogni relazione diplomatica con
l'imperiale governo germanico.
Ognuno capì subito che questa
rottura di relazioni diplomatiche
era il preludio buono e semplice
di una conseguente dichiarazio
ne di guerra, perchè quelle ragio
ni che persistettero per rompere
le relazioni diplomatiche eran più
che sufficienti per la dichiara
zione di guerra. Mancava però il
semplice fatto circostanziale per
la definizione del "casus belli", e
questo fatto non tardò a verifi
carsi perchè i sottomarini tede
schi, senza riguardo alcuno, af
fondarono nelle zone dichiarate
di guerra parecchi piroscafi «me
>* ITA LI A N w E R
I )evoted to welfare and BP» Italia ns in Aifcii
S. LIBERATORE, Direttore
ricani che ardirono attraversarle, j
Checché se ne possa dire in j
contrario, non sono stati gli Stati
Uniti a provocare la Germania.
Essi non chiedevano e non pre- !
tesero che il solo rispetto alle |
leggi ed ai trattati intemazionali
circa il modo di guerreggiare, sia
nei rapporti delle nazioni belli
geranti, che rispettivamente ai
neutri. E allo intento di conse
guire un tale effetto, gli Stati
Uniti sono stati di una longani
mità proverbiale, di una pazien
za più che santa; hanno espe
rite tutte le vie bonarie, dalla
nota umilmente consigliera a
quella risentita, energica, velata
mente minacciante. La Germania
però hà creduto di tener duro e
non ha nemmeno di una virgola
voluto modificare il suo piano di
guerra marinaro, adducendo ra
gioni e pretesti che, a rigore di
buona logica, non potettero me
ritare l'accoglimento nemmeno
parziale da parte di questa nazio
ne che, sebbene democratica e ■
pacifista per indole di popolo e •
per liberalità di costituzione, sa
all'occorrenza avere i suoi scatti
di opportunità psichica, quegli
scatti cioè che sono legittimi in
chiunque si creda offeso viva
mente ed ingiustamente nelle
sue ragioni e nei suoi privilegi di
persona, di ente, di collettività
PHILADELPHIA, PA.,
avente diritto al libero esercizio
di certe attribuzioni che trovano
consentimento pieno e largo nelle
teorie che affermano pieno e lar
go il diritto di natura e delle
genti.
Non v'è chi possa disconoscere,
sempre tra quelli cui non fa
difetto il lume della ragione,
che gli Stati Uniti sieno stati
proprio tirati, trascinati per i
capelli alla dichiarazione di uno
stato di guerra con la Germania.
Quali saranno le conseguenze af
fini ad un tale stato di cose, nes
suno potrebbe con matematica
disquisizione dirlo oggi; la qui
stione è oltremodo complessa;
per cui di difficile discussione.
Avranno gli Stati Uniti la for- j
za di vincere e debellare la sel
vaggia caparbietà tedesca?
Sarà quistìone semplicemente da
vedersi.
E' sempre prudenza, in giorna
lismo specialmente, farsi, stabi
lirsi, o positivamente e con pru
denza imporsi delle riserve, delle
vaste, grandi, oculate riserve.
Onde è necessario si venga alla
seguente, breve, concisa, spassio-
nata locuzione : La Gei-mania ha
provocato gli Stati Uniti; lo ha
fatto forse senza raziocinio belli
co? Sarebbe da audaci a dire che
sì, sarebbe da ingenui a dire che
no. Negli Stati Uniti vi sono cer
velli che ragionano, indubbia
mente; sarebbe da imbecilli sol
mwm m ama»
dirittura. E' prudente quindi, per
il momento almeno dichiarare
che dall'una e dall'altra parte il
caso ha voluto che sia mancato il
vero punto di appoggio sul qua
le basare un accordo qualsiasi. Le
pagine di una prossima o lontana
storia sapranno dirci approssi
mativamente da parte di chi sta
la ragione; noi per altro la cre
diamo, ora come ora dalla parte
degli Stati Uniti, di questa gran
de nazione democratica sempre
pronta a farsi sentire ogni qual
volta l'eco di una qualsiasi
concussione di diritto delle genti
potesse arrivare fino all'orizzon
te serenissimo della più grande,
più prospera, più generosa e più
ospitale delle repubbliche del
mondo.
Molti si domandano: a che co
sa potrà portare l'entrata in
guerra degli Stati U. con la Ger
| mania? Sarà veramente combat
tuta tra le due nazioni una guer
ra nel vero significato del termine
così come si combatte in Euro
pa?...... No, assolutamente no, di
ciamo noi oggi e lo diremo
chissà per quanto tempo ancora,
fino a quando il verificarsi di fat
ti in senso opposto al nostro av
viso non venisse per farci ricre
dere in modo bruscamente logico
e matematico.
Noi non crediamo che gli Stati
Uniti, dichiarando guerra alla
Germania possano presto o tardi
intraprendere le loro azioni belli
che nei campi d'Europa. Non vi
avrebbero di che guadagnare,
non solo, ma nemmeno di che
efficacemente operare. La Ger
mania, e con essa i suoi alleati,
godono nelle terre di Europa
di una posizione strategica
mente forte, sia dal lato del
le coste, guardate da piazzeforti
'assolutamente imprendibili, che
per le posizioni terrestri forti ol
tre ogni dire, e per tentare la
conquista delle quali le potenze
; alleate hanno saputo sapiente
! mente mettere su tante forze da
1 non avere assolutamente il biso
gno che altri vadano a coadiu
. varie in qualche modo.
Faranno, a >H
dere contro la,
porti del
peo, quello
Giappone
scoppiò il
flitto che
chiamare
solo, ma
due terzi del mondo. Fenomeni
curioso di velleità bellica *. di
sempre condannabile velleità im
perialistica ! Ad ogni modo pe
;l ò certe cose, dolorose peraltro
sempre si son verificate; si ha il
dovere ora di fronteggiarle a do
vere e gli Stati Uniti non sapran
no venirvi da meno. E' un gran
popolo questo, capace di grande
pazienza e di grandi scatti pu
ranco. Non abbiamo bisogno di !
COMUNICATO A PAGAMENTO
Lettera aperta al Sig. R. Lombardo
Presidente della "Sons of Italy State Bank"
Philadelphia, Pa.
Egregio Signor Lombardo,
La colonia è stala turlupinata :
voi siete una vittima di coloro i
quali agognando di raggiungere
una posizione sociale che mai po
tevano sperate, si sono fatti u
sbergo del vostro nome, sono riu
sciti nel loro* ■intesto, e, fra bre
"tuvi virìatf le
ami'.
Voi siete debole,
- -
perchè galantuomo; siete una
persona che non ha studiato il
Loiola e quindi non sapete le ar
ti che adoreranno contro di
voi; ma cadrete, infallibilmente
cadrete, se continuerete ad esse
re parte passiva nelle mani dei
gesuiti e chiudo gli occhi per
non vedere il rogo che già hanno
preparato per voi.
Ora sulle vostre spalle gravano
le terribili responsabilità della
carica; ricordatelo, e non abbia
te a male l'avviso mio.
La colonia non, ha mai creduto
che voi, Presidente della Sons of
Italy State Bank, con Giovanni
Di Silvestro a fianco, avreste po
tuto esercitare con serenità le
vostre mansioni. Ed è purtroppo
così.
E tanto per incominciare dirò
a voi le impressioni da me ricevu
te sin dal primo giorno.
Il 19 marzo, onomastico di Giu
seppe Di Silvestro (non fu scel
to da voi per l'apertura), con
grande distribuzione di stampati
a mezzo della posta, (oh quante
spese!) si avvertiva il pubblico
dell'apertuta della Sons of ltaly
State Bank facendo caldo appel
lo a tutti di aprire conti corren
ti ed eseguire depositi, ed agli a
zionisti la preghiera di regola
rizzare i loro pagamenti a saldo
ammontare del capitale sotto
scritto.
Infatti portai anch'io un ceck
di sls della South Phila. State
Bank delle 11 strade, e con det
to ammontare pagai lo "assess
ment" dovuto su tre azioni di
SSO ognuna pagate integralmen
te fin dall'ottobre scorso. Ebbi
anche il piacere di salutarvi e
non mancai di far.e gli auguri di
occasione.
Sarebbe stato meglio che io
non ci fossi venuto!! Non avrei
visto il cassiere sig. Luigi Coro
na, mentre allo sportello faceva
operazioni ed il pubblico in linea
aspettava il turno, abbandonare
elettricamente il posto perchè
chiamato dal sig. Giuseppe Di
Silvestro e trattenersi per 5 mi-
■Rßßb'o non prep
■ora indubhiamentjKi \jn pae
■K dei Uniti è
come gli co.
esser capace in que
Bil'italiiini, di mi
■ti. sentono il
Bfcersi ad essi ligi e per<
i Vutto, non
njirte della fazk.
leati'*, ma anche perch/è
niente ospitati da questo
che, da Washington a Lincoln,
Roosevelt a Wilson, primeggia
tra le nazioni che si vogliono dire j
fedeli custodi degl'interessi veri
delle classi iti eh o abbienti che si (l
dibattono continuamente per la c
| lotta dell'esistenza. n
S. L. n
r
liuti fuori dell'ufficio. li Giusep
pe Di iSiivestro non e che un a
wonista dell'uitim'ora !
inoll aviei constatalo pure 111
cno niouo barocco si ialino cene
operazioni: *.jna donna recia
niava li pagamento ui due gior
nate (li ìfcvoro per la p"lnura uei
V«i, alvi:. i_.on., jirdo,
garle ipi.òU. Ebbene, egregio sig.
Lionibaido, ini dispiace dnvelo,
non dovevate permettere che ve
nisse pagata la donna così come
venne latto, giacche non basta,
per giustificare la spesa, segnare
su un qualunque pezzo di carta
sia pure strappata dal desco, lo
ammontare sborsato, ma è neces
sario che ci sia analoga ricevuta
per poter controllare domani le
spese di cui voi siete tenuto a
rendere conto soldo per soldo,
dollaro per dollaro a tutti gli a
zionisti ed al revisore dello Stato.
Vengo ora al mio l'atto perso
nale: Dove avete imparato che!
allorquando trattasi di tornare
documenti presentati per opera
zioni bancarie, e non avvenute,
si debbano far recapitare a ma
no, senza alcuna spiegazione di
sorta, in busta chiusa e senza
nemmeno due righi soltanto di
gesuitica lettera giustificando le
ragioni del rifiuto?
Come potete sviluppare il ì
commercio bancario se vi manca
il principio fondamentale dell'e- ;
ducazione ?
Siete voi la persona adatta ad |
amministrare i miei $165?
Presidente: rammentate la
mattina dei 24 corr. marzo? Voi
siete venuto in automobile nel
mio negozio; abbiamo parlato!
della nota, mi avete assicurato |
che, presentando la stessa trat
ta dalla Ditta e girata da me sa
rebbe stata accettata, e con que
ste assicurazioni mi recai in ban
ca, vi consegnai la nota rinno
vata giusta la vostra richiesta, e
vi pregai di restituirmi la prima.
Voi inutilmente cercaste ; do
mandaste poscia ai vostri impie
gati e vi risposero che la porta
va "Giovannino". Oh ! Apriti
cielo! Gjardate!! Sentite!! E'
proprio il colmo!! Un Vice Pri
mo Presidente che la sera chiu
de la Banca e si porta la moneta
a casa, a vostra insaputa. Dico
moneta perchè la cambiale è mo
neta per chi non lo sappia. Que
sto è un fatto ette accade solo in
America, ossia solo nella Sons of
'ltaly State Bank.
pa- do, un'altra cViderazwn* *
estasio avveniva iVnedi 24 m ■
ij. e cambiale si trkva
tee. ' vannino" anchla do' Ul 1 '.. A j
;s ti 1 die il sabato, cfe.
an-ile correttezzaVy.
ian i Non dico alti per
c hè lo permettete cip
•Al-1 "Povera coloniaiColoni^^M
lial- ; Ui ogni scandaltialbei
Con stima
Ikj GaiibaldH
ò' V
Non
e lavoray^^^^^^^^^HÌTi
non brani
li'attenzione 1
coinmei i
tic che,
mente di spunto
chissà da quale
lamento passar ,-^^H
un
portanza nei i
iella "Sons of e libera
ha testé resi-
Buoi sportelli
anche
iente in ao .
Un
potes.se a j
tempi in
Dottor De
jran bel H
•itto (li cucciarsi in saccoccia una cam
pale consegnata alla Banca per l'um
nissione allo sconto. Non c'era forse
ina cassa forte negli uffici della "Sons
>f Italy State liank" ove custodire la
ambiale del signor Felici perchè il
irimo vice presidente di essa sentis
se il bisogno di affidarla alla propria
saccoccia? E poi, quale correttezza è
questa, noi domandiamo? Un banchie
re privato qualsiasi, sia anche dell'in
fima specie, di quelli appunto che
Lanto facevano ombra a Giovanni Di
Silvestro ai tempi in cui piacevagli di
esser socialista, non avrebbe mai o
sato, gelosi come si è sempre dalla
parte di' tutti del principio di scrupo
losamente custodire ogni cosa che sap
pia e dica di pubblica amministrazioi.
ne di portarsi a casa o custodire ito
saccoccia gl'induminenti contabili del
l'azienda.
Se uno sconcio di tal genere si fos
se dovuto deplorare nei rapporti di al
tre aziende, di altre persone, di altri
uomini cui' la fregola della pretesa,
della priorità e dell'apostolismo non
prese mai, meno male; ma dirimpetto
id un'istituzione bancaria che ebbe la
pretesa di' sorgere in colonia solo pel
combattere, fino alla distruzione, il
bossismo, lo sfruttamento, l'angaria,
la vessazione che semplicemente ed ar
tificiosamente si vollero supporre q
debito di altri, la cosa assume tuttd
l'aspetto della più imperdonabile grai
vita.
Quello che il signor Garibaldi Felici
ha esposto nel suo comunicato è, spe
cie nella parte da noi presa a com
mentare, troppo eloquente perchè fa
cessero bisogno ulteriori postille c
maggiori discussioni allo scopo di di
re sufficientemente ai nostri lettori
Ecco perchè facciamo in merito alla
quistione punto e basta per questo nu
mero.
Al momentc di mandare in macchi
na il giornale ci vien comunicato cht
il sig. Garibaldi Felici sia stato rim
borsato dell'ammontare integro dell*
azioni da lui pagate alla "Sons of Italj
State lìank". Chi ne abbia comprato
l'effettivo non lo sappiamo, nè voglia
mo saperlo. E' rilevante però il fatt<
' che, per una sola e semplice prò
testa o superficiale recriminazione
5 soldi la copia
UFFICIO: 920 So. lOth Street
| a ">l" u zione uella specie della "tìons
11 ttaiy .itale Uank aorta col
W'j .-.pecioso ui combattere iettimi
.imo u lo - ooverctìierie nei
Lupo bancario tn Keoerale, si mduea
I J,I ■ ntemtnte latitare . reclami e...
I aan>>' ie recrinunieiu».. geni*,
I ...unione tro-
I iprio-
I nafct, venne con gx sollecitata
I sue stato mag
I Ulule.
I Liuunque übbia rimborsato i'am-
L mornare delle azioni al sig. Felici, sia
/ itala la banca come ente, oppure un
Mtagjiiisi interessato che abbia avuto
'/t'inttimone di l'irlo per tornaconto
/ Proptu, non c'importa saperlo; certo è
Però ito una tuie liquidazione non può'
UL semplicemente strana a
U ""' ue aboia il' merito o il torto che
■ 1 di ragionare con i lumi della
mente.
imborsare le azioni a Garibaldi Fe-
u "ostro modestissimo modo di
■ l> a significato sentenza di ra
ie completi al suo reclamo. Perchè
KÌMB^Ì^ 1 "W tosse stato, il Pelici a
vrebbe avuto di che sbraitare a suo
Dell agio, sei>za che l'&mministrazio-
Ine della banca avesse sentito il ben
j che menomo dovere di preoccuparsene
uè molto e re poco. Tutto questo è, a
j nostro avviso sincero e spassionato,
| .semplicemente deplorevole.
Non avremmo voluto, in fede nostra,
trovarci per nessuna ragione a de
plorare un fatto di tal genere, anima
ti come siamo e come siamo sempre
lati dai principio di volerci sempre
pjratulare, anziché dispiacere, delle
Italiani all'Est.
CT CT JW CT TO .CT jmaiUMUMQKURigLmi^
Cicale* Grilli
€ Zanzare
Rinascenza bancaria Sono
;oite a i'hiladelphia, cioè a dito
il mezzo uJlu nostra Colonia due
janche statali: la "South filila
ielphia State Bank" e la "Sons
)i italy State Bank". La prima
:on programma eminentemente
commerciale, scevra da ogni
jrincipio di concorrenza alle mol
,e banche private che abbiamo in
mezzo a noi ; la seconda, la "Sons
jl Italy State Bank, è venuta su,
pur senza versale ancora l'intero
capitale dalla legge richiesto, con
jrincipii gesuiticamente liberato
ri. Dove stieno le qualità eccel
lentemente dimostrative d'un ta
le programma, nessuno lo sa, nè
nessuno può capirlo, nè altri se
lo sanno in alcun modo spiegare.
Pel momento ci limitiamo soltan
to a metter rilievo una cosa: una
cosa semplicissima peraltro: vi
annetta chi vuole la debita im
portanza :
La "Sons of Italy Statei Bank"
si ripromette di fare la guerra
un'accanita guerra alle tante
banche private che esistono in
Colonia. Con una miriade di stra
ni e strampalati programmi ri
cettati sulle colonne di un gior
nale ligio, troppo ligio al ma
novratore della banca, Gio
vanni Di Silvestro non t'offen
dere se l'abbiamo con te, proprio
con te, socialista in un tempo,
perchè povero, d'idee cosmopolite
oggi, perchè conduttore di una
ben fornita bottiglieria e di altro
ancora, s'è preteso di dire
alle genti di questo e quell'altro
mondo, che la "Sons of Italy
State Bank si costituiva solo per
combattere gli altrui abusi e so
prusi, le frodi e le malversazio
ni di tutte le altre istituzioni del
genere.
Non sappiamo fino ad oggi se


# kiara\kiara\examples\data\text_corpus\La_Rassegna\sn84037025_1917-04-14_ed-1_seq-1_ocr.txt
La Rassegna
Jjoth Phones
ANNO L No. 2
BASTA !...
Da qualche tempo a questa
arte, da due o tre anni cioè,
[amo andati assistendo in Colo
;ia ad un fenomeno curioso di
Drze evolutive che, in altri ter
mini, potrebbe ben definirsi svi
ippo completo e rigoglioso del
! poche energie degenerate e
ìalefiche esistenti in mezzo a
oi, contro il ristagno completo,
Bsoluto, dispiacentissimo peral
ro, di ogni forza motrice van
ente la sua origine da congegni
Dciali di fede indubbia, di bon
à ed onestà provatiesime.
La nostra Colonia, adunque,
ia comè ente virtualmente loca
», sia come gran parte della no
tra gente stabilitasi all'Estero,
on può dirsi oggi più in mano di
Dchi o di molti pionieri che la vi-
Bro nascere, crescere e 112 elice
lente prosperare attraverso
tolti anni di pacifismo colonia
(. Essa, fatalmente, troppo
è caduta nelle mani
cianche di un piccolo gruppo di
Bri e proprii malversatori della
Jde pubblica, di pochi "azzecca
arbugli", di una combriccola di
asnadieri in guanti gialli cioè
pve i "Cuocolo", gli "Alfano",
li "Abbatemaggio" e tutta la
ega caterva dogli eroi delle Rub
lo e delle scranne della Corte di
tsisi di Viterbo avrebbero avu
► d? cw Tfrtlillrm'^'rfrr'
studio, l'interpretazione e la
>noscelìza di certe tavole di
bdagogia e di lestofantismo so
lali che librano il loro volo si
[stro è minaccioso dall'alto di
pa base che vuol dire pericolo
isoluto in tema di collettività
Coniale.
[ Un pericolo questo, invero, del
Uale bisognerà guardarsi ocula
imente, cercando ogni via, usan
-0 di ogni mezzo per scongiurar
| addirittura.
[ Sonò dei lupi orribilmente af
finati, insaziabili a tutta prova
lie minacciano di divorare tutto
\ nostro normale, corretto ed o
festo organismo sociale, ed a
Uesti lupi, disgraziatamente,
a corona un gran numero di
sne, di sciacalli immondi, seni-
Ire pronti allo spolpamento raf
irato e spregevole all'istesso
£mpo di ossi lasciati lungo il
entiero di una truce carneficina.
E E' inutile sperare ancora, è so
erchio, proprio soverchio segui
rsi a fare delle illusioni : i buo
-1 sono terribilmente minacciati
ai cattivi ; i disonesti, i ladri, i
«ratti ed i ruffiani stanno per
rendere camorristicamente il
Dpravvento sui buoni e sugli o
icsti ; tutto è in pèricolo in mez
-0 a noi; è necessàrio, essenzial
mente necessario che si aprano
fli occhi e si diventi non solo
juardinghi, ma aggressivi anche
jerchè la malavita coloniale non
irogredisca ulteriormente, non
tionfi alla fine nei buoni rappor
-1 delle ragioni di altri.
112 Dovunque si abbia ragione di
redere un mascalzone, lo si com
fatta coraggiosamente; nessuno
i lasci intimidire dalle chiac
ihiere e dalle voci che sogliono
►render vita dai corpi sociali, dai
estaurants e dalle cantine. Si
ibbia sempre il coraggio di ri
fondere a tuono alle rodomon
late dei soliti bravi della mac
hia e della ignomia propria; è
[uesto il solo mezzo per isgo
(lentare gl'impudenti e gli sfac
ciati ; sara' questa la sola via per
Iwiare la nostra massa coloniale
iulla via di un salutare risana
Devoted to welfare and advancement of the Italiana in America
S. LIBERATORE, Direttore
mento, su quella via cioè che do
vrà sempre dire, a parere dei
buoni, di vera, pura, assoluta- ;
mente positiva rigenerazione dei
la nostra collettività.
Noi siamo di parere assoluta
mente fermo che molti, moltissi
mi ci sapranno seguire in Colo
nia in un'azione giornalistica as
solutamente altruistica, informa
ta sinceramente e correttamente
ai principii della più alta finali
; tà coloniale consentita legittima
mente da quelle idee che sanno e
che dovranno sempre sapere, per
ogni italiano all'Estero, di onesta
! affermazione dei meriti e dei
1 privilegi eoe solo le genti delle
italiche terre seppero far parla
re di sè le pagine di una storia
millenaria e gloriosa all'istesso
! tempo.
E' necessario, assolutamente
1 necessario che in Colonia i buo
ni si risveglino, si destino cioè
dal sonno letargico in cui vollero
1 o furono costretti cadere, perchè,
iove ciò essi non facessero, l'ele
mento corrotto e malintenziona
to avrebbe di che poter prendere
il sopravvento.
Colonia non deve essere
turlupinata, sfruttata e ulterior
mente rubata da pochi campioni
del mercimonio sociale. Ognuno
drfrfrp di mfrfViener fer
me le man» in tasca a guardia
vigile e contante delle poche gra
na che ess > potessero contenere e
custodire, giacché a Philadelphia,
in mezzo alla nostra Colonia, al
la distanza di un solo passo da
noi, un branco di masnadieri in
' guanti gialli, di farabutti in cra
vatta bianca e esco
quotidianamente di buon mattino
per cortesemente aggredire e
magnanimamente svaligiare.
Basta! noi gridiamo oggi e
grideremo sempre a questa gen
te. Il vostro regno non dovrà ul
teriormente durare a lungo; voi
siète disonesti e gli altri non lo
jsono; voi dovrete infallibilmente
I cadere perchè contro di voi sa
premo pazientemente organizza
re e disciplinare una grande, una
immensa marea di coscienze ret
te i di cui flutti dovranno irre
missibilmente sbalzarvi contro
! scogli durissimi capaci a fracas
! sarvi per sempre il cranio.
LA RASSEGNA
ìttìh
Una delicata quistione Si è
telegrafato da Washington che
, Mr. Webb, chairman dell' "House
i ludiciary Committee" stia per
: presentare un progetto di legge
i che permetta ai Governi Alleati
, di "reclutare i loro sudditi in
questo paese per il servizio all'E
stero."
Così la dizione laconica del te
legramma da Washington, tele
gramma sul quale s'è anche fu
gacemente fermato qualche gior
nale in nostra lingua allo intento
di commentarlo in qualche mo
do. Dato però che non s'è con la
dovuta chiarezza voluto o potu
to preannunziare lo scopo chiaro
cui vuol mirare Mr. Webb col suo
progetto di legge, non è possibi
le in alcun modo nè a voi e nè a
chiunque altro, una discussione a
fondo, esauriente dal punto di vi
sta giuridico internazionale, giac
ché non è che su questo campo
ITALIAN WEEKLY NEWSPAPER
PHILADELPHIA, PA., SABATO, 1 I APRILE 1917
che oggi vertono e si dibattono
tutti i provvedimenti extra legi
slativi delle nazioni in guerra.
La notizia del progetto di leg
ge che sta per presentare Mr.
Webb ha avuto però l'effetto, al
meno per il moemnto, di far ri
tornare sul tappeto della discus
sione l'eterna, la tanto dibattuta
quistione dei nostri renitenti re
sidenti negli Stati Uniti. In gene
rale, giacché v'è sempre qual
cuno dalla vista un pochino lun
ga che usa vedere piccolo laddo
ve gli altri vedono grosso grosso,
la notizia del progetto di leg
ge Webb ha prodotto scompiglio
e scoraggiamento in mezzo ai no
stri moltissimi renitenti di leva
che, se non risposero alla chiama
ta del patrio Governo al momen
to in cui l'ltalia nostra si lancia
va ardimentosamente in una
guerra difficile e scabrosa, non lo
fecero se per ragioni d'indole as
solutamente economiche dalle
quali non era possibile prescin
dere.
Questi renitenti sono stati pre
si dallo spauracchio che gli Stati
Uniti, con una legge qualsiasi,
potesse un giorno obbligarli al
reclutamento o per conto suo o
per conto degli Alleati.
No, noi non crediamo ad un
pericolo di tal genere; molti ne
parlano, ne discutono è vero; ma
sarebbe assolutamente follia pen
sare che in una terra libera come
Ài signori de' la
'Sons of Italy State Bank
( Noi non abbiamo avuta nèt
( l'intenzione e nè la pretesa di at-1
, taccarvi come voi supinamente e
stupidamente avete ritenuto che
, fosse. Ci si è data l'occasione di
discutervi e lo abbiamo fatto, lo
abbiamo fatto notate con
quella obiettività' di argomenti e
di ragionamenti che dissero, di
cono e diranno sempre ed a me
raviglia di noi, del giornale che
scriviamo, di tutta quella falan
ge di onesti, rispettabilissimi co
loni che, senza mai pagarci un
solo soldo di mercede, va superba,
altera, contentissima dell'opera
nostra.
Da voi, o signori affaristi, pa
gnottisti, scribi e farisei della
. "Sons of Italy State Bank" ci a
spettavamo più seria, più positi
va, più logica risposta al comuni
cato del sig. Felice Garibaldi ed
agli opportuni commenti nostri
che lo seguirono. Ci aspettava
i mo una più logica, coerente ri
; sposta, ripetiamo, perchè parti
-5 vamo dal principio che l'ineffa
• bile Angio'o Curi, cancelliere
ì affezionatissimo all'impero glo
i bo-vino-terraqueo di Donna Gio-,
i vannina, avesse saputo scri
- vere per l'occasione un po' meglio I
di quello che sia riuscito a fare. |
Avendolo sempre stimato per e
simio, celebre masturbatore di
ginnasiali reminiscenze scolasti
che, ritenevamo, eravamo quasi
certi che, per combatterci, a-j
vrebbe ricorso a qualche bel pun
to di mitologia, oppure a delle |
preziosissime nozioni di storia
greca pronunzianti sentenze su
blimi se non nel campo della real
tà, almeno in quello dell'astrazio- j
ne.
Non è stato così, invero, e ce
ne addoloriamo immensamente,
illustri signori della "Sons of;
3 Italy State Bank", perchè avrem
gli Stati Uniti si potessero com
piere di simili anormalità' legi
slative, contradicenti in ogni
punto, in ogni passo le regole più
elementari di diritto intemazio
nale non solo, ma anche ogni
principio civico delle genti intor
no al quale ì nostri giuresperiti e
legislatori sonimi hanno sempre
detto e scritto vastamente, non
solo, ma scientificamente anche.
"11 Progresso Italo America
no" è stato, bisogna dirlo, il solo
a dare l'allarme interne a tale
quistione ; incerto e peritoso pe
rò perchè, ove avesse avuta l'in
tenzione vera di parlar chiaro ai
nostri Coloni, avrebbe dovuto es
sere di forma più chiara, più e
splicita, più ragionatrice.
"Il Progresso" ha saputo, in
vece, usare di un'arma a doppio
taglio; ha usato cioè di quell'ar
ma che, all'occorrenza, come
mezzo paliativo per far quattrini
in mezzo ai gonzi, è purtroppo
efficace per pervenire allo scopo
di certe, di talune obligue ed an
fibie finalità'. Ha prima difesa la
causa dei renitenti e poi l'ha or
ribilmente offesa. E così si usa
fare in America, è questo che
conviene fare in America per far
quattrini alle spalle dei gonzi, per
affermarsi alle spalle dei gonzi,
per rendere popolare un giorna
le in mezzo ai gonzi,
i Verità»
i mo voluto, desiderato almeno che
ci aveste fornito di un terreno
1 più solido sul quale discutervi e
combattervi. Voi, invece, non
sappiamo se per malizia o insi
piènza, ci sfuggite come pe
sci dalla rete, ed in fede nostra
non sappiamo se voi siete per la
circostanza più degni di pietà o di
disprezzo. Di pietà crediamo ad
ogni modo ; perchè ove volessimo
toccarvi coi frustino del nostro
disprezzo, ora come ora, alla stre
gua dei comunicati che avete vo
luto diramare su tutti i nostri
settimanali, sapcemmo di fare
opera c di commettere azione nè
più e nè meno di quello che Ma
ramaldo ebbe il cinismo di con
sumare ai danni di Ferruccio.
Noi non siamo peraltro dei fe
roci sanguinari, dei Maramaldo
prepotenti cui piace inveire e col
pire da bravi contro i deboli e gli
impotenti; nè voi, o egregi si
gnori della Sons of Italy State
Bank, siete addirittura dei nobi
li "Ferruccio". Per cui vi rispar
miamo fino ad un certo punto,
giacché la vostra debolezza sa
semplicemente, più che di impo
tenza assoluta nella difesa, ori
ginata da una naturale, impre
scindibile forza maggiore, di
debolezza ingenita proveniente e
scaturiente dai vostri difetti ori
ginali, dalle vostre colpe volonta
rie, dalla vostra falsa presunzio
ne e dalla vostra arrogante pre
tenzione financo.
j Altri vi ha esposti fatti brevi,
concisi, compendiosi che valeva
no un mondo per la loro natura e
per la loro eloquenza; noi li com
mentammo appena. Di rimando
ci avete solo saputo gratificare
di una buona frittata di parole,
paroline e paroloni cui nessun
lettore dal naso fino e dalla vi
sta buona ha potuto attribuire
j nemmeno il merito della momen-
I tanea coesione logica di idee e di I
argomenti.
Noi, parlando così di passaggio
intorno al comunicato Felici,
demmo la dovuta importanza a
quattro fatti positivamente seri
della vostra banca :"L'ammontar
delle azioni non ancora versate", |
'una cambiale per tre giorni cu- j
stodita nella saccoccia di "Gio j
j vannino", "la pretesa insulsa di •
! redimere il nostro emigrato dal- ,
lo sfruttamento supposto e pre- ,
sunto semplicemente ad opera ,
dei banchieri privati", "le parec- v
chie migliaia di dollari di cui ,
qualche direttore della Sons of'|
Italy State Bank è sempre debi- ,
tore verso parecchi dei nostri j
banchieri privati locali." (
Invece di risponderci a tuono, |
si giuoca di parole, di sole paro- ,
le al di cui affastellamento scor- 112
retto può sempre essere prepo- |
sto ed incaricato un imbecille ,
qualsiasi, fosse anche della na- r
tura di quelli pagati a ventitré ,
pezze la settimana per la sola ri
conoscenza di certi piccoli servi- c ,
zi sporchi resi in gabinetto pu-
! litO. ,
Ai fatti è necessario, quando si c
vuole stare ben* in gamba in u
na qualsiasi arena di combatti
mento, sempre contrap- (
1 porre i fatti, ove non si abbia la j
| voglia di cadere nel cattivo con
cetto del pubblico e nel ridicolo
anche nei rapporti di chiunque è y
uso appassionarsi alle altrui ten- ,
Jtfl: - • • • 1
Si sarebbe potuto essere più j
forti e più virili in una risposta
giacché s'era deciso di farcela. (
Meglio sarebbe stato però a non v
darla una tale risposta quando si 112
doveva riuscire così meschini,
tanto piccini in argomenti ed ar- (
gomentazioni.
Gli scribi che si son voluto as- 112
sumere l'incarico di certe difese, }
ci fanno semplicemente pietà, <
giacché essi mancano addirittu- |
radi argomentazione non solo, t
ma di raziocinio anche. A noi ,
piace avere avversari forti ; solo |
allora c'è cara e piacevole la lot- ,
ta, anche se dovessimo sempre ,
perderla. I giuocatori di parola, ,
i disquisitori masturmanti senza (
rima e senza concetto alla Angio- |
lo Curi, ci fanno pena semplice- \
mente; li lasceremmo in pace, .
senza un cenno qualsiasi di ri- |
sposta, ove non avessimo la di-],
sgrazia di trovarci in un ambien- ]
te coloniale dove non sempre si
trovano giudici sereni e spassio- .
nati, più amanti della eloquenza j,
dei fatti che delle filastrocche di |
parole polemiche.
Abbiamo delle ragioni positi
vamente positive per finirla in
questo numero. Ritorneremo pe
rò alla carica prossimamente, per
essere ancora più precisi e più
dettagliati intomo a taluni altri:
argomenti che la ristrettezza del
: tempo non ci consente oggi di po
ter trattare come vorremmo.
Sarà per i prossimi numeri.
La Cavalletta
IL PROSSIMO NUMERO DE
"LA RASSEGNA" CONTERRÀ'
ARTICOLI INTERESSANTI E
DI GRANDE NOVITÀ'. NON
MANCATE DI LEGGERLO E
FARLO CONOSCERE ANCHE
AI VOSTRI AMICI.
Cicale, Grilli
e-Zanzare
Ci si minaccia! Per un po
co poco appena di verità che ab
biamo voluto pubblicare e dove
rosamente contentare col nostro
primo numero a riguardo della
"Sons of Italy State Bank", ci
siamo sentiti tuonare attorno al
nostro povero capo tutti i fulmini
di cui Giove abbia mai potuto di
sporre a questo ed a quell'altro ;
mondo. Tuoni fragorosi e terribi
li in verità; ma essi non ci han
no scosso ne punto e nè poco, nè
impensieriti menomamente. Dac
ché Franklin fece la scoverta del
parafulmine, gli uomini impara
rono ad arrestare l'azione fune
sta del fulmine di Dio. Noi
parafrasando il principio, fisica
mente assiomatico ci sentia
mo di poter dire, dalle sponde di
un giornale ultra-indipendente
che nessun fulmine ci fa paura,
ci spaventa o ci rgomenta, cada o j
tuoni essi vicino o distante da
noi. Siamo muniti noi di parec- 1
chi e ben potenti parafulmini.
I soliti giannizzeri e sparafu
cili di Peppuccio Romano e di
Giovannino Alfano si son dato un i
bel da fare in questa settimana
per incutere timore, non a noi
che non abbiamo mai saputo che
volesse dire a questo mondo ti
moro e r. giialcung dei
'pòchi' coraggiosi clrrisi "STm (fo
lliti unire a noi per un'azione in- !
dipendentemente detta nel campo
del giornalismo coloniale, al solo
scopo di radicalmente purgare ed
epurare il nostro ambiente da
certi elementi nocivi e funesti per j
esso nel senso vero della parola.
Minacce a destra e minacce a
sinistra; giornali che vi sono e
giornali che sorgeranno per at
taccare, violentemente, implaca
bilmente attaccare Tizio e Caio,
Sempronio e Mevio. Appunta
menti di qua e chiamate a tele
fono di là; tutto un movimento
di battaglioni e di batterie in ri
serva, a chi dandi ed a chi pro
mettendo Tutte manovre queste
che ci fanno ridere, semplicetpen
te ridere, perche non dicono al- '
tro se non mostrarci la pagina j
aperta, chiara e nitida di quel li-j
bro che ha di già scritto qualche j
cosa sul crollo immancabile, ine- 1
vimabile di certe consorterie co
loniali che a noi non sono mai
andate a genio per nessun verso
e che, per giunta, abbiamo sem
pre combattute ad oltranza, con
tutta la somma delle forze di cui
ci è stato consentito disporre.
Lo sappiamo, lo abbiamo sem
pre pensato: qualche foglietto
affidato alla direzione pro-forma
di un irresponsabile qualsiasi ri
vedrà presto la iuce. Vi scara
bocchieranno Angiolo Curi, Don
na Giovannina e financo il ranoc
chiaio Notar Peppe "il grande".
Peccato che per il momento non
ci sia quel grande mascalzone di
Peppe Landolfi, del quale poter
si servire a bell'agio, giacché quel
delinquente, tanto caro a certa
canaglia che ebbe financo il co
raggio di salutarlo con un ban
chetto, venne, appena pochi gior
ni (jopo la sua assunzione all'uf
ficio di assistente "manager" del
la Casa Armour a Milano, licen
ziato vergognosamente dall'uffi
cio per un giusto reclamo della
locale ditta "Philadelphia Maca
ronì Co." alla quale ebbe l'abilità
di truffare l'ammontare di due
cento casse di pasta proprio al
momento in cui Giovanni Di Sii-
5 soldi la copia
UFFICIO: 920 So. lOth Street
i vestro, il focoso, il grande, il con
cettoso, il facondo oratore delle
nostre Colonie, lo definì "un gio
vane degno di grande avvenire,
dal passato, e dal futuro finan-
L'O, fulgidi e sereni come le stel
' le mattutine.
Vengano, vengano pur fuori di
questi giornali ; ne avremo sem
pre per tutti. Compassione e mi
sericordia per gli straccioni che
vi possano prestare il nóme a
guisa di gerenti irresponsabili;
canzoni e biografie per tutti
tinelli che vi saranno parte inte
ressata. Ricorrano o non i man
datari alla prosa prezzolata del
socialistoide o a quella assassina
I dell'anarchico, noi non faremo
j mai un sol passo indietro. Sapre
mo affrontare gli sgherri con
l'arma al piede e, non si dubiti,
| sapremo anche aver la forza di
i colpire, mortalmente colpire.
Giù la visiera, adunque, ed a
| vanti Nè avrà chi più non ne
: saprà mettere. Ci vorranno fatti
però e non chiacchiere, perchè la
nostra religione è quella di la
sciarci sempre guidare dai fatti,
| dalla filosofia dei fatti, dalla lo
jgica e dalla ragione di essi, da
nient'altro. Si stia pur sicuri.
Felice Cavallotti e Giovanni Di
Silvestro Un assemblea di sti
mabili azionisti della Sons of lta
iy Stale Bank, riunita a solenne
seduta per protestare contro il
commento ad un comunicato in
ferito nel nostro giornale, in un
ottime del giorno brillante solo di
mancanza di veritàÌS dì rettitudi
ne bancaria "proclama aperta
mente che Giovanni Di Silvestro
rimane sempre per la massa ope
raia la figura simpatica del Fe
lice Cavallotti coloniale."
Tanta prosa gonfiata a mo' di
| cornamusa, tanto grandioso pa
ragone tra due nomi che fanno a
calci, si rincorrono, si persegui
tano così come può fare un cane
ed un gatto, un lupo ed un agnel
lo, una volpe ed una gallina, là
sul piazzale vasto dell'aia di un
selvaggio contado, non ci fa, nei
rapporti del nostro grande cam
pione senza valore signor Gio
vanni Di Silvestro, se non l'effet
to dell'ossigeno cautamente e
i pazientemente somministrato ad
j un ammalato cui non rimangono
se non pochi istanti di vita.
Forre Giovanni Di Silvestro,
eroe di ogni nostra porcheria co
loniale, a paragone di Felice Ca
vallotti il campione vero della
democrazia italiana che non si
sognò mai di chiamare figlio di
p un Conte Naselli, nè si fece
tacitare certe fameliche pretese
nella campagna contro gl'imbro
gli che in un tempo si facevano
per la leva militare, nè si recò in
un tempo a Scranton per inneg
giare al regicida Bresci, è co
sa che raggiunge solo il colmo del
la imbecillaggine umana, passata
attraverso il filtro di ventitre
dollari la settimana pel mastur
batoli Angiolo Curi, di una mez
za dozzina di bottiglie di buona o
cattiva birra che siasi nei ri
guardi dei focosi, stimabili, sem
pre illustri votanti uji ordine del
giorno cui solamente i pesci stan
tii e puzzolenti che si vendono al
i le nove strade potrebbero batte
• re fragorosamente le mani.
1 Ci rivedremo, signori, ci rive-
I dremo, siatene certi.
Don Procopio


# kiara\kiara\examples\data\text_corpus\La_Rassegna\sn84037025_1917-04-14_ed-2_seq-1_ocr.txt
Both Phones
ANNO I. No. 2
BASTA !...
uà quaiene tempo a questa
parie, ua uue o tre ama cioè,
Maino anelati asaisienuo ni colo
ma atl un ieuomeiio curioso ui
iorze evolutive cne, ni ai in ter
nuiu, poireuue oen uer limai svi
luppo completo e rigoglioso uei
ie poetiti energie degenerate e
inaieiiclie esistenti ni mezzo a
1101, conilo il ristagno completo,
assoluto, flispiacentissnno peral
tro, di ogni lorza motrice van
tante la sua origine aa congegni
sociali di rede indubbia, di Don- !
ta ed onesta provatiasmie.
La nostra Colonia, adunque,
sia come ente virtualmente loca
le, sia come gran parte della no
stra gente stabilitasi all'Estero,
non può dirsi oggi più in mano di
pochi o di molti pionieri che la vi
dero nascere, crescere e felice
mente prosperare attraverso
molti anni di paciiismo colonia
le. Essa, fatalmente, troppo
fatalmente è caduta nelle mani
adunche di un piccolo gruppo di
veri e proprii malversatori della
fede pubblica, di pochi "azzecca
garbugli", di una combriccola di
masnadieri in guanti gialli cioè
dove i "Cuocolo", gii "Alfano",
gli "Abbatemaggio" e tutta la
biega caterva dagli eroi ueile gab
bie e delle scialine delia Corte eli
ASSISI di Viterbo avrebbero avu
io di che sempre invidiare circa
10 studio, l'interpretazione e la
conoscenza di certe tavole di
pedagogia e di lestofantismo so
ciali che librano il loro volo si
nistro e minaccioso dal! alto di
una base che vuol dire pericolo
assoluto in téma di collettività
coloniale.
Un pericolo questo, invero, del
quale bisognerà guardarsi ocula
tamente, cercando ogni via, usan
do di ogni mezzo per scongiurar
lo addirittura.
Sono dei lupi orribilmente af
famati, insaziabili a tutta prova
che minacciano di divorare tutto
11 nostro normale, corretto ed o
nesto organismo sociale, ed a
questi lupi, disgraziatamente,
fa corona un gran numero di
iene, di sciacalli immondi, sem
pre pronti allo spolpamento raf
i finato e spregevole alfistesso
[tempo di ossi lasciati lungo il
[sentiero di una truce carneficina.
E' inutile sperare ancora, è so
verchio, proprio soverchio segui
tarsi a fare delle illusioni : i buo
ni sono terribilmente minacciati
dai cattivi; i disonesti, i ladri, i
baratti ed i ruffiani stanno per
prendere camorristicamente il
sopravvento sui buoni e sugli o
nesti ; tutto è in pericolo in mez
zo a noi; è necessario, essenzial
mente necessario che si aprano
gli occhi e si divenfì non solo
guardinghi, ma aggressivi anche
perchè la malavita coloniale non
progredisca ulteriormente, non
trionfi alla fine nei buoni rappor
ti delle ragioni di altri.
Dovunque si abbia ragione di
vedere un mascalzone, lo si com
batta coraggiosamente; nessuno
6i lasci intimidire dalle chiac
chiere e dalle voci che sogliono
prender vita dai corpi sociali, dai
Testaurants e dalle cantine. Si
abbia sempre il coraggio di ri
spondere a tuono alle rodomon-
Jate dei soliti bravi della mac
:hia e della ignomia propria; è
9uesto il solo mezzo- per {sgo
mentare gl'impudenti e gli sfac
ciati ; sara' questa la sola via per
avviare la nostra massa coloniale
Sulla via di un salutare risana-
.x IT ALI AN WEEKLY NEWSPAPER
Devoted to welfare and advancement of the Italian» in America
S. LIBERATORE, Direttore
mento, su quella via cioè che do
vrà sempre due, a parere dei
buoni, di vera, pura, assoluta
mente positiva rigenerazione del
la nostra collettività.
tbfck
Noi siamo di parere assoluta
mente fermo che molti, moltissi
mi ci sapranno seguire in Colo
nia in un'azione giornalistica as
solutamente altruistica, informa
ta sinceramente e correttamente
ai principii della più alta finali
i tà coloniale consentita legittima
f mente da quelle idee che sanno e
che dovranno sempre sapere, per
! ogni italiano all'Estero, di onesta
affermazione dei meriti e dei
privilegi che solo le genti delle
italiche terre seppero far parla
| re di sè le pagine di una storia
millenaria e gloriosa all'istesso
tempo.
E' necessario, assolutamente
necessario che in Colonia i buo
ni si risveglino, si destino cioè
dal sonno letargico in cui vollero
0 furono costretti cadere, perchè,
ove ciò essi non facessero, l'ele
mento corrotto e malintenziona
to avrebbe di che poter prendere
il sopravvento.
La Colonia non deve essere
turlupinata, sfruttata e ulterior
mente rubata da pochi campioni
del mercimonio sociale. Ognuno
sente il dovere di mantener fer
me le man» in tasca a guardia
vigile e costante delle poche gra
na che ess.3 potessero contenere e
custodire, giacché a Philadelphia,
in mezzo alla nostra Colonia, al
la distanza di un solo passo da
noi, un branco di masnadieri in
( guanti gialli, di farabutti in cra
vatta bianca e "redingote" esco
! quotidianamente di buon mattino
pei - cortesemente aggredire e
: magnanimamente svaligiare.
Basta! noi gridiamo oggi e
grideremo sempre a questa gen
te. Il vostro regno non dovrà ul
teriormente durare a lungo; voi
siete disonesti e gli altri non lo
sono; voi dovrete infallibilmente
cadere perchè contro di voi sa
liremo pazientemente organizza
re e disciplinare una grande, una
immensa marea di coscienze ret
te i di cui flutti dovranno irre
missibilmente sbalzarvi contro
scogli durissimi capaci a fracas
-1 sarvi per sempre il cranio.
LA RASSEGNA
NOIE e iIMII
Una delicata questione Si è
telegrafato da Washington che
Mr. Webb, chairman dell' "House
ludiciary Committee" stia per
presentare un progetto di legge
che permetta ai Governi Alleati
di "reclutare i loro sudditi in
questo paese per il servizio all'E
stero."
Così la dizione laconica del te
legramma da Washington, tele
gramma sul quale s'è anche fu
gacemente fermato qualche gior
i naie in nostra lingua allo intento
di commentarlo in qualche mo
do. Dato però che non s'è con la
dovuta chiarezza voluto o potu
to preannunziare lo scopo chiaro
cui vuol mirare Mr. Webb col suo
progetto di legge, non è possibi
le in alcun modo nè a voi e nè a
chiunque altro, una discussione a
fondo, esauriente dal punto di vi
sta giuridico internazionale, giac
ché non è che su questo campo
PHILADELPHIA, FA., SABATO, 14 APRILE 1917
cue oggi vertono e si dibattono ,
' lutti 1 piovveuimenti extra legi
slativi tiene naziom in guerra. \ ;
L,a notizia dei progetto di ieg
igè cne sta per presentare Mr.
WebO Ha avuto pero 1 elietto, ai- :
meno per il moemnto, di tar ri
tornare sul tappeto della discus
sione 1 eterna, la tanto dibattuta
quisuone dei nostri renitenti re- <
sidenti negli Stati Uniti. In gene- ;
j
rale, giacché v'è sempre qual
cuno dalia vista un pochino lun
ga che usa vedere piccolo laddo
ve gli altri vedono grosso grosso,
la notizia del progetto di leg
ge YVebb ha prodotto scompiglio!
e scoraggiamento in mezzo ai no
stri moltissimi renitenti di leva
che, se non risposero alla chiama
ta del patrio Governo al momen
to in cui l'ltalia nostra si lancia
va ardimentosamente in una
guerra difficile e scabrosa, non lo
fecero se per ragioni d'indole as
solutamente economiche dalle
quali non era possibile prescin
dere.
Questi renitenti sono stati pre
si dallo spauracchio che gli Stati
Uniti, con una legge qualsiasi, 112
potesse un giorno obbligarli al
reclutamento o per conto suo o
per conto degli Alleati.
No, noi non crediamo ad un
pericolo di tal genere; molti ne
parlano, ne discutono è vero; ma
•sarebbe assolutamente follia pen
sare clic in una terra libera cornei
Ài signori de' la
"Sons of Italy State Bank'
) Noi non abbiamo avuta nè
) l'intenzione e nè la pretesa di at
taccarvi come voi supinamente e
stupidamente avete ritenuto che
, fosse. Ci si è data l'occasione di '
discutervi e lo abbiamo fatto, lo !
abbiamo fatto notate con
quella obiettività' di argomenti e
di ragionamenti che dissero, di
cono e diranno sempre ed a me- '
raviglia di noi, del giornale che
scriviamo, di tutta quella falan
ge di onesti, rispettabilissimi co
loni che, senza mai pagarci un
solo soldo di mercede, va superba,
altera, contentissima dell'opera
nostra.
Da voi, o signori affaristi, pa
gnottisti, scribi e farisei della
. "Sons of ltaly State Bank" ci a
spettavamo più seria, più positi- j
j va, più logica risposta al comuni-
I cato del sig. Felice Garibaldi ed
agli opportuni commenti nostri j
che lo seguirono. Ci aspettava-!
; mo una più logica, coerente ri-1
? sposta, ripetiamo, perchè parti
; vamo dal principio che l'ineffa
• bile Angiolo Curi, cancelliere
? affezionatissimo all'impero glo
i bo-vino-terraqueo di Donna Gio
-1 vannina, avesse saputo scri
- vere per l'occasione un po' meglio
di quello che sia riuscito a fare.
Avendolo sempre stimato per e
simio, celebre masturbatore di
ginnasiali reminiscenze scolasti
che, ritenevamo, eravamo quasi
certi che, per combatterci, a
vrebbe ricorso a qualche bel pun
to di mitologia, oppure a delle
preziosissime nozioni di storia i
greca pronunzianti sentenze su
blimi se non nel campo della real
tà, almeno in quello dell'astrazio
'"ne. i
Non è stato così, invero, e ce
ne addoloriamo immensamente,
illustri signori della "Sons of
Italy State Bank", perchè avrem
gu £>tati Uniti si potessero com
piere di simili anormalità' legi
slative, eontradicénti in ogni
punto, in ogni passo le regole più
elementari di diritto internazio
nale non solo, ma anche ogni
principio civico delle genti intor
no al quale ì nostri .giuresperiti e
legislatori sonimi hanno sempre
detto e scritto vastamente, non
solo, ma scientificamente anche.
"il Progresso Italo America
no" è stato, Insogna dillo, il solo
a ilare l'allarme intorno a tale
quistione; incerto e peritoso pe
rò perche, ove avesse avuta l'in
tenzione vera di parlar chiaro ai
nostri Coloni, avrebbe dovuto es
sere di forma più chiara, più e
splicita, più ragionatrice.
"11 Progresso" ha saputo, in
vece, usare di un'arma a doppio
taglio; ha usato cioè ili quell'ar
ma che, all'occorrenza, come
mezzo paliativo per far quattrini
in mezzo ai gonzi, è purtroppo
efficace per pervenire allo scopo
| di certe, di talune obligue ed an
fibie finalità'. Ha prima difesa la
! causa dei renitenti e poi l'ha or
ribilmente offesa. E così si usa
fare in America, è questo che
conviene fare in America per far
quattrini alie spalle dei gonzi, per
affermarsi alle spalle dei gonzi,
per rendere popolare un giorna
le in mezzo ai gonzi,
i Verità»
ino voluto, desiderato almeno che
ci uveale 101111 to ui un terreno
più solido sui yuaie discutervi e
combattervi. Voi, invece, non
| sappiamo se pei' malizia o insi
pienza, ci sr uggì te come pe
! sci dalla rete, ed 111 lede nostra
! non sappiamo se voi siete per la
I circostanza più degni di pieia o di
'disprezzo. Di pietà crediamo ad
ogni modo ; perche ove volessimo
toccarvi coi frustino del nostro
disprezzo, ora come ora, alla stre
gua dei comunicati che avete vo
luto diramare su tutti i nostri
settimanali, sapremmo di l'are
opera e di commettere azione né
più e nè meno di quello che Ma
ramaldo ebbe il cinismo di con
sumare ai danni di Ferruccio.
Noi non siamo peraltro dei fe
roci sanguinari, dei Maramaldo
prepotenti cui piace inveire e col
pire da bravi contro i deboli e gli
impotenti; nè voi, o egregi si
gnori della Sons of Italy State
Bank, siete addirittura dei nobi
li "Ferruccio". Per cui vi rispar
miamo fino ad un certo punto,
giacché la vostra debolezza sa
; semplicemente, più che di impo
tenza assoluta nella difesa, ori
ginata da una naturale, impre
scindibile forza maggiore, di
debolezza ingenita proveniente e
scaturiente dai vostri difetti ori
ginali, dalle vostre colpe volonta
rie, dalla vostra falsa presunzio
ne e dalla vostra arrogante pre
tenzione financo.
Altri vi ha esposti l'atti brevi,
concisi, compendiosi che valeva
no un mondo per la loro natura e
per la loro eloquenza ; noi li com
mentammo appena. Di rimando
ci avete solo saputo gratificare
di una buona frittata di parole,
paroline e paroloni cui nessun
lettore dal naso fino e dalla vi
sta buona ha potuto attribuire
I nemmeno il memo uena momeii-
I canea coesione logica ui idee e ui
ai g omenti.
in 01, parlando cosi di passaggio
intorno al comunicato r elici,
■
demmo la dovuta importanza a
quattro l'atti positivamente seri
uena vostra banca:"L/ammontar
ueiie azioni non ancora versate",
una cambiale per tre giorni cu
stodita nella saccoccia di "Gio
vannino", "la pretesa insulsa di
redimere il nostro emigrato dal
lo sfruttamento supposto e pre
sunto semplicemente ad opera
dei banchieri privati", "le parec
chie migliaia di dollari di cui
qualche direttore della Sons ol
italy State Bank è sempre debi
tore verso parecchi dei nostri
banchieri privati locali."
invece di risponderci a tuono,
si guioca di parole, di sole paro
le al di cui affastellamento scor
retto può sempre essere prepo
sto ed incaricato un imbecille
qualsiasi, fosse anche della na
tura di quelli pagati a ventitré
pezze la settimana* per la sola ri
conoscenza di certi piccoli servi
zi sporchi resi in gabinetto pu
lito.
Ai fatti è necessario, quando si
vuole stare bene in gamba in u
na qualsiasi arena di combatti
mento, sempi'e contrap
porre i fatti, ove non si abbia la
voglia di cadere nel cattivo con
cetto del pubblico e nel ridicolo
anche nei rapporti di chiunque è
> liso appassionarsi alle altrui ten
doni.
Si sarebbe potuto essere più
forti e più virili in una risposta
giacché s'era deciso di farcela.
Meglio sarebbe stato però a non
darla una tale risposta quando si |
doveva riuscire così meschini,!
tanto piccini in argomenti ed ar
gomentazioni.
Gli scribi che si son voluto as
sumere l'incarico di certe difese,
ci fanno semplicemente pietà,
giacché essi mancano addirittu
ra di argomentazione non solo,
ma di raziocinio anche. A noi
1 piace avere avversari forti; solo
allora c'è cara e piacevole la lot
ta, anche se dovessimo sempre
perderla. 1 giuocatori di parola,
1 i disquisitoli masturmanti senza
rima e senza concetto alla Angio
lo Curi, ci fanno pena semplice
mente ; li lasceremmo in pace,
: senza un cenno qualsiasi di ri
' sposta, ove non avessimo la di
sgrazia di trovarci in un ambien
te coloniale dove non sempre si
trovano giudici sereni e spassio
nati, più amanti della eloquenza
i dei fatti che delle filastrocche di
parole polemiche.
Abitiamo delle ragioni positi
vamente positive per finirla in
1 questo numero. Ritorneremo pe
rò alla carica prossimamente, per
essere ancora più precisi e più
■ dettagliati intorno a taluni altri
• argomenti che la ristrettezza del
j tempo non ci consente oggi di po
! ter trattare come voi-remmo.
Sarà per i prossimi numeri.
La Cavalletti!
IL PROSSIMO NUMERO DE
"LA RASSEGNA" CONTERRA'
ARTICOLI INTERESSANTI E
DI GRANDE NOVITÀ'. NON
i
MANCATE DI LEGGERLO E
i FARLO CONOSCERE ANCHE
: AI VOSTRI AMICI.
Cieafep Grilli
® Zanzare
Li sì minacciai Per un po
co poco appella di venta che au
bianio voluto pubblicare e dove
rosamente conieniare col nosiru
Plinio numero a riguardo deità
"Sons oi italy State Bank", ci
siamo sentiti tuonare attorno ai
nostro poveio capo tutti i fulmini
di cui Giove abbia mai potuto di
sporre a questo ed a quell'altro
inondo. Tuoni fragorosi e terribi
li in verità; ma essi non ci han
no scosso ne punto e nè poco, nè
impensieriti menomamente. Dac
ché Franklin lece la scoverta del
parafulmine, gli uomini impara
rono ad arrestare l'azione fune
sta del fulmine di Dio. Noi
parafrasando il principio, fisica
mente assiomatico ci sentia
mo di poter dire, dalle sponde di
un giornale ultra-indipendente
che nessun fulmine ci fa paura,
ci spaventa o ci sgomenta, cada o
tuoni essi vicino o distante da
noi. Siamo muniti noi di parec
chi e ben potenti parafulmini.
112 soliti giannizzeri e sparafu
cih di r'eppuccio Romano e di
Giovannino Alfano si son dato un
bei da lare in questa settimana
per incutere timore, non a noi
che non abbiamo mai saputo che
voìesse dire a qfi'ìste mondo ti
more e paura, ma a qualcuno dei
pochi coraggiosi che si son vo
luti unire a noi per un'azione in
dipendentemente detta nei campo
del giornalismo coloniale, al solo
scopo di radicalmente purgare ed
, epurare il nostro ambiente da
I certi elementi nocivi e funesti per
esso nel senso vero della parola.
1 Minacce a destra e minacce a
sinistra; giornali che vi sono e
giornali che sorgeranno per at
taccare, violentemente, implaca
bilmente attaccare Tizio e Caio,
Sempronio e Mevio. Appunta
menti di qua e chiamate a tele
fono di là; tutto un movimento
di battaglioni e di batterie in ri
serva, a chi dandi ed a chi pro
mettendo Tutte manovre queste
che ci fanno ridere, semplicemen
te ridere, perchè non dicono al
tro se non mostrarci la pagina
aperta, chiara e nitida di quel li
bro che ha di già scritto qualche
cosa sul crollo immancabile, ine
vimabile di certe consorterie co
loniali che a noi non sono mai
| andate a genio per nessun verso
e che, per giunta, abbiamo sem
pre combattute ad oltranza, con
; tutta la somma delle forze di cui
j ci è stato consentito disporre.
; Lo sappiamo, lo abbiamo sem
pre pensato : qualche foglietto
■ affidato alla direzione pro-l'orma
i di un irresponsabile qualsiasi ri
vedrà predo la luce. Vi scara-
I bocchieranno Angiolo Curi, Don
na Giovannina e financo il ranoc
chi iaro Notar Peppe "il grande".
Peccato che per il momento non
ci sia quel grande mascalzone di
Peppe Landolfi, del quale poter
: si servire a bell'agio, giacché quel
delinquente, tanto caro a certa
I canaglia che ebbe financo il co
raggio di salutarlo con un ban
' chetto, venne, appena pochi gior
ni dopo la sua assunzione all'uf-
I ficio di assistente "manager" del
la Casa Armour a Milano, licen
f ziato vergognosamente dall'uffi
cio per un giusto reclamo della
'< locale ditta "Philadelphia Jfaca
roni Co." alla quale ebbe l'abilità
I di truffare l'ammontare di due
cento casse di pasta proprio al
momento in cui Giovanni Di Sii
5 soldi la copia
UFFICIO . 920 So. lOth Street
vestro, il focoso, il grande, il con
cettoso, il facondo oratore delle
nostre Colonie, lo definì "un gio
vane degno di grande avvenire,
dal passato, e dal futuro finan
co, fulgidi e sereni come le stel
le mattutine.
Vengano, vengano pur fuori di
questi giornali ; ne avremo sem
pre per tutti. Compassione e mi
sericordia per gli straccioni che
vi possano prestale il nome a
guisa di gerenti irresponsabili;
canzoni e biografie per tutti
quelli che vi saranno parte inte
ressata. Ricorrano o non i man
datari alla prosa prezzolata del
socialistoide o a quella assassina
dell'anarchico, noi non faremo
mai un sol passo indietro. Sapre
mo affrontare gli sgherri con
l'arma al piede e, non si dubiti,
sapremo anche aver la forza di
colpire, mortalmente colpire.
Giù la visiera, adunque, ed a
vanti Nè avrà chi più non ne
saprà mettere. Ci vorranno fatti
però e non chiacchiere, perchè la
nostra religione è quella di la
sciarci sempre guidare dai fatti,
dalla filosofia dei fatti, dalla lo
gica e dalla ragione di essi, da
nient'altro. Si stia pur sicuri.
r elice Cavalloni e Giovanni ili
ailvestio Un assemblea di sti
mabili azionisti tlella tìons oi ita
ly tìtate Bank, riunita a solenne
seuuia per protestale contro il
conunento aU un comunicato in
serito nel nostro giornale in un
ordine del giorno brillante solo ai
mancanza di verità e di rettitudi
ne bancaria "proclama aperta
mente che Ciò vaimi Di Silvestro
limane sempre per la massa ope
raia la figura simpatica del Fe
lice Cavallotti coloniale."
Tanta prosa gonfiata a mo' di
cornamusa, tanto grandioso pa
ragone tra due nomi che fanno a
calci, si rincorrono, si persegui
tano così come può fare un cane
ed un gatto, un lupo ed un agnel
lo, una volpe ed una gallina, là
sul piazzale vasto dell'aia di un
selvaggio contado, non ci fa, nei
rapporti del nostro grande cam
pione senza valore signor Gio
vanni Dì Silvestro, se non l'effet
to dell'ossigeno cautamente e
pazientemente somministrato ad
un ammalato cui non rimangono
se non pochi istanti di vita.
forre Giovanni Di Silvestro,
eroe ai ogni nostra porcheria co
loniale, a paragone di Felice Ca
vallotti il campione vero della
democrazia italiana che non si
sognò mai di chiamare figlio di
p un Conte Naselli, nè si fece
tacitare certe fameliche pretese
nella campagna contro gl'imbro
gli che in un tempo si facevano
per la leva militare, nè si recò in
un tempo a Scranton per inneg
giare al regicida Bresci, è co
sa che raggiunge solo il colmo del
la imbecillaggine umana, passata
attraverso il filtro di ventitre
dollari la settimana pel mastur
batoli Angiolo Curi, di una mez
za dozzina di bottiglie di buona o
cattiva birra che siasi nei ri
guardi dei focosi, stimabili, sem
pre illustri votanti un ordine del
giorno cui solamente i pesci stan
tii e puzzolenti che si vendono al
le nove strade potrebbero batte
re fragorosamente le mani.
Ci rivedremo, signori, ci rive
dremo, siatene certi.
Don Procopio


# kiara\kiara\examples\data\text_corpus\La_Rassegna\sn84037025_1917-04-21_ed-1_seq-1_ocr.txt
■ jSrìt** W?? iIK 38®f- i^M
F< 5É -Ì V ;Ht p T '"'- ., .^4)4——ff
Both Phones
ANNO IL No. S
LE COSE A POSTO
ibi va dicendo, si va susurran-
Uo, ai va insinuando in Colonia,
con una gran dose ui malignila
s inteuue, perone ove la mali
gnità venisse meno, cene cose
non potrebbero sussistere, cue
noi uè "Ux Kassegna ' siamo sor- !
li per combattere i ordine ìgli
d Italia e la iianca dei J? ìgli ui-1
calia.
Niente di più sbagliato, di più
errato, di più assurdo.
Chi pubblicamente o privata
mente, magari, ci accusa di tanto
non fa che compiere opera malva
gia, ove nell'azione prevalgano
sentimenti di animo perfido e
maligno ; non fa che sempre con
sumare opera strana ed imbecil
le ove solo il senso della superfi
ciale osservazione delie cose po
tesse consentire l'accesso al di- j
ritto del giudizio e della critica.
No. Noi non siamo nè contro!
l'Ordine dei Figli d'ltalia, nè con
tro la Banca che dei Figli d'ltalia
na voluto opportunamente pren
dere il nome.
Sono entrambe per noi delle ri
spettabili istituzioni cui va sem
pre latto tanto di cappello. Ab
ominio di tutte e due parlato sem
pre bene; quello pero che non ci
va i genio in proposito di esse e il
fatto del controllo assoluto, del
l'egemonia la più impudente, la
più sfacciata che una cricca di
furfanti e di snaturati, da tempo
affliggente la nostra Colonia,
pretende di potere esercitare sul
l'una e sull'altra senza che dalla
parte di altri, da parte degli one
sti cioè, si potesse dire o ridire
all'occorrenza in modo opportuno
o, in qualche maniera, logicamen
te.
Abbiamo scritto in altri rin
contri dell'Ordine Figli d'ltalia,
lodandone incondizionatamente la
forte compagine che, nella sua
sintesi, sta plausibilmente a di
mostrare, nè più e nè meno
peraltro di come si fa dalla par
te degl'lndipendenti, che gl'i
taliani all'Estero incominciano a
sentire il bisogno di organizzarsi
in grandi famiglie per aver dirit
to in alcun modo alla considera
zione ed al rispetto legittima
mente dovuti in mezzo a popo
lo che ci ospita, in mezzo agli a
mericani cioè che, solo da qual
che tempo, incominciano a guar
darci con occhio più benigno di
quello che non avessero mai fatto
prima. E se ciò fanno è, di
spiace a noi più di tutti il dirlo,
non per merito acquisito delle
nostre masse all'Estero, ma per
tutt'altre ragioni che trovano e
vantano la loro genesi nello svol
gimento di ragioni di indole na
zionale interna, nei rapporti im
mediati, obbligatoriamente cioè
consecutivi a quelli di politica e
stera, dato il conflitto europeo.
Abbiamo sempre e sinceramen
te inneggiato alla fusione delle
masse degl'italiani all'Estero
perchè, spassionatamente, ab
biamo sempre ritenuto che solo
da una fusione stretta, compat
ta, cosciente, informata a princi
pi fattivi e non distruttivi, noi
italiani potessimo dire sempre e
sufficientemente all'estero. Di
sgraziatamente però all'estero il
più grande, il maggiore dei nemi
ci dell'italiano, del nome italiano,
del prestigio italiano, è l'italiano,
sempre l'italiano.
Perchè si segua e si coltivi tan
ta condotta, noi non siamo mai
riusciti a comprendere, nè a spie
garci in alcun modo. E' cosa cer-
ITALIAN WEEKLY IMEWSPAPER
I )evoted to welfare and advancement of the Italians in America
S. LIBERATORE, Direttore
! ta pero ciie tutta questa nostra 1
' strana condotta, tutto questo no- 1
suo strano procedere non la cne
nuocere, seriamente e positiva
mente nuocere agl'italiani clie
cieuono di riporre, di auiuare a
! gì italiani maggiori la loro lidu
j eia, quella iiducia cioè che, altri
menti, andrebbe chiamata Ouona
leue neil aiiidare ad altri il man
! dato per 1 esperimento delle pro
« prie ragioni.
Per quanto significa l'istitu
zione della Banca Statale "Figli
d'italia", noi non abbiamo a ri
petere che sia essa la benvenuta
, in mezzo a noi, solo però se viene
con intendimenti per davvero e
| satti circa l'espletamento di una
funzione dicente a pieno o che po
i tesse dire anche in piccolo modo
Idi evoluzione coloniale, di interes
ise coloniale. Perchè ove il pro
gramma generale della massa de
gli azionisti dovesse essere il con
trario, dovesse avere cioè di mira
la lotta ad altre istituzioni ita
liane uei genere, oppure l'assurda
pretesa di volere e saper fare
meglio degli altri e contro degli
altri, noi, dai nostro povero po
sto ai giornalisti che non sono u
si a scrivere per accondiscendere
passivamente alle volontà altrui,
sentiremo sempre, per impulso
c scatto di coscienza semplice
mente e non per altro, il doveri
Idi parlare Insorgere a dovere.;
Riepilogando, sia nei rapporti
dell'Ordine dei Figli d'ltalia, sia
in quelli che riflettono l'istitu
zione che da esso ha voluto pren
dere il nome, noi dobbiamo, sen
tiamo il dovere di rispettare, in
massima, e l'una e l'altra istitu
zione. Tanta professione di fede ,
e di rispetto non ci è dettata,
si noti e si noti bene, da sensi ;
di paura o di qualsiasi altra cosa
del genere; è solo in base ad un
solido principio di disquisizione e
di critica giornalistica che noi
parliamo.
Si noti però che noi siamo de
terminati, assolutamente deter
minati di alzare la voce, folte
mente e vibratamente alzarla o
gni qualvolta nel seno di queste
due rispettabili istituzioni si ab
bia menomamente ragione a ri
dire per l'infiltrazione preponte
rante e malintenzionata in mez
zo ad esse di elementi che la più
parte della nostra Colonia seppe
da tempo riprovare non solo, ma
aborrire anche.
La Rassegna
Comunicato
Sig. S. Liberatore
direttore de
"La Rassegna"
Sono un appartenente all'Ordi
ne Figli d'ltalia; partecipo però
dell'Ordine fino quando mi si
riesca a dimostrare che esso sap
pia veramente giovare agl'inte
ressi operai, cioè a dire all'inte
resse di quell'operaio che usa e sa
veramente usare delle sue ener
gie, delle proprie energie per as
solvere dignitosamente il com
pito difficilissimo della vita di
oggigiorno.
Io sono membro della "Loggia
Italia"; fui presente, passiva
mente presente alla seduta che
detta Loggia tenne la sera di
martedì scorso. Dico passivamen
te presente, perchè io, poco o
niente desideroso di emergere in
PHILADELPHIA, PA., SABATO, 21 APRILE 1917
mezzo alle assemblee, non sono
uso farmi notare per partegiantel
dell'una o dell'altra fazione che in I
mezzo a tale loggia si contendo- ì
no e si contrastano il primate (
semplicemente oratorio.
lo che scrivo si noti sono
un operaio. Sentii in quella sera
parlare molta gente in favore
della classe operaia. Vincenzo Ti
tolo da una parte, i fratelli Di
Silvestro dall'altra. Gli operai ap
plaudivano, sentivano giusto ad
applaudire iti discorsi, o per me
glio dire alla chiacchiere incon
cludenti dell'uno e degli altri ; ap
plausi in "sine fine" e ovazioni a
"not plus ultra" agli spunti ora
tori dell'una e dell'altra parte.
Però, io credo, ben pochi capiva
no che tutta quella roba era pro
venienza di un armadio assoluta
mente affaristico, vergognosa
mente affaristico. Titolo disse
contro i Di Silvestro; i Di Silve
stro seppero rispondere a Titolo.
Tutta questa gentilissima gente
volle e seppe artificiosamente
parlare in nome della classe ope
raia.
Quando si trattò però di ad
divenire alla nomina dei delegati
per la prossima suprema con
venzione dei Figli d'ltalia, i fa
voriti dell'urna cieca e sempre
imbecille sono stati i fratelli Gio
vanili e Giuseppe Di Silvestro e
Vincenzo Titolo, nessuno di que
sti può dirsi con coscienza vera
di essere operaio ; quelli vera
»•***»»»•*>»»»»<»<>>»»»»>•
Al" * • Jj ÌE m " » !
Affli uscieri dalia
"Per copia conforme"
Vi siete affannati, vi siete
scalmanati, vi siete indolii o li
dotli a risponderci per conto
non vostro, giacche voi siete as
solutamente delle meschinità in
materia di vita coloniale, ma
per mandato imperativamente
dovuto agii inetti ed agi impoten
ti. Nemmeno sotto la fal
sariga impostavi dai vostri pa
droni, ci siete apparsi nè servi
coscienti, nè uomini e nè tampo
co giornalisti. Vi dovremmo su
bito dire: andate al diavolo, alla
malora, perchè siete semplice
mente dei grandi imbecilli e
quindi non sispondervi nemme
no ; ma siamo pur grandi e gene
rosi noi per non sentire il bisogno
di prendervi sotto la protezione
del nostro manto sociale-giorna
bstico coloniale.
Stiamo da tanto tempo assi
stendo allo scandalo veramente
deplorevole e vergognoso all'i
stesso tempo di gente che, pur in
Italia essendo andati oltre il gin
nasio, il liceo e l'università, dan
no prova in Colonia di professio
ne giornalistica bastarda e mer
cimoniosa. Deploriamo questo
fatto con tutte le forze dell'animo
nostro, e ce ne addoloriamo, al
contempo, sia per quelli che si
prestano a tanto giuoco, sia per
gli altri che per tanta povera,
stupida, mercimoniosa opera,
credono di farsi sgabello per
montare sublimi nel campo delle
nostre cose coloniali.
Don Tommaso l'ineffabile,
il sempre ineffabile. Don Tomma
so Catalogna, immemore di pre
cedenti dichiarazioni che Io han
no sempre fatto compatire nei ri
trovi pubblici o privati dove gli
viene, sempre per pura e sempli
ce elemosina o che, in altri ter
mini, si voglia o si possa dire pu
mente dalla coscienza pulita, co-,
' me può sempre essere la coscien
za di un onesto ope."aio. Potran
no essi essere dei gruiid'uomini
| in altro campo, ma in quello ope
raio no, assolutamente no.
E' curioso, egregio direttore
de "La Rassegna" che certa gen
te vinca, riesca a vincere dello
cause in nome della classe ope
raia, solo in base ad un program
ma che è tutto un insulto, tutto
un oltraggio alla classe operaia.
Quando arriveranno a compren
dere tutto questo i nostri operai?
10 sono di quelli, egregio signor
Liberatore, che combatto ad ol
tranza per l'Ordine dei Figli d'l
talia ; però certi cibi, all'occorren
za, in occasione, non vogliono,
non possono assolutamente anda
re giù. Conviene, allora, ricorrere
al medico ed il medico, in questo
caso, in questo difficilissimo ca
so siete voi. Sappiate curare n
dovere.
Un membro della
"Loggia Italia"
11 comunicato cui abbiamo da
to pubblicazione non ha bisogno
di commenti ; è eloquente a me
ravioli a in tutta la sua interez
za. Lo conserviamo nel suo origi
nalo, anzi lo manteniamo a dispo
sizione di chiunque potesse es
ser colto dalla debolezza $ tocca
re sempre con le mani pfr crede
re.
n .d r.
ra e semplice pelosa carità fra
terna, accordato un pochino di
compiacente attenzione, s'e im
pennato a guisa di asino di
pautelieria ferocemente e, gui
dato ed assistito in parecchi
I punti dall'altrui falsariga, ha
tentato di andare in alto, divo-
in allo, a guisa di Icaro, di
menticando o ignorando addirit
tura che le ali di cera son destina
te sempre a perdere ogni virtù al
semplice, immediato contatto dei
'aggi portentosi del sole, raggi
scovritoli e purificatori di tanti
germi maligni ed insidiatori del
l'uman genere e di tutto ciò che
di genti e genere delle genti po
tesse sempre dire in qualunque
modo.
Don Tommaso Catalogna, sem
pre sfruttato in tutto il succo
delle sue invertebrate forme dai
compari di cui oggi ardisce pren
dere le difese fu sempre com
patito da noi nelle sue supi
ne velleiteà giornalistiche, e
tutto questo per solo rispetto a
monna senilità che, burbera e
minacciosa, s'erge sul di lui ca
i po più a castigo della sua perfi
dia d'animo che della somma dei
suoi parecchi anni.
Don Tommaso Catalogna, dopo
aver permesso ad altri di scrive
re, giacché egli non ha mai a
vuto nè il diritto e nè l'abilità di
scrivere sulle colonne di un gior
-1 naie,— insulsaggini e vituperi a
riguardo e nei riguardi dei ban
chieri privati, allorché si trattò
di gittare e far fecondare il se
me per la banca statale che oggi
lo ha pensionato' a guisa d'invali
do reduce valorosissimo delle pa
trie bottiglie, se ne vien fuori
con delle frasi paliatìve a riguar
do dei banchieri privati; e tutto
questo lo fa con una prosa che sa
più di lavandaia che di ordinario,
, coni une giornalista coloniale il,
' quaie non sa, non può guardare I
più oltre dei proprio naso prono
scemale.
IN on sapremmo definire tanta
annoia conuoita da parte di chi
posa per davvero a giornalista
maglio e sommo ili mezzo al no
stro amoiente, ove non ci fosse
dato di conoscere a fondo uomini
e cose, storie e disgrazie, vita e
I miracoli di tutto foibe coloniale
di un ventennio a questa parte,
trattandosi perù di un Tommaso
Catalogna, celebre, per le requisi
torie che usa fare contro i fra
telli Siamesi ogni qual volta qual
cuno gli paga da bere per farlo
parlale, la cosa può anche passa
re sotto il manto caritatevole di
una generosità d'animo che ci fe
ce sempre distinguere in mezzo
alle genti di nostra Colonia.
A voler contentare tutta la prosa,
stupida, illogica, assolutamente
meccanica del nostro Don Tom
maso, significherebbe fare il
giuoco semplicemente di altri e
non suo, giacché il poveruomo,
l'eroe del "teaccuino del pubbli
co" non ha scritto, oppure non si
è fatto tenere la penna scrivendo,
se non sotto la minaccia di poter
perdere i venticinque scudi men
sili dell'avviso per il giornale ser
votta e le venti pezzarelle setti
manali che, appena la scadenza
del primo semestre costringeran
no la banca nostra ad un assess
ment superiore di quello che non
avrebbe mai potuto avere ove al
l'ufficio di impiegati non avesse
si'imulisti che, con '
la loro coltura,", vanno viux, a
liceo e l'università.
E' pregio dell'opera però, e sa- ;
remmo degli asini se non lo fa
cessimo, mettere in rilievo come
Don Tommaso, cliissa forse se
presente o assente la nostra sim
patica Donna Giovannina, si sia
voluto dichiarare uno dei rivendi
catori più strenui della nostra o
pera, del nostro apostolato gior
nalistico.
Sentano i lettori che cosa ci ha
egli voluto dire con la sua epi
stola :
"Nel giro di pochi anni sono
surte in colonia tre aziende ban
carie italiane: la ltalian Co-Ope
rative Banking Association, tra
sformatasi ultimamente in South
Philudelphia State Bank ; la Eco
nomical Co-Operative Banking
Association ela Sons of ltaly
State Bank.
"Intorno alle due prime la
stampa non ha trovato nulla a
ridire, anzi fu loro piuttosto lar
ga di incoraggiamento, pur sa
pendo che a danno dei nostri po
veri connazionali bisognosi si o
perava un certo tal quale strozzi
naggio, in virtù del quale era pos
sibile dare agli azionisti larghi
dividendi, che han riparto loro
il capitale investito.
"Se vi fu alcuno che mosse un
attacco contro una di esse, e se
non andiamo errati, contro due e
due, fu appunto il pennaiuolo che
ora di entrambe si è fatto pala
-1 dino".
Anche un principiante mastur
batore in giornalismo saprebbe
1 leggere attraverso tanta eloquen
tissima prosa: La Co-Operative
1 Banking Association e la Econo
mical Co-Operative Banking Ass.
1 vennero su in colonia con deter
minata intenzione di esercitare lo
strozzinaggio. Intorno alle opera
zioni di queste due banche dice
ed asserisce Don Tommaso sulla
i fede dei suoi cinque lustri di vi
• ta giornalistica "la stampa
) non trovò nulla a ridire, anzi fu
i loro piuttosto di incoraggiamen
,ì to ecc. ecc." Tra questa stam-
Ipa ìncoraggiatnce stavano Don
I iommaso, i suoi padroni ed il ri
manente uei giornalisti che van
no oltre il ginnasio, il liceo e l'u
niversità. "Se vi ìu alcuno che
mosse un attacco contro una di
esse seguita a sciorinare Don
lommaso Catalogna e se non
andiamo errati contro tutte e
due (sì, proprio contro tutte e
due, n. d. r.) fu il pennaiuolo
ecc. "
ÌNOÌ non auuiaino mai voluto
menare n vanto ui essere riusci
ti con una sene compieta ai aj.li
con serrati ad indurre le banche
co-operative, e più specialmen
te quella delle undici strade, a
cessare dallo strozzinaggio cui a
vevano credulo darsi per ragioni
lutie interne di azienda, giac
che non fu mai prerogativa no
stra quella deii'auto-deoantazio
ne ma se altri ci riconoscono
il merito oi certe azioni, non ci
rimane che cortesemente ringra
ziare. a urono allora gli altri a
tollerale, anzi ad incoraggiale lo
strozzinaggio; noi a combatterlo
uno a vincerlo. Lettori, avete
bisogno di commenti ulteriori '(
Non crediamo perchè non abbia
mo mai credulo che voi foste di
quella tal quale ignoranza che vi
si vuole sovente attribuire dai no
stri giornalisti sommi.
be poi dalla banca delie undici i
strade "avemmo" del denaro a!
campagna imita, diremo in al-!
, tro numero. i\oi siamo sempre
qua per dare conto di noi, dei no
»tro presente, di tutto ciò che
riguarda noi e quelli che stanno
i Con noi. <_>* pin i.. A
dovunque, e se altri tenta di to
gliercela, noi non sapremo per
donare.
Ed ora una paiola, una sola
parola all'orecchio ai grande gior
nalista Catalano che osa unpen
narsi senza ragione e che solo
per mandato ricevuto corre al |
salvataggio di taluni naufraghi
pericolanti :
Tieni o non tieni sempre a
mente tutta la maldicenza che
per mesi, per anni interi sei an
data facendo contro i fratelli Di
Silvestro? Non ti ricordi quanto
ne hai detto e contro l'uno e con
tro l'altro, in ispecial modo con
tro Peppuccio che secondo le
tue asserzioni ti aveva rubato il
frutto dei sacrifici di cinque lu
stri di giornalismo? Senti anco
ra, Don Tommaso carissimo, av
vicinati un po' più alla nostra vo
ce e tendi bene le asinine orec
chie: Ti ricordi quando sei anda
to, mentre facevi la campagna
prò istituenda banca Figli d'lta
lia, a premurare il signor Frank
Bisciotti perchè dicesse a quelli
della South Phila. State Bank che
tu, armi e bagagli—fortunamen
te sfiatati—con tutto il tuo gior
-1 naie saresti stato disposto a pas
sare dalla parte loro e schierarti
contro gli altri ove ti avessero as
sicurato un compenso di cen
: to dollari al mese ?.
Con la proposta facesti ridere e
più di tutti rise—anti ti compian
se, cosa peraltro che non avrà
mai ragione di fare a riguardo
: nostro, quel grand'uomo di
- Ciccio Palumbo che tu hai pur
ì tante volte insultato non ostante
- ti pagasse puntualmente sei pez
• zarelle al mese per l'avviso.
Via, Don Tommaso, non ardi
> re di parlare ; lascia stare la pen
na, cessa di fare il giornalista o
? per lo meno non conformare
i quello che solo altri ti costringo
- no a pubblicare e rimani, rima
i ni sempre quel maligno compi
i latore che sei de "Il Taccuino del
- Pubblico" e del "Diario della
- Guerra". Quando un'altra volta
5 soldi la copia
UFFICIO : 920 So. lOth Street
. iì voi-l'anno lanciale contro quei
j untante ui oh vio i-uoeiiuore sap-
Ivi iiùpoiiucic, uegaiiuoti: "An
uiuiiiu a iaiL' un uiccuiere
oli viti ciuciatole e un osso
nuppu uuiu percne certe mascel
le squassale potessero essere
capaci ui intaccarlo menoma
mente.
ora a te, o .angiolo Curi.
AU nai scritto tre lunghe, rit
te, compatte e uen nutrite colon
ne in prosa cne vanno oltre il gin
nasio, ii i.ceo e sorpassano pure
i università.
iNoi provenienti dalia terza
classe elementare non abbiamo
potuio comprendere delle tue tre
colonne se non 1 uiluno periodo:
io posso bene permettermi il
ÌUSSO di pisciar e allegramente so
pra ì vostri squallidi singulti"
ecc
iiiuuene, caro Angiolo Curi: se
hai creduto or pisciare tu sulle
lue tre coionne di prosa, non vo
gliamo commettere ia scostuma
tezza di pisciarci noi. ijiamo edu
cati noi della terza classe elemen
tare più di quello che non sap
piano essere ì provenienti dai h
i cei e che lambirono pure l'univer
sità.
ri galateo innanzi tutto.
bilvio La bela tuie
A cui litica iiiierissare
AUorquanuo òhvio .Liberatore
nn richiese per la costruzione di
mum toammnaiii ttHii ."ina .HpHft
quale por sono stato nominato
presidente, aderii di buon grado
Julia richiesta per due semplicis
sime ragioni: la pinna perchè
! conoscevo da parecclti mini il si
gnor liberatore, e mi sentivo a
I lui ligato da vincoli di buona aiui
ì cizia per stimarlo e come uomo e
j come giornalista ; la seconda per
i che mi convulsi delle ragioni che
' un si addussero intorno alla ne
, ! cessila d'avere iu Colonia un fo-
glio ben lutto non solo, ma indi
pendente anche. Queste garanzie
peraltro le trovavo non nelle sem
plici affermazioni, mu in tutto un
passato di movimentato ed one
sto giornalismo coloniale che sta
e starà sempre per dire di Silvio
Liberatore.
Accettando poi la presidenza
della compagnia editrice, seppi di
assumere delle responsabilità e
le assunsi con piena coscienza.
Non si venga quindi a parlare di
presidente "travicello" o di altre
storie, perchè tutto questo non
potrebbe rimanere che una sem
plice, stupida affermazione di
gente abituata a vedere sempre
con gli occhi degli altri.
Angelo Cusano
RIDI, PAGLIACCIO,..
Il generale austriaco Koevess
ha detto qualche cosa che non bi
sogna lasciar perdere disatten
tamente. Ha detto: Oh, l'eser
cito era scettico per la possibili
tà di una pace rapida; e "la causa
del suo scetticismo era il contat
to col nemico". Se i politicanti
vedono le cose a modo loro e se,
lungi dai campi di battaglia, i
"contatti" possono essere d'ogni
i specie e mettiamo anche d'ogni
; risma, sui campi di battaglia do
■ ve non rimane posto o comodità
• sufficiente per gl'intrighi c'è una
• sola ma capitale materia di giudi
! zio. 1 soldati austriaci del gene
t rale Koevess notavano che il ne
t mico il nemico combattente


# kiara\kiara\examples\data\text_corpus\La_Rassegna\sn84037025_1917-04-21_ed-2_seq-1_ocr.txt
■Both Phones
ANNO 11. No. 5
LE COSE A POSTO
Si va dicendo, si va susurran
do, si va insinuando in Colonia,
i con una gran dose di malignità
; s'intende, perchè ove la mali
gnità venisse meno, certe cose
non potrebbero sussistere, che
noi de "La Rassegna" siamo sor
ti per combattere l'Ordine Figli
d'ltalia e la Banca dei Figli d'l
talia.
Niente di più sbagliato, di più
errato, di più assurdo.
Chi pubblicamente o privata
mente, magari, ci accusa di tanto
non fa che compiere opera malva
gia, ove nell'azione prevalgano
sentimenti di animo perfido e
maligno ; non fa che sempre con
sumare opera strana ed imbecil
le ove solo il senso della superfi
ciale osservazione delle cose po
tesse consentire l'accesso al di
ritto del giudizio e della critica.
No. Noi non siamo nè contro
l'Ordine dei Figli d'ltalia, nè con
tro la Banca che dei Figli d'ltalia
ha voluto opportunamente pren
dere il nome.
Sono entrambe per noi delle ri
spettabili istituzioni cui va sem
pre l'atto tanto di cappello. Ab
biamo di tutte e due parlato sem
pre bene; quello però che non ci
va i genio in proposito di esse è il
l'atto del controllo assoluto, del
l'egemonia la più impudente, la
più sfacciata che una cricca di
furfanti e di snaturati, da tempo
affliggente la nostra Colonia,
pretende di potere esercitare sul
l'una e sull'altra senza che dalla
parte di altri, da parte degli one
sti cioè, si potesse dire o ridire
all'occorrenza in modo opportuno
o, in qualche maniera, logicamen
te.
Abbiamo scritto in altri rin
contri dell'Ordine Figli d'ltalia,
lodandone incondizionatamente la
forte compagine che, nella sua
sintesi, sta plausibilmente a di
mostrare, nè più e nè meno
peraltro di come si fa dalla par
te degl'lndipendenti, che gl'i
taliani all'Estero incominciano a
sentire il bisogno di organizzarsi
in grandi famiglie per aver dirit
to in alcun modo alla considera
zione ed al rispetto legittima
mente dovuti in mezzo a popo
lo che ci ospita, in mezzo agli a
mericani cioè che, solo da qual
che tempo, incominciano a guai 1 -
darci con occhio più benigno di
quello che non avessero mai fatto
prima. E se ciò fanno è, di
spiace a noi più di tutti il dirlo,
non per merito acquisito delle
nostre masse all'Estero, ma per
tutt'altre ragioni che trovano e
vantano la loro genesi nello svol
gimento di ragioni di indole na
zionale interna, nei rapporti im
mediati, obbligatoriamente cioè
consecutivi a quelli di politica e
stera, dato il conflitto europeo.
Abbiamo sempre e sinceramen
te inneggiato alla fusione delle
masse degl'italiani all'Estero
perchè, spassionatamente, ab
biamo sempre ritenuto che solo
da una fusione stretta, compat
ta, cosciente, informata a princi
pii fattivi e non distruttivi, noi
italiani potessimo dire sempre e
sufficientemente all'estero. Di
sgraziatamente però all'estero il
più grande, il maggiore dei nemi
ci dell'italiano, del nome italiano,
del prestigio italiano, è l'italiano,
sempre l'italiano.
Perchè si segua e si coltivi tan
ta condotta, noi non siamo mai
riusciti a comprendere, nè a spie
garci in alcun modo. E' cosa cer
<* IT ALI A N WEEKLY NEWSPAPER
Devoted to welfare and advancement of the Italiana in America
S. LIBERATORE, Direttore
•ita però che tutta questa nostra
, strana condotta, tutto questo no
i stro strano procedere non fa che
•j nuocere, seriamente e positiva
s mente nuocere agl'italiani che
: j credono di riporre, di affidale a
■ gl italiani maggiori la loro fidu
i eia, quella fiducia cioè che, altri
menti, andrebbe chiamata buona
fede nell'affidare ad altri il man
i dato per l'esperimento delle pro
prie ragioni.
i Per quanto significa l'istitu
. zione della Banca Statale "Figli
i d'ltalia", noi non abbiamo a ri
: petere che sia essa la benvenuta
. in mezzo a noi, solo però se viene
. con intendimenti per davvero e
. | satti circa l'espletamento di una
funzione dicente a pieno o che po
. I tesse dire anche in piccolo modo
di evoluzione coloniale, di interes
-11 se coloniale. Perchè ove il pro
gramma generale della massa de
t gii azionisti dovesse essere il con
trario, dovesse avere cioè di mira
la lotta ad altre istituzioni ita
. nane del genere, oppure l'assurda
. pretesa di volere e saper fare
. meglio degli altri e contro degli
. altri, noi, dal nostro povero po
i sto di giornalisti che non sono u
| si a scrivere per accondiscendere
passivamente alle volontà altrui,
i sentiremo sempre, per impulso
i e scatto di coscienza semplicc
i mente e non per altro, il dovere
, di patiate ed v: :or«e*£ awrtovwc v
Riepilogando, sia nei rapporti
dell'Ordine dei Figli d'ltalia, sia
in quelli che riflettono l'istitu
zione che da esso ha voluto pren
dere il nome, noi dobbiamo, sen
tiamo il dovere di rispettare, in
massima, e l'una e l'altra istitu
• zione. Tanta professione di fede
1 e di rispetto non ci è dettata,
I si noti esi noti bene, da sensi
di paura o di qualsiasi altra cosa
1 del genere; è solo in base ad un
solido principio di disquisizione e
di critica giornalistica che noi
parliamo.
Si noti però che noi siamo de
terminati, assolutamente deter
minati di alzare la voce, forte
mente e vibratamente alzarla o
gni qualvolta nel seno di queste
due rispettabili istituzioni si ab
bia menomamente ragione a ri
dire per l'infiltrazione preponte
' rante e malintenzionata in mez
' zo ad esse di elementi che la più
parte della nostra Colonia seppe
da tempo riprovare non solo, ma
aborrire anche.
La Rassegna
Comunicato
Sig. S. Liberatore
direttore de
"La Rassegna"
Sono un appartenente all'Ordi
; ne Figli d'ltalia; partecipo però
> dell'Ordine fino quando mi si
• riesca a dimostrare che esso sap
) pia veramente giovare agl'inte
ressi operai, cioè a dire ali inte
■ resse di quell'operaio che usa e sa
i veramente usare delle sue ener
; gie, delle proprie energie per as
■ solvere dignitosamente il com-
I pito difficilissimo della vita di
• oggigiorno.
Io sono membro della "Loggia
, Atalia"; fui presente, passiva
mente presente alla seduta che
detta Loggia tenne la sera di
i martedì scorso. Dico passivamen
• te presente, perchè io, poco o
■ niente desideroso di emergere in
PHILADELPHIA, PA., SABATO, 21 APRILE 1917
mezzo alle assemblee, non sono
uso farmi notare per partegiante '
dell'una o dell'altra fazione che in '
mezzo a tale loggia si contendo
no e si contrastano il primato j
semplicemente oratorio.
Io che scrivo si noti sono
un operaio. Sentii in quella sera
parlare molta gente in favore
della classo operaia. Vincenzo Ti
tolo da una parte, i fratelli Di
Silvestro dall'altra. Gli operai ap
plaudivano, sentivano giusto ad
applaudire ai discorsi, o per me
glio dire alla chiacchiere incon
cludenti dell'uno e degli altri; ap
plausi in "sine fine" e ovazioni a
"not plus ultra" agli spunti ora
tori dell'una e dell'altra parte.
Però, io credo, ben pochi capiva
no che tutta quella roba era pro
venienza di un armadio assoluta
mente affaristico, vergognosa
mente affaristico. Titolo disse
contro i Di Silvestro; i Di Silve
stro seppero rispondere a Titolo.
Tutta questa gentilissima gente
volle e seppe artificiosamente
parlare in nome della classe ope
raia.
Quando si trattò però di ad
divenire alla nomina dei delegati
per la prossima suprema con
venzione dei Figli d'ltalia, i fa
voliti dell'urna cieca e sempre
imbecille sono stati i fratelli Gio
vanni e Giuseppe Di Silvestro e
Vincenzo Titolo, nessuno di que
sti può dirsi con coscienza vera
di essere operaio; quelli vera-
Agli uscieri dalla
"Per copia conforme"
Vi siete affannati, vi siete
scalmanati, vi siete indotti o ri
dotti a risponderci per conto
non vostro, giacché voi siete as
solutamente deile meschinità in
materia di vita coloniale, ma
per mandato imperativamente
dovuto agli inetti ed agl'impoten
ti. Nemmeno sotto la fal
sariga impostavi dai vostri pa
droni, ci siete apparsi nè servi
coscienti, nè uomini e nè tampo
co giornalisti. Vi dovremmo su
bito dire: andate al diavolo, alla
malora, perchè siete semplice
mente dei grandi imbecilli e
quindi non sispondervi nemme
no; ma siamo pur grandi e gene
rosi noi per non sentire il bisogno
di prendervi sotto la protezione
ilei nostro manto sociale-giorna
listico coloniale.
Stiamo da tanto tempo assi
stendo allo scandalo veramente
deplorevole e vergognoso all'i
stesso tempo di gente che, pur in
Italia essendo andati oltre il gin
nasio, il liceo e l'università, dan
no prova in Colonia di professio
ne giornalistica bastarda e mer
cimoniosa. Deploriamo questo
fatto con tutte le forze dell'animo
nostro, e ce ne addoloriamo, al
contempo, sia per quelli che si
prestano a tanto giuoco, sia per
gli altri che per tanta povera,
stupida, mercimoniosa opera,
credono di farsi sgabello per
montare sublimi nel campo delle
nostre cose coloniali.
ls- i/
7Tsi sì
Don Tommaso l'ineffabile,
il sempre ineffabile Don Tomma
so Catalogna, immemore di pre
cedenti dichiarazioni che lo han
no sempre fatto compatire nei ri
trovi pubblici o privati dove gli
viene, sempre per pura e sempli
ce elemosina o che, in altri ter
mini, si voglia o si possa dire pu
mente dalla coscienza pulita, co
me può sempre essere la coscien
za di un onesto operaio. Potran
no essi essere dei grand'uomini
in altro campo, ma in quello ope
raio no, assolutamente no.
E' curioso, egregio direttore
de "La Rassegna" che certa gen
te vinca, riesca a vincere delle
cause in nome della classe ope
raia, solo in base ad un program
ma che è tutto un insulto, tutto
un oltraggio alla classe operaia.
Quando arriveranno a compren
dere tutto questo i nostri operai?
10 sono di quelli, egregio signor
Liberatore, che combatto ad ol
tranza per l'Ordine dei* Figli d'l
talia; però certi cibi, alibccorren
za, in occasione, ntìn 4 vogliono,
non possono assolutamente anda
re giù. Conviene, allora, ricorrere
al modico ed il medico, in questo
caso, in questo difficilissimo ca
so siete voi. Sappiate curare a
dovere.
Un membro della
"Loggia Italia"
11 comunicato cui abbiamo da
to pubblicazione non ha bisogno
•li commenti; è eloquente a me
r;\vi«rlia in tutta la sua interez
za. I ,o conserviamo nel suo origi
nale, anzi lo manteniamo a dispo
sizione di chiunque potesse es
ser colto dalla debolezza di tocca
re empre con lo mani per crede
re.
n .d r.
ia e semplice pelosa carità fra
terna, accordato un pochino di
compiacente attenzione, s'è im
pennato a guisa di asino di
pantelleria ferocemente e, gui
dato ed assistito in parecchi
punti dall'altrui falsariga, ha
tentato di andare in alto, di vo
lare in aito, a guisa di Icaro, di
menticando o ignorando addirit
tura che le ali di cera son destina
te sempre a perdere ogni virtù al
semplice, immediato contatto dei
raggi portentosi del sole, raggi
scovritoli e purificatori di tanti
germi maligni ed insidiatori del
l'uman genere e di tutto ciò che
di genti e genere delle genti po
tesse sempre dire in qualunque
modo.
Don Tommaso Catalogna, sem
pre sfruttato in tutto il succo
delle sue invertebrate forme dai
compari di cui oggi ardisce pren
dere le difese fu sempre com
patito da noi nelle sue supi
ne velleiteà giornalistiche, e
tutto questo per solo rispetto a
monna senilità che, burbera e
minacciosa, s'erge sul di lui ca
po più a castigo della sua perfi
dia d'animo che della somma dei
suoi parecchi anni.
Don Tommaso Catalogna, dopo
aver permesso ad altri di scrive
re, giacché egli'non ha mai a
vuto nè il diritto e nè l'abilità di
scrivere sulle colonne di un gior
nale, — insulsaggini e vituperi a
riguardo e nei riguardi dei ban
chieri privati, allorché si trattò
di gittare e far fecondare il se
me per la banca statale che oggi
lo ha pensionato 1 a guisa d'invali
do reduce valorosissimo delle pa
trie bottiglie, se ne vien fuori
con delle frasi paliative a riguar
do dei banchieri privati ; e tutto
questo lo fa con una prosa che sa
più di lavandaia che di ordinario,
icomune giornalista coloniale il
quale non sa, non può guardare
più oltre del proprio naso probo
sceidale.
Non sapremmo definire tanta
anfibia condotta da parte di chi
posa per davvero a giornalista
magno e sommo in mezzo al no
stro ambiente, ove non ci fosse
dato di conoscere a fondo uomini
e cose, storie e disgrazie, vita e
miracoli di tutto l'orbe coloniale
!di un ventennio a questa parte,
i Trattandosi però di un Tommaso
Catalogna, celebre per le requisi
torie che usa fare contro i fra
telli Siamesi ogni qual volta qual
cuno gli paga da bere per farlo
parlare, la cosa può anche passa
re sotto il manto caritatevole di
una generosità d'animo che ci fe
ce sempre distinguere in mezzo
alle genti di nostra Colonia.
A voler contentare tutta la prosa,
stupida, illogica, assolutamente
meccanica del nostro Don Tom
maso, significherebbe fare il
giuoco semplicemente di altri e
non suo, giacché il poveruomo,
l'eroe del "teaccuino del pubbli
co" non ha scritto, oppure non si
è fatto tenere la penna scrivendo,
se non sotto la minaccia di poter
perdere i venticinque scudi men
sili dell'avviso per il giornale ser
votta e le venti pezzarelle setti
manali che, appena la scadenza
lei primo semestre costringeran
no la banca nostra ad un assess
ment superiore di quello che non
avrebbe mai potuto avere ove al
j l'ufficio di impiegati non avesse
preposti dei giornalisti che, con
U. loro coltura, vr.n » q oltre il
liceo e l'università.
: E' pregio dell'opera però, e sa
remmo degli asini se non lo fa
cessimo, mettere in rilievo come j
Don Tommaso, chissà forse se
presente o assente la nostra sim
patica Donna Giovannina, si sia
voluto dichiarare uno dei rivendi
catori più strenui della nostra o
pera, del nostro apostolato gior
nalistico.
Sentano i lettori che cosa ci ha
egli voluto dire con la sua epi
stola :
"Nel giro di pochi anni sono
surte in colonia tre aziende ban
carie italiane : la Italian Co-Ope
rative Banking Associatimi, tra- !
sformatasi ultimamente in South
Philadelphia State Bank ; la Eco
nomical Co-Operative Banking
Association e la Sons of Italy
State Bank.
"Intorno alle due prime la
stampa non ha trovato nulla a
ridire, anzi fu loro piuttosto lar
ga di incoraggiamento, pur sa- \
pendo che a danno dei nostri po
veri connazionali bisognosi si o
perava un certo tal quale strozzi
naggio, in virtù del quale era pos
sibile dare agli azionisti larghi
dividendi, che han ripagato loro
il capitale investito.
"Se vi fu alcuno che mosse un
attacco contro una di esse, e se
non andiamo errati, contro due e
due, fu appunto il pennaiuolo che
ora di entrambe si è fatto pala
dino".
Anche un principiante mastur
batore in giornalismo saprebbe
leggere attraverso tanta eloquen
tissima prosa: La Co-Operative
Banking Association e la Econo
mical Co-Operative Banking Ass.
1 vennero su in colonia con deter
minata intenzione di esercitare lo
strozzinaggio. Intorno alle opera
zioni di queste due banche dice
ed asserisce Don Tommaso sulla
fede dei suoi cinque lustri di vi
ta giornalistica "la stampa
> non trovò nulla a ridire, anzi fu
loro piuttosto di incoraggiamen
to ecc. ecc." Tra questa stam
ipa incoraggiatrice stavano Don
Tommaso, i suoi padroni ed il ri
manente dei giornalisti che van
no oltre il ginnasio, il liceo e l'u
niversità. "Se vi fu alcuno che
mosse un attacco contro una di
esse seguita a sciorinare Don
Tommaso Catalogna e se non
andiamo errati contro tutte e
due (sì, proprio contro tutte e
due, n. d. r.) fu il pennaiuolo
ecc "
Noi non abbiamo mai voluto
1 menare il vanto di essere riusci
ti con una serie completa di arti- 1
coli serrati ad indurre le banche
co-operative, e più specialmen |
te quella delle undici strade, a
cessare dallo strozzinaggio cui a- '
vevano creduto darsi per ragioni J
tutte interne di azienda, giac
ché non fu mai prerogativa no
stra quella dell'auto-decantazio
ne ma se altri ci riconoscono
il merito di certe azioni, non ci
rimane che cortesemente ringra
ziare. Furono allora gli altri a
tollerare, anzi ad incoraggiare lo
strozzinaggio ; noi a combatterlo I
t ino a vincerlo. Lettori, avete j i
bisogno di commenti ulteriori?!
Non crediamo perchè non abbia-1
mo mai creduto che voi foste di <
quella tal quale ignoranza che vi ;
si vuole sovente attribuire dai no
stri giornalisti sommi.
Se poi dalia banca delle undici ,
strade "avemmo" dei denaro a i
campagna Unita, diremo in al- !
no numero, rsoi siamo sempre '
qua per dare conto di noi, del no
stro presente, ui tutto ciò che|
riguarda noi «..quelli che stannoj
t*on noi. Ci piace la luce sempre e
dovunque, e se altri tenta di to
gliercela, noi non sapremo per
donare.
Ed ora una parola, una sola ■
parola all'orecchio al grande gior- 1
nalista Catalano che osa impen
narsi senza ragione e che solo
per mandato ricevuto corre al 1
salvataggio di taluni naufraghi 1
pericolanti :
Tieni o non tieni sempre a
niente tutta la maldicenza che
per mesi, per anni interi sei an
data facendo contro i fratelli Di
Silvestro? Non ti ricordi quanto
ne hai detto e contro l'uno e con
| tro l'altro, in ispecial modo con
! tro Peppuccio che secondo le
tue asserzioni ti aveva rubato il
frutto dei sacrifici di cinque lu
stri di giornalismo? Senti anco
ra, Don Tommaso carissimo, av
vicinati un po' p:ù alla nostra vo
ce e tendi bene le asinine orec
chie : Ti ricordi quando sei anda
to, mentre facevi la campagna
prò istituenda banca Figli d'lta
lia, a premurare il signor Frank
Bisciotti perchè dicesse a quelli
della South Phila. State Bank che
tu, armi e bagagli—fortunamen
te sfiatati—con tutto il tuo gior
nale saresti stato disposto a pas
sare dalla parte loro e schierarti
contro gli altri ove ti avessero as
sicurato un compenso di cen
to dollari al mese ?
Con la proposta facesti ridere e
più di tutti rise—anti ti compian
se, cosa peraltro che non avrà
mai ragione di fare a riguardo
: nostro, quel grand'uomo di
Ciccio Palumbo che tu hai pur
; tante volte insultato non ostante
ti pagasse puntualmente sei pez
zarelle al mese per l'avviso.
Via, Don Tommaso, non ardi
re di parlare ; lascia stare la pen
na, cessa di fare il giornalista o
■ per lo meno non conformare
i quello che solo altri ti costringo
no a pubblicare e rimani, rima
i ni sempre quel maligno compi
i latore che sei de "Il Taccuino del
Pubblico" e del "Diario della
■ Guerra". Quando un'altra volta
5 soldi la copia
UFFICIO : 920 So. lOth Street
ti vorranno lanciare contro quel
brigante di Silvio Liberatore sap
pi rispondere, negandoti: "An
diamo a fare un bicchiere "
Silvio Liberatore è un osso
troppo duro perchè certe mascel
le sganassate potessero essere
capaci di intaccarlo menoma
mente.
Ed ora a te, o Angiolo Curi.
Tu hai scritto tre lunghe, fit
te, compatte e ben nutrite colon
ne di prosa che vanno oltre il gin
nasio, il liceo e sorpassano pure
l'università.
Noi provenienti dalla terza
classe elementare non abbiamo
potuto comprendere delle tue tre
colonne se non l'ultimo periodo:
"Io posso bene permettermi il
lusso di pisciat e allegramente po
lirà i vostri squallidi singulti"
I ecc.
Ebbene, caro Angiolo Curi : se
hai creduto di pisciare tu sulle
tue tre colonne di prosa, non vo
gliamo commettere la scostuma-
I tezza di pisciarci noi. Siamo edu
! cati noi della terza classe elemen
■ tare più di quello che non sap-
I piano essere i provenienti dai li
cei e che lambirono pure l'univer
; sita.
Il galateo innanzi tutto.
Silvio Liberatore
A ciii possa imerissare
Allorquando Silvio Liberatore
| mi richiese per la costituzione di
una compagnia editrice, delia
quale poi sono stato nominato
presidente, aderii di buon grado
alla richiesta per due semplicis
sime ragioni: la prima perchè
conoscevo da parecchi anni il si
gnor Liberatore, e mi sentivo a
lui ligato da vincoli di buona ami
cizia per stimarlo e come uomo e
come giornalista ; la seconda per
che mi convinsi delle ragioni che
mi si addussero intorno alla ne
cessità d'avere in Colonia un fo
glio ben l'atto non solo, ma indi
pendente anche. Queste garenzie
peraltro le trovavo non nelle sem
plici affermazioni, ma in tutto un
passato di movimentato ed one
sto giornalismo coloniale che sta
e starà sempre per dire di Silvio
Liberatore.
Accettando poi la presidenza
della compagnia editrice, seppi di
assumere delle responsabilità e
le assunsi con piena coscienza.
Non si venga quindi a parlare di
presidente "travicello" o di altre
storie, perchè tutto questo non
potrebbe rimanere che una sem
plice, stupida affermazione di
gente abituata a vedere sempre
con gli occhi degli altri.
Angelo Cusano
RIDI, PAGLIACCIO...
11 generale austriaco Koevess
ha detto qualche cosa che non bi
sogna lasciar perdere disatten
tamente. Ha detto: Oh, l'eser
cito era scettico per la possibili
tà di una pace rapida; e "la causa
; del suo scetticismo era il contat
to col nemico". Se i politicanti
vedono le cose a modo loro e se,
lungi dai campi di battaglia, i
"contatti" possono essere d'ogni
specie e mettiamo anche d'ogni
risma, sui campi di battaglia do
ve non rimane posto o comodità
sufficiente per gl'intrighi c'è una
sola ma capitale materia di giudi
l zio. 1 soldati austriaci del gene
i rale Koevess notavano che il ne
i mico il nemico combattente


# kiara\kiara\examples\pipelines\mock_pipeline_1.yaml
pipeline_name: mock_pipeline_1
doc: A pipeline only using the mock module
steps:
  - step_id: step_1
    module_type: mock
    module_config:
      inputs_schema:
        first:
          type: string
          doc: The first string
        second:
          type: string
          doc: The second string
      outputs:
        combined:
          field_schema:
            type: string
            doc: The combined string
          data: "Hello World!"

input_aliases:
  step_1.first: first
  step_1.second: second


# kiara\kiara\examples\scripts\import_df.py
# -*- coding: utf-8 -*-
import pandas as pd

from kiara.context import Kiara

kiara = Kiara.instance()

d = {"col1": [1, 2], "col2": [3, 4]}

example_df = pd.DataFrame(data=d)
table_value = kiara.data_registry.register_data(data=example_df, schema="table")
print(table_value.model_dump())
print(table_value.data.arrow_table)


# kiara\kiara\scripts\documentation\gen_api_doc_pages.py
# -*- coding: utf-8 -*-

"""Generate the code reference pages and navigation."""

from pathlib import Path

import mkdocs_gen_files

nav = mkdocs_gen_files.Nav()

for path in sorted(Path("src/kiara").rglob("*.py")):
    if "resources" in path.as_posix():
        continue
    module_path = path.relative_to("src").with_suffix("")
    doc_path = path.relative_to("src").with_suffix(".md")
    full_doc_path = Path("reference", doc_path)

    parts = list(module_path.parts)

    if parts[-1] == "__init__":
        parts = parts[:-1]
    elif parts[-1] == "__main__":
        continue

    nav[parts] = doc_path  #

    with mkdocs_gen_files.open(full_doc_path, "w") as fd:
        ident = ".".join(parts)
        print("::: " + ident, file=fd)

    mkdocs_gen_files.set_edit_path(full_doc_path, path)

with mkdocs_gen_files.open("reference/SUMMARY.md", "w") as nav_file:  #
    nav_file.writelines(nav.build_literate_nav())  #


# kiara\kiara\scripts\documentation\gen_info_pages.py
# -*- coding: utf-8 -*-

import builtins

from kiara.context import Kiara, KiaraContextInfo
from kiara.doc.gen_info_pages import generate_detail_pages

pkg_name = "kiara"

kiara: Kiara = Kiara.instance()
context_info = KiaraContextInfo.create_from_kiara_instance(
    kiara=kiara, package_filter=pkg_name
)

generate_detail_pages(
    context_info=context_info, sub_path="included_components", add_summary_page=True
)

builtins.plugin_package_context_info = context_info


# kiara\kiara\scripts\documentation\gen_schemas.py
# -*- coding: utf-8 -*-
import inspect
import os
import typing

import mkdocs_gen_files

from kiara.doc.mkdocs_macros_kiara import KIARA_MODEL_CLASSES


def class_namespace(cls: typing.Type):

    module = cls.__module__
    if module is None or module == str.__class__.__module__:
        return cls.__name__
    else:
        return module + "." + cls.__name__


overview_file_path = os.path.join("development", "entities", "index.md")
overview = """# Schemas overviews

This page contains an overview of the available models and their associated schemas used in *kiara*.

"""

for category, classes in KIARA_MODEL_CLASSES.items():

    overview = overview + f"## {category.capitalize()}\n\n"

    file_path = os.path.join("development", "entities", f"{category}.md")

    content = f"# {category.capitalize()}\n\n"

    for cls in classes:

        doc = cls.__doc__

        if doc is None:
            doc = ""

        doc = inspect.cleandoc(doc)

        doc_short = doc.split("\n")[0]
        if doc_short:
            doc_str = f": {doc_short}"
        else:
            doc_str = ""

        overview = (
            overview
            + f"  - [``{cls.__name__}``]({category}{os.path.sep}#{cls.__name__.lower()}){doc_str}\n"
        )

        namescace = class_namespace(cls)
        download_link = f'<a href="{cls.__name__}.json">{cls.__name__}.json</a>'

        # content = content + f"## {cls.__name__}\n\n" + "{{ get_schema_for_model('" + class_namespace(cls) + ") }}\n\n"
        content = content + f"## {cls.__name__}\n\n"
        content = content + doc + "\n\n"
        content = content + "#### References\n\n"
        content = (
            content + f"  - model class reference: [{cls.__name__}][{namescace}]\n"
        )
        content = content + f"  - JSON schema file: {download_link}\n\n"
        content = content + "#### JSON schema\n\n"
        content = (
            content
            + "``` json\n{{ get_schema_for_model('"
            + namescace
            + "') }}\n```\n\n"
        )

    with mkdocs_gen_files.open(file_path, "w") as f:
        f.write(content)

with mkdocs_gen_files.open(overview_file_path, "w") as f:
    f.write(overview)


# kiara\kiara\src\kiara\api.py
# -*- coding: utf-8 -*-
__all__ = [
    "Kiara",
    "KiaraAPI",
    "KiaraConfig",
    "KiaraModule",
    "KiaraModuleConfig",
    "JobDesc",
    "Pipeline",
    "PipelineStructure",
    "RunSpec",
    "Value",
    "ValueMap",
    "ValueMapSchema",
    "ValueSchema",
    "KiArchive",
]

from .context import Kiara
from .context.config import KiaraConfig
from .interfaces.python_api.kiara_api import KiaraAPI
from .interfaces.python_api.models.archive import KiArchive
from .interfaces.python_api.models.job import JobDesc, RunSpec
from .models.module.pipeline.pipeline import Pipeline, PipelineStructure
from .models.values.value import Value, ValueMap
from .models.values.value_schema import ValueSchema
from .modules import KiaraModule, KiaraModuleConfig, ValueMapSchema


# kiara\kiara\src\kiara\defaults.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import os
import typing
import uuid
from enum import Enum
from pathlib import Path

from appdirs import AppDirs

kiara_app_dirs = AppDirs("kiara", "DHARPA")

# if getattr(sys, 'oxidized', False):
#     KIARA_MODULE_BASE_FOLDER = "xxx"
#     raise NotImplementedError()
# elif not hasattr(sys, "_MEIPASS"):
#     KIARA_MODULE_BASE_FOLDER = os.path.dirname(__file__)
#     """Marker to indicate the base folder for the `kiara` module."""
# else:
#     KIARA_MODULE_BASE_FOLDER = os.path.join(sys._MEIPASS, "kiara")  # type: ignore
#     """Marker to indicate the base folder for the `kiara` module."""

# KIARA_RESOURCES_FOLDER = os.path.join(KIARA_MODULE_BASE_FOLDER, "resources")
# """Default resources folder for this package."""

KIARA_CONFIG_FILE_NAME = "kiara.config"
KIARA_DEV_CONFIG_FILE_NAME = "dev.config"
KIARA_MAIN_CONFIG_FILE = os.path.join(
    kiara_app_dirs.user_config_dir, KIARA_CONFIG_FILE_NAME
)
KIARA_DEV_CONFIG_FILE = os.path.join(
    kiara_app_dirs.user_config_dir, KIARA_DEV_CONFIG_FILE_NAME
)
KIARA_MAIN_CONTEXTS_PATH = os.path.join(kiara_app_dirs.user_config_dir, "contexts")
KIARA_MAIN_CONTEXT_DATA_PATH = os.path.join(
    kiara_app_dirs.user_data_dir, "context_data"
)
KIARA_MAIN_CONTEXT_LOCKS_PATH = os.path.join(
    kiara_app_dirs.user_data_dir, "context_locks"
)


KIARA_DEFAULT_STAGES_EXTRACTION_TYPE = "early"

INIT_EXAMPLE_NAME = "init"

# USER_PIPELINES_FOLDER = os.path.join(kiara_app_dirs.user_config_dir, "pipelines")


MODULE_TYPE_KEY = "module_type"
"""The key to specify the type of a module."""

STEP_ID_KEY = "step_id"
"""The key to specify the step id."""

# INVALID_VALUE_NAMES = [
#     "kiara",
#     "registry",
#     "items_are_valid",
#     "set_values",
#     "set_value",
#     "ALL",
#     "all",
#     "metadata",
#     "value",
#     "value_obj",
#     "items",
#     "keys",
#     "values",
#     "data",
#     "callbacks",
#     "trigger_callbacks",
#     "shared_metadata",
# ]
INVALID_VALUE_NAMES = [
    "kiara",
    "callbacks",
]
INVALID_ALIAS_NAMES = [
    "kiara",
    "__default__",
    "alias",
    "value",
    "value_id",
    "kiarchive",
]
"""List of reserved names, inputs/outputs can't use those."""
DEFAULT_STORE_MARKER = "default_store"

DEFAULT_DATA_STORE_MARKER = "default_data_store"
"""Name for the default context data store."""

DEFAULT_METADATA_STORE_MARKER = "default_metadata_store"
"""Name for the default context metadata store."""

DEFAULT_JOB_STORE_MARKER = "default_job_store"
"""Name for the default context job store."""

DEFAULT_ALIAS_STORE_MARKER = "default_alias_store"
"""Name for the default context alias store."""

DEFAULT_WORKFLOW_STORE_MARKER = "default_workflow_store"
"""Name for the default context workflow store."""

METADATA_PROPERTY_MARKER = "metadata"
"""Name for the default context destiny store."""

PIPELINE_PARENT_MARKER = "__pipeline__"
"""Marker string in the pipeline structure that indicates a parent pipeline element."""

DEFAULT_EXCLUDE_DIRS = [".git", ".tox", ".cache"]
"""List of directory names to exclude by default when walking a folder recursively."""

DEFAULT_EXCLUDE_FILES = [".DS_Store"]
"""List of file names to exclude by default when reading folders."""

VALID_PIPELINE_FILE_EXTENSIONS = ["yaml", "yml", "json"]
"""File extensions a kiara pipeline/workflow file can have."""

MODULE_TYPE_NAME_KEY = "module_type_name"
"""The string for the module type name in a module configuration dict."""

DEFAULT_PIPELINE_PARENT_ID = "__kiara__"
"""Default parent id for pipeline objects that are not associated with a workflow."""

DEFAULT_NO_DESC_VALUE = "-- n/a --"
NOT_AVAILBLE_MARKER = "-- n/a --"

KIARA_MODULE_METADATA_ATTRIBUTE = "KIARA_METADATA"

KIARA_DEFAULT_ROOT_NODE_ID = "__self__"

KIARA_SQLITE_STORE_EXTENSION = "kiara"

VALUE_ATTR_DELIMITER = "::"
VALID_VALUE_QUERY_CATEGORIES = ["data", "properties"]

CHUNK_CACHE_BASE_DIR = Path(kiara_app_dirs.user_cache_dir) / "data" / "chunks"
CHUNK_CACHE_DIR_DEPTH = 2
CHUNK_CACHE_DIR_WIDTH = 1


class SpecialValue(Enum):

    NOT_SET = "__not_set__"
    NO_VALUE = "__no_value__"


DEFAULT_PRETTY_PRINT_CONFIG = {
    "max_no_rows": 32,
    "max_row_height": 1,
    "max_cell_length": 80,
}

NO_HASH_MARKER = "--no-hash--"
"""Marker string to indicate no hash was calculated."""

NO_VALUE_ID_MARKER = "--no-value-id--"
"""Marker string to indicate no value id exists."""
DEFAULT_TO_JSON_CONFIG: typing.Mapping[str, typing.Any] = {
    "indent": 2,
}

COLOR_LIST = [
    "green",
    "blue",
    "bright_magenta",
    "dark_red",
    "gold3",
    "cyan",
    "orange1",
    "light_yellow3",
    "light_slate_grey",
    "deep_pink4",
]

VOID_KIARA_ID = uuid.UUID("00000000-0000-0000-0000-000000000000")
NOT_SET_VALUE_ID = uuid.UUID("00000000-0000-0000-0000-000000000001")
NONE_VALUE_ID = uuid.UUID("00000000-0000-0000-0000-000000000002")
NONE_STORE_ID = uuid.UUID("00000000-0000-0000-0000-000000000003")
ORPHAN_PEDIGREE_OUTPUT_NAME = "__orphan__"

NO_MODULE_TYPE = "EXTERNAL_DATA"

INVALID_HASH_MARKER = ""

INVALID_SIZE_MARKER = -1
NO_SERIALIZATION_MARKER = "-- serialization not supported --"
KIARA_ROOT_TYPE_NAME = "__kiara__"

SERIALIZED_data_type_name = "serialized_data"
LOAD_CONFIG_data_type_name = "load_config"

PYDANTIC_USE_CONSTRUCT: bool = False
STRICT_CHECKS: bool = False

ANY_TYPE_NAME = "any"

DEFAULT_ENV_HASH_KEY = "default"

LOAD_CONFIG_PLACEHOLDER = "__placeholder__"

DATA_TYPE_CATEGORY_ID = "metadata.type"
DATA_TYPES_CATEGORY_ID = "data_types"
DATA_TYPE_CLASS_CATEGORY_ID = "data_type_class"

DATA_WRAP_CATEGORY_ID = "instance.datawrap"
UNOLOADABLE_DATA_CATEGORY_ID = "instance.unloadable_data"
VALUE_CATEGORY_ID = "instance.value"
VALUES_CATEGORY_ID = "instance.values"
VALUE_METADATA_CATEGORY_ID = "instance.value_metadata"

MODULE_CONFIG_SCHEMA_CATEGORY_ID = "module_config_schema"
MODULE_CONFIG_CATEGORY_ID = "module_config"
MODULE_CONFIG_METADATA_CATEGORY_ID = "metadata.module_config"

MODULE_TYPE_CATEGORY_ID = "metadata.module"
MODULE_TYPES_CATEGORY_ID = "modules"

BATCH_CONFIG_TYPE_CATEGORY_ID = "instance.batch_config"

PIPELINE_TYPE_CATEGORY_ID = "metadata.pipeline"
PIPELINE_TYPES_CATEGORY_ID = "pipelines"
PIPELINE_STEP_TYPE_CATEGORY_ID = "instance.pipeline_step"
PIPELINE_CONFIG_TYPE_CATEGORY_ID = "instance.pipeline_config"
PIPELINE_STRUCTURE_TYPE_CATEGORY_ID = "instance.pipeline_structure"

PIPELINE_STEP_DETAILS_CATEGORY_ID = "instance.pipeline_step_details"

OPERATION_TYPE_CATEGORY_ID = "metadata.operation_type"
OPERATION_TYPES_CATEGORY_ID = "operation_types"
OPERATIONS_CATEGORY_ID = "operations"
OPERATION_CATEOGORY_ID = "instance.operation"
OPERATION_CONFIG_CATEOGORY_ID = "instance.operation_config"
OPERATION_DETAILS_CATEOGORY_ID = "instance.operation_details"
OPERATION_INPUTS_SCHEMA_CATEOGORY_ID = "instance.operation_input_schema"
OPERATION_OUTPUTS_SCHEMA_CATEOGORY_ID = "instance.operation_output_schema"

ENVIRONMENT_TYPE_CATEGORY_ID = "instance.environment"
DOCUMENTATION_CATEGORY_ID = "documentation"

VALUE_SCHEMA_CATEGORY_ID = "value_schema"

JOB_CATEGORY_ID = "instance.job"
JOB_LOG_CATEGORY_ID = "job_log"

DESTINY_CATEGORY_ID = "instance.destiny"

CONTEXT_INFO_CATEGORY_ID = "info.context"

CONTEXT_METADATA_CATEOGORY_ID = "metadata.context"
AUTHORS_METADATA_CATEGORY_ID = "metadata.authors"

JOB_CONFIG_TYPE_CATEGORY_ID = "instance.job"
JOB_RECORD_TYPE_CATEGORY_ID = "instance.job_record"
VALUE_PEDIGREE_TYPE_CATEGORY_ID = "instance.value_pedigree"

FILE_MODEL_CATEOGORY_ID = "instance.model.file"
FILE_BUNDLE_MODEL_CATEOGORY_ID = "instance.model.file_bundle"

ARRAY_MODEL_CATEOGORY_ID = "instance.model.array"
TABLE_MODEL_CATEOGORY_ID = "instance.model.table"
DEFAULT_CONTEXT_NAME = "default"

KIARA_MODEL_ID_KEY = "kiara_model_id"
KIARA_MODEL_DATA_KEY = "data"
KIARA_MODEL_SCHEMA_KEY = "schema"

ENVIRONMENT_MARKER_KEY = "environment"
"""Constant string to indicate this is a metadata entry of type 'environment'."""

SYMLINK_ISSUE_MSG = """Your operating system does not support symlinks, which is a requirement for kiara to work.

You can enable developer mode to fix this issue:

- open 'Settings'
- click 'Updates & Security'
- click 'For developers'
- make sure 'Developer Mode' is turned on
- log out of your Windows session, and log back in again

For more information, please visit:
- https://dharpa.org/kiara.documentation/latest/installation/#enable-developer-mode
- https://learn.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development
"""


OFFICIAL_KIARA_PLUGINS = [
    "core_types",
    "tabular",
    "onboarding",
    "network_analysis",
    "language_processing",
]


class CHUNK_COMPRESSION_TYPE(Enum):
    NONE = 0
    ZSTD = 1
    LZMA = 2
    LZ4 = 3


DEFAULT_CHUNK_COMPRESSION = CHUNK_COMPRESSION_TYPE.ZSTD

ARCHIVE_NAME_MARKER = "archive_name"
DATA_ARCHIVE_DEFAULT_VALUE_MARKER = "default_value"
TABLE_NAME_ARCHIVE_METADATA = "archive_metadata"
TABLE_NAME_DATA_METADATA = "data_value_metadata"
TABLE_NAME_DATA_SERIALIZATION_METADATA = "data_serialization_metadata"
TABLE_NAME_DATA_CHUNKS = "data_chunks"
TABLE_NAME_DATA_PEDIGREE = "data_value_pedigree"
TABLE_NAME_DATA_DESTINIES = "data_value_destiny"
REQUIRED_TABLES_DATA_ARCHIVE = {
    TABLE_NAME_ARCHIVE_METADATA,
    TABLE_NAME_DATA_METADATA,
    TABLE_NAME_DATA_SERIALIZATION_METADATA,
    TABLE_NAME_DATA_CHUNKS,
    TABLE_NAME_DATA_PEDIGREE,
    TABLE_NAME_DATA_DESTINIES,
}

TABLE_NAME_ALIASES = "aliases"
REQUIRED_TABLES_ALIAS_ARCHIVE = {
    TABLE_NAME_ARCHIVE_METADATA,
    TABLE_NAME_ALIASES,
}

TABLE_NAME_JOB_RECORDS = "job_records"
REQUIRED_TABLES_JOB_ARCHIVE = {
    TABLE_NAME_ARCHIVE_METADATA,
    TABLE_NAME_JOB_RECORDS,
}

TABLE_NAME_METADATA = "metadata"
TABLE_NAME_METADATA_SCHEMAS = "metadata_schemas"
TABLE_NAME_METADATA_REFERENCES = "metadata_references"
REQUIRED_TABLES_METADATA = {
    TABLE_NAME_ARCHIVE_METADATA,
    TABLE_NAME_METADATA,
    TABLE_NAME_METADATA_SCHEMAS,
    TABLE_NAME_METADATA_REFERENCES,
}


ALL_REQUIRED_TABLES = set(REQUIRED_TABLES_DATA_ARCHIVE)
ALL_REQUIRED_TABLES.update(REQUIRED_TABLES_ALIAS_ARCHIVE)
ALL_REQUIRED_TABLES.update(REQUIRED_TABLES_JOB_ARCHIVE)
ALL_REQUIRED_TABLES.update(REQUIRED_TABLES_METADATA)


# kiara\kiara\src\kiara\exceptions.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)
import uuid
from typing import TYPE_CHECKING, Any, Iterable, List, Mapping, Type, Union

import orjson

from kiara.defaults import NOT_AVAILBLE_MARKER
from kiara.utils.json import orjson_dumps

if TYPE_CHECKING:
    from rich.console import RenderableType
    from rich.table import Table

    from kiara.data_types import DataType
    from kiara.models.module.jobs import ActiveJob
    from kiara.models.module.manifest import Manifest
    from kiara.models.module.pipeline import PipelineConfig
    from kiara.models.values.value import Value
    from kiara.modules import KiaraModule


class KiaraException(Exception):
    @classmethod
    def get_root_details(
        cls, e: Exception, default: Union[None, str] = None
    ) -> Union[str, None]:

        if isinstance(e, KiaraException):
            return e.root_details()
        else:
            if default is None:
                return str(e)
            else:
                return default

    def __init__(self, msg: str, parent: Union[Exception, None] = None, **kwargs):

        self._msg = msg
        self._parent: Union[Exception, None] = parent
        self._properties = kwargs
        super().__init__(msg)

    @property
    def msg(self):
        return self._msg

    @property
    def details(self) -> Union[str, None]:

        result: Union[None, str] = self._properties.get("details", None)
        return result

    @property
    def parent(self) -> Union[Exception, None]:
        return self._parent

    @property
    def root_cause(self) -> Exception:

        current: Exception = self
        while hasattr(current, "parent") and current.parent is not None:  # type: ignore
            current = current.parent  # type: ignore

        return current

    def root_details(self) -> Union[str, None]:

        current: Exception = self
        if hasattr(self, "details"):
            current_details = self.details  # type: ignore
        else:
            current_details = None
        while hasattr(current, "parent") and current.parent is not None:  # type: ignore
            current = current.parent  # type: ignore

            if hasattr(current, "details") and current.details:  # type: ignore
                current_details = current.details  # type: ignore
            else:
                current_details = str(current)

        return current_details

    def create_renderable(self, **config) -> "RenderableType":

        from rich.console import Group

        rows: List[RenderableType] = [f"[red]Error[/red]: {self._msg}"]
        root_details = self.root_details()
        if root_details:
            from rich.markdown import Markdown

            rows.append("")
            rows.append(Markdown(root_details))

        return Group(*rows)

    def __str__(self) -> str:
        msg = super().__str__()
        root_details = self.root_details()
        if root_details:
            msg += f"\n\n{root_details}"
        return msg


class InvalidCommandLineInvocation(KiaraException):
    def __init__(
        self,
        msg: str,
        parent: Union[Exception, None] = None,
        error_code: int = 0,
        **kwargs,
    ):

        self.error_code: int = error_code

        super().__init__(msg, parent=parent, **kwargs)


class KiaraContextException(KiaraException):
    def __init__(self, msg: str, context_id: uuid.UUID):

        self._context_id: uuid.UUID = context_id
        super().__init__(msg)


class KiaraModuleConfigException(KiaraException):
    def __init__(
        self,
        msg: str,
        module_cls: Type["KiaraModule"],
        config: Mapping[str, Any],
        parent: Union[Exception, None] = None,
    ):

        self._module_cls = module_cls
        self._config = config

        if not msg.endswith("."):
            _msg = msg + "."
        else:
            _msg = msg

        super().__init__(_msg, parent=parent)


class InvalidManifestException(KiaraException):
    def __init__(
        self,
        msg: str,
        module_type: str,
        module_config: Union[None, Mapping[str, Any]] = None,
        available_module_types: Union[None, Iterable[str]] = None,
        parent: Union[Exception, None] = None,
    ):

        self._module_type = module_type
        self._module_config = module_config
        self._available_module_types = available_module_types
        super().__init__(msg, parent=parent)

    @property
    def details(self) -> Union[str, None]:

        if not self._available_module_types:
            return None

        else:
            msg = "Available module types:\n\n"
            for module_type in self._available_module_types:
                msg += f"- {module_type}\n"
            return msg


class ValueTypeConfigException(KiaraException):
    def __init__(
        self,
        msg: str,
        type_cls: Type["DataType"],
        config: Mapping[str, Any],
        parent: Union[Exception, None] = None,
    ):

        self._type_cls = type_cls
        self._config = config

        if not msg.endswith("."):
            _msg = msg + "."
        else:
            _msg = msg

        super().__init__(_msg, parent=parent)


class DataTypeUnknownException(KiaraException):
    def __init__(
        self,
        data_type: str,
        msg: Union[str, None] = None,
        value: Union[None, "Value"] = None,
    ):

        self._data_type = data_type
        if msg is None:
            msg = f"Data type '{data_type}' not registered in current context."
        self._msg = msg
        self._value = value

        super().__init__(msg)

    @property
    def data_type(self) -> str:
        return self._data_type

    @property
    def value(self) -> Union[None, "Value"]:
        return self._value

    def create_renderable(self, **config: Any) -> "Table":

        from rich import box
        from rich.table import Table

        table = Table(box=box.SIMPLE, show_header=False)

        table.add_column("key", style="i")
        table.add_column("value")

        table.add_row("error", self._msg)
        table.add_row("data type", self._data_type)
        table.add_row(
            "solution", "Install the Python package that provides this data type."
        )

        if self._value is not None:
            table.add_row("value", self._value.create_renderable())

        return table


class KiaraValueException(KiaraException):
    def __init__(
        self,
        data_type: Type["DataType"],
        value_data: Any,
        parent: Exception,
    ):
        self._data_type: Type["DataType"] = data_type
        self._value_data: Any = value_data

        exc_msg = str(parent)
        if not exc_msg:
            exc_msg = "no details available"

        super().__init__(f"Invalid value of type '{data_type._data_type_name}': {exc_msg}", parent=parent)  # type: ignore


class NoSuchExecutionTargetException(KiaraException):
    def __init__(
        self,
        selected_target: str,
        available_targets: Iterable[str],
        msg: Union[str, None] = None,
    ):

        if msg is None:
            msg = f"Specified run target '{selected_target}' is an operation, additional module configuration is not allowed."

        self.avaliable_targets: Iterable[str] = available_targets
        super().__init__(msg)


class KiaraProcessingException(KiaraException):
    def __init__(
        self,
        msg: Union[str, Exception],
        module: Union["KiaraModule", None] = None,
        inputs: Union[Mapping[str, "Value"], None] = None,
    ):
        self._module: Union["KiaraModule", None] = module
        self._inputs: Union[Mapping[str, Value], None] = inputs
        _properties = None

        if isinstance(msg, KiaraException):
            _parent: Union[Exception, None] = msg.parent
            _msg = msg.msg
            _properties = msg._properties
        elif isinstance(msg, Exception):
            _parent = msg
            _msg = str(msg)
        else:
            _parent = None
            _msg = msg
        if _properties:
            super().__init__(msg=_msg, parent=_parent, **_properties)
        else:
            super().__init__(_msg)

    @property
    def module(self) -> "KiaraModule":
        return self._module  # type: ignore

    @property
    def inputs(self) -> Mapping[str, "Value"]:
        return self._inputs  # type: ignore

    @property
    def parent_exception(self) -> Union[Exception, None]:
        return self._parent


class InvalidValuesException(KiaraException):
    def __init__(
        self,
        msg: Union[None, str, Exception] = None,
        invalid_values: Union[Mapping[str, str], None] = None,
    ):

        if invalid_values is None:
            invalid_values = {}

        self.invalid_inputs: Mapping[str, str] = invalid_values

        if msg is None:
            if not self.invalid_inputs:
                _msg = "Invalid values. No details available."
            else:
                msg_parts = []
                for k, v in invalid_values.items():
                    msg_parts.append(f"{k}: {v}")
                _msg = f"Invalid values: {', '.join(msg_parts)}"
        elif isinstance(msg, Exception):
            self._parent: Union[Exception, None] = msg
            _msg = str(msg)
        else:
            self._parent = None
            _msg = msg

        super().__init__(_msg)

    @property
    def details(self) -> str:
        result = ""
        for k, v in self.invalid_inputs.items():
            result += f" - {k}: {v}"

        return result

    def create_renderable(self, **config: Any) -> "Table":

        from rich import box
        from rich.console import RenderableType
        from rich.table import Table

        table = Table(box=box.SIMPLE, show_header=True)

        table.add_column("field name", style="i")
        table.add_column("[red]error[/red]")

        for field_name, error in self.invalid_inputs.items():

            row: List[RenderableType] = [field_name]
            row.append(error)
            table.add_row(*row)

        return table


class JobConfigException(KiaraException):
    def __init__(
        self,
        msg: Union[str, Exception],
        manifest: "Manifest",
        inputs: Mapping[str, Any],
    ):

        self._manifest: Manifest = manifest
        self._inputs: Mapping[str, Any] = inputs

        if isinstance(msg, Exception):
            self._parent: Union[Exception, None] = msg
            _msg = str(msg)
        else:
            self._parent = None
            _msg = msg

        super().__init__(_msg)

    @property
    def manifest(self) -> "Manifest":
        return self._manifest

    @property
    def inputs(self) -> Mapping[str, Any]:
        return self._inputs


class FailedJobException(KiaraException):
    def __init__(
        self,
        job: "ActiveJob",
        msg: Union[str, None] = None,
        parent: Union[Exception, None] = None,
    ):
        self.job: ActiveJob = job

        if isinstance(parent, KiaraException):
            super().__init__(msg=parent.msg, parent=parent.parent, **parent._properties)
        else:
            if msg is None:
                msg = "Job failed."
            super().__init__(msg=msg, parent=parent)

    # @property
    # def details(self) -> Union[str, None]:
    #     return None

    def create_renderable(self, **config: Any):

        from rich import box
        from rich.console import Group
        from rich.panel import Panel
        from rich.table import Table

        table = Table(show_header=False, box=box.SIMPLE)
        table.add_column("key", style="i")
        table.add_column("value")

        table.add_row("job_id", str(self.job.job_id))
        table.add_row("module_type", self.job.job_config.module_type)

        group = Group(
            Panel(f"[red]Error[/red]: [i]{self.msg}[i]", box=box.SIMPLE), table
        )
        return group


class NoSuchValueException(KiaraException):

    pass


class NoSuchValueIdException(NoSuchValueException):
    def __init__(self, value_id: uuid.UUID, msg: Union[str, None] = None):
        self.value_id: uuid.UUID
        if not msg:
            msg = f"No value with id: {value_id}."
        super().__init__(msg)


class NoSuchValueAliasException(NoSuchValueException):
    def __init__(self, alias: str, msg: Union[str, None] = None):
        self.value_id: uuid.UUID
        if not msg:
            msg = f"No value with alias: {alias}."
        super().__init__(msg)


class NoSuchWorkflowException(KiaraException):
    def __init__(self, workflow: Union[uuid.UUID, str], msg: Union[str, None] = None):
        self._workflow: Union[str, uuid.UUID] = workflow
        if not msg:
            msg = f"No such workflow: {workflow}"
        super().__init__(msg)

    @property
    def alias_requested(self) -> bool:

        if isinstance(self._workflow, str):
            try:
                uuid.UUID(self._workflow)
                return False
            except Exception:
                return True
        else:
            return False


class NoSuchOperationException(KiaraException):
    def __init__(
        self,
        operation_id: str,
        available_operations: Iterable[str],
        msg: Union[None, str] = None,
    ):

        self._operation_id: str = operation_id
        self._available_operations: Iterable[str] = available_operations

        if not msg:
            msg = f"No operation with id '{operation_id}' available."

        super().__init__(msg)

    @property
    def available_operations(self) -> Iterable[str]:
        return self._available_operations

    @property
    def operation_id(self) -> str:
        return self._operation_id


class InvalidOperationException(KiaraException):
    def __init__(self, operation_details: Mapping[str, Any]):

        self._all_details: Mapping[str, Any] = operation_details
        msg = operation_details.get("operation_id", None)
        if msg is None:
            msg = operation_details.get("pipeline_name", None)
        parent = operation_details.get("parent", None)
        super().__init__(f"Invalid operation: {msg}", parent=parent)

    @property
    def module_id(self) -> str:
        return self._all_details.get("module_id", NOT_AVAILBLE_MARKER)

    @property
    def module_config(self) -> Union[Mapping[str, Any], str]:
        return self._all_details.get("module_config", {})

    @property
    def details(self) -> Union[str, None]:
        if self.parent:
            if hasattr(self.parent, "details"):
                return self.parent.details  # type: ignore
            else:
                return None
        else:
            return self._all_details.get("details", None)


class InvalidPipelineStepConfig(KiaraException):
    def __init__(self, msg: str, step_config: Mapping[str, Any]):

        self._step_config: Mapping[str, Any] = step_config
        super().__init__(msg)

    @property
    def details(self) -> str:
        config = orjson_dumps(self._step_config, option=orjson.OPT_INDENT_2)

        details = f"Invalid step config:\n\n```\n{config}\n```"
        return details


class InvalidPipelineConfig(KiaraException):
    def __init__(self, msg: str, config: "PipelineConfig", details: str):

        self._config = config
        self._details = details
        super().__init__(msg)

    @property
    def pipeline_config(self) -> "PipelineConfig":
        return self._config

    @property
    def details(self) -> Union[str, None]:

        return self._details


# kiara\kiara\src\kiara\__init__.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

# isort: skip_file

__all__ = [
    "get_version",
]
import logging
import os
import sys

import structlog
import typing

from .utils import is_develop, is_debug
from .utils.class_loading import (
    KiaraEntryPointItem,
    find_kiara_model_classes_under,
    find_kiara_renderers_under,
)

try:
    builtins = __import__("__builtin__")
except ImportError:
    builtins = __import__("builtins")


# =================================================================
# global init stuff

# default logging, unless set somewhere else
structlog.configure(
    wrapper_class=structlog.make_filtering_bound_logger(logging.WARNING),
)
# check if run in Jupyter
if "google.colab" in sys.modules or "jupyter_client" in sys.modules:
    from kiara.interfaces import set_console_width

    set_console_width()

try:
    from rich import inspect
    from rich import print as rich_print

    setattr(builtins, "insp", inspect)

    def dbg(
        *objects: typing.Any,
        sep: str = " ",
        end: str = "\n",
        file: typing.Union[typing.IO[str], None] = None,
        flush: bool = False,
    ):

        for obj in objects:
            if hasattr(obj, "create_renderable"):
                obj = obj.create_renderable()
            try:
                rich_print(obj, sep=sep, end=end, file=file, flush=flush)
            except Exception:
                rich_print(
                    f"[green]{obj}[/green]", sep=sep, end=end, file=file, flush=flush
                )

    setattr(builtins, "dbg", dbg)

    def DBG(
        *objects: typing.Any,
        sep: str = " ",
        end: str = "\n",
        file: typing.Union[typing.IO[str], None] = None,
        flush: bool = False,
    ):

        objs = (
            ["[green]----------------------------------------------[/green]"]  # noqa
            + list(objects)
            + ["[green]----------------------------------------------[/green]"]
        )
        dbg(*objs, sep=sep, end=end, file=file, flush=flush)

    setattr(builtins, "DBG", DBG)

except ImportError:  # Graceful fallback if IceCream isn't installed.
    pass

if is_develop() or is_debug():
    try:
        from icecream import install

        install()
    except ImportError:  # Graceful fallback if IceCream isn't installed.
        pass

"""Top-level package for kiara."""


__author__ = """Markus Binsteiner"""
"""The author of this package."""
__email__ = "markus@frkl.io"
"""Email address of the author."""


KIARA_METADATA = {
    "authors": [{"name": __author__, "email": __email__}],
    "description": "Kiara Python package",
    "references": {
        "source_repo": {
            "desc": "The kiara project git repository.",
            "url": "https://github.com/DHARPA-Project/kiara",
        },
        "documentation": {
            "desc": "The url for kiara documentation.",
            "url": "https://dharpa.org/kiara_documentation/",
        },
    },
    "tags": [],
    "labels": {"package": "kiara"},
}

find_model_classes: KiaraEntryPointItem = (
    find_kiara_model_classes_under,
    "kiara.models",
)
find_model_classes_api: KiaraEntryPointItem = (
    find_kiara_model_classes_under,
    "kiara.interfaces.python_api.models",
)
find_renderer_classes: KiaraEntryPointItem = (
    find_kiara_renderers_under,
    "kiara.renderers.included_renderers",
)


def get_version() -> str:
    """Return the current version of *Kiara*."""
    from pkg_resources import DistributionNotFound, get_distribution

    try:
        # Change here if project is renamed and does not equal the package name
        dist_name = __name__
        __version__ = get_distribution(dist_name).version
    except DistributionNotFound:

        try:
            version_file = os.path.join(os.path.dirname(__file__), "version.txt")

            if os.path.exists(version_file):
                with open(version_file, encoding="utf-8") as vf:
                    __version__ = vf.read()
            else:
                __version__ = "unknown"

        except Exception:
            __version__ = "unknown"

        if __version__ is None:
            __version__ = "unknown"

    return __version__


# kiara\kiara\src\kiara\context\config.py
# -*- coding: utf-8 -*-
import contextlib

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)
import os
import uuid
from pathlib import Path
from typing import (
    TYPE_CHECKING,
    Any,
    Dict,
    Iterable,
    List,
    Literal,
    Mapping,
    Type,
    Union,
)

import structlog
from pydantic import BaseModel, ConfigDict, field_validator, model_validator
from pydantic.fields import Field, PrivateAttr
from pydantic_settings import BaseSettings, SettingsConfigDict
from ruamel import yaml as r_yaml

from kiara.context.runtime_config import KiaraRuntimeConfig
from kiara.defaults import (
    DEFAULT_ALIAS_STORE_MARKER,
    DEFAULT_CONTEXT_NAME,
    DEFAULT_DATA_STORE_MARKER,
    DEFAULT_JOB_STORE_MARKER,
    DEFAULT_METADATA_STORE_MARKER,
    DEFAULT_WORKFLOW_STORE_MARKER,
    KIARA_CONFIG_FILE_NAME,
    KIARA_MAIN_CONFIG_FILE,
    KIARA_MAIN_CONTEXTS_PATH,
    kiara_app_dirs,
)
from kiara.exceptions import KiaraException
from kiara.registries.ids import ID_REGISTRY
from kiara.utils import log_message
from kiara.utils.files import get_data_from_file

if TYPE_CHECKING:
    from kiara.context import Kiara
    from kiara.models.context import ContextInfo
    from kiara.registries import BaseArchive, KiaraArchive

logger = structlog.getLogger()

yaml = r_yaml.YAML(typ="safe", pure=True)
yaml.default_flow_style = False


def config_file_settings_source(settings: BaseSettings) -> Dict[str, Any]:
    if os.path.isfile(KIARA_MAIN_CONFIG_FILE):
        config: Dict[str, Any] = get_data_from_file(
            KIARA_MAIN_CONFIG_FILE, content_type="yaml"
        )
        if not isinstance(config, Mapping):
            raise ValueError(
                f"Invalid config file format, can't parse file: {KIARA_MAIN_CONFIG_FILE}"
            )
    else:
        config = {}
    return config


class KiaraArchiveConfig(BaseModel):
    """Configuration data that can be used to load an existing kiara archive."""

    # archive_alias: str = Field(description="The unique archive id.")
    archive_type: str = Field(description="The archive type.")
    config: Mapping[str, Any] = Field(
        description="Archive type specific config.", default_factory=dict
    )


class KiaraArchiveReference(BaseModel):
    @classmethod
    def load_existing_archive(
        cls,
        archive_uri: str,
        store_type: Union[str, None, Iterable[str]] = None,
        allow_write_access: bool = False,
        archive_name: Union[str, None] = None,
        **kwargs: Any,
    ) -> "KiaraArchiveReference":

        from kiara.utils.class_loading import find_all_archive_types

        archive_types = find_all_archive_types()

        archive_configs: List[KiaraArchiveConfig] = []
        archives: List[KiaraArchive] = []

        if store_type:
            if isinstance(store_type, str):
                archive_cls: Union[Type[KiaraArchive], None] = archive_types.get(
                    store_type, None
                )
                if archive_cls is None:
                    raise Exception(
                        f"Can't create context: no archive type '{store_type}' available. Available types: {', '.join(archive_types.keys())}"
                    )
                data = archive_cls.load_archive_config(
                    archive_uri=archive_uri,
                    allow_write_access=allow_write_access,
                    **kwargs,
                )
                archive_config = archive_cls._config_cls(**data)
                archive: KiaraArchive = archive_cls(
                    archive_config=archive_config, archive_name=archive_name
                )
                wrapped_archive_config = KiaraArchiveConfig(
                    archive_type=store_type, config=data
                )
                archive_configs.append(wrapped_archive_config)
                archives.append(archive)
            else:
                for st in store_type:
                    archive_cls = archive_types.get(st, None)
                    if archive_cls is None:
                        raise Exception(
                            f"Can't create context: no archive type '{store_type}' available. Available types: {', '.join(archive_types.keys())}"
                        )
                    data = archive_cls.load_archive_config(
                        archive_uri=archive_uri,
                        allow_write_access=allow_write_access,
                        **kwargs,
                    )
                    archive_config = archive_cls._config_cls(**data)
                    archive = archive_cls(
                        archive_config=archive_config, archive_name=archive_name
                    )
                    wrapped_archive_config = KiaraArchiveConfig(
                        archive_type=st, config=data
                    )
                    archive_configs.append(wrapped_archive_config)
                    archives.append(archive)
        else:
            for archive_type, archive_cls in archive_types.items():
                data = archive_cls.load_archive_config(
                    archive_uri=archive_uri,
                    allow_write_access=allow_write_access,
                    **kwargs,
                )

                if data is None:
                    continue

                archive_config = archive_cls._config_cls(**data)
                archive = archive_cls(
                    archive_config=archive_config, archive_name=archive_name
                )
                wrapped_archive_config = KiaraArchiveConfig(
                    archive_type=archive_type, config=data
                )
                archive_configs.append(wrapped_archive_config)
                archives.append(archive)

        if archives is None:
            raise Exception(
                f"Can't create context: no valid archive found at '{archive_uri}'"
            )

        result = cls(
            archive_uri=archive_uri,
            allow_write_access=allow_write_access,
            archive_configs=archive_configs,
            # archive_alias=archive_alias,
        )
        result._archives = archives
        return result

    archive_uri: str = Field(description="The uri that points to the archive.")
    # archive_alias: str = Field(
    #     description="The alias that is used for the archives contained in here."
    # )
    allow_write_access: bool = Field(
        description="Whether to allow write access to the archives contained here.",
        default=False,
    )
    archive_configs: List[KiaraArchiveConfig] = Field(
        description="All the archives this kiara context can use and the aliases they are registered with."
    )
    _archives: Union[None, List["KiaraArchive"]] = PrivateAttr(default=None)

    @property
    def archives(self) -> List["KiaraArchive"]:

        if self._archives is not None:
            return self._archives

        from kiara.utils.class_loading import find_all_archive_types

        archive_types = find_all_archive_types()

        archive_alias = None

        result = []
        for config in self.archive_configs:
            if config.archive_type not in archive_types.keys():
                raise Exception(
                    f"Can't create context: no archive type '{config.archive_type}' available. Available types: {', '.join(archive_types.keys())}"
                )

            archive_cls = archive_types[config.archive_type]
            archive_config_data = archive_cls.load_archive_config(
                archive_uri=self.archive_uri,
                allow_write_access=self.allow_write_access,
            )
            archive_config = archive_cls._config_cls(**archive_config_data)
            archive = archive_cls(
                archive_config=archive_config, archive_name=archive_alias
            )
            result.append(archive)

        self._archives = result
        return self._archives


class KiaraContextConfig(BaseModel):
    @classmethod
    def create_from_sqlite_db(cls, db_path: Path) -> "KiaraContextConfig":

        import sqlite3

        if not db_path.exists():
            context_id = str(uuid.uuid4())
            conn = sqlite3.connect(db_path)
            c = conn.cursor()
            c.execute(
                """CREATE TABLE context_metadata
                         (key text PRIMARY KEY , value text NOT NULL)"""
            )
            c.execute(
                "INSERT INTO context_metadata VALUES ('context_id', ?)", (context_id,)
            )
            c.execute(
                """CREATE TABLE archive_metadata
                         (key text PRIMARY KEY , value text NOT NULL)"""
            )
            c.execute(
                "INSERT INTO archive_metadata VALUES ('archive_id', ?)", (context_id,)
            )

            conn.commit()
            conn.close()
        else:
            try:

                with sqlite3.connect(db_path) as conn:
                    context_id = conn.execute(
                        "SELECT value FROM context_metadata WHERE key = 'context_id'"
                    ).fetchone()[0]
            except Exception as e:
                raise KiaraException(
                    f"Can't read context from sqlite db '{db_path}': {e}"
                )

        base_path = os.path.abspath(kiara_app_dirs.user_data_dir)
        stores_base_path = os.path.join(base_path, "stores")
        workflow_base_path = os.path.join(
            stores_base_path, "filesystem_stores", "workflows"
        )
        workflow_store_path = os.path.join(workflow_base_path, context_id)

        data_store_config = KiaraArchiveConfig(
            archive_type="sqlite_data_store",
            config={"sqlite_db_path": db_path.as_posix()},
        )
        alias_store_config = KiaraArchiveConfig(
            archive_type="sqlite_alias_store",
            config={"sqlite_db_path": db_path.as_posix()},
        )
        job_store_config = KiaraArchiveConfig(
            archive_type="sqlite_job_store",
            config={"sqlite_db_path": db_path.as_posix()},
        )
        workflow_store_config = KiaraArchiveConfig(
            archive_type="filesystem_workflow_store",
            config={"archive_path": workflow_store_path},
        )

        archives = {
            DEFAULT_DATA_STORE_MARKER: data_store_config,
            DEFAULT_ALIAS_STORE_MARKER: alias_store_config,
            DEFAULT_JOB_STORE_MARKER: job_store_config,
            DEFAULT_WORKFLOW_STORE_MARKER: workflow_store_config,
        }

        context_config = cls(
            context_id=context_id,
            archives=archives,
        )

        return context_config

    model_config = ConfigDict(extra="forbid")

    context_id: str = Field(description="A globally unique id for this kiara context.")

    archives: Dict[str, KiaraArchiveConfig] = Field(
        description="All the archives this kiara context can use and the aliases they are registered with."
    )
    extra_pipelines: List[str] = Field(
        description="Paths to local folders that contain kiara pipelines.",
        default_factory=list,
    )
    _context_config_path: Union[Path, None] = PrivateAttr(default=None)

    def add_pipelines(self, *pipelines: str):

        for pipeline in pipelines:
            if os.path.exists(pipeline):
                self.extra_pipelines.append(pipeline)
            else:
                logger.info(
                    "ignore.pipeline", reason="path does not exist", path=pipeline
                )

    # def create_archive(
    #     self, archive_alias: str, allow_write_access: bool = False
    # ) -> "KiaraArchive":
    #     """Create the kiara archive with the specified alias.
    #
    #     Make sure you know what you are doing when setting 'allow_write_access' to True.
    #     """
    #
    #     store_config = self.archives[archive_alias]
    #     store = create_store(
    #         archive_id=store_config.archive_uuid,
    #         store_type=store_config.archive_type,
    #         store_config=store_config.config,
    #         allow_write_access=allow_write_access,
    #     )
    #     return store


class KiaraSettings(BaseSettings):
    model_config = SettingsConfigDict(
        extra="forbid", validate_assignment=True, env_prefix="kiara_setting_"
    )

    syntax_highlight_background: str = Field(
        description="The background color for code blocks when rendering to terminal, Jupyter, etc.",
        default="default",
    )


KIARA_SETTINGS = KiaraSettings()


def create_default_store_config(
    store_type: str, stores_base_path: str, use_wal_mode: bool = False
) -> KiaraArchiveConfig:

    from kiara.utils.archives import find_archive_types

    # env_registry = EnvironmentRegistry.instance()
    # find_archive_types = find_archive_types()
    # kiara_types: "KiaraTypesRuntimeEnvironment" = env_registry.environments["kiara_types"]  # type: ignore
    available_archives = find_archive_types()

    assert store_type in available_archives.item_infos.keys()

    from kiara.models.archives import ArchiveTypeInfo

    archive_info: ArchiveTypeInfo = available_archives.item_infos[store_type]
    cls: Type[BaseArchive] = archive_info.python_class.get_class()  # type: ignore

    log_message(
        "create_new_store",
        stores_base_path=stores_base_path,
        store_type=cls.__name__,
    )

    config = cls._config_cls.create_new_store_config(
        store_base_path=stores_base_path, use_wal_mode=use_wal_mode
    )

    # store_id: uuid.UUID = config.get_archive_id()

    data_store = KiaraArchiveConfig(
        archive_type=store_type,
        config=config.model_dump(),
    )
    return data_store


DEFAULT_STORE_TYPE: Literal["sqlite"] = "sqlite"


class KiaraConfig(BaseSettings):

    model_config = SettingsConfigDict(
        env_prefix="kiara_", extra="forbid", use_enum_values=True
    )

    @classmethod
    def create_in_folder(cls, path: Union[Path, str]) -> "KiaraConfig":

        if isinstance(path, str):
            path = Path(path)
        path = path.absolute()
        if path.exists():
            raise Exception(
                f"Can't create new kiara config, path exists: {path.as_posix()}"
            )

        config = KiaraConfig(base_data_path=path.as_posix())
        config_file = path / KIARA_CONFIG_FILE_NAME

        config.save(config_file)

        return config

    @classmethod
    def load_from_file(cls, path: Union[Path, str, None] = None) -> "KiaraConfig":

        if path is None:
            path = Path(KIARA_MAIN_CONFIG_FILE)
        elif isinstance(path, str):
            path = Path(path)

        if not path.exists():
            raise Exception(
                f"Can't load kiara config, path does not exist: {path.as_posix()}"
            )

        if path.is_dir():
            path = path / KIARA_CONFIG_FILE_NAME
            if not path.exists():
                raise Exception(
                    f"Can't load kiara config, path does not exist: {path.as_posix()}"
                )

        with path.open("rt") as f:
            data = yaml.load(f)

        config = KiaraConfig(**data)
        config._config_path = path
        return config

    context_search_paths: List[str] = Field(
        description="The base path to look for contexts in.",
        default=[KIARA_MAIN_CONTEXTS_PATH],
    )
    base_data_path: str = Field(
        description="The base path to use for all data (unless otherwise specified.",
        default=kiara_app_dirs.user_data_dir,
    )
    stores_base_path: str = Field(
        description="The base path for the stores of this context."
    )
    default_context: str = Field(
        description="The name of the default context to use if none is provided.",
        default=DEFAULT_CONTEXT_NAME,
    )
    # default_store_type: Literal["sqlite", "filesystem"] = Field(
    #     description="The default store type to use when creating new stores.",
    #     default=DEFAULT_STORE_TYPE,
    # )
    auto_generate_contexts: bool = Field(
        description="Whether to auto-generate requested contexts if they don't exist yet.",
        default=True,
    )
    runtime_config: KiaraRuntimeConfig = Field(
        description="The kiara runtime config.", default_factory=KiaraRuntimeConfig
    )

    _contexts: Dict[uuid.UUID, "Kiara"] = PrivateAttr(default_factory=dict)
    _available_context_files: Dict[str, Path] = PrivateAttr(default=None)
    _context_data: Dict[str, KiaraContextConfig] = PrivateAttr(default_factory=dict)
    _config_path: Union[Path, None] = PrivateAttr(default=None)

    @field_validator("context_search_paths")
    @classmethod
    def validate_context_search_paths(cls, v):

        if not v or not v[0]:
            v = [KIARA_MAIN_CONTEXTS_PATH]

        return v

    @model_validator(mode="before")
    @classmethod
    def _set_paths(cls, values: Any):

        base_path = values.get("base_data_path", None)
        if not base_path:
            base_path = os.path.abspath(kiara_app_dirs.user_data_dir)
            values["base_data_path"] = base_path
        elif isinstance(base_path, Path):
            base_path = base_path.absolute().as_posix()
            values["base_data_path"] = base_path

        stores_base_path = values.get("stores_base_path", None)
        if not stores_base_path:
            stores_base_path = os.path.join(base_path, "stores")
            values["stores_base_path"] = stores_base_path

        context_search_paths = values.get("context_search_paths")
        if not context_search_paths:
            context_search_paths = [os.path.join(base_path, "contexts")]
            values["context_search_paths"] = context_search_paths

        return values

    @property
    def available_context_names(self) -> Iterable[str]:

        if self._available_context_files is not None:
            return self._available_context_files.keys()

        result = {}
        for search_path in self.context_search_paths:
            sp = Path(search_path)
            for path in sp.rglob("*.yaml"):
                rel_path = path.relative_to(sp)
                alias = rel_path.as_posix()[0:-5]
                alias = alias.replace(os.sep, ".")
                result[alias] = path
        self._available_context_files = result
        return self._available_context_files.keys()

    @property
    def context_configs(self) -> Mapping[str, KiaraContextConfig]:

        return {a: self.get_context_config(a) for a in self.available_context_names}

    def get_context_config(
        self,
        context_name: Union[str, None] = None,
        auto_generate: Union[bool, None] = None,
    ) -> KiaraContextConfig:

        if auto_generate is None:
            auto_generate = self.auto_generate_contexts

        if context_name is None:
            context_name = self.default_context

        if context_name not in self.available_context_names:
            if not auto_generate and not context_name == DEFAULT_CONTEXT_NAME:
                raise Exception(
                    f"No kiara context with name '{context_name}' available."
                )
            else:
                return self.create_context_config(context_alias=context_name)

        if context_name in self._context_data.keys():
            return self._context_data[context_name]

        context_file = self._available_context_files[context_name]
        context_data = get_data_from_file(context_file, content_type="yaml")

        if not context_data:
            raise KiaraException(
                f"Empty/corrupted context file '{context_file.as_posix()}': delete file and try again."
            )

        changed = False
        if "extra_pipeline_folders" in context_data.keys():
            epf = context_data.pop("extra_pipeline_folders")
            context_data.setdefault("extra_pipelines", []).extend(epf)
            changed = True

        context = KiaraContextConfig(**context_data)

        if not changed:
            changed = self._validate_context(context_config=context)

        if changed:
            logger.debug(
                "write.context_file",
                context_config_file=context_file.as_posix(),
                context_name=context_name,
                reason="context changed after validation",
            )
            context_file.parent.mkdir(parents=True, exist_ok=True)
            with open(context_file, "wt") as f:
                yaml.dump(context.model_dump(), f)

        context._context_config_path = context_file

        self._context_data[context_name] = context
        return context

    def _validate_context(self, context_config: KiaraContextConfig) -> bool:

        changed = False

        sqlite_base_path = os.path.join(self.stores_base_path, "sqlite_stores")
        filesystem_base_path = os.path.join(self.stores_base_path, "filesystem_stores")

        def create_default_sqlite_archive_config(use_wal_mode: bool) -> Dict[str, Any]:

            store_id = str(uuid.uuid4())
            file_name = f"{store_id}.karchive"
            archive_path = Path(
                os.path.abspath(os.path.join(sqlite_base_path, file_name))
            )

            if archive_path.exists():
                raise Exception(
                    f"Archive path '{archive_path.as_posix()}' already exists."
                )

            archive_path.parent.mkdir(exist_ok=True, parents=True)

            # Connect to the SQLite database (or create it if it doesn't exist)
            import sqlite3

            conn = sqlite3.connect(archive_path)

            # Create a cursor object
            c = conn.cursor()
            # Create table
            c.execute(
                """CREATE TABLE archive_metadata
                         (key text PRIMARY KEY , value text NOT NULL)"""
            )
            c.execute(
                "INSERT INTO archive_metadata VALUES ('archive_id', ?)", (store_id,)
            )
            conn.commit()
            conn.close()

            return {
                "sqlite_db_path": archive_path.as_posix(),
                "use_wal_mode": use_wal_mode,
            }

        default_sqlite_config: Union[Dict[str, Any], None] = None

        use_wal_mode: bool = True
        default_store_type = "sqlite"

        if default_store_type == "auto":

            # if windows, we want sqlite as default, because although it's slower, it does not
            # need the user to enable developer mode
            if os.name == "nt":
                data_store_type = "sqlite"
            else:
                data_store_type = "filesystem"

            metadata_store_type = "sqlite"
            alias_store_type = "sqlite"
            job_store_type = "sqlite"
            workflow_store_type = "sqlite"
        elif default_store_type == "filesystem":
            metadata_store_type = "filesystem"
            data_store_type = "filesystem"
            alias_store_type = "filesystem"
            job_store_type = "filesystem"
            workflow_store_type = "filesystem"
        elif default_store_type == "sqlite":
            metadata_store_type = "sqlite"
            data_store_type = "sqlite"
            alias_store_type = "sqlite"
            job_store_type = "sqlite"
            workflow_store_type = "sqlite"
        else:
            raise Exception(f"Unknown store type: {default_store_type}")

        if DEFAULT_METADATA_STORE_MARKER not in context_config.archives.keys():

            if metadata_store_type == "sqlite":
                default_sqlite_config = create_default_sqlite_archive_config(
                    use_wal_mode=use_wal_mode
                )
                metaddata_store = KiaraArchiveConfig(
                    archive_type="sqlite_metadata_store", config=default_sqlite_config
                )
            elif metadata_store_type == "filesystem":
                default_sqlite_config = create_default_sqlite_archive_config(
                    use_wal_mode=use_wal_mode
                )
                metaddata_store = KiaraArchiveConfig(
                    archive_type="sqlite_metadata_store", config=default_sqlite_config
                )
            else:
                raise Exception(
                    f"Can't create default metadata store: invalid default store type '{metadata_store_type}'"
                )

            context_config.archives[DEFAULT_METADATA_STORE_MARKER] = metaddata_store
            changed = True

        if DEFAULT_DATA_STORE_MARKER not in context_config.archives.keys():

            if data_store_type == "sqlite":
                if default_sqlite_config is None:
                    default_sqlite_config = create_default_sqlite_archive_config(
                        use_wal_mode=use_wal_mode
                    )

                data_store = KiaraArchiveConfig(
                    archive_type="sqlite_data_store", config=default_sqlite_config
                )
            elif data_store_type == "filesystem":
                data_store_type = "filesystem_data_store"
                data_store = create_default_store_config(
                    store_type=data_store_type,
                    stores_base_path=os.path.join(filesystem_base_path, "data"),
                )
            else:
                raise Exception(
                    f"Can't create default data store: invalid default store type '{data_store_type}'."
                )

            context_config.archives[DEFAULT_DATA_STORE_MARKER] = data_store
            changed = True

        if DEFAULT_JOB_STORE_MARKER not in context_config.archives.keys():

            if job_store_type == "sqlite":

                if default_sqlite_config is None:
                    default_sqlite_config = create_default_sqlite_archive_config(
                        use_wal_mode=use_wal_mode
                    )

                job_store = KiaraArchiveConfig(
                    archive_type="sqlite_job_store", config=default_sqlite_config
                )
            elif job_store_type == "filesystem":
                job_store_type = "filesystem_job_store"
                job_store = create_default_store_config(
                    store_type=job_store_type,
                    stores_base_path=os.path.join(filesystem_base_path, "jobs"),
                )
            else:
                raise Exception(
                    f"Can't create default job store: invalid default store type '{job_store_type}'."
                )

            context_config.archives[DEFAULT_JOB_STORE_MARKER] = job_store
            changed = True

        if DEFAULT_ALIAS_STORE_MARKER not in context_config.archives.keys():

            if alias_store_type == "sqlite":

                if default_sqlite_config is None:
                    default_sqlite_config = create_default_sqlite_archive_config(
                        use_wal_mode=use_wal_mode
                    )

                alias_store = KiaraArchiveConfig(
                    archive_type="sqlite_alias_store", config=default_sqlite_config
                )
            elif alias_store_type == "filesystem":
                alias_store_type = "filesystem_alias_store"
                alias_store = create_default_store_config(
                    store_type=alias_store_type,
                    stores_base_path=os.path.join(filesystem_base_path, "aliases"),
                )
            else:
                raise Exception(
                    f"Can't create default alias store: invalid default store type '{alias_store_type}'."
                )

            context_config.archives[DEFAULT_ALIAS_STORE_MARKER] = alias_store
            changed = True

        if DEFAULT_WORKFLOW_STORE_MARKER not in context_config.archives.keys():

            # TODO: impolement sqlite type, or remove workflows entirely

            workflow_store_type = "filesystem_workflow_store"
            # workflow_store_type = "sqlite_workflow_store"

            workflow_store = create_default_store_config(
                store_type=workflow_store_type,
                stores_base_path=os.path.join(filesystem_base_path, "workflows"),
            )
            context_config.archives[DEFAULT_WORKFLOW_STORE_MARKER] = workflow_store
            changed = True

        return changed

    def create_context_config(
        self, context_alias: Union[str, None] = None
    ) -> KiaraContextConfig:

        if not context_alias:
            context_alias = DEFAULT_CONTEXT_NAME

        if context_alias in self.available_context_names:
            raise Exception(
                f"Can't create kiara context '{context_alias}': context with that alias already registered."
            )

        if context_alias.endswith(".kontext"):
            context_db_file = Path(context_alias)
            context_config: KiaraContextConfig = (
                KiaraContextConfig.create_from_sqlite_db(db_path=context_db_file)
            )
            self._validate_context(context_config=context_config)
            context_config._context_config_path = context_db_file
        else:

            if os.path.sep in context_alias:
                raise Exception(
                    f"Can't create context with alias '{context_alias}': no special characters allowed."
                )

            context_file = (
                Path(os.path.join(self.context_search_paths[0]))
                / f"{context_alias}.yaml"
            )

            archives: Dict[str, KiaraArchiveConfig] = {}
            # create_default_archives(kiara_config=self)
            context_id = ID_REGISTRY.generate(
                obj_type=KiaraContextConfig,
                comment=f"new kiara context '{context_alias}'",
            )

            context_config = KiaraContextConfig(
                context_id=str(context_id), archives=archives, extra_pipelines=[]
            )

            self._validate_context(context_config=context_config)

            context_file.parent.mkdir(parents=True, exist_ok=True)
            with open(context_file, "wt") as f:
                yaml.dump(context_config.model_dump(), f)

            context_config._context_config_path = context_file
            self._available_context_files[context_alias] = context_file

        self._context_data[context_alias] = context_config

        return context_config

    def create_context(
        self,
        context: Union[None, str, uuid.UUID, Path] = None,
        extra_pipelines: Union[None, str, Iterable[str]] = None,
    ) -> "Kiara":

        if not context:
            context = self.default_context
        else:
            with contextlib.suppress(Exception):
                context = uuid.UUID(context)  # type: ignore

        if isinstance(context, str) and (
            os.path.exists(context) or context.endswith(".kontext")
        ):
            context = Path(os.path.abspath(context))

        if isinstance(context, Path):
            if context.name.endswith(".kontext"):
                context_config = KiaraContextConfig.create_from_sqlite_db(
                    db_path=context
                )
            else:
                try:
                    with context.open("rt") as f:
                        data = yaml.load(f)
                except Exception as e:
                    raise KiaraException(
                        f"Can't read context from file '{context}': {e}"
                    )
                context_config = KiaraContextConfig(**data)
        elif isinstance(context, str):
            context_config = self.get_context_config(context_name=context)
        elif isinstance(context, uuid.UUID):
            context_config = self.find_context_config(context_id=context)
        else:
            raise Exception(
                f"Can't retrieve context, invalid context config type '{type(context)}'."
            )

        assert context_config.context_id not in self._contexts.keys()

        if extra_pipelines:
            if isinstance(extra_pipelines, str):
                extra_pipelines = [extra_pipelines]
            context_config.add_pipelines(*extra_pipelines)

        from kiara.context import Kiara

        kiara = Kiara(config=context_config, runtime_config=self.runtime_config)
        assert kiara.id == uuid.UUID(context_config.context_id)
        self._contexts[kiara.id] = kiara

        return kiara

    def find_context_config(self, context_id: uuid.UUID) -> KiaraContextConfig:
        raise NotImplementedError()

    def save(self, path: Union[Path, None] = None):
        if path is None:
            path = Path(KIARA_MAIN_CONFIG_FILE)

        if path.exists():
            raise Exception(
                f"Can't save config file, path already exists: {path.as_posix()}"
            )

        path.parent.mkdir(parents=True, exist_ok=True)

        data = self.model_dump(
            exclude={
                "context",
                "auto_generate_contexts",
                "stores_base_path",
                "context_search_paths",
                "default_context",
                "runtime_config",
            }
        )

        with path.open("wt") as f:
            yaml.dump(
                data,
                f,
            )

        self._config_path = path

    def delete(
        self, context_name: Union[str, None] = None, dry_run: bool = True
    ) -> Union["ContextInfo", None]:
        """Deletes the context with the specified name."""

        if context_name is None:
            context_name = self.default_context

        from kiara.context import Kiara
        from kiara.models.context import ContextInfo

        context_config = self.get_context_config(
            context_name=context_name, auto_generate=False
        )

        context_summary = None

        try:
            kiara = Kiara(config=context_config, runtime_config=self.runtime_config)

            context_summary = ContextInfo.create_from_context(
                kiara=kiara, context_name=context_name
            )

            if dry_run:
                return context_summary

            for archive in kiara.get_all_archives().keys():
                archive.delete_archive(archive_id=archive.archive_id)
        except Exception as e:
            log_message("delete.context.error", context_name=context_name, error=e)

        if not dry_run:
            if context_config._context_config_path is not None:
                os.unlink(context_config._context_config_path)

        return context_summary

    def create_renderable(self, **render_config: Any):
        from kiara.utils.output import create_recursive_table_from_model_object

        return create_recursive_table_from_model_object(
            self, render_config=render_config
        )


# class KiaraCurrentContextConfig(KiaraBaseConfig):
#     """Configuration that holds the currently active context, as well as references to other available contexts."""
#
#     class Config:
#         env_prefix = "kiara_context_"
#         extra = Extra.forbid
#
#         @classmethod
#         def customise_sources(
#             cls,
#             init_settings,
#             env_settings,
#             file_secret_settings,
#         ):
#             return (
#                 init_settings,
#                 env_settings,
#             )
#
#     kiara_config: KiaraConfig = Field(
#         description="The base kiara configuration.", default_factory=KiaraConfig
#     )
#     context: str = Field(
#         description=f"The path to an existing folder that houses the context, or the name of the context to use under the default kiara app data directory ({kiara_app_dirs.user_data_dir})."
#     )
#     context_configs: Dict[str, KiaraContextConfig] = Field(
#         description="The context configuration."
#     )
#     # overlay_config: KiaraConfig = Field(description="Extra config options to add to the selected context.")
#
#     @classmethod
#     def find_current_contexts(
#         cls, kiara_config: KiaraConfig
#     ) -> Dict[str, KiaraContextConfig]:
#
#         contexts: Dict[str, KiaraContextConfig] = {}
#
#         if not os.path.exists(kiara_config.context_base_path):
#             return contexts
#
#         for f in os.listdir(kiara_config.context_base_path):
#
#             config_dir = os.path.join(kiara_config.context_base_path, f)
#             k_config = cls.load_context(config_dir)
#             if k_config:
#                 contexts[k_config.context_alias] = k_config
#
#         return contexts
#
#     @classmethod
#     def create_context(
#         cls,
#         path: str,
#         context_id: str,
#         kiara_config: KiaraConfig,
#         context_alias: Optional[str],
#     ) -> KiaraContextConfig:
#
#         if os.path.exists(path):
#             raise Exception(f"Can't create kiara context folder, path exists: {path}")
#
#         os.makedirs(path, exist_ok=False)
#
#         config = {}
#         config["context_id"] = context_id
#         if not context_alias:
#             context_alias = config["context_id"]
#         config["context_alias"] = context_alias
#         config["context_folder"] = path
#
#         config["archives"] = create_default_archives(kiara_config=kiara_config)
#
#         kiara_context_config = KiaraContextConfig(**config)
#         config_file = os.path.join(path, "kiara_context.yaml")
#
#         with open(config_file, "wt") as f:
#             yaml.dump(kiara_config.dict(), f)
#
#         return kiara_context_config
#
#     @classmethod
#     def load_context(cls, path: str):
#
#         if path.endswith("kiara_context.yaml"):
#             path = os.path.dirname(path)
#
#         if not os.path.isdir(path):
#             return None
#
#         config_file = os.path.join(path, "kiara_context.yaml")
#         if not os.path.isfile(config_file):
#             return None
#
#         try:
#             config = get_data_from_file(config_file)
#             k_config = KiaraContextConfig(**config)
#         except Exception as e:
#             log_message("config.parse.error", config_file=config_file, error=e)
#             return None
#
#         return k_config
#
#     @root_validator(pre=True)
#     def validate_global_config(cls, values):
#
#         create_context = values.pop("create_context", False)
#
#         kiara_config = values.get("kiara_config", None)
#         if kiara_config is None:
#             kiara_config = KiaraConfig()
#             values["kiara_config"] = kiara_config
#
#         contexts = cls.find_current_contexts(kiara_config=kiara_config)
#
#         assert "context_configs" not in values.keys()
#         assert "overlay_config" not in values.keys()
#
#         context_name: Optional[str] = values.get("context", None)
#         if context_name is None:
#             context_name = kiara_config.default_context
#         loaded_context: Optional[KiaraContextConfig] = None
#
#         assert context_name != "kiara_context.yaml"
#
#         if context_name != DEFAULT_CONTEXT_NAME:
#             context_dir: Optional[str] = None
#             if context_name.endswith("kiara_context.yaml"):
#                 context_dir = os.path.dirname(context_name)
#             elif os.path.isdir(context_name):
#                 context_config = os.path.join(context_name, "kiara_context.yaml")
#                 if os.path.exists(context_config):
#                     context_dir = context_name
#
#             if context_dir is not None:
#                 loaded_context = loaded_context(context_dir)
#             elif create_context and os.path.sep in context_name:
#                 # we assume this is meant to be a path that is outside of the 'normal' kiara data directory
#                 if context_name.endswith("kiara_context.yaml"):
#                     context_dir = os.path.dirname(context_name)
#                 else:
#                     context_dir = os.path.abspath(os.path.expanduser(context_name))
#                 context_id = str(uuid.uuid4())
#                 loaded_context = cls.create_context(
#                     path=context_dir, context_id=context_id, kiara_config=kiara_config
#                 )
#
#         if loaded_context is not None:
#             contexts[loaded_context.context_alias] = loaded_context
#             context_name = loaded_context.context_alias
#         else:
#             match = None
#
#             for context_alias, context in contexts.items():
#
#                 if context.context_id == context_name:
#                     if match:
#                         raise Exception(
#                             f"More then one kiara contexts with id: {context.context_id}"
#                         )
#                     match = context_name
#                 elif context.context_alias == context_name:
#                     if match:
#                         raise Exception(
#                             f"More then one kiara contexts with alias: {context.context_id}"
#                         )
#                     match = context_name
#
#             if not match:
#                 if not create_context and context_name != DEFAULT_CONTEXT_NAME:
#                     raise Exception(f"Can't find context with name: {context_name}")
#
#                 context_id = str(uuid.uuid4())
#                 context_dir = os.path.join(kiara_config.context_base_path, context_id)
#
#                 kiara_config = cls.create_context(
#                     path=context_dir,
#                     context_id=context_id,
#                     context_alias=context_name,
#                     kiara_config=kiara_config,
#                 )
#                 contexts[context_name] = kiara_config
#             else:
#                 context_name = match
#
#         values["context"] = context_name
#         values["context_configs"] = contexts
#         values["archives"] = contexts[context_name].archives
#
#         return values
#
#     def get_context(self, context_name: Optional[str] = None) -> KiaraContextConfig:
#
#         if not context_name:
#             context_name = self.context
#
#         if context_name not in self.context_configs.keys():
#             raise Exception(
#                 f"Kiara context '{context_name}' not registered. Registered contexts: {', '.join(self.context_configs.keys())}"
#             )
#
#         selected_dict = self.context_configs[context_name].dict()
#         overlay = self.dict(exclude={"context", "context_configs", "kiara_config"})
#         selected_dict.update(overlay)
#
#         kc = KiaraContextConfig(**selected_dict)
#         return kc
#
#     def create_renderable(self, **config) -> RenderableType:
#         return create_table_from_model_object(self)


# kiara\kiara\src\kiara\context\orm.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)


# import uuid
# from datetime import datetime
# from sqlalchemy import (
#     JSON,
#     Boolean,
#     Column,
#     ForeignKey,
#     Integer,
#     String,
#     Table,
#     UniqueConstraint,
# )
# from sqlalchemy.ext.declarative import DeclarativeMeta, declarative_base
# from sqlalchemy.orm import relationship
# from sqlalchemy_utc import UtcDateTime, utcnow
# from sqlalchemy_utils import UUIDType
# from typing import Any, Dict, List, Union
#
# Base: DeclarativeMeta = declarative_base()
#
#
# class MetadataSchemaOrm(Base):
#     __tablename__ = "metadata_schema_lookup"
#
#     id: Column[Union[int, None]] = Column(Integer, primary_key=True)
#     metadata_schema_hash: Column[int] = Column(Integer, index=True, nullable=False)
#     metadata_type: Column[str] = Column(String, nullable=False)
#     metadata_schema: Column[Union[Dict[Any, Any], List[Any]]] = Column(
#         JSON, nullable=False
#     )
#     metadata_payloads = relationship("EnvironmentOrm")
#
#     UniqueConstraint(metadata_schema_hash)
#
#
# class EnvironmentOrm(Base):
#     __tablename__ = "environments"
#
#     id: Column[Union[int, None]] = Column(Integer, primary_key=True)
#     metadata_hash: Column[int] = Column(Integer, index=True, nullable=False)
#     metadata_schema_id = Column(
#         Integer, ForeignKey("metadata_schema_lookup.id"), nullable=False
#     )
#     metadata_payload: Column[Union[Dict[Any, Any], List[Any]]] = Column(
#         JSON, nullable=False
#     )
#
#     UniqueConstraint(metadata_hash)
#
#
# class ManifestOrm(Base):
#     __tablename__ = "manifests"
#
#     id: Column[Union[int, None]] = Column(Integer, primary_key=True)
#     module_type: Column[str] = Column(String, index=True, nullable=False)
#     module_config: Column[Union[Dict[Any, Any], List[Any]]] = Column(
#         JSON, nullable=False
#     )
#     manifest_hash: Column[int] = Column(Integer, index=True, nullable=False)
#     is_idempotent: Column[bool] = Column(Boolean, nullable=False)
#
#     UniqueConstraint(module_type, manifest_hash)
#
#
# jobs_env_association_table = Table(
#     "job_environments",
#     Base.metadata,
#     Column("jobs_id", ForeignKey("jobs.id"), primary_key=True),
#     Column("environment_id", ForeignKey("environments.id"), primary_key=True),
# )
#
#
# class JobsOrm(Base):
#
#     __tablename__ = "jobs"
#     id: Column[Union[int, None]] = Column(Integer, primary_key=True)
#     manifest_id: Column[int] = Column(
#         Integer, ForeignKey("manifests.id"), nullable=False
#     )
#     inputs: Column[Union[Dict[Any, Any], List[Any]]] = Column(JSON, nullable=False)
#     input_hash: Column[str] = Column(String, nullable=False)
#     is_idempotent: Column[bool] = Column(Boolean, nullable=False)
#     created: Column[datetime] = Column(UtcDateTime(), default=utcnow(), nullable=False)
#     started: Column[Union[datetime, None]] = Column(UtcDateTime(), nullable=True)
#     duration_ms: Column[Union[int, None]] = Column(Integer, nullable=True)
#     environments = relationship("EnvironmentOrm", secondary=jobs_env_association_table)
#
#
# class ValueTypeOrm(Base):
#     __tablename__ = "data_types"
#
#     id: Column[Union[int, None]] = Column(Integer, primary_key=True)
#     type_config_hash: Column[int] = Column(Integer, index=True, nullable=False)
#     type_name: Column[str] = Column(String, nullable=False, index=True)
#     type_config: Column[Union[Dict[Any, Any], List[Any]]] = Column(JSON, nullable=False)
#
#     UniqueConstraint(type_config_hash, type_name)
#
#
# value_env_association_table = Table(
#     "value_environments",
#     Base.metadata,
#     Column("value_id", ForeignKey("values.id"), primary_key=True),
#     Column("environment_id", ForeignKey("environments.id"), primary_key=True),
# )
#
#
# class ValueOrm(Base):
#     __tablename__ = "values"
#
#     id: Column[Union[int, None]] = Column(Integer, primary_key=True)
#     global_id: Column[uuid.UUID] = Column(UUIDType(binary=True), nullable=False)
#     data_type_id: Column[int] = Column(
#         Integer, ForeignKey("data_types.id"), nullable=False
#     )
#     data_type_name: Column[str] = Column(String, index=True, nullable=False)
#     value_size: Column[int] = Column(Integer, index=True, nullable=False)
#     value_hash: Column[str] = Column(String, index=True, nullable=False)
#     environments = relationship("EnvironmentOrm", secondary=value_env_association_table)
#
#     UniqueConstraint(value_hash, value_size, data_type_id)
#
#
# class Pedigree(Base):
#     __tablename__ = "pedigrees"
#
#     id: Column[Union[int, None]] = Column(Integer, primary_key=True)
#     manifest_id: Column[int] = Column(
#         Integer, ForeignKey("manifests.id"), nullable=False
#     )
#     inputs: Column[Union[Dict[Any, Any], List[Any]]] = Column(JSON, nullable=False)
#
#
# class DestinyOrm(Base):
#     __tablename__ = "destinies"
#
#     id: Column[Union[int, None]] = Column(Integer, primary_key=True)
#     value_id: Column[int] = Column(Integer, ForeignKey("values.id"), nullable=False)
#     category: Column[str] = Column(String, nullable=False, index=False)
#     key: Column[str] = Column(String, nullable=False, index=False)
#     manifest_id: Column[int] = Column(
#         Integer, ForeignKey("manifests.id"), nullable=False
#     )
#     inputs: Column[Union[Dict[Any, Any], List[Any]]] = Column(
#         JSON, index=False, nullable=False
#     )
#     output_name: Column[str] = Column(String, index=False, nullable=False)
#     destiny_value: Column[Union[int, None]] = Column(
#         Integer, ForeignKey("values.id"), nullable=True
#     )
#     description: Column[Union[str, None]] = Column(String, nullable=True)
#
#     UniqueConstraint(value_id, category, key)
#
#
# class AliasOrm(Base):
#
#     __tablename__ = "aliases"
#
#     id: Column[Union[int, None]] = Column(Integer, primary_key=True)
#     alias: Column[str] = Column(String, index=True, nullable=False)
#     created: Column[datetime] = Column(UtcDateTime(), nullable=False, index=True)
#     version: Column[int] = Column(Integer, nullable=False, index=True)
#     value_id: Column[Union[uuid.UUID, None]] = Column(
#         UUIDType(binary=True), nullable=True
#     )
#
#     UniqueConstraint(alias, version)


# kiara\kiara\src\kiara\context\runtime_config.py
# -*- coding: utf-8 -*-
from enum import Enum
from typing import Literal

from pydantic import Field
from pydantic_settings import BaseSettings, SettingsConfigDict


class JobCacheStrategy(Enum):

    no_cache = "no_cache"
    value_id = "value_id"
    data_hash = "data_hash"


class KiaraRuntimeConfig(BaseSettings):
    """The runtime configuration for a *kiara* backend.

    The most important option here is the 'job_cache' setting, which determines how the runtime will match a new job against the records of past ones, in order to find a matching one and not have to re-run the possibly expensive job again. By default, no matching is done, other options are matching based on exact input value ids, or (more expensive) matching based on the input data hashes.
    """

    model_config = SettingsConfigDict(
        extra="forbid", validate_assignment=True, env_prefix="kiara_runtime_"
    )

    job_cache: JobCacheStrategy = Field(
        description="Name of the strategy that determines when to re-run jobs or use cached results.",
        default=JobCacheStrategy.no_cache,
    )
    allow_external: bool = Field(
        description="Whether to allow external external pipelines.", default=True
    )
    lock_context: bool = Field(
        description="Whether to lock context(s) on creation.", default=False
    )
    runtime_profile: Literal["default", "dharpa"] = Field(
        description="The runtime profile to use, this determines for example whether comments need to be provided when running a job.",
        default="dharpa",
    )

    # ignore_errors: bool = Field(
    #     description="If set, kiara will try to ignore most errors (that can be ignored).",
    #     default=False,
    # )


# kiara\kiara\src\kiara\context\__init__.py
# -*- coding: utf-8 -*-
import atexit
import os
import uuid
from pathlib import Path
from typing import TYPE_CHECKING, Any, Dict, Iterable, List, Mapping, Set, Type, Union

import structlog

# from alembic import command  # type: ignore
from pydantic import Field

from kiara.context.config import KiaraArchiveConfig, KiaraConfig, KiaraContextConfig
from kiara.context.runtime_config import KiaraRuntimeConfig
from kiara.data_types import DataType
from kiara.exceptions import KiaraContextException
from kiara.interfaces import get_console
from kiara.interfaces.python_api.models.info import (
    DataTypeClassesInfo,
    InfoItemGroup,
    ItemInfo,
    KiaraModelClassesInfo,
    ModuleTypeInfo,
    ModuleTypesInfo,
    OperationGroupInfo,
    OperationTypeClassesInfo,
)
from kiara.interfaces.python_api.value import StoreValueResult, StoreValuesResult
from kiara.models import KiaraModel
from kiara.models.context import ContextInfo
from kiara.models.module.manifest import Manifest
from kiara.models.runtime_environment import RuntimeEnvironment
from kiara.models.values.value import ValueMap
from kiara.registries import KiaraArchive, SqliteArchiveConfig
from kiara.registries.aliases import AliasRegistry
from kiara.registries.data import DataRegistry
from kiara.registries.environment import EnvironmentRegistry
from kiara.registries.events.metadata import CreateMetadataDestinies
from kiara.registries.events.registry import EventRegistry
from kiara.registries.ids import ID_REGISTRY
from kiara.registries.jobs import JobRegistry
from kiara.registries.metadata import MetadataRegistry
from kiara.registries.models import ModelRegistry
from kiara.registries.modules import ModuleRegistry
from kiara.registries.operations import OperationRegistry
from kiara.registries.rendering import RenderRegistry
from kiara.registries.types import TypeRegistry
from kiara.registries.workflows import WorkflowRegistry
from kiara.utils import log_exception, log_message
from kiara.utils.class_loading import find_all_archive_types
from kiara.utils.operations import filter_operations
from kiara.utils.stores import check_external_archive

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)


if TYPE_CHECKING:
    from kiara.modules import KiaraModule


logger = structlog.getLogger()


def explain(item: Any, kiara: Union[None, "Kiara"] = None):
    """Pretty print information about an item on the terminal."""
    if isinstance(item, type):
        from kiara.modules import KiaraModule

        if issubclass(item, KiaraModule):
            if kiara is None:
                kiara = Kiara.instance()
            item = ModuleTypeInfo.create_from_type_class(type_cls=item, kiara=kiara)

    console = get_console()
    console.print(item)


class Kiara(object):
    """
    The core context of a kiara session.

    The `Kiara` object holds all information related to the current environment the user does works in. This includes:

      - available modules, operations & pipelines
      - available value data_types
      - available metadata schemas
      - available data items
      - available controller and processor data_types
      - misc. configuration options

    It's possible to use *kiara* without ever manually touching the 'Kiara' class, by default all relevant classes and functions
    will use a default instance of this class (available via the `Kiara.instance()` method.

    The Kiara class is highly dependent on the Python environment it lives in, because it auto-discovers available sub-classes
    of its building blocks (modules, value data_types, etc.). So, you can't assume that, for example, a pipeline you create
    will work the same way (or at all) in a different environment. *kiara* will always be able to tell you all the details
    of this environment, though, and it will attach those details to things like data, so there is always a record of
    how something was created, and in which environment.
    """

    @classmethod
    def instance(cls) -> "Kiara":
        """The default *kiara* context. In most cases, it's recommended you create and manage your own, though."""

        raise NotImplementedError("Kiara.instance() is not implemented yet.")
        # return BaseAPI.instance().context

    def __init__(
        self,
        config: Union[KiaraContextConfig, None] = None,
        runtime_config: Union[KiaraRuntimeConfig, None] = None,
    ) -> None:

        kc: Union[KiaraConfig, None] = None
        if not config:
            kc = KiaraConfig()
            config = kc.get_context_config()

        if not runtime_config:
            if kc is None:
                kc = KiaraConfig()
            runtime_config = kc.runtime_config

        self._id: uuid.UUID = ID_REGISTRY.generate(
            id=uuid.UUID(config.context_id), obj=self
        )
        ID_REGISTRY.update_metadata(self._id, kiara_id=self._id)
        self._config: KiaraContextConfig = config
        self._runtime_config: KiaraRuntimeConfig = runtime_config

        self._env_mgmt: EnvironmentRegistry = EnvironmentRegistry()

        self._event_registry: EventRegistry = EventRegistry(kiara=self)
        self._type_registry: TypeRegistry = TypeRegistry(self)
        self._data_registry: DataRegistry = DataRegistry(kiara=self)
        self._metadata_registry: MetadataRegistry = MetadataRegistry(kiara=self)
        self._job_registry: JobRegistry = JobRegistry(kiara=self)
        self._module_registry: ModuleRegistry = ModuleRegistry(kiara=self)
        self._operation_registry: OperationRegistry = OperationRegistry(kiara=self)

        self._kiara_model_registry: ModelRegistry = ModelRegistry.instance()

        self._alias_registry: AliasRegistry = AliasRegistry(kiara=self)
        # self._destiny_registry: DestinyRegistry = DestinyRegistry(kiara=self)

        self._workflow_registry: WorkflowRegistry = WorkflowRegistry(kiara=self)

        self._render_registry = RenderRegistry(kiara=self)

        metadata_augmenter = CreateMetadataDestinies(kiara=self)
        self._event_registry.add_listener(
            metadata_augmenter, *metadata_augmenter.supported_event_types()
        )

        self._context_info: Union[KiaraContextInfo, None] = None

        # initialize stores
        self._archive_types = find_all_archive_types()
        self._archives: Dict[str, KiaraArchive] = {}

        for archive_alias, archive in self._config.archives.items():

            # TODO: this is just to make old context that still had that not error out
            if "_destiny_" in archive.archive_type:
                continue

            if (
                archive_alias == "default_job_store"
                and archive.archive_type == "filesystem_job_store"
            ):

                # this is a temporary solution for contexts that still have the old filesystem job store
                # TODO: remove this at some stage

                archive_path = Path(archive.config["archive_path"])
                file_name = f"{archive_path.name}.kiarchive"

                js_config = SqliteArchiveConfig.create_new_store_config(
                    store_base_path=archive.config["archive_path"],
                    file_name=file_name,
                    use_wal_mode=True,
                )
                archive = KiaraArchiveConfig(
                    archive_type="sqlite_job_store", config=js_config.model_dump()
                )

            archive_cls = self._archive_types.get(archive.archive_type, None)

            if archive_cls is None:
                raise Exception(
                    f"Can't create context: no archive type '{archive.archive_type}' available. Available types: {', '.join(self._archive_types.keys())}"
                )

            config_cls = archive_cls._config_cls
            archive_config = config_cls(**archive.config)
            archive_obj = archive_cls(archive_name=archive_alias, archive_config=archive_config)  # type: ignore
            for supported_type in archive_obj.supported_item_types():
                if supported_type == "metadata":
                    self.metadata_registry.register_metadata_archive(archive_obj)  # type: ignore
                if supported_type == "data":
                    self.data_registry.register_data_archive(
                        archive_obj,  # type: ignore
                    )
                if supported_type == "job_record":
                    self.job_registry.register_job_archive(archive_obj)  # type: ignore

                if supported_type == "alias":
                    self.alias_registry.register_archive(archive_obj)  # type: ignore

                # if supported_type == "destiny":
                #     self.destiny_registry.register_destiny_archive(archive_obj)  # type: ignore

                if supported_type == "workflow":
                    self.workflow_registry.register_archive(archive_obj)  # type: ignore

        if self._runtime_config.lock_context:
            self.lock_context()

    def lock_context(self):
        """Lock the context, so that it can't be used by other processes."""
        aquired = ID_REGISTRY.lock_context(self.id)

        if not aquired:
            raise KiaraContextException(
                "Can't lock context: already locked by another process.",
                context_id=self.id,
            )

        atexit.register(self.unlock_context)

    def unlock_context(self):

        ID_REGISTRY.unlock_context(self.id)

    @property
    def id(self) -> uuid.UUID:
        return self._id

    @property
    def context_config(self) -> KiaraContextConfig:
        return self._config

    @property
    def runtime_config(self) -> KiaraRuntimeConfig:
        return self._runtime_config

    def update_runtime_config(self, **settings) -> KiaraRuntimeConfig:

        for k, v in settings.items():
            setattr(self.runtime_config, k, v)

        return self.runtime_config

    @property
    def context_info(self) -> "KiaraContextInfo":

        if self._context_info is None:
            self._context_info = KiaraContextInfo.create_from_kiara_instance(kiara=self)
        return self._context_info

    # ===================================================================================================
    # registry accessors

    @property
    def environment_registry(self) -> EnvironmentRegistry:

        return self._env_mgmt

    @property
    def type_registry(self) -> TypeRegistry:
        return self._type_registry

    @property
    def module_registry(self) -> ModuleRegistry:
        return self._module_registry

    @property
    def kiara_model_registry(self) -> ModelRegistry:
        return self._kiara_model_registry

    @property
    def alias_registry(self) -> AliasRegistry:
        return self._alias_registry

    # @property
    # def destiny_registry(self) -> DestinyRegistry:
    #     return self._destiny_registry

    @property
    def job_registry(self) -> JobRegistry:
        return self._job_registry

    @property
    def operation_registry(self) -> OperationRegistry:
        op_registry = self._operation_registry
        return op_registry

    @property
    def data_registry(self) -> DataRegistry:
        return self._data_registry

    @property
    def metadata_registry(self) -> MetadataRegistry:
        return self._metadata_registry

    @property
    def workflow_registry(self) -> WorkflowRegistry:
        return self._workflow_registry

    @property
    def event_registry(self) -> EventRegistry:
        return self._event_registry

    @property
    def render_registry(self) -> RenderRegistry:
        return self._render_registry

    # ===================================================================================================
    # context specific types & instances

    @property
    def current_environments(self) -> Mapping[str, RuntimeEnvironment]:
        return self.environment_registry.environments

    @property
    def data_type_classes(self) -> Mapping[str, Type[DataType]]:
        return self.type_registry.data_type_classes

    @property
    def data_type_names(self) -> List[str]:
        return self.type_registry.get_data_type_names(include_profiles=True)

    @property
    def module_type_classes(self) -> Mapping[str, Type["KiaraModule"]]:
        return self._module_registry.module_types

    @property
    def module_type_names(self) -> Iterable[str]:
        return self._module_registry.get_module_type_names()

    # ===================================================================================================
    # kiara session API methods

    def register_external_archive(
        self,
        archive: Union[str, KiaraArchive, Iterable[Union[KiaraArchive, str]]],
        allow_write_access: bool = False,
    ) -> Dict[str, str]:
        """Register one or several external archives with the context.

        In case you provide KiaraArchive instances, they will be modified in case the provided 'allow_write_access' is different from the 'is_force_read_only' attribute of the archive.
        """

        archive_instances = check_external_archive(
            archive=archive, allow_write_access=allow_write_access
        )

        result = {}
        for archive_type, _archive_inst in archive_instances.items():
            log_message(
                "register.external.archive",
                archive=_archive_inst.archive_name,
                allow_write_access=allow_write_access,
            )

            _archive_inst.set_force_read_only(not allow_write_access)

            if archive_type == "data":
                result["data"] = self.data_registry.register_data_archive(_archive_inst)  # type: ignore
                log_message(
                    "archive.registered",
                    archive=_archive_inst.archive_name,
                    archive_type="data",
                )
            elif archive_type == "metadata":
                result["metadata"] = self.metadata_registry.register_metadata_archive(_archive_inst)  # type: ignore
                log_message(
                    "archive.registered",
                    archive=_archive_inst.archive_name,
                    archive_type="metadata",
                )
            elif archive_type == "alias":
                result["alias"] = self.alias_registry.register_archive(_archive_inst)  # type: ignore
                log_message(
                    "archive.registered",
                    archive=_archive_inst.archive_name,
                    archive_type="alias",
                )
            elif archive_type == "job_record":
                result["job_record"] = self.job_registry.register_job_archive(_archive_inst)  # type: ignore
                log_message(
                    "archive.registered",
                    archive=_archive_inst.archive_name,
                    archive_type="job_record",
                )
            else:
                raise Exception(f"Can't register archive of type '{archive_type}'.")

        return result

    def create_manifest(
        self, module_or_operation: str, config: Union[Mapping[str, Any], None] = None
    ) -> Manifest:

        if config is None:
            config = {}

        if module_or_operation in self.module_type_names:

            manifest: Manifest = Manifest(
                module_type=module_or_operation, module_config=config
            )

        elif module_or_operation in self.operation_registry.operation_ids:

            if config:
                raise Exception(
                    f"Specified run target '{module_or_operation}' is an operation, additional module configuration is not allowed (yet)."
                )
            manifest = self.operation_registry.get_operation(module_or_operation)

        elif os.path.isfile(module_or_operation):
            raise NotImplementedError()

        else:
            raise Exception(
                f"Can't assemble operation, invalid operation/module name: {module_or_operation}. Must be registered module or operation name, or file."
            )

        return manifest

    # def create_module(self, manifest: Union[Manifest, str]) -> "KiaraModule":
    #     """Create a [KiaraModule][kiara.module.KiaraModule] object from a module configuration.
    #
    #     Arguments:
    #         manifest: the module configuration
    #     """
    #
    #     return self._module_registry.create_module(manifest=manifest)

    def queue(
        self, manifest: Manifest, inputs: Mapping[str, Any], wait: bool = False
    ) -> uuid.UUID:
        """
        Queue a job with the specified manifest and inputs.

        Arguments:
        ---------
           manifest: the job manifest
           inputs: the job inputs
           wait: whether to wait for the job to be finished before returning

        Returns:
        -------
            the job id that can be used to look up job status & results
        """
        return self.job_registry.execute(manifest=manifest, inputs=inputs, wait=wait)

    def process(self, manifest: Manifest, inputs: Mapping[str, Any]) -> ValueMap:
        """
        Queue a job with the specified manifest and inputs.

        Arguments:
        ---------
           manifest: the job manifest
           inputs: the job inputs
           wait: whether to wait for the job to be finished before returning

        Returns:
        -------
        """
        return self.job_registry.execute_and_retrieve(manifest=manifest, inputs=inputs)

    def save_values(
        self, values: ValueMap, alias_map: Mapping[str, Iterable[str]]
    ) -> StoreValuesResult:

        _values = {}
        for field_name in values.field_names:
            value = values.get_value_obj(field_name)
            _values[field_name] = value
            self.data_registry.store_value(value=value)
        stored = {}
        for field_name, field_aliases in alias_map.items():

            value = _values[field_name]
            try:
                if field_aliases:
                    self.alias_registry.register_aliases(
                        value_id=value.value_id, aliases=field_aliases
                    )

                stored[field_name] = StoreValueResult(
                    value=value,
                    aliases=sorted(field_aliases),
                    error=None,
                    persisted_data=None,
                )

            except Exception as e:
                log_exception(e)
                stored[field_name] = StoreValueResult(
                    value=value,
                    aliases=sorted(field_aliases),
                    error=str(e),
                    persisted_data=None,
                )

        return StoreValuesResult(root=stored)

    def create_context_summary(self) -> ContextInfo:
        return ContextInfo.create_from_context(kiara=self)

    def get_all_archives(self) -> Dict[KiaraArchive, Set[str]]:

        result: Dict[KiaraArchive, Set[str]] = {}

        archive: KiaraArchive
        for alias, archive in self.metadata_registry.metadata_archives.items():
            result.setdefault(archive, set()).add(alias)
        for alias, archive in self.data_registry.data_archives.items():
            result.setdefault(archive, set()).add(alias)
        for alias, archive in self.alias_registry.alias_archives.items():
            result.setdefault(archive, set()).add(alias)
        # for alias, archive in self.destiny_registry.destiny_archives.items():
        #     result.setdefault(archive, set()).add(alias)
        for alias, archive in self.job_registry.job_archives.items():
            result.setdefault(archive, set()).add(alias)
        for alias, archive in self.workflow_registry.workflow_archives.items():
            result.setdefault(archive, set()).add(alias)

        return result


class KiaraContextInfo(KiaraModel):
    @classmethod
    def create_from_kiara_instance(
        cls, kiara: "Kiara", package_filter: Union[str, None] = None
    ):

        data_types = kiara.type_registry.get_context_metadata(
            only_for_package=package_filter
        )
        modules = kiara.module_registry.get_context_metadata(
            only_for_package=package_filter
        )
        operation_types = kiara.operation_registry.get_context_metadata(
            only_for_package=package_filter
        )
        operations = filter_operations(
            kiara=kiara, pkg_name=package_filter, **kiara.operation_registry.operations
        )

        model_registry = kiara.kiara_model_registry
        if package_filter:
            kiara_models = model_registry.get_models_for_package(
                package_name=package_filter
            )
        else:
            kiara_models = model_registry.all_models

        # metadata_types = find_metadata_models(only_for_package=package_filter)

        return KiaraContextInfo(
            kiara_id=kiara.id,
            package_filter=package_filter,
            data_types=data_types,
            module_types=modules,
            kiara_model_types=kiara_models,
            # metadata_types=metadata_types,
            operation_types=operation_types,
            operations=operations,
        )

    kiara_id: uuid.UUID = Field(description="The id of the kiara context.")
    package_filter: Union[str, None] = Field(
        description="Whether this context is filtered to only include information included in a specific Python package."
    )
    data_types: DataTypeClassesInfo = Field(description="The included data types.")
    module_types: ModuleTypesInfo = Field(
        description="The included kiara module types."
    )
    kiara_model_types: KiaraModelClassesInfo = Field(
        description="The included model classes."
    )
    # metadata_types: MetadataTypeClassesInfo = Field(
    #     description="The included value metadata types."
    # )
    operation_types: OperationTypeClassesInfo = Field(
        description="The included operation types."
    )
    operations: OperationGroupInfo = Field(description="The included operations.")

    def _retrieve_id(self) -> str:
        if not self.package_filter:
            return str(self.kiara_id)
        else:
            return f"{self.kiara_id}.package_{self.package_filter}"

    def _retrieve_data_to_hash(self) -> Any:
        return {"kiara_id": self.kiara_id, "package": self.package_filter}

    def get_info(self, item_type: str, item_id: str) -> ItemInfo:

        if item_type in ("data_type", "data_types"):
            group_info: InfoItemGroup = self.data_types
        elif "module" in item_type:
            group_info = self.module_types
        # elif "metadata" in item_type:
        #     group_info = self.metadata_types
        elif "operation_type" in item_type or "operation_types" in item_type:
            group_info = self.operation_types
        elif "operation" in item_type:
            group_info = self.operations
        elif "kiara_model" in item_type:
            group_info = self.kiara_model_types
        else:
            item_types = [
                "data_type",
                "module_type",
                "kiara_model_type",
                "operation_type",
                "operation",
            ]
            raise Exception(
                f"Can't determine item type '{item_type}', use one of: {', '.join(item_types)}"
            )
        result: ItemInfo = group_info.item_infos[item_id]
        return result

    def get_all_info(self, skip_empty_types: bool = True) -> Dict[str, InfoItemGroup]:

        result: Dict[str, InfoItemGroup] = {}
        if self.data_types or not skip_empty_types:
            result["data_types"] = self.data_types
        if self.module_types or not skip_empty_types:
            result["module_types"] = self.module_types
        if self.kiara_model_types or not skip_empty_types:
            result["kiara_model_types"] = self.kiara_model_types
        # if self.metadata_types or not skip_empty_types:
        #     result["metadata_types"] = self.metadata_types
        if self.operation_types or not skip_empty_types:
            result["operation_types"] = self.operation_types
        if self.operations or not skip_empty_types:
            result["operations"] = self.operations

        return result


# def delete_context(kiara_config: KiaraConfig, context_name: str):
#
#     kiara_context_config = kiara_config.get_context_config(context_name=context_name)
#     kiara = Kiara(config=kiara_context_config)
#
#     data_archives = kiara.data_registry.data_archives.values()
#     alias_archives = kiara.alias_registry.alias_archives.values()
#     job_archives = kiara.job_registry.job_archives.values()
#     destiny_archives = kiara.destiny_registry.destiny_archives.values()
#
#     clashes: Dict[str, List[KiaraArchive]] = {}
#     for context_name, context_config in kiara_config.context_configs.items():
#         k = Kiara(config=context_config)
#         for da in k.data_registry.data_archives.values():
#             if da in data_archives:
#                 clashes.setdefault("data", []).append(da)
#         for aa in k.alias_registry.alias_archives.values():
#             if aa in alias_archives:
#                 clashes.setdefault("alias", []).append(aa)
#         for ja in k.job_registry.job_archives.values():
#             if ja in job_archives:
#                 clashes.setdefault("job", []).append(ja)
#         for dea in k.destiny_registry.destiny_archives.values():
#             if dea in destiny_archives:
#                 clashes.setdefault("destiny", []).append(dea)
#
#     if clashes:
#         # TODO: only delete non-clash archives and don't throw exception
#         raise Exception(
#             f"Can't delete context '{context_name}', some archives are used in other contexts: {clashes}"
#         )
#
#     for da in data_archives:
#         da.delete_archive(archive_id=da.archive_id)
#
#     for aa in alias_archives:
#         aa.delete_archive(archive_id=aa.archive_id)
#
#     for ja in job_archives:
#         ja.delete_archive(archive_id=ja.archive_id)
#
#     for dea in destiny_archives:
#         dea.delete_archive(archive_id=dea.archive_id)


# kiara\kiara\src\kiara\data_types\__init__.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

"""
This is the base module that contains everything data type-related in *kiara*.

I'm still not 100% sure how to best implement the *kiara* type system, there are several ways it could be done, for example
based on Python type-hints, using JSON-schema, Avro (which is my 2nd favourite option), as well as by implementing a
custom type-class hierarchy. Which is what I have choosen to try first. For now, it looks like it'll work out,
but there is a chance requirements I haven't forseen will crop up that could make this become ugly.

Anyway, the way it works (for now) is that *kiara* comes with a set of often used data_types (the standard set of: scalars,
list, dict, table & array, etc.) which each come with 2 functions that can serialize and deserialize values of that
type in a persistant fashion -- which could be storing as a file on disk, or as a cell/row in a database. Those functions
will most likley be *kiara* modules themselves, with even more restricted input/output type options.

In addition, packages that contain modules can implement their own, custom data_types, if suitable ones are not available in
core-*kiara*. Those can either be 'serialized/deserialized' into *kiara*-native data_types (which in turn will serialize them
using their own serializing functions), or will have to implement custom serializing functionality (which will probably
be discouraged, since this might not be trivial and there are quite a few things to consider).

"""
import abc
import uuid
from typing import TYPE_CHECKING, Any, Generic, Mapping, Tuple, Type, TypeVar, Union

import structlog
from deepdiff import DeepHash
from pydantic import BaseModel, ConfigDict, PrivateAttr, ValidationError
from rich import box
from rich.console import Console, ConsoleOptions, RenderResult
from rich.rule import Rule
from rich.syntax import Syntax
from rich.table import Table

from kiara.defaults import (
    INVALID_HASH_MARKER,
    INVALID_SIZE_MARKER,
    NO_SERIALIZATION_MARKER,
    SpecialValue,
)
from kiara.exceptions import KiaraValueException, ValueTypeConfigException
from kiara.models.python_class import PythonClass
from kiara.models.values import DataTypeCharacteristics, ValueStatus
from kiara.models.values.value_schema import ValueSchema
from kiara.utils.hashing import KIARA_HASH_FUNCTION

#
#     if obj.__class__.__module__ == "builtins":
#         return obj.__class__.__name__
#     else:
#         return f"{obj.__class__.__module__}.{obj.__class__.__name__}"

if TYPE_CHECKING:
    from kiara.models.values.value import (
        DataTypeInfo,
        SerializedData,
        Value,
        ValuePedigree,
    )

logger = structlog.getLogger()

# def get_type_name(obj: Any):
#     """Utility function to get a pretty string from the class of an object."""


class DataTypeConfig(BaseModel):
    """
    Base class that describes the configuration a [``DataType``][kiara.data.data_types.DataType] class accepts.

    This is stored in the ``_config_cls`` class attribute in each ``DataType`` class. By default,
    a ``DataType`` is not configurable, unless the ``_config_cls`` class attribute points to a sub-class of this class.
    """

    model_config = ConfigDict(extra="forbid")

    @classmethod
    def requires_config(cls) -> bool:
        """Return whether this class can be used as-is, or requires configuration before an instance can be created."""
        for field_name, field in cls.model_fields.items():
            if field.is_required() and field.default is None:
                return True
        return False

    _config_hash: Union[int, None] = PrivateAttr(default=None)

    def get(self, key: str) -> Any:
        """Get the value for the specified configuation key."""
        if key not in self.model_fields.keys():
            raise Exception(
                f"No config value '{key}' in module config class '{self.__class__.__name__}'."
            )

        return getattr(self, key)

    @property
    def config_hash(self) -> int:

        if self._config_hash is None:
            _d = self.model_dump()
            hashes = DeepHash(_d)
            self._config_hash = hashes[_d]
        return self._config_hash

    def __eq__(self, other):

        if self.__class__ != other.__class__:
            return False

        return self.model_dump() == other.model_dump()

    def __hash__(self):

        return self.config_hash

    def __rich_console__(
        self, console: Console, options: ConsoleOptions
    ) -> RenderResult:

        my_table = Table(box=box.MINIMAL, show_header=False)
        my_table.add_column("Field name", style="i")
        my_table.add_column("Value")
        for field in self.model_fields.keys():
            my_table.add_row(field, getattr(self, field))

        yield my_table


TYPE_PYTHON_CLS = TypeVar("TYPE_PYTHON_CLS")
TYPE_CONFIG_CLS = TypeVar("TYPE_CONFIG_CLS", bound=DataTypeConfig)


class DataType(abc.ABC, Generic[TYPE_PYTHON_CLS, TYPE_CONFIG_CLS]):
    """
    Base class that all *kiara* data_types must inherit from.

    *kiara* data_types have 3 main responsibilities:

     - serialize into / deserialize from persistent state
     - data validation
     - metadata extraction

     Serializing being the arguably most important of those, because without most of the data management features of
     *kiara* would be impossible. Validation should not require any explanation. Metadata extraction is important, because
     that metadata will be available to other components of *kiara* (or frontends for it), without them having to request
     the actual data. That will hopefully make *kiara* very efficient in terms of memory management, as well as data
     transfer and I/O. Ideally, the actual data (bytes) will only be requested at the last possible moment. For example when a
     module needs the input data to do processing on it -- and even then it might be that it only requests a part of the
     data, say a single column of a table. Or when a frontend needs to display/visualize the data.
    """

    @classmethod
    def retrieve_available_type_profiles(cls) -> Mapping[str, Mapping[str, Any]]:
        return {}

    @classmethod
    @abc.abstractmethod
    def python_class(cls) -> Type[TYPE_PYTHON_CLS]:
        """The Python class that the internal 'data' attribute of a value has."""

    @classmethod
    def data_type_config_class(cls) -> Type[TYPE_CONFIG_CLS]:
        """The Python class that holds the (optional) configuration for a data type instance."""
        return DataTypeConfig  # type: ignore

    @classmethod
    def _calculate_data_type_hash(
        cls, data_type_config: Union[Mapping[str, Any], DataTypeConfig]
    ) -> int:

        if isinstance(data_type_config, Mapping):
            data_type_config = cls.data_type_config_class()(**data_type_config)  # type: ignore

        obj = {
            "type": cls._data_type_name,  # type: ignore
            "type_config": data_type_config.config_hash,
        }
        h = DeepHash(obj, hasher=KIARA_HASH_FUNCTION)
        result: int = h[obj]
        return result

    def __init__(self, **type_config: Any):

        try:
            self._type_config: TYPE_CONFIG_CLS = (
                self.__class__.data_type_config_class()(**type_config)
            )
        except ValidationError as ve:
            raise ValueTypeConfigException(
                f"Error creating object for type: {ve}",
                self.__class__,
                type_config,
                ve,
            )

        self._data_type_hash: Union[int, None] = None
        self._characteristics: Union[DataTypeCharacteristics, None] = None
        self._info: Union[DataTypeInfo, None] = None

    @property
    def data_type_name(self) -> str:
        return self._data_type_name  # type: ignore

    @property
    def data_type_hash(self) -> int:
        if self._data_type_hash is None:
            self._data_type_hash = self.__class__._calculate_data_type_hash(
                self._type_config
            )
        return self._data_type_hash

    @property
    def info(self) -> "DataTypeInfo":

        if self._info is not None:
            return self._info

        from kiara.models.values.value import DataTypeInfo

        self._info = DataTypeInfo(
            data_type_name=self.data_type_name,
            data_type_config=self.type_config.model_dump(),
            characteristics=self.characteristics,
            data_type_class=PythonClass.from_class(self.__class__),
        )
        self._info._data_type_instance = self
        return self._info

    @property
    def characteristics(self) -> DataTypeCharacteristics:
        if self._characteristics is not None:
            return self._characteristics

        self._characteristics = self._retrieve_characteristics()
        return self._characteristics

    def _retrieve_characteristics(self) -> DataTypeCharacteristics:
        return DataTypeCharacteristics()

    # @abc.abstractmethod
    # def is_immutable(self) -> bool:
    #     pass

    def calculate_hash(self, data: "SerializedData") -> str:
        """Calculate the hash of the value."""
        return data.instance_id

    def calculate_size(self, data: "SerializedData") -> int:
        """Calculate the size of the value."""
        return data.data_size

    def serialize_as_json(self, data: Any) -> "SerializedData":

        _data = {"data": {"type": "inline-json", "inline_data": data, "codec": "json"}}

        serialized_data = {
            "data_type": self.data_type_name,
            "data_type_config": self.type_config.model_dump(),
            "data": _data,
            "serialization_profile": "json",
            "metadata": {
                "environment": {},
                "deserialize": {
                    "python_object": {
                        "module_type": "deserialize.from_json",
                        "module_config": {"result_path": "data"},
                    }
                },
            },
        }
        from kiara.models.values.value import SerializationResult

        serialized = SerializationResult(**serialized_data)
        return serialized

    def serialize(self, data: TYPE_PYTHON_CLS) -> Union[None, str, "SerializedData"]:

        logger.debug(
            "ignore.serialize_request",
            data_type=self.data_type_name,
            reason="no 'serialize' method imnplemented",
        )
        return NO_SERIALIZATION_MARKER
        # raise NotImplementedError(f"Data type '{self.data_type_name}' does not support serialization.")
        #
        # try:
        #     import pickle5 as pickle
        # except Exception:
        #     import pickle  # type: ignore
        #
        # pickled = pickle.dumps(data, protocol=5)
        # _data = {"python_object": {"type": "chunk", "chunk": pickled, "codec": "raw"}}
        #
        # serialized_data = {
        #     "data_type": self.data_type_name,
        #     "data_type_config": self.type_config.dict(),
        #     "data": _data,
        #     "serialization_profile": "pickle",
        #     "serialization_metadata": {
        #         "profile": "pickle",
        #         "environment": {},
        #         "deserialize": {
        #             "object": {
        #                 "module_name": "value.unpickle",
        #                 "module_config": {
        #                     "value_type": "any",
        #                     "target_profile": "object",
        #                     "serialization_profile": "pickle",
        #                 },
        #             }
        #         },
        #     },
        # }
        # from kiara.models.values.value import SerializationResult
        #
        # serialized = SerializationResult(**serialized_data)
        # return serialized

    @property
    def type_config(self) -> TYPE_CONFIG_CLS:
        return self._type_config

    def _pre_examine_data(
        self, data: Any, schema: ValueSchema
    ) -> Tuple[Any, Union[str, "SerializedData"], ValueStatus, str, int]:

        assert data is not None

        if data is SpecialValue.NOT_SET:
            status = ValueStatus.NOT_SET
            data = None
        elif data is SpecialValue.NO_VALUE:
            status = ValueStatus.NONE
            data = None
        else:
            status = ValueStatus.SET

        # if data is None and schema.default not in [
        #     None,
        #     SpecialValue.NO_VALUE,
        #     SpecialValue.NOT_SET,
        # ]:
        #
        #     status = ValueStatus.DEFAULT
        #     if callable(schema.default):
        #         data = schema.default()
        #     else:
        #         data = copy.deepcopy(schema.default)

        if data is None or data is SpecialValue.NOT_SET:
            # if schema.default in [None, SpecialValue.NO_VALUE]:
            #     data = SpecialValue.NO_VALUE
            #     status = ValueStatus.NONE
            # elif schema.default == SpecialValue.NOT_SET:
            #     data = SpecialValue.NOT_SET
            #     status = ValueStatus.NOT_SET

            size = 0
            value_hash = INVALID_HASH_MARKER
            serialized: Union[None, str, "SerializedData"] = NO_SERIALIZATION_MARKER
        else:

            from kiara.models.values.value import SerializedData

            if isinstance(data, SerializedData):
                # TODO: assert value is in schema lineage
                # assert data.data_type == schema.type
                # assert data.data_type_config == schema.type_config
                serialized = data
                not_serialized: bool = False
            else:
                data = self.parse_python_obj(data)
                if data is None:
                    raise Exception(
                        f"Invalid data, can't parse into a value of type '{schema.type}'."
                    )
                self._validate(data)

                serialized = self.serialize(data)
                if serialized is None:
                    serialized = NO_SERIALIZATION_MARKER

                if isinstance(serialized, str):
                    not_serialized = True
                else:
                    not_serialized = False

            if not_serialized:
                size = INVALID_SIZE_MARKER
                value_hash = INVALID_HASH_MARKER
            else:
                size = serialized.data_size  # type: ignore
                value_hash = serialized.instance_id  # type: ignore

        assert serialized is not None
        result = (data, serialized, status, value_hash, size)
        return result

    def assemble_value(
        self,
        value_id: uuid.UUID,
        data: Any,
        schema: ValueSchema,
        environment_hashes: Mapping[str, Mapping[str, str]],
        serialized: Union[str, "SerializedData"],
        status: Union[ValueStatus, str],
        value_hash: str,
        value_size: int,
        pedigree: "ValuePedigree",
        kiara_id: uuid.UUID,
        pedigree_output_name: str,
    ) -> Tuple["Value", Any]:

        from kiara.models.values.value import Value

        if isinstance(status, str):
            status = ValueStatus(status).name

        if status in [ValueStatus.SET, ValueStatus.DEFAULT]:
            try:

                value = Value(
                    value_id=value_id,
                    kiara_id=kiara_id,
                    value_status=status,
                    value_size=value_size,
                    value_hash=value_hash,
                    value_schema=schema,
                    environment_hashes=environment_hashes,
                    pedigree=pedigree,
                    pedigree_output_name=pedigree_output_name,
                    data_type_info=self.info,
                )

            except Exception as e:
                raise KiaraValueException(
                    data_type=self.__class__, value_data=data, parent=e
                )
        else:
            value = Value(
                value_id=value_id,
                kiara_id=kiara_id,
                value_status=status,
                value_size=value_size,
                value_hash=value_hash,
                value_schema=schema,
                environment_hashes=environment_hashes,
                pedigree=pedigree,
                pedigree_output_name=pedigree_output_name,
                data_type_info=self.info,
            )

        value._value_data = data
        value._serialized_data = serialized
        return value, data

    def parse_python_obj(self, data: Any) -> TYPE_PYTHON_CLS:
        """
        Parse a value into a supported python type.

        This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object).
        If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to
        avoid adding or removing information from the data (e.g. by changing the resolution of a date).

        Arguments:
        ---------
            v: the value

        Returns:
        -------
            'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object
        """

        # this would in most cases be overwritten by an implementing class
        # if not, then the _validate method should catch the issue
        return data  # type: ignore

    def _validate(self, value: TYPE_PYTHON_CLS) -> None:
        """Validate the value. This expects an instance of the defined Python class (from 'backing_python_type)."""
        if not isinstance(value, self.__class__.python_class()):
            raise ValueError(
                f"Invalid python type '{type(value)}', must be: {self.__class__.python_class()}"
            )

    def create_renderable(self, **config):

        show_type_info = config.get("show_type_info", False)

        table = Table(show_header=False, box=box.SIMPLE)
        table.add_column("key")
        table.add_column("value", style="i")
        table.add_row("type_name", self.data_type_name)
        config_json = self.type_config.model_dump_json(exclude_unset=True, indent=2)
        config = Syntax(config_json, "json", background_color="default")
        table.add_row("type_config", config)

        if show_type_info:
            from kiara.interfaces.python_api.models.info import DataTypeClassInfo

            info = DataTypeClassInfo.create_from_type_class(self.__class__)
            table.add_row("", "")
            table.add_row("", Rule())
            table.add_row("type_info", info)

        return table


# class ValueTypeInfo(object):
#     def __init__(self, type_cls: typing.Type[ValueTypeOrm]):
#
#         self._value_type_cls: typing.Type[ValueTypeOrm] = type_cls
#
#     @property
#     def doc(self) -> str:
#         return self._value_type_cls.doc()
#
#     @property
#     def desc(self) -> str:
#         return self._value_type_cls.desc()


# kiara\kiara\src\kiara\data_types\included_core_types\filesystem.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

from typing import TYPE_CHECKING, Any, ClassVar, Dict, Mapping, Type, Union

import humanfriendly
import structlog
from pydantic import Field
from rich import box
from rich.console import Group, RenderableType
from rich.panel import Panel
from rich.table import Table

from kiara.data_types import DataTypeConfig
from kiara.data_types.included_core_types import AnyType, KiaraModelValueBaseType
from kiara.models.filesystem import KiaraFile, KiaraFileBundle
from kiara.models.values.value import Value
from kiara.utils.output import create_table_from_data_and_schema

if TYPE_CHECKING:
    from kiara.models.values.value import SerializedData

logger = structlog.getLogger()


class FileTypeConfig(DataTypeConfig):

    content_type: Union[str, None] = Field(
        description="The content type of this file.", default=None
    )


SUPPORTED_FILE_TYPES = ["csv", "json", "text", "binary"]


class FileValueType(KiaraModelValueBaseType[KiaraFile, FileTypeConfig]):
    """A file."""

    _data_type_name: ClassVar[str] = "file"

    @classmethod
    def retrieve_available_type_profiles(cls) -> Mapping[str, Mapping[str, Any]]:
        result = {}
        for ft in SUPPORTED_FILE_TYPES:
            result[f"{ft}_file"] = {"content_type": ft}
        return result

    @classmethod
    def python_class(cls) -> Type:
        return KiaraFile

    @classmethod
    def data_type_config_class(cls) -> Type[FileTypeConfig]:
        return FileTypeConfig

    def serialize(self, data: KiaraFile) -> "SerializedData":

        # metadata = orjson_dumps(data.metadata)
        # metadata_schemas = orjson_dumps(data.metadata_schema)
        _data = {
            data.file_name: {
                "type": "file",
                "codec": "raw",
                "file": data.path,
            },
            "__file_metadata__": {
                "type": "inline-json",
                "codec": "json",
                "inline_data": {
                    "file_name": data.file_name,
                    # "import_time": data.import_time,
                    "metadata": data.metadata,
                    "metadata_schemas": data.metadata_schemas,
                },
            },
        }

        serialized_data = {
            "data_type": self.data_type_name,
            "data_type_config": self.type_config.model_dump(),
            "data": _data,
            "serialization_profile": "copy",
            "metadata": {
                # "profile": "",
                "environment": {},
                "deserialize": {
                    "python_object": {
                        "module_type": "deserialize.file",
                        "module_config": {
                            "value_type": "file",
                            "target_profile": "python_object",
                            "serialization_profile": "copy",
                        },
                    }
                },
            },
        }
        from kiara.models.values.value import SerializationResult

        serialized = SerializationResult(**serialized_data)
        return serialized

    def create_model_from_python_obj(self, data: Any) -> KiaraFile:

        if isinstance(data, Mapping):
            return KiaraFile(**data)
        if isinstance(data, str):
            return KiaraFile.load_file(source=data)
        else:
            raise Exception(f"Can't create FileModel from data of type '{type(data)}'.")

    def _pretty_print_as__string(
        self, value: "Value", render_config: Mapping[str, Any]
    ) -> Any:

        data: KiaraFile = value.data
        max_lines = render_config.get("max_lines", 34)
        try:
            lines = []
            with open(data.path, "r", encoding="utf-8") as f:
                for idx, line in enumerate(f):
                    if idx > max_lines:
                        lines.append("...\n")
                        lines.append("...")
                        break
                    lines.append(line)

            # TODO: syntax highlighting
            return "\n".join(lines)
        except UnicodeDecodeError:
            # found non-text data
            lines = [
                "Binary file or non-utf8 enconding, not printing content...",
                "",
                "[b]File metadata:[/b]",
                "",
                data.model_dump_json(indent=2),
            ]
            return "\n".join(lines)

    def _pretty_print_as__terminal_renderable(
        self, value: "Value", render_config: Mapping[str, Any]
    ) -> Any:

        data: KiaraFile = value.data
        max_lines = render_config.get("max_lines", 34)
        try:
            lines = []
            with open(data.path, "r", encoding="utf-8") as f:
                for idx, line in enumerate(f):
                    if idx > max_lines:
                        lines.append("...")
                        lines.append("...")
                        break
                    lines.append(line.rstrip())

            preview: RenderableType = Group(*lines)
        except UnicodeDecodeError:
            # found non-text data
            lines = [
                "",
                "[b]File metadata:[/b]",
                "",
                data.model_dump_json(indent=2),
            ]
            preview = Panel(
                "Binary file or non-utf8 enconding, not printing content...",
                box=box.HORIZONTALS,
                padding=(2, 2),
            )

        table = Table(show_header=False, box=box.SIMPLE)
        table.add_column("key", style="i")
        table.add_column("value")

        table.add_row("Preview", preview)
        if data.metadata:
            metadata_table = create_table_from_data_and_schema(
                data=data.metadata, schema=data.metadata_schemas
            )
            table.add_row("Metadata", metadata_table)

        return table


class FileBundleValueType(AnyType[KiaraFileBundle, FileTypeConfig]):
    """A bundle of files (like a folder, zip archive, etc.)."""

    _data_type_name: ClassVar[str] = "file_bundle"

    @classmethod
    def retrieve_available_type_profiles(cls) -> Mapping[str, Mapping[str, Any]]:
        result = {}
        for ft in SUPPORTED_FILE_TYPES:
            result[f"{ft}_file_bundle"] = {"content_type": ft}
        return result

    @classmethod
    def python_class(cls) -> Type:
        return KiaraFileBundle

    @classmethod
    def data_type_config_class(cls) -> Type[FileTypeConfig]:
        return FileTypeConfig

    def serialize(self, data: KiaraFileBundle) -> "SerializedData":

        file_data: Dict[str, Any] = {}
        file_metadata = {}
        for rel_path, file in data.included_files.items():
            file_data[rel_path] = {"type": "file", "codec": "raw", "file": file.path}
            file_metadata[rel_path] = {
                "file_name": file.file_name,
                "size": file.size,
                "mime_type": file.mime_type,
                "metadata": file.metadata,
                "metadata_schemas": file.metadata_schemas,
            }

        # bundle_metadata = orjson_dumps(data.metadata)
        # bundle_metadata_schema = orjson_dumps(data.metadata_schema)
        metadata: Dict[str, Any] = {
            "included_files": file_metadata,
            "bundle_name": data.bundle_name,
            # "import_time": data.import_time,
            "size": data.size,
            "number_of_files": data.number_of_files,
            "metadata": data.metadata,
            "metadata_schemas": data.metadata_schemas,
        }

        assert "__file_metadata__" not in file_data

        file_data["__file_metadata__"] = {
            "type": "inline-json",
            "codec": "json",
            "inline_data": metadata,
        }

        serialized_data = {
            "data_type": self.data_type_name,
            "data_type_config": self.type_config.model_dump(),
            "data": file_data,
            "serialization_profile": "copy",
            "metadata": {
                "environment": {},
                "deserialize": {
                    "python_object": {
                        "module_type": "deserialize.file_bundle",
                        "module_config": {
                            "value_type": "file_bundle",
                            "target_profile": "python_object",
                            "serialization_profile": "copy",
                        },
                    }
                },
            },
        }
        from kiara.models.values.value import SerializationResult

        serialized = SerializationResult(**serialized_data)
        return serialized

    def parse_python_obj(self, data: Any) -> KiaraFileBundle:

        if isinstance(data, KiaraFileBundle):
            return data
        elif isinstance(data, str):
            return KiaraFileBundle.import_folder(source=data)
        else:
            raise Exception(
                f"Can't create FileBundle from data of type '{type(data)}'."
            )

    def _pretty_print_as__terminal_renderable(
        self, value: "Value", render_config: Mapping[str, Any]
    ) -> Any:

        bundle: KiaraFileBundle = value.data
        renderable = bundle.create_renderable(**render_config)

        table = Table(show_header=False, box=box.SIMPLE)
        table.add_column("key", style="i")
        table.add_column("value")

        table.add_row("File bundle info", renderable)
        if bundle.metadata:
            metadata_table = create_table_from_data_and_schema(
                data=bundle.metadata, schema=bundle.metadata_schemas
            )
            table.add_row("Metadata", metadata_table)

        return table

    def _pretty_print_as__string(
        self, value: "Value", render_config: Mapping[str, Any]
    ) -> Any:

        bundle: KiaraFileBundle = value.data
        result = []
        result.append(f"File bundle '{bundle.bundle_name}")
        result.append(f"  size: {humanfriendly.format_size(bundle.size)}")
        result.append("  contents:")
        for rel_path, file in bundle.included_files.items():
            result.append(f"    - {rel_path}: {file.file_name}")

        return "\n".join(result)


# kiara\kiara\src\kiara\data_types\included_core_types\metadata.py
# -*- coding: utf-8 -*-
from typing import TYPE_CHECKING, Any, ClassVar, Dict, Mapping, Type

from kiara.data_types import DataTypeConfig
from kiara.data_types.included_core_types import KiaraModelValueBaseType
from kiara.models import KiaraModel

if TYPE_CHECKING:
    from kiara.models.values.value import SerializedData


class Metadata(KiaraModel):

    _kiara_model_id: ClassVar = "instance.metadata"


class MetadataTypeConfig(DataTypeConfig):

    pass


class MetadataValueType(KiaraModelValueBaseType[Metadata, MetadataTypeConfig]):
    """A file."""

    _data_type_name: ClassVar[str] = "file"

    @classmethod
    def retrieve_available_type_profiles(cls) -> Mapping[str, Mapping[str, Any]]:
        return {}

    @classmethod
    def python_class(cls) -> Type:
        return Metadata

    @classmethod
    def data_type_config_class(cls) -> Type[MetadataTypeConfig]:
        return MetadataTypeConfig

    def serialize(self, data: Metadata) -> "SerializedData":

        # _data = {
        #     data.file_name: {
        #         "type": "file",
        #         "codec": "raw",
        #         "file": data.path,
        #     },
        #     "__file_metadata__": {
        #         "type": "inline-json",
        #         "codec": "json",
        #         "inline_data": {
        #             "file_name": data.file_name,
        #             # "import_time": data.import_time,
        #         },
        #     },
        # }
        _data: Dict[str, Any] = {}

        serialized_data = {
            "data_type": self.data_type_name,
            "data_type_config": self.type_config.model_dump(),
            "data": _data,
            "serialization_profile": "copy",
            "metadata": {
                # "profile": "",
                "environment": {},
                "deserialize": {
                    "python_object": {
                        "module_type": "deserialize.file",
                        "module_config": {
                            "value_type": "file",
                            "target_profile": "python_object",
                            "serialization_profile": "copy",
                        },
                    }
                },
            },
        }
        from kiara.models.values.value import SerializationResult

        serialized = SerializationResult(**serialized_data)
        return serialized

    def create_model_from_python_obj(self, data: Any) -> Metadata:

        # if isinstance(data, Mapping):
        #     return Metadata(**data)
        # if isinstance(data, str):
        #     return Metadata.load_file(source=data)
        # else:
        raise Exception(
            f"Can't create Metadata instance from data of type '{type(data)}'."
        )


# kiara\kiara\src\kiara\data_types\included_core_types\serialization.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)


from typing import Any, Mapping, Type

from kiara.data_types import DataTypeConfig
from kiara.data_types.included_core_types.internal import InternalType
from kiara.defaults import INVALID_HASH_MARKER, INVALID_SIZE_MARKER
from kiara.models.values.value import SerializedData, Value


class PythonObjectType(InternalType[object, DataTypeConfig]):
    """
    A 'plain' Python object.

    This data type is mostly used internally, for hading over data in (de-)serialization operations.
    """

    @classmethod
    def python_class(cls) -> Type:
        return object

    def parse_python_obj(self, data: Any) -> object:
        return data

    def calculate_hash(self, data: SerializedData) -> str:
        """Calculate the hash of the value."""
        return INVALID_HASH_MARKER

    def calculate_size(self, data: SerializedData) -> int:
        return INVALID_SIZE_MARKER

    def _pretty_print_as__terminal_renderable(
        self, value: Value, render_config: Mapping[str, Any]
    ):

        return str(value.data)


# kiara\kiara\src\kiara\data_types\included_core_types\__init__.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import abc
from typing import (
    TYPE_CHECKING,
    Any,
    ClassVar,
    Dict,
    Generic,
    Iterable,
    List,
    Mapping,
    Type,
    TypeVar,
    Union,
)

import orjson
from pydantic import BaseModel, Field
from rich import box
from rich.console import RenderableType
from rich.syntax import Syntax
from rich.table import Table

from kiara.data_types import TYPE_CONFIG_CLS, TYPE_PYTHON_CLS, DataType, DataTypeConfig
from kiara.defaults import INVALID_HASH_MARKER, SpecialValue
from kiara.exceptions import DataTypeUnknownException, KiaraProcessingException
from kiara.models import KiaraModel
from kiara.models.data_types import KiaraDict
from kiara.models.python_class import PythonClass
from kiara.models.rendering import RenderScene, RenderValueResult
from kiara.models.values import DataTypeCharacteristics
from kiara.utils.json import orjson_dumps

if TYPE_CHECKING:
    from kiara.models.module.manifest import Manifest
    from kiara.models.values.value import SerializedData, Value


SCALAR_CHARACTERISTICS = DataTypeCharacteristics(
    is_scalar=True, is_json_serializable=True
)


class NoneType(DataType[SpecialValue, DataTypeConfig]):
    """Type indicating a 'None' value."""

    _data_type_name: ClassVar[str] = "none"

    @classmethod
    def python_class(cls) -> Type:
        return SpecialValue

    # def is_immutable(self) -> bool:
    #     return False

    def calculate_hash(self, data: Any) -> str:
        return INVALID_HASH_MARKER

    def calculate_size(self, data: Any) -> int:
        return 0

    def parse_python_obj(self, data: Any) -> SpecialValue:
        return SpecialValue.NO_VALUE

    def pretty_print_as__string(
        self, value: "Value", render_config: Mapping[str, Any]
    ) -> Any:

        return "None"

    def pretty_print_as__terminal_renderable(
        self, value: "Value", render_config: Mapping[str, Any]
    ):

        return "None"


class AnyType(
    DataType[TYPE_PYTHON_CLS, TYPE_CONFIG_CLS],
    Generic[TYPE_PYTHON_CLS, TYPE_CONFIG_CLS],
):
    """
    'Any' type, the parent type for most other types.

    This type acts as the parents for all (or at least most) non-internal value types. There are some generic operations
    (like 'persist_value', or 'pretty_print') which are implemented for this type, so it's descendents have a fallback
    option in case no subtype-specific operations are implemented for it. In general, it is not recommended to use the 'any'
    type as module input or output, but it is possible. Values of type 'any' are not allowed to be persisted (at the moment,
    this might or might not change).
    """

    _data_type_name: ClassVar[str] = "any"

    @classmethod
    def python_class(cls) -> Type:
        return object

    def pretty_print_as__string(
        self, value: "Value", render_config: Mapping[str, Any]
    ) -> str:

        if hasattr(self, "_pretty_print_as__string"):
            return self._pretty_print_as__string(value=value, render_config=render_config)  # type: ignore

        try:
            return str(value.data)
        except DataTypeUnknownException as dtue:
            return str(dtue)

    def pretty_print_as__terminal_renderable(
        self, value: "Value", render_config: Mapping[str, Any]
    ) -> RenderableType:

        if hasattr(self, "_pretty_print_as__terminal_renderable"):
            return self._pretty_print_as__terminal_renderable(value=value, render_config=render_config)  # type: ignore

        try:
            data = value.data
        except DataTypeUnknownException as dtue:
            rendered: RenderableType = dtue.create_renderable(**render_config)
            from rich.panel import Panel

            return Panel(
                rendered,
                title=f"Unsupported data type: {dtue.data_type}",
                title_align="left",
            )
        except Exception as e:
            raise KiaraProcessingException(
                f"Error getting data for value '{value.value_id}': {e}"
            )

        from pydantic import BaseModel

        if isinstance(data, BaseModel):
            from kiara.utils.output import create_table_from_model_object

            rendered = create_table_from_model_object(
                model=data, render_config=render_config
            )
        elif isinstance(data, Iterable):
            import pprint

            rendered = pprint.pformat(data)
        else:
            rendered = str(data)
        return rendered

    def render_as__string(
        self, value: "Value", render_config: Mapping[str, Any], manifest: "Manifest"
    ) -> str:
        if hasattr(self, "_render_as__string"):
            return self._render_as__string(value=value, render_scene=render_config, manifest=manifest)  # type: ignore
        else:
            return self.pretty_print_as__string(value=value, render_config={})

    def render_as__terminal_renderable(
        self, value: "Value", render_config: Mapping[str, Any], manifest: "Manifest"
    ) -> RenderableType:

        if not hasattr(self, "_render_as__terminal_renderable"):

            try:
                value.data
                return self.render_as__string(
                    value=value, render_config=render_config, manifest=manifest
                )
            except DataTypeUnknownException:
                return self.pretty_print_as__terminal_renderable(
                    value=value, render_config=render_config
                )

        else:
            return self._render_as__terminal_renderable(value=value, render_config=render_config, manifest=manifest)  # type: ignore


class BytesType(AnyType[bytes, DataTypeConfig]):
    """An array of bytes."""

    _data_type_name: ClassVar[str] = "bytes"

    @classmethod
    def python_class(cls) -> Type:
        return bytes

    def serialize(self, data: bytes) -> "SerializedData":

        _data = {"bytes": {"type": "chunk", "chunk": data, "codec": "raw"}}

        serialized_data = {
            "data_type": self.data_type_name,
            "data_type_config": self.type_config.model_dump(),
            "data": _data,
            "serialization_profile": "raw",
            "metadata": {
                "environment": {},
                "deserialize": {
                    "python_object": {
                        "module_name": "load.bytes",
                        "module_config": {
                            "value_type": "bytes",
                            "target_profile": "python_object",
                            "serialization_profile": "raw",
                        },
                    }
                },
            },
        }
        from kiara.models.values.value import SerializationResult

        serialized = SerializationResult(**serialized_data)
        return serialized

    def _pretty_print_as__string(
        self, value: "Value", render_config: Mapping[str, Any]
    ) -> Any:

        data: bytes = value.data
        return data.decode()


class StringTypeConfig(DataTypeConfig):

    allowed_strings: Union[None, List[str]] = Field(
        description="A list of allowed strings, if empty or None, any string is allowed.",
        default=None,
    )


class StringType(AnyType[str, StringTypeConfig]):
    """A string.

    Can be configured to only allow a list of specific strings by using the `allowed_strings` configuration option.
    """

    _data_type_name: ClassVar[str] = "string"

    @classmethod
    def data_type_config_class(cls) -> Type[TYPE_CONFIG_CLS]:
        """The Python class that holds the (optional) configuration for a data type instance."""
        return StringTypeConfig  # type: ignore

    @classmethod
    def python_class(cls) -> Type:
        return str

    def serialize(self, data: str) -> "SerializedData":

        _data = {
            "string": {"type": "chunk", "chunk": data.encode("utf-8"), "codec": "raw"}
        }

        serialized_data = {
            "data_type": self.data_type_name,
            "data_type_config": self.type_config.model_dump(),
            "data": _data,
            "serialization_profile": "raw",
            "metadata": {
                "environment": {},
                "deserialize": {
                    "python_object": {
                        "module_type": "load.string",
                        "module_config": {
                            "value_type": "string",
                            "target_profile": "python_object",
                            "serialization_profile": "raw",
                        },
                    }
                },
            },
        }
        from kiara.models.values.value import SerializationResult

        serialized = SerializationResult(**serialized_data)
        return serialized

    def _retrieve_characteristics(self) -> DataTypeCharacteristics:
        return SCALAR_CHARACTERISTICS

    def parse_python_obj(self, data: Any) -> str:

        # TODO: check if this is actually ok to do always
        return str(data)

    def _validate(self, value: Any) -> None:

        if not isinstance(value, str):
            raise ValueError(f"Invalid type '{type(value)}': string required")

        if (
            self.type_config.allowed_strings
            and value not in self.type_config.allowed_strings
        ):
            raise ValueError(
                f"Invalid value '{value}': not in allowed values {self.type_config.allowed_strings}"
            )

    def pretty_print_as__bytes(self, value: "Value", render_config: Mapping[str, Any]):
        value_str: str = value.data
        return value_str.encode()


class BooleanType(AnyType[bool, DataTypeConfig]):

    "A boolean."

    _data_type_name: ClassVar[str] = "boolean"

    @classmethod
    def python_class(cls) -> Type:
        return bool

    def serialize(self, data: bool) -> "SerializedData":
        result = self.serialize_as_json(data)
        return result

    def _retrieve_characteristics(self) -> DataTypeCharacteristics:
        return SCALAR_CHARACTERISTICS

    # def calculate_size(self, data: bool) -> int:
    #     return 24
    #
    # def calculate_hash(cls, data: bool) -> int:
    #     return 1 if data else 0

    def parse_python_obj(self, data: Any) -> bool:

        if data is True or data is False:
            return data
        elif data == 0:
            return False
        elif data == 1:
            return True
        elif isinstance(data, str):
            if data.lower() == "true":
                return True
            elif data.lower() == "false":
                return False
        raise Exception(f"Can't parse value '{data}' as boolean.")

    def validate(cls, value: Any):
        pass


class DictValueType(AnyType[KiaraDict, DataTypeConfig]):
    """
    A dictionary.

    In addition to the actual dictionary value, this value type comes also with an optional schema, describing the
    dictionary. In case no schema was attached, a simple generic one is attached. This data type is backed by the
    [DictModel][kiara_plugin.core_types.models.DictModel] class.
    """

    _data_type_name: ClassVar[str] = "dict"

    @classmethod
    def python_class(cls) -> Type:
        return KiaraDict

    # def calculate_size(self, data: DictModel) -> int:
    #     return data.size
    #
    # def calculate_hash(self, data: DictModel) -> int:
    #     return data.value_hash

    def _retrieve_characteristics(self) -> DataTypeCharacteristics:
        return DataTypeCharacteristics(is_scalar=False, is_json_serializable=True)

    def parse_python_obj(self, data: Any) -> KiaraDict:

        python_cls = data.__class__
        dict_data = None
        schema = None

        if isinstance(data, Mapping):

            if (
                len(data) == 3
                and "dict_data" in data.keys()
                and "data_schema" in data.keys()
                and "python_class" in data.keys()
            ):
                dict_model = KiaraDict(
                    dict_data=data["dict_data"],
                    data_schema=data["data_schema"],
                    python_class=data["python_class"],
                )
                return dict_model

            schema = {"title": "dict", "type": "object"}
            dict_data = data
        elif isinstance(data, BaseModel):
            dict_data = data.model_dump()
            schema = data.model_json_schema()
        elif isinstance(data, str):
            try:
                dict_data = orjson.loads(data)
                schema = {"title": "dict", "type": "object"}
            except Exception:
                try:
                    from kiara.utils.cli import dict_from_cli_args

                    tokens = data.split(",")
                    dict_data = dict_from_cli_args(*tokens)
                    schema = {"title": "dict", "type": "object"}
                except Exception:
                    pass

        if dict_data is None or schema is None:
            raise Exception(f"Invalid data for value type 'dict': {data}")

        result = {
            "dict_data": dict_data,
            "data_schema": schema,
            "python_class": PythonClass.from_class(python_cls).model_dump(),
        }
        return KiaraDict(**result)

    def _validate(self, data: KiaraDict) -> None:

        if not isinstance(data, KiaraDict):
            raise Exception(f"Invalid type: {type(data)}.")

    # def render_as__string(self, value: Value, render_config: Mapping[str, Any]) -> str:
    #
    #     data: DictModel = value.data
    #     return orjson_dumps(data.dict_data, option=orjson.OPT_INDENT_2)

    def _pretty_print_as__terminal_renderable(
        self, value: "Value", render_config: Mapping[str, Any]
    ):

        show_schema = render_config.get("show_schema", True)

        table = Table(show_header=False, box=box.SIMPLE)
        table.add_column("key", style="i")
        table.add_column("value")

        data: KiaraDict = value.data
        data_json = orjson_dumps(
            data.dict_data, option=orjson.OPT_INDENT_2 | orjson.OPT_NON_STR_KEYS
        )
        table.add_row(
            "dict data", Syntax(data_json, "json", background_color="default")
        )

        if show_schema:
            schema_json = orjson_dumps(data.data_schema, option=orjson.OPT_INDENT_2)
            table.add_row(
                "dict schema", Syntax(schema_json, "json", background_color="default")
            )

        return table

    def serialize(self, data: KiaraDict) -> "SerializedData":

        result = self.serialize_as_json(data.model_dump())
        return result

    def render_as__terminal_renderable(
        self, value: "Value", render_config: Mapping[str, Any], manifest: "Manifest"
    ) -> RenderableType:

        render_item = render_config.get("render_item", "data")
        width = render_config.get("display_width", 0)

        related_scenes: Dict[str, Union[None, RenderScene]] = {}
        if render_item == "data":
            dict_data = value.data.dict_data
            json_string = orjson_dumps(dict_data, option=orjson.OPT_INDENT_2)

            if width > 0:
                new_lines = []
                for line in json_string.split("\n"):
                    if len(line) > width:
                        new_lines.append(line[0 : width - 3] + "...")
                    else:
                        new_lines.append(line)

            json_string = "\n".join(new_lines)

            rendered = Syntax(json_string, "json")
            related_scenes["data"] = None
            related_scenes["schema"] = RenderScene(
                title="schema",
                description="The (json) schema for the data.",
                manifest_hash=manifest.manifest_hash,
                render_config={"render_item": "schema"},
            )

        elif render_item == "schema":
            schema = value.data.data_schema
            json_string = orjson_dumps(schema, option=orjson.OPT_INDENT_2)

            rendered = Syntax(json_string, "json")
            related_scenes["data"] = RenderScene(
                title="data",
                description="The actual data of the dictionary.",
                manifest_hash=manifest.manifest_hash,
                render_config={"render_item": "data"},
            )
            related_scenes["schema"] = None

        else:
            raise KiaraProcessingException(
                f"Invalid render item '{render_item}', allowed: 'data', 'schema'."
            )

        result = RenderValueResult(
            value_id=value.value_id,
            render_config=render_config,
            render_manifest=manifest.manifest_hash,
            related_scenes=related_scenes,
            manifest_lookup={manifest.manifest_hash: manifest},
            rendered=rendered,
        )

        return result


KIARA_MODEL_CLS = TypeVar("KIARA_MODEL_CLS", bound=KiaraModel)


class KiaraModelValueBaseType(
    AnyType[KIARA_MODEL_CLS, TYPE_CONFIG_CLS], Generic[KIARA_MODEL_CLS, TYPE_CONFIG_CLS]
):
    """
    A value type that is used internally.

    This type should not be used by user-facing modules and/or operations.
    """

    _data_type_name: ClassVar[str] = None  # type: ignore

    @classmethod
    def data_type_config_class(cls) -> Type[TYPE_CONFIG_CLS]:
        return DataTypeConfig  # type: ignore

    @abc.abstractmethod
    def create_model_from_python_obj(self, data: Any) -> KIARA_MODEL_CLS:
        pass

    def parse_python_obj(self, data: Any) -> KIARA_MODEL_CLS:

        if isinstance(data, self.__class__.python_class()):
            return data  # type: ignore

        _data = self.create_model_from_python_obj(data)
        return _data

    def _validate(self, data: KiaraModel) -> None:

        if not isinstance(data, self.__class__.python_class()):
            raise Exception(
                f"Invalid type '{type(data)}', must be: {self.__class__.python_class().__name__}, or subclass."
            )


# kiara\kiara\src\kiara\data_types\included_core_types\internal\render_value.py
# -*- coding: utf-8 -*-
from typing import TYPE_CHECKING, Any, ClassVar, Mapping, Type, Union

from pydantic import Field
from rich import box
from rich.syntax import Syntax
from rich.table import Table

from kiara.data_types import DataTypeConfig
from kiara.data_types.included_core_types.internal import InternalType
from kiara.models.rendering import RenderScene, RenderValueResult
from kiara.utils.class_loading import find_all_kiara_model_classes
from kiara.utils.output import extract_renderable

if TYPE_CHECKING:
    from kiara.models.values.value import Value


class RenderSceneTypeConfig(DataTypeConfig):

    kiara_model_id: Union[str, None] = Field(
        description="The id of the model backing this render (Python class must sub-class 'RenderScene').",
        default=None,
    )


class RenderSceneDataType(InternalType[RenderScene, RenderSceneTypeConfig]):
    """A value type to contain information about how to render a value in a specific render scenario."""

    _data_type_name: ClassVar[str] = "render_scene"

    def __init__(self, **type_config: Any):

        self._cls_cache: Union[Type[RenderScene], None] = None
        super().__init__(**type_config)

    @classmethod
    def python_class(cls) -> Type:
        return RenderScene

    @classmethod
    def data_type_config_class(cls) -> Type[RenderSceneTypeConfig]:
        return RenderSceneTypeConfig

    @property
    def model_cls(self) -> Type[RenderScene]:

        if self._cls_cache is not None:
            return self._cls_cache

        kiara_model_id = self.type_config.kiara_model_id
        if not kiara_model_id:
            kiara_model_id = RenderScene._kiara_model_id

        if kiara_model_id == RenderScene._kiara_model_id:
            model_cls = RenderScene
        else:
            all_models = find_all_kiara_model_classes()
            if kiara_model_id not in all_models.keys():
                raise Exception(f"Invalid model id: {kiara_model_id}")
            # TODO: check type is right?
            model_cls = all_models[kiara_model_id]  # type: ignore

        assert issubclass(model_cls, RenderScene)
        self._cls_cache = model_cls
        return self._cls_cache

    def parse_python_obj(self, data: Any) -> RenderScene:

        if isinstance(data, RenderScene):
            return data
        elif isinstance(data, Mapping):
            return self.model_cls(**data)
        else:
            raise ValueError(
                f"Can't parse data, invalid type '{type(data)}': must be subclass of 'KiaraModel' or Mapping."
            )

    def _validate(self, value: RenderScene) -> None:

        if not isinstance(value, RenderScene):
            raise Exception(f"Invalid type: {type(value)}.")

    def _pretty_print_as__terminal_renderable(
        self, value: "Value", render_config: Mapping[str, Any]
    ) -> Any:

        data: RenderScene = value.data

        ri_json = data.model_dump_json(indent=2)
        return Syntax(ri_json, "json", background_color="default")


class RenderValueResultDataType(InternalType[RenderValueResult, DataTypeConfig]):
    """A value type to contain information about how to render a value in a specific render scenario."""

    _data_type_name: ClassVar[str] = "render_value_result"

    def __init__(self, **type_config: Any):

        self._cls_cache: Union[Type[RenderValueResult], None] = None
        super().__init__(**type_config)

    @classmethod
    def python_class(cls) -> Type:
        return RenderValueResult

    def parse_python_obj(self, data: Any) -> RenderValueResult:

        if data is None:
            raise ValueError(
                "Can't parse render_scene_result data: no source data provided (None)."
            )
        elif isinstance(data, RenderValueResult):
            return data
        elif isinstance(data, Mapping):
            return RenderValueResult(**data)
        else:
            raise ValueError(
                f"Can't parse data, invalid type '{type(data)}': must be subclass of 'RenderValueResult' or Mapping."
            )

    def _validate(self, value: Any) -> None:

        if not isinstance(value, RenderValueResult):
            raise Exception(f"Invalid type: {type(value)}.")

    def _pretty_print_as__terminal_renderable(
        self, value: "Value", render_config: Mapping[str, Any]
    ) -> Any:

        data: RenderValueResult = value.data

        ri_json = data.model_dump_json(indent=2, exclude={"rendered"})
        rendered = extract_renderable(data.rendered)

        metadata = Syntax(ri_json, "json", background_color="default")
        table = Table(show_header=True, box=box.SIMPLE)
        table.add_column("Rendered item")
        table.add_column("Render metadata")
        table.add_row(rendered, metadata)
        return table


# kiara\kiara\src\kiara\data_types\included_core_types\internal\__init__.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)
from typing import TYPE_CHECKING, Any, ClassVar, Generic, Iterable, Mapping, Type, Union

import structlog
from pydantic import Field, PrivateAttr
from rich import box
from rich.console import RenderableType
from rich.panel import Panel
from rich.syntax import Syntax
from rich.table import Table

from kiara.data_types import TYPE_CONFIG_CLS, TYPE_PYTHON_CLS, DataType, DataTypeConfig
from kiara.defaults import NO_SERIALIZATION_MARKER
from kiara.models import KiaraModel
from kiara.models.documentation import DocumentationMetadataModel
from kiara.models.python_class import PythonClass
from kiara.models.values.value import SerializedData, Value

if TYPE_CHECKING:
    from kiara.models.module.manifest import Manifest
    from kiara.models.rendering import RenderScene

logger = structlog.getLogger()


class InternalType(
    DataType[TYPE_PYTHON_CLS, TYPE_CONFIG_CLS],
    Generic[TYPE_PYTHON_CLS, TYPE_CONFIG_CLS],
):
    """'A 'marker' base data type for data types that are (mainly) used internally in kiara.."""

    _data_type_name: ClassVar[str] = "internal"

    @classmethod
    def python_class(cls) -> Type:
        return object

    def pretty_print_as__string(
        self, value: "Value", render_config: Mapping[str, Any]
    ) -> Any:

        if hasattr(self, "_pretty_print_as__string"):
            return self._pretty_print_as_string(value=value, render_config=render_config)  # type: ignore

        return str(value.data)

    def pretty_print_as__terminal_renderable(
        self, value: "Value", render_config: Mapping[str, Any]
    ) -> RenderableType:

        if hasattr(self, "_pretty_print_as__terminal_renderable"):
            return self._pretty_print_as__terminal_renderable(value=value, render_config=render_config)  # type: ignore

        data: Any = value.data

        from pydantic import BaseModel

        if isinstance(data, BaseModel):
            from kiara.utils.output import create_table_from_model_object

            rendered: RenderableType = create_table_from_model_object(
                model=data, render_config=render_config
            )
        elif isinstance(data, Iterable):
            import pprint

            rendered = pprint.pformat(data)
        else:
            rendered = str(data)
        return rendered

    def render_as__string(
        self, value: "Value", render_config: "RenderScene", manifest: "Manifest"
    ):

        if hasattr(self, "_render_as__string"):
            return self._render_as__string(value=value, render_config=render_config, manifest=manifest)  # type: ignore
        else:
            return self.pretty_print_as__string(value=value, render_config={})

    def render_as__terminal_renderable(
        self, value: "Value", render_config: "RenderScene", manifest: "Manifest"
    ):

        if hasattr(self, "_render_as__terminal_renderable"):
            return self._render_as__terminal(value=value, render_config=render_config, manifest=manifest)  # type: ignore
        return self.render_as__string(
            value=value, render_config=render_config, manifest=manifest
        )


class TerminalRenderable(InternalType[object, DataTypeConfig]):
    """
    A list of renderable objects, used in the 'rich' Python library, to print to the terminal or in Jupyter.

    Internally, the result list items can be either a string, a 'rich.console.ConsoleRenderable', or a 'rich.console.RichCast'.
    """

    _data_type_name: ClassVar[str] = "terminal_renderable"

    @classmethod
    def python_class(cls) -> Type:
        return object

    def _pretty_print_as__terminal_renderable(
        self, value: "Value", render_config: Mapping[str, Any]
    ) -> Any:

        renderable = value.data

        table = Table(show_header=False, show_lines=False, box=box.SIMPLE)
        table.add_column("key", style="i")
        table.add_column("value")
        cls = PythonClass.from_class(renderable.__class__)
        table.add_row("python class", cls)
        table.add_row("preview", Panel(renderable, height=20))

        return table


class InternalModelTypeConfig(DataTypeConfig):

    kiara_model_id: Union[str, None] = Field(
        description="The Python class backing this model (must sub-class 'KiaraModel')."
    )


class InternalModelValueType(InternalType[KiaraModel, InternalModelTypeConfig]):
    """
    A value type that is used internally.

    This type should not be used by user-facing modules and/or operations.
    """

    _data_type_name: ClassVar[str] = "internal_model"
    _cls_cache: Union[Type[KiaraModel], None] = PrivateAttr(default=None)

    @classmethod
    def data_type_config_class(cls) -> Type[InternalModelTypeConfig]:
        return InternalModelTypeConfig  # type: ignore

    def serialize(self, data: KiaraModel) -> Union[str, SerializedData]:

        if self.type_config.kiara_model_id is None:
            logger.debug(
                "ignore.serialize_request",
                data_type="internal_model",
                cls=data.__class__.__name__,
                reason="no model id in module config",
            )
            return NO_SERIALIZATION_MARKER

        _data = {
            "data": {
                "type": "inline-json",
                "inline_data": data.model_dump(),
                "codec": "json",
            },
        }

        serialized_data = {
            "data_type": self.data_type_name,
            "data_type_config": self.type_config.model_dump(),
            "data": _data,
            "serialization_profile": "json",
            "metadata": {
                "environment": {},
                "deserialize": {
                    "python_object": {
                        "module_type": "load.internal_model",
                        "module_config": {
                            "value_type": "internal_model",
                            "target_profile": "python_object",
                            "serialization_profile": "json",
                        },
                    }
                },
            },
        }
        from kiara.models.values.value import SerializationResult

        serialized = SerializationResult(**serialized_data)
        return serialized

    @classmethod
    def python_class(cls) -> Type:

        return KiaraModel

    # @property
    # def model_cls(self) -> Type[KiaraModel]:
    #
    #     if self._cls_cache is not None:
    #         return self._cls_cache
    #
    #     model_type_id = self.type_config.kiara_model_id
    #     assert model_type_id is not None
    #
    #     model_registry = ModelRegistry.instance()
    #
    #     model_cls = model_registry.get_model_cls(
    #         model_type_id, required_subclass=KiaraModel
    #     )
    #
    #     self._cls_cache = model_cls
    #     return self._cls_cache

    def parse_python_obj(self, data: Any) -> KiaraModel:

        if isinstance(data, KiaraModel):
            return data
        # elif isinstance(data, Mapping):
        #     return self.model_cls(**data)
        else:
            raise ValueError(
                f"Can't parse data, invalid type '{type(data)}': must be subclass of 'KiaraModel' or Mapping."
            )

    def _validate(self, value: KiaraModel) -> None:

        if not isinstance(value, KiaraModel):
            raise Exception(f"Invalid type: {type(value)}.")

    def _pretty_print_as__terminal_renderable(
        self, value: "Value", render_config: Mapping[str, Any]
    ):

        data: KiaraModel = value.data
        json_str = data.model_dump_json(indent=2)
        return Syntax(json_str, "json", background_color="default")


class DocumentationModelValueType(InternalModelValueType):
    """Documentation for an internal entity."""

    _data_type_name: ClassVar[str] = "doc"

    def parse_python_obj(self, data: Any) -> DocumentationMetadataModel:

        return DocumentationMetadataModel.create(data)

    @classmethod
    def python_class(cls) -> Type:
        return DocumentationMetadataModel

    def _pretty_print_as__terminal_renderable(
        self, value: "Value", render_config: Mapping[str, Any]
    ):

        data: DocumentationMetadataModel = value.data
        json_str = data.model_dump_json(indent=2)
        return Syntax(json_str, "json", background_color="default")


# kiara\kiara\src\kiara\doc\generate_api_doc.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2020-2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)


import importlib
import os
import typing
from pathlib import Path
from types import ModuleType


def gen_pages_for_module(
    module: typing.Union[str, ModuleType], prefix: str = "api_reference"
):
    """Generate modules for a set of modules (using the [mkdocstring](https://github.com/mkdocstrings/mkdocstrings) package."""
    result = {}
    modules_info = get_source_tree(module)
    for module_name, path in modules_info.items():

        page_name = module_name

        if page_name.endswith("__init__"):
            page_name = page_name[0:-9]
        if page_name.endswith("._frkl"):
            continue

        doc_path = f"{prefix}{os.path.sep}{page_name}.md"
        p = Path("..", path["abs_path"])
        if not p.read_text().strip():
            continue

        main_module = path["main_module"]
        if page_name == main_module:
            title = page_name
        else:
            title = page_name.replace(f"{main_module}.", "➜ ")  # noqa

        result[doc_path] = {
            "python_src": path,
            "content": f"---\ntitle: {title}\n---\n# {page_name}\n\n::: {module_name}",
        }

    return result


def get_source_tree(module: typing.Union[str, ModuleType]):
    """Find all python source files for a module."""
    if isinstance(module, str):
        module = importlib.import_module(module)

    if not isinstance(module, ModuleType):
        raise TypeError(
            f"Invalid type '{type(module)}', input needs to be a string or module."
        )

    module_file = module.__file__
    assert module_file is not None
    module_root = os.path.dirname(module_file)
    module_name = module.__name__

    src = {}

    for path in Path(module_root).glob("**/*.py"):

        rel = os.path.relpath(path, module_root)
        mod_name = f"{module_name}.{rel[0:-3].replace(os.path.sep, '.')}"
        rel_path = f"{module_name}{os.path.sep}{rel}"
        src[mod_name] = {
            "rel_path": rel_path,
            "abs_path": path,
            "main_module": module_name,
        }

    return src


# kiara\kiara\src\kiara\doc\gen_info_pages.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2022, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)
import os
from typing import Mapping

import mkdocs_gen_files
from jinja2 import PackageLoader

from kiara.context import KiaraContextInfo

# from kiara.defaults import KIARA_RESOURCES_FOLDER
from kiara.interfaces.python_api.models.info import ItemInfo

_jinja_env = None


def get_jina_env():

    global _jinja_env
    if _jinja_env is None:
        from jinja2 import Environment

        _loader = PackageLoader(
            package_name="kiara",
            package_path=os.path.join("resources", "templates", "doc_gen"),
            encoding="utf8",
        )
        _jinja_env = Environment(loader=_loader, autoescape=True)
        # _jinja_env = Environment(
        #     loader=FileSystemLoader(
        #         os.path.join(KIARA_RESOURCES_FOLDER, "templates", "doc_gen"),
        #         encoding="utf8",
        #     )
        # )
    return _jinja_env


def render_item_listing(
    item_type: str, items: Mapping[str, ItemInfo], sub_path: str = "info"
):

    list_template = get_jina_env().get_template("info_listing.j2")

    render_args = {"items": items, "item_type": item_type}
    rendered = list_template.render(**render_args)

    path = f"{sub_path}/{item_type}.md"
    with mkdocs_gen_files.open(path, "w") as f:
        f.write(rendered)

    return path


def generate_detail_pages(
    context_info: KiaraContextInfo,
    sub_path: str = "info",
    add_summary_page: bool = False,
):

    pages = {}
    summary = []

    all_info = context_info.get_all_info(skip_empty_types=True)

    for item_type, items_info in all_info.items():
        summary.append(f"* [{item_type}]({item_type}.md)")
        path = render_item_listing(
            item_type=item_type, items=items_info.item_infos, sub_path=sub_path
        )
        pages[item_type] = path

    if summary:
        if add_summary_page:
            summary.insert(0, "* [Summary](index.md)")

        with mkdocs_gen_files.open(f"{sub_path}/SUMMARY.md", "w") as f:
            f.write("\n".join(summary))

    return pages


# kiara\kiara\src\kiara\doc\mkdocs_macros_cli.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2020-2021, Markus Binsteiner
#
#  Mozilla Public License Version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import copy
import os
import sys
from pathlib import Path
from subprocess import PIPE, Popen
from timeit import default_timer as timer
from typing import Dict, Mapping, Union

import orjson
from deepdiff import DeepHash

from kiara.defaults import kiara_app_dirs

KIARA_DOC_BUILD_CACHE_DIR = os.path.join(kiara_app_dirs.user_cache_dir, "doc_gen")
if not os.path.exists(KIARA_DOC_BUILD_CACHE_DIR):
    os.makedirs(KIARA_DOC_BUILD_CACHE_DIR)

os_env_vars = copy.deepcopy(os.environ)
os_env_vars["CONSOLE_WIDTH"] = "80"
os_env_vars["KAIRA_DATA_STORE"] = os.path.join(
    kiara_app_dirs.user_cache_dir, "data_store_1"
)


def log_msg(msg: str):
    print(msg)  # noqa


def define_env(env):
    """
    Helper macros for Python project documentation.

    Currently, those macros are available (check the source code for more details):

    ## ``cli``

    Execute a command on the command-line, capture the output and return it to be used in a documentation page.

    ## ``inline_file_as_codeblock``

    Read an external file, and return its content as a markdown code block.
    """
    # env.variables["baz"] = "John Doe"

    @env.macro
    def cli(
        *command,
        print_command: bool = True,
        code_block: bool = True,
        split_command_and_output: bool = True,
        max_height: Union[int, None] = None,
        cache_key: Union[str, None] = None,
        extra_env: Union[Dict[str, str], None] = None,
        fake_command: Union[str, None] = None,
        fail_ok: bool = False,
        repl_dict: Union[Mapping[str, str], None] = None,
    ):
        """Execute the provided command, save the output and return it to be used in documentation modules."""
        hashes = DeepHash(command)
        hash_str = hashes[command]
        hashes_env = DeepHash(extra_env)
        hashes_env_str = hashes_env[extra_env]

        hash_str = hash_str + "_" + hashes_env_str
        if cache_key:
            hash_str = hash_str + "_" + cache_key

        cache_file: Path = Path(os.path.join(KIARA_DOC_BUILD_CACHE_DIR, str(hash_str)))
        failed_cache_file: Path = Path(
            os.path.join(KIARA_DOC_BUILD_CACHE_DIR, f"{hash_str}.failed")
        )
        cache_info_file: Path = Path(
            os.path.join(KIARA_DOC_BUILD_CACHE_DIR), f"{hash_str}.command"
        )

        _run_env = dict(os_env_vars)
        if extra_env:
            _run_env.update(extra_env)

        if cache_file.is_file():
            stdout_str = cache_file.read_text()
            if repl_dict:
                for k, v in repl_dict.items():
                    stdout_str = stdout_str.replace(k, v)
        else:
            start = timer()

            cache_info = {
                "command": command,
                "extra_env": extra_env,
                "cmd_hash": hash_str,
                "cache_key": cache_key,
                "fail_ok": fail_ok,
                "started": start,
                "repl_dict": repl_dict,
            }

            log_msg(f"RUNNING: {' '.join(command)}")

            p = Popen(command, stdout=PIPE, stderr=PIPE, env=_run_env)
            stdout, stderr = p.communicate()

            stdout_str = stdout.decode("utf-8")
            stderr_str = stderr.decode("utf-8")

            if repl_dict:
                for k, v in repl_dict.items():
                    stdout_str = stdout_str.replace(k, v)
                    stderr_str = stderr_str.replace(k, v)

            log_msg("stdout:")
            log_msg(stdout_str)
            log_msg("stderr:")
            log_msg(stderr_str)

            cache_info["exit_code"] = p.returncode

            end = timer()
            if p.returncode == 0:

                # result = subprocess.check_output(command, env=_run_env)

                # stdout = result.decode()
                cache_file.write_bytes(stdout)
                cache_info["size"] = len(stdout)
                cache_info["duration"] = end - start
                cache_info["success"] = True
                cache_info["output_file"] = cache_file.as_posix()
                cache_info_file.write_bytes(orjson.dumps(cache_info))

                if failed_cache_file.exists():
                    failed_cache_file.unlink()
            else:

                cache_info["duration"] = end - start

                if fail_ok:
                    cache_info["size"] = len(stdout)
                    cache_info["success"] = True
                    cache_file.write_bytes(stdout)
                    cache_info["output_file"] = cache_file.as_posix()
                    cache_info_file.write_bytes(orjson.dumps(cache_info))
                    if failed_cache_file.exists():
                        failed_cache_file.unlink()
                else:
                    cache_info["size"] = len(stdout)
                    cache_info["success"] = False
                    failed_cache_file.write_bytes(stdout)
                    cache_info["output_file"] = failed_cache_file.as_posix()
                    cache_info_file.write_bytes(orjson.dumps(cache_info))
                    # stdout = f"Error: {e}\n\nStdout: {e.stdout}\n\nStderr: {e.stderr}"
                    # cache_info["size"] = len(stdout)
                    # cache_info["success"] = False
                    # print("stdout:")
                    # print(e.stdout)
                    # print("stderr:")
                    # print(e.stderr)
                    # failed_cache_file.write_text(stdout)
                    # cache_info["output_file"] = failed_cache_file.as_posix()
                    # cache_info_file.write_bytes(orjson.dumps(cache_info))
                    if os.getenv("FAIL_DOC_BUILD_ON_ERROR") == "true":
                        sys.exit(1)

        if fake_command:
            command_str = fake_command
        else:
            command_str = " ".join(command)

        if split_command_and_output and print_command:
            _c = f"\n```bash\n{command_str}\n```\n"
            _output = "```\n" + stdout_str + "\n```\n"
            if max_height is not None and max_height > 0:
                _output = f"<div style='max-height:{max_height}px;overflow:auto'>\n{_output}\n</div>"
            _stdout = _c + _output
        else:
            if print_command:
                _stdout = f"> {command_str}\n{stdout_str}"
            if code_block:
                _stdout = "```\n" + _stdout + "\n```\n"

            if max_height is not None and max_height > 0:
                _stdout = f"<div style='max-height:{max_height}px;overflow:auto'>\n{_stdout}\n</div>"

        return _stdout

    @env.macro
    def inline_file_as_codeblock(path, format: str = ""):
        """Import external file and return its content as a markdown code block."""
        f = Path(path)
        return f"```{format}\n{f.read_text()}\n```"


# kiara\kiara\src\kiara\doc\mkdocs_macros_kiara.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import builtins
import inspect
from pydoc import locate
from typing import Any, Type, Union

from pydantic import BaseModel

from kiara.context import Kiara, KiaraContextInfo
from kiara.utils.yaml import StringYAML

yaml = StringYAML()
kiara_obj = Kiara.instance()


def define_env(env):
    """
    This is the hook for defining variables, macros and filters.

    - variables: the dictionary that contains the environment variables
    - macro: a decorator function, to declare a macro.
    """
    # env.variables["baz"] = "John Doe"

    @env.macro
    def get_schema_for_model(model_class: Union[str, Type[BaseModel]]):

        if isinstance(model_class, str):
            _class: Type[BaseModel] = locate(model_class)  # type: ignore
        else:
            _class = model_class

        schema_json = _class.schema_json(indent=2)

        return schema_json

    @env.macro
    def get_src_of_object(obj: Union[str, Any]):

        try:
            if isinstance(obj, str):
                _obj: Type[BaseModel] = locate(obj)  # type: ignore
            else:
                _obj = obj

            src = inspect.getsource(_obj)
            return src
        except Exception as e:
            return f"Can't render object source: {e}"

    @env.macro
    def get_context_info() -> KiaraContextInfo:

        return builtins.plugin_package_context_info  # type: ignore

    # @env.macro
    # def get_module_info(module_type: str):
    #
    #     try:
    #
    #         m_cls = Kiara.instance().module_registry.get_module_class(module_type)
    #         info = KiaraModuleTypeInfo.from_module_class(m_cls)
    #
    #         from rich.console import Console
    #
    #         console = Console(record=True)
    #         console.print(info)
    #
    #         html = console.export_text()
    #         return html
    #     except Exception as e:
    #         return f"Can't render module info: {str(e)}"
    #
    # @env.macro
    # def get_info_item_list_for_category(
    #     category: str, limit_to_package: typing.Optional[str] = None
    # ) -> typing.Dict[str, KiaraInfoModel]:
    #     return _get_info_item_list_for_category(
    #         category=category, limit_to_package=limit_to_package
    #     )
    #
    # def _get_info_item_list_for_category(
    #     category: str, limit_to_package: typing.Optional[str] = None
    # ) -> typing.Dict[str, KiaraInfoModel]:
    #
    #     infos = kiara_context.find_subcomponents(category=category)
    #
    #     if limit_to_package:
    #         temp = {}
    #         for n_id, obj in infos.items():
    #             if obj.context.labels.get("package", None) == limit_to_package:
    #                 temp[n_id] = obj
    #         infos = temp
    #
    #     docs = {}
    #     for n_id, obj in infos.items():
    #         docs[obj.get_id()] = obj.documentation.description
    #
    #     return docs
    #
    # @env.macro
    # def get_info_for_categories(
    #     *categories: str, limit_to_package: typing.Optional[str] = None
    # ):
    #
    #     TITLE_MAP = {
    #         "metadata.module": "Modules",
    #         "metadata.pipeline": "Pipelines",
    #         "metadata.type": "Value data_types",
    #         "metadata.operation_type": "Operation data_types",
    #     }
    #     result = {}
    #     for cat in categories:
    #         infos = _get_info_item_list_for_category(
    #             cat, limit_to_package=limit_to_package
    #         )
    #         if infos:
    #             result[cat] = {"items": infos, "title": TITLE_MAP[cat]}
    #
    #     return result
    #
    # @env.macro
    # def get_module_list_for_package(
    #     package_name: str,
    #     include_core_modules: bool = True,
    #     include_pipelines: bool = True,
    # ):
    #
    #     modules = kiara_obj.module_registry.find_modules_for_package(
    #         package_name,
    #         include_core_modules=include_core_modules,
    #         include_pipelines=include_pipelines,
    #     )
    #
    #     result = []
    #     for name, info in modules.items():
    #         type_md = info.get_type_metadata()
    #         result.append(
    #             f"[``{name}``][kiara_info.modules.{name}]: {type_md.documentation.description}"
    #         )
    #
    #     return result
    #
    # @env.macro
    # def get_data_types_for_package(package_name: str):
    #
    #     data_types = kiara_obj.type_registry.find_data_type_classes_for_package(
    #         package_name
    #     )
    #     result = []
    #     for name, info in data_types.items():
    #         type_md = info.get_type_metadata()
    #         result.append(f"  - ``{name}``: {type_md.documentation.description}")
    #
    #     return "\n".join(result)
    #
    # @env.macro
    # def get_metadata_models_for_package(package_name: str):
    #
    #     metadata_schemas = kiara_obj.metadata_mgmt.find_all_models_for_package(
    #         package_name
    #     )
    #     result = []
    #     for name, info in metadata_schemas.items():
    #         type_md = info.get_type_metadata()
    #         result.append(f"  - ``{name}``: {type_md.documentation.description}")
    #
    #     return "\n".join(result)
    #
    # @env.macro
    # def get_kiara_context() -> KiaraContext:
    #     return kiara_context


# def on_post_build(env):
#     "Post-build actions"
#
#     site_dir = env.conf["site_dir"]
#
#     for category, classes in KIARA_MODEL_CLASSES.items():
#
#         for cls in classes:
#             schema_json = cls.schema_json(indent=2)
#
#             file_path = os.path.join(
#                 site_dir, "development", "entities", category, f"{cls.__name__}.json"
#             )
#             os.makedirs(os.path.dirname(file_path), exist_ok=True)
#             with open(file_path, "w") as f:
#                 f.write(schema_json)


# kiara\kiara\src\kiara\doc\__init__.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2020-2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License Version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

"""Main module for code that helps with documentation auto-generation in supported projects."""

import os
import tempfile
import urllib.parse

import mkdocs.utils
from mkdocs.config import Config, config_options  # noqa
from mkdocs.plugins import BasePlugin
from mkdocs.structure.files import File, Files
from mkdocs.structure.nav import (
    Navigation,
    Section,
    _add_parent_links,
    _add_previous_and_next_links,
)
from mkdocs.structure.pages import Page

from kiara.doc.generate_api_doc import gen_pages_for_module


class FrklDocumentationPlugin(BasePlugin):
    """
    [mkdocs](https://www.mkdocs.org/) plugin to render API documentation for a project.

    To add to a project, add this to the 'plugins' section of a mkdocs config file:

    ```yaml
    - frkl-docgen:
        main_module: "module_name"
    ```

    This will add an ``API reference`` navigation item to your page navigation, with auto-generated entries for every
    Python module in your package.
    """

    config_scheme = (("main_module", mkdocs.config.config_options.Type(str)),)

    def __init__(self):
        self._doc_paths = None
        self._dir = tempfile.TemporaryDirectory(prefix="frkl_doc_gen_")
        self._doc_files = None
        super().__init__()

    def on_files(self, files: Files, config: Config) -> Files:

        self._doc_paths = gen_pages_for_module(self.config["main_module"])
        self._doc_files = {}

        for k in sorted(self._doc_paths, key=lambda x: os.path.splitext(x)[0]):
            content = self._doc_paths[k]["content"]
            _file = File(
                k,
                src_dir=self._dir.name,
                dest_dir=config["site_dir"],
                use_directory_urls=config["use_directory_urls"],
            )

            os.makedirs(os.path.dirname(_file.abs_src_path), exist_ok=True)

            with open(_file.abs_src_path, "w") as f:
                f.write(content)

            self._doc_files[k] = _file
            files.append(_file)

        return files

    def on_page_content(self, html, page: Page, config: Config, files: Files):

        repo_url = config.get("repo_url", None)
        python_src = config.get("edit_uri", None)

        if page.file.src_path in self._doc_paths.keys():
            src_path = self._doc_paths.get(page.file.src_path)["python_src"]["rel_path"]
            rel_base = urllib.parse.urljoin(repo_url, f"{python_src}/../src/{src_path}")
            page.edit_url = rel_base

        return html

    def on_nav(self, nav: Navigation, config: Config, files: Files):

        for item in nav.items:
            if item.title and "Api reference" in item.title:
                return nav

        pages = []
        for _file in self._doc_files.values():
            pages.append(_file.page)

        section = Section(title="API reference", children=pages)
        nav.items.append(section)
        nav.pages.extend(pages)

        _add_previous_and_next_links(nav.pages)
        _add_parent_links(nav.items)

        return nav

    def on_post_build(self, config: Config):

        self._dir.cleanup()


# kiara\kiara\src\kiara\doc\mkdocstrings\collector.py
# -*- coding: utf-8 -*-
# #  Copyright (c) 2022-2022, Markus Binsteiner
# #
# #  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)
#
# from __future__ import annotations
#
# import builtins
#
# from mkdocstrings.handlers.base import CollectionError, CollectorItem, BaseHandler
# from mkdocstrings.loggers import get_logger
#
# from kiara.context import Kiara, KiaraContextInfo
# from kiara.interfaces.python_api.models.info import ItemInfo
# from kiara.utils import log_exception
#
# logger = get_logger(__name__)
#
#
# class KiaraCollector(BaseHandler):
#
#     """
#     The class responsible for loading Jinja templates and rendering them.
#     It defines some configuration options, implements the `render` method,
#     and overrides the `update_env` method of the [`BaseRenderer` class][mkdocstrings.handlers.base.BaseRenderer].
#     """
#
#     default_config: dict = {"docstring_style": "google", "docstring_options": {}}
#     """The default selection options.
#     Option | Type | Description | Default
#     ------ | ---- | ----------- | -------
#     **`docstring_style`** | `"google" | "numpy" | "sphinx" | None` | The docstring style to use. | `"google"`
#     **`docstring_options`** | `dict[str, Any]` | The options for the docstring parser. | `{}`
#     """
#
#     fallback_config: dict = {"fallback": True}
#
#     def __init__(self) -> None:
#         """Initialize the collector."""
#         self._kiara: Kiara = Kiara.instance()
#
#     def collect(self, identifier: str, config: dict) -> CollectorItem:
#         """
#         Collect the documentation tree given an identifier and selection options.
#
#         Arguments:
#         ---------
#             identifier: The dotted-path of a Python object available in the Python path.
#             config: Selection options, used to alter the data collection done by `pytkdocs`.
#
#
#         Raises:
#         ------
#             CollectionError: When there was a problem collecting the object documentation.
#
#
#         Returns:
#         -------
#             The collected object-tree.
#         """
#         tokens = identifier.split(".")
#
#         if tokens[0] != "kiara_info":
#             return None
#
#         item_type = tokens[1]
#         item_id = ".".join(tokens[2:])
#         if not item_id:
#             raise CollectionError(f"Invalid id: {identifier}")
#
#         ctx: KiaraContextInfo = builtins.plugin_package_context_info  # type: ignore
#         try:
#             item: ItemInfo = ctx.get_info(item_type=item_type, item_id=item_id)
#         except Exception as e:
#             log_exception(e)
#             raise CollectionError(f"Invalid id: {identifier}")
#
#         return {"obj": item, "identifier": identifier}


# kiara\kiara\src\kiara\doc\mkdocstrings\handler.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2022-2022, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import typing

from mkdocstrings.handlers.base import BaseHandler, CollectionError, CollectorItem

__all__ = ["get_handler"]

from kiara.context import KiaraContextInfo

# from kiara.defaults import KIARA_RESOURCES_FOLDER
# from kiara.doc.mkdocstrings.collector import KiaraCollector
# from kiara.doc.mkdocstrings.renderer import KiaraInfoRenderer
from kiara.interfaces.python_api.models.info import ItemInfo
from kiara.utils import log_exception


class KiaraHandler(BaseHandler):
    """
    The kiara handler class.

    Attributes:
    ----------
        domain: The cross-documentation domain/language for this handler.
        enable_inventory: Whether this handler is interested in enabling the creation
            of the `objects.inv` Sphinx inventory file.
    """

    domain: str = "kiara"
    enable_inventory: bool = True

    def collect(
        self, identifier: str, config: typing.MutableMapping[str, typing.Any]
    ) -> CollectorItem:
        """
        Collect the documentation tree given an identifier and selection options.

        Arguments:
        ---------
            identifier: The dotted-path of a Python object available in the Python path.
            config: Selection options, used to alter the data collection done by `pytkdocs`.


        Raises:
        ------
            CollectionError: When there was a problem collecting the object documentation.


        Returns:
        -------
            The collected object-tree.
        """
        tokens = identifier.split(".")

        if tokens[0] != "kiara_info":
            return None

        item_type = tokens[1]
        item_id = ".".join(tokens[2:])
        if not item_id:
            raise CollectionError(f"Invalid id: {identifier}")

        ctx: KiaraContextInfo = builtins.plugin_package_context_info  # type: ignore  # noqa
        try:
            item: ItemInfo = ctx.get_info(item_type=item_type, item_id=item_id)
        except Exception as e:
            log_exception(e)
            raise CollectionError(f"Invalid id: {identifier}")

        return {"obj": item, "identifier": identifier}

    def get_anchors(self, data: CollectorItem) -> typing.Tuple[str, ...]:

        if data is None:
            return ()

        return (data["identifier"], data["kiara_id"], data["obj"].get_id())

    def render(
        self, data: CollectorItem, config: typing.Mapping[str, typing.Any]
    ) -> str:

        # final_config = ChainMap(config, self.default_config)

        obj = data["obj"]
        html: str = obj.create_html()
        return html


def get_handler(
    theme: str,
    custom_templates: typing.Union[str, None] = None,
    config_file_path: typing.Union[None, str] = None,
) -> KiaraHandler:
    """
    Simply return an instance of `PythonHandler`.

    Arguments:
    ---------
        theme: The theme to use when rendering contents.
        custom_templates: Directory containing custom templates.
        **config: Configuration passed to the handler.


    Returns:
    -------
        An instance of `PythonHandler`.
    """
    if custom_templates is not None:
        raise Exception("Custom templates are not supported for the kiara renderer.")

    # custom_templates = os.path.join(
    #     KIARA_RESOURCES_FOLDER, "templates", "info_templates"
    # )

    return KiaraHandler(
        "kiara",
        theme=theme,
        custom_templates=custom_templates,
    )


# kiara\kiara\src\kiara\doc\mkdocstrings\renderer.py
# -*- coding: utf-8 -*-
#
# #  Copyright (c) 2022-2022, Markus Binsteiner
# #
# #  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)
#
# import typing
#
# from mkdocstrings.handlers.base import BaseRenderer, CollectorItem
#
#
# class AliasResolutionError:
#     pass
#
#
# class KiaraInfoRenderer(BaseRenderer):
#
#     default_config: dict = {}
#
#     def get_anchors(self, data: CollectorItem) -> typing.List[str]:
#
#         if data is None:
#             return []
#
#         return [data["identifier"], data["kiara_id"], data["obj"].get_id()]
#
#     def render(self, data: typing.Dict[str, typing.Any], config: dict) -> str:
#
#         # final_config = ChainMap(config, self.default_config)
#
#         obj = data["obj"]
#         html: str = obj.create_html()
#         return html


# kiara\kiara\src\kiara\doc\mkdocstrings\__init__.py
# -*- coding: utf-8 -*-
#  Copyright (c) 2022, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)


# kiara\kiara\src\kiara\interfaces\__init__.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

"""Implementation of interfaces for *Kiara*."""
import contextlib
import os
import sys
import uuid
from typing import (
    TYPE_CHECKING,
    Any,
    ClassVar,
    Dict,
    Iterable,
    Iterator,
    Literal,
    Union,
)

from kiara.exceptions import KiaraException
from kiara.utils.cli import terminal_print

if TYPE_CHECKING:
    from rich.console import Console

    from kiara.context import Kiara
    from kiara.context.config import KiaraConfig
    from kiara.interfaces.python_api.base_api import BaseAPI

# log = structlog.getLogger()

# Global console used by alternative print
_console: Union["Console", None] = None


def create_console(
    width: Union[None, int],
    color_system: Union[
        None, Literal["auto", "standard", "256", "truecolor", "windows"]
    ] = None,
) -> "Console":
    """Create a console instance."""
    from rich.console import COLOR_SYSTEMS
    from rich.highlighter import RegexHighlighter
    from rich.theme import Theme

    STYLE_OPTION = "bold cyan"
    STYLE_ARGUMENT = "bold cyan"
    STYLE_SWITCH = "bold green"
    STYLE_METAVAR = "bold yellow"
    STYLE_METAVAR_SEPARATOR = "dim"
    STYLE_USAGE = "yellow"

    theme = Theme(
        {
            "option": STYLE_OPTION,
            "argument": STYLE_ARGUMENT,
            "switch": STYLE_SWITCH,
            "metavar": STYLE_METAVAR,
            "metavar_sep": STYLE_METAVAR_SEPARATOR,
            "usage": STYLE_USAGE,
        }
    )

    class OptionHighlighter(RegexHighlighter):
        """Highlights our special options."""

        highlights: ClassVar = [  # type: ignore
            r"(^|\W)(?P<switch>\-\w+)(?![a-zA-Z0-9])",
            r"(^|\W)(?P<option>\-\-[\w\-]+)(?![a-zA-Z0-9])",
            r"(^|\W)(?P<argument>[A-Z0-9\_]+)(?![_a-zA-Z0-9])",
            r"(?P<metavar>\<[^\>]+\>)",
            r"(?P<usage>Usage: )",
        ]

    highlighter = OptionHighlighter()
    FORCE_TERMINAL = (
        True
        if os.getenv("GITHUB_ACTIONS")
        or os.getenv("FORCE_COLOR")
        or os.getenv("PY_COLORS")
        else None
    )

    console_width = None
    if width is not None:
        console_width = width
    else:
        _console_width = os.environ.get("CONSOLE_WIDTH", None)
        if _console_width:
            try:
                console_width = int(_console_width)
            except Exception:
                pass

    from rich.console import Console

    if color_system is not None:
        _color_system = color_system
    else:
        _color_system = "auto"

    if _color_system != "auto" and _color_system not in COLOR_SYSTEMS.keys():
        raise Exception(
            f"Invalid color system '{_color_system}': available options: auto, {', '.join(COLOR_SYSTEMS.keys())}."
        )

    _console = Console(
        width=console_width,
        theme=theme,
        highlighter=highlighter,
        color_system=_color_system,
        force_terminal=FORCE_TERMINAL,
    )
    return _console


def get_console() -> "Console":
    """
    Get a global Console instance.

    Returns:
    -------
        Console: A console instance.
    """
    global _console
    if _console is None:
        _console = create_console(width=None, color_system=None)

    return _console


@contextlib.contextmanager
def get_proxy_console(
    width: Union[None, int] = None,
    color_system: Union[
        None, Literal["auto", "standard", "256", "truecolor", "windows"]
    ] = None,
    restore_default_console: bool = True,
) -> Iterator["Console"]:
    """
    Get a console that proxies a remote one.

    This should only be used in a single-threaded way.
    """
    global _console

    default_console = get_console()
    old_width = default_console.width

    changed = False
    changed_width = False
    # TODO: maybe use thread-local?
    if width is None and color_system is None:
        result = default_console
    else:
        if width is None:
            width = default_console.width

        if color_system is None:
            color_system = default_console.color_system  # type: ignore

        if color_system != default_console.color_system:
            result = create_console(width=width, color_system=color_system)
            _console = result
            changed = True
        elif width != default_console.width:
            default_console.width = width
            changed_width = True
        else:
            result = default_console

    yield result

    if changed and restore_default_console:
        _console = default_console

    if changed_width and restore_default_console:
        default_console.width = old_width


def set_console_width(width: Union[int, None] = None, prefer_env: bool = True):

    global _console
    if prefer_env or not width:
        _width: Union[None, int] = None
        try:
            _width = int(os.environ.get("CONSOLE_WIDTH", None))  # type: ignore
        except Exception:
            pass
        if _width:
            width = _width

    if width:
        try:
            width = int(width)
        except Exception as e:
            import structlog

            log = structlog.getLogger()
            log.debug("invalid.console_width", error=str(e))

    from rich.console import Console

    _console = Console(width=width)

    if not width:
        if "google.colab" in sys.modules or "jupyter_client" in sys.modules:
            width = 140

    if width:
        import rich

        con = rich.get_console()
        con.width = width


class BaseAPIWrap(object):
    """A wrapper class to help with lazy loading.

    This is mostly relevant in terms of Python imports and the cli, because that allows
    to avoid importing lots of Python modules if only `--help` is called.
    """

    def __init__(
        self,
        config: Union[str, None],
        context: Union[str, None],
        pipelines: Union[None, Iterable[str]] = None,
        ensure_plugins: Union[str, Iterable[str], None] = None,
        exit_process: bool = True,
    ):

        if not context:
            context = os.environ.get("KIARA_CONTEXT", None)

        if context and context.endswith(".kontext") and os.path.exists(context):
            context = os.path.abspath(context)

        self._config: Union[str, None] = config
        self._context: Union[str, None] = context

        self._pipelines: Union[None, Iterable[str]] = pipelines
        self._ensure_plugins: Union[str, Iterable[str], None] = ensure_plugins

        self._kiara_config: Union["KiaraConfig", None] = None
        self._api: Union[BaseAPI, None] = None

        self._reload_process_if_plugins_installed = True

        self._items: Dict[str, Any] = {}
        self._exit_process: bool = exit_process

    @property
    def kiara_context_name(self) -> str:

        if not self._context:
            self._context = self.kiara_config.default_context

        return self._context

    @property
    def exit_process(self) -> bool:
        return self._exit_process

    @exit_process.setter
    def exit_process(self, exit_process: bool):
        self._exit_process = exit_process

    def exit(self, msg: Union[None, Any] = None, exit_code: int = 1):

        if self._exit_process:
            if msg:
                terminal_print(msg)
            sys.exit(exit_code)
        else:
            if not msg:
                msg = "An error occured."
            raise KiaraException(str(msg))

    @property
    def current_kiara_context_id(self) -> uuid.UUID:

        return self.base_api.context.id

    @property
    def kiara(self) -> "Kiara":
        return self.base_api.context

    @property
    def kiara_config(self) -> "KiaraConfig":

        if self._kiara_config is not None:
            return self._kiara_config

        try:
            from kiara.utils.config import assemble_kiara_config

            kiara_config = assemble_kiara_config(config_file=self._config)
        except Exception as e:
            terminal_print()
            terminal_print(f"Error loading kiara config: {e}")
            sys.exit(1)

        # kiara_config.runtime_config.runtime_profile = "default"

        self._kiara_config = kiara_config
        return self._kiara_config

    def lock_file(self, context: str) -> str:
        """The path to the lock file for this context."""
        return "asdf"

    @property
    def base_api(self) -> "BaseAPI":

        if self._api is not None:
            return self._api

        from kiara.utils import log_message

        context = self._context
        if not context:
            context = self.kiara_config.default_context

        from kiara.interfaces.python_api.base_api import BaseAPI

        api = BaseAPI(kiara_config=self.kiara_config)
        if self._ensure_plugins:
            installed = api.ensure_plugin_packages(self._ensure_plugins, update=False)
            if installed and self._reload_process_if_plugins_installed:
                log_message(
                    "replacing.process",
                    reason="reloading this process, in order to pick up new plugin packages",
                )
                os.execvp(sys.executable, (sys.executable,) + tuple(sys.argv))  # noqa

        import fasteners

        fasteners.InterProcessReaderWriterLock(self.lock_file(context))

        api.set_active_context(context, create=True)

        if self._pipelines:
            for pipeline in self._pipelines:
                ops = api.context.operation_registry.register_pipelines(pipeline)
                for op_id in ops.keys():
                    log_message("register.pipeline", operation_id=op_id)

        self._api = api
        return self._api

    def add_item(self, key: str, item: Any):

        self._items[key] = item

    def get_item(self, key: str) -> Any:

        if key not in self._items.keys():
            raise ValueError(f"No item with key '{key}'")

        return self._items[key]


# kiara\kiara\src\kiara\interfaces\cli\proxy_cli.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

"""A command-line interface for *Kiara*."""

import logging

import rich_click as click
import structlog

from kiara.utils import is_debug
from kiara.utils.class_loading import find_all_cli_subcommands

click.rich_click.USE_RICH_MARKUP = True

# TODO: rich_click refactoring, how to backport this?
# click.rich_click._get_rich_console = get_console


if is_debug():
    logger = structlog.get_logger()

    structlog.configure(
        wrapper_class=structlog.make_filtering_bound_logger(logging.DEBUG),
    )
else:
    structlog.configure(
        wrapper_class=structlog.make_filtering_bound_logger(logging.WARNING),
    )

CLICK_CONTEXT_SETTINGS = {"help_option_names": ["-h", "--help"]}


@click.group(context_settings=CLICK_CONTEXT_SETTINGS)
@click.pass_context
def proxy_cli(
    ctx,
):

    assert ctx.obj is not None


for plugin in find_all_cli_subcommands():
    proxy_cli.add_command(plugin)


# kiara\kiara\src\kiara\interfaces\cli\run.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

"""The 'run' subcommand for the cli."""

import os.path
import sys
import typing
from pathlib import Path
from typing import Any, Dict, Iterable, List, Mapping, Union

import rich_click as click

from kiara.exceptions import InvalidCommandLineInvocation
from kiara.utils import log_message
from kiara.utils.cli import dict_from_cli_args, terminal_print
from kiara.utils.cli.exceptions import handle_exception
from kiara.utils.files import get_data_from_file

if typing.TYPE_CHECKING:
    from kiara.interfaces.python_api.base_api import BaseAPI


@click.command()
@click.argument("module_or_operation", nargs=1, metavar="MODULE_OR_OPERATION")
@click.argument("inputs", nargs=-1, required=False)
@click.option(
    "--module-config",
    "-c",
    required=False,
    help="(Optional) module configuration, only valid when run target is a module name.",
    multiple=True,
)
@click.option(
    "--explain",
    "-e",
    help="Display information about the selected operation and exit.",
    is_flag=True,
)
@click.option(
    "--output", "-o", help="The output format and configuration.", multiple=True
)
@click.option(
    "--comment", "-c", help="Add comment metadata to the job you run.", required=False
)
@click.option(
    "--save",
    "-s",
    help="Save one or several of the outputs of this run. If the argument contains a '=', the format is [output_name]=[alias], if not, the values will be saved as '[alias]-[output_name]'.",
    required=False,
    multiple=True,
)
@click.option(
    "--print-properties",
    "-p",
    help="Also display the properties of the result values.",
    required=False,
    is_flag=True,
)
@click.option("--help", "-h", help="Show this message and exit.", is_flag=True)
@click.pass_context
@handle_exception()
def run(
    ctx,
    module_or_operation: str,
    module_config: Iterable[str],
    inputs: Iterable[str],
    output: Iterable[str],
    comment: Union[str, None],
    explain: bool,
    save: Iterable[str],
    print_properties: bool,
    help: bool,
):
    """Run a kiara operation."""
    from kiara.api import JobDesc, RunSpec
    from kiara.utils.cli.run import (
        _validate_save_option,
        calculate_aliases,
        execute_job,
        set_and_validate_inputs,
        validate_operation_in_terminal,
    )
    from kiara.utils.output import OutputDetails

    # =========================================================================
    # initialize a few variables

    if module_config:
        module_config = dict_from_cli_args(*module_config)
    else:
        module_config = {}

    _validate_save_option(save)

    output_details = OutputDetails.from_data(output)
    silent = False
    if output_details.format == "silent":
        silent = True

    force_overwrite = output_details.config.get("force", False)

    if output_details.target != "terminal":
        if output_details.target == "file":
            target_dir = Path.cwd()
            target_file = target_dir / f"{module_or_operation}.{output_details.format}"
        else:
            target_file = Path(
                os.path.realpath(os.path.expanduser(output_details.target))
            )

        if target_file.exists() and not force_overwrite:
            terminal_print()
            terminal_print(
                f"Can't run workflow, the target files already exist, and '--output force=true' not specified: {target_file}"
            )

            sys.exit(1)

    api: BaseAPI = ctx.obj.base_api  # type: ignore

    cmd_arg = ctx.params["module_or_operation"]
    cmd_help = f"[yellow bold]Usage: [/yellow bold][bold]kiara run [OPTIONS] [i]{cmd_arg}[/i] [INPUTS][/bold]"

    # if module_config:
    #     op: Union[str, Mapping[str, Any]] = {
    #         "module_type": module_or_operation,
    #         "module_config": module_config,
    #     }
    # else:
    #     op = module_or_operation

    # base_inputs: Union[Mapping[str, Any], None] = None
    # extra_save: Union[None, Dict[str, str]] = None
    run_type = None
    job_descs: List[JobDesc] = []

    if not module_config and os.path.isfile(module_or_operation):

        path: Path = Path(module_or_operation)
        data = get_data_from_file(path)
        repl_dict: Dict[str, Any] = {"this_dir": path.parent.absolute().as_posix()}
        alias = path.stem

        if isinstance(data, list):
            raise NotImplementedError()
        elif isinstance(data, Mapping):
            if "operation" in data.keys():
                run_type = "job"
                job_desc = JobDesc.create_from_data(
                    data, var_repl_dict=repl_dict, alias=alias
                )
                job_descs.append(job_desc)

            elif "jobs" in data.keys():
                run_type = "run"

                if inputs:
                    terminal_print()
                    terminal_print(
                        "Can't specify inputs when running file with a run spec."
                    )
                    sys.exit(1)

                run_desc = RunSpec.create_from_data(
                    data, var_repl_dict=repl_dict, alias=alias
                )
                job_descs.extend(run_desc.jobs)
            elif "steps" not in data.keys():

                terminal_print()
                terminal_print(
                    f"Can't run file '{path}', it does not contain a valid pipeline, job or run specification."
                )
                sys.exit(1)
            else:
                # TODO: check if valid pipeline, otherwise check if 'module_or_operation is an operation name

                from kiara.models.module.jobs import ExecutionContext
                from kiara.models.module.pipeline import PipelineConfig

                pipeline_dir = os.path.abspath(os.path.dirname(path))
                execution_context = ExecutionContext(pipeline_dir=pipeline_dir)
                pc = PipelineConfig.from_config(
                    data, execution_context=execution_context, kiara=api.context
                )
                job_desc = JobDesc(
                    operation="pipeline",
                    module_config=pc.model_dump(),
                    job_alias="local_pipeline",
                )
                job_descs.append(job_desc)

    else:
        if module_config:
            job_desc = JobDesc(
                operation=module_or_operation,
                module_config=module_config,
                job_alias="default",
            )
        else:
            job_desc = JobDesc(operation=module_or_operation, job_alias="default")

        job_descs.append(job_desc)

    assert len(job_descs) > 0

    for job_desc in job_descs:

        if job_desc.module_config:
            op: Union[str, Mapping[str, Any]] = {
                "module_type": job_desc.operation,
                "module_config": job_desc.module_config,
            }
        else:
            op = job_desc.operation

        try:
            kiara_op = validate_operation_in_terminal(api=api, module_or_operation=op)
        except InvalidCommandLineInvocation as e:
            ctx.obj.exit(msg=None, exit_code=e.error_code)
            return

        log_message(f"run_arg.is.{run_type}")

        final_aliases = calculate_aliases(
            operation=kiara_op, alias_tokens=save, extra_aliases=job_desc.save
        )

        try:
            inputs_value_map = set_and_validate_inputs(
                api=api,
                operation=kiara_op,
                inputs=inputs,
                explain=explain,
                print_help=help,
                click_context=ctx,
                cmd_help=cmd_help,
                base_inputs=job_desc.inputs,
            )
            if inputs_value_map is None:
                ctx.obj.exit(msg=None, exit_code=0)
                return
        except InvalidCommandLineInvocation as e:
            ctx.obj.exit(msg=None, exit_code=e.error_code)
            return

        execute_job(
            api=api,
            operation=kiara_op,
            inputs=inputs_value_map,
            comment=comment,
            silent=silent,
            save_results=bool(final_aliases),
            aliases=final_aliases,
            properties=print_properties,
        )


# kiara\kiara\src\kiara\interfaces\cli\__init__.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

"""A command-line interface for *Kiara*."""

import logging
import sys
from typing import Tuple, Union

import rich_click as click
import structlog

from kiara.interfaces import BaseAPIWrap
from kiara.utils import is_debug, log_message
from kiara.utils.class_loading import find_all_cli_subcommands
from kiara.utils.cli import (
    kiara_runtime_info_option,
    kiara_version_option,
    terminal_print,
)

click.rich_click.USE_RICH_MARKUP = True
# TODO: rich_click backport this
# click.rich_click._get_rich_console = get_console


if is_debug():
    logger = structlog.get_logger()

    structlog.configure(
        wrapper_class=structlog.make_filtering_bound_logger(logging.DEBUG),
    )
else:
    structlog.configure(
        wrapper_class=structlog.make_filtering_bound_logger(logging.WARNING),
    )

CLICK_CONTEXT_SETTINGS = {"help_option_names": ["-h", "--help"]}


@click.group(context_settings=CLICK_CONTEXT_SETTINGS)
@click.option(
    "--config",
    "-cnf",
    help="A kiara config file (or folder containing one named 'kiara.config').",
    required=False,
)
@click.option(
    "--context",
    "-ctx",
    "-c",
    help="The name of the kiara context to use (or the path to a context file).",
    required=False,
)
@click.option(
    "--pipelines",
    "-p",
    help="File(s) and folder(s) that contain extra pipeline definitions.",
    multiple=True,
    required=False,
)
@click.option(
    "--plugin",
    "-P",
    help="Ensure the provided plugin package(s) are installed in the virtual environment.",
)
@click.option(
    "--use-background-service",
    "-b",
    is_flag=True,
    help="Always use the background service (start if not already running).",
    default=False,
)
@kiara_version_option()
@kiara_runtime_info_option()
@click.pass_context
def cli(
    ctx,
    config: Union[str, None],
    context: Union[str, None],
    pipelines: Tuple[str],
    plugin: Union[str, None],
    use_background_service: Union[bool, None] = None,
):
    """
    [i b]kiara[/b i] ia a data-orchestration framework; this is the command-line frontend for it.

    For more information, visit the [i][b]kiara[/b] homepage[/i]: https://dharpa.org/kiara.documentation .
    """
    # check if windows symlink work

    # if not check_symlink_works():
    #
    #     terminal_print()
    #     from rich.markdown import Markdown
    #
    #     terminal_print(Markdown(SYMLINK_ISSUE_MSG))
    #     sys.exit(1)

    context_subcommand = ctx.invoked_subcommand == "context"
    if context_subcommand and use_background_service:
        log_message(
            "ignore.background_service_request",
            reason="Not using background service for 'context' subcommand.",
        )

    if use_background_service and not context_subcommand:

        from kiara.zmq import (
            KiaraZmqServiceDetails,
            get_context_details,
            start_zmq_service,
        )
        from kiara.zmq.client import KiaraZmqClient

        if context is None:
            context = "default"

        context_details_data = get_context_details(context_name=context)
        if context_details_data is None:

            timeout = 120 * 1000  # 2 minutes default timeout
            api_wrap = BaseAPIWrap(config, context, pipelines, plugin)
            context_details = start_zmq_service(
                api_wrap=api_wrap,
                host=None,
                port=None,
                monitor=False,
                stdout=None,
                stderr=None,
                timeout=timeout,
            )
        else:
            context_details = KiaraZmqServiceDetails(**context_details_data)

        CONTEXT_ARG_NAMES = [
            "--config",
            "--context",
            "--pipelines",
            "--plugin",
            "-cnf",
            "-ctx",
            "-c",
            "-p",
            "-P",
        ]
        CONTEXT_FLAG_NAMES = ["--use-background-service", "-b"]

        arguments = []
        still_prefix = True
        for arg in sys.argv[1:]:
            if still_prefix and arg in CONTEXT_ARG_NAMES:
                # TODO: check arg with running service
                raise NotImplementedError(
                    "Custom context args for background service client not supported yet."
                )

            if still_prefix and arg in CONTEXT_FLAG_NAMES:
                continue
            else:
                still_prefix = False

            arguments.append(arg)

        assert context_details is not None

        zmq_client = KiaraZmqClient(
            host=context_details.host, port=context_details.port
        )

        try:
            zmq_client.request_cli(args=arguments)
            sys.exit(0)
        except KeyboardInterrupt:
            terminal_print("\nInterrupted by user, closing connection to service...")
            sys.exit(1)
        finally:
            zmq_client.close()

    else:
        lazy_wrapper = BaseAPIWrap(config, context, pipelines, plugin)
        ctx.obj = lazy_wrapper


for plugin in find_all_cli_subcommands():
    cli.add_command(plugin)

if __name__ == "__main__":
    cli()


# kiara\kiara\src\kiara\interfaces\cli\archive\commands.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)
import rich_click as click

from kiara.defaults import (
    CHUNK_COMPRESSION_TYPE,
    DEFAULT_CHUNK_COMPRESSION,
)
from kiara.utils.cli import (
    output_format_option,
    terminal_print_model,
)
from kiara.utils.cli.exceptions import handle_exception


@click.group()
@click.pass_context
def archive(ctx):
    """Kiara archive related sub-commands."""


@archive.command("explain")
@click.argument("archive", nargs=1, required=True)
@output_format_option()
@click.pass_context
@handle_exception()
def explain_archive(
    ctx,
    format: str,
    archive: str,
):
    """Print details of an archive file."""

    from kiara.interfaces.python_api.base_api import BaseAPI

    kiara_api: BaseAPI = ctx.obj.base_api

    info = kiara_api.retrieve_archive_info(archive)

    terminal_print_model(info, format=format, in_panel=f"Archive info: {archive}")


@archive.command("export")
@click.argument("path", nargs=1, required=True)
@click.option(
    "--compression",
    "-c",
    help="The compression inside the archive. If not provided, 'zstd' will be used. Ignored if archive already exists and 'append' is used.",
    type=click.Choice(["zstd", "lz4", "lzma", "none"]),
    default=DEFAULT_CHUNK_COMPRESSION.ZSTD.name.lower(),  # type: ignore
)
@click.option("--append", "-a", help="Append data to existing archive.", is_flag=True)
@click.option("--no-aliases", "-na", help="Do not store aliases.", is_flag=True)
@click.pass_context
@handle_exception()
def export_archive(ctx, path: str, compression: str, append: bool, no_aliases: bool):

    from kiara.interfaces.python_api.base_api import BaseAPI

    api: BaseAPI = ctx.obj.base_api

    target_store_params = {"compression": CHUNK_COMPRESSION_TYPE[compression.upper()]}
    result = api.export_archive(
        target_archive=path,
        append=append,
        target_store_params=target_store_params,
        no_aliases=no_aliases,
    )

    render_config = {"add_field_column": False}
    terminal_print_model(
        result,
        format="terminal",
        empty_line_before=None,
        in_panel="Exported values",
        **render_config,
    )


@archive.command("import")
@click.argument("path", nargs=1, required=True)
@click.option("--no-aliases", "-na", help="Do not store aliases.", is_flag=True)
@click.pass_context
@handle_exception()
def import_archive(ctx, path: str, no_aliases: bool):
    """Import an archive file."""

    from kiara.interfaces.python_api.base_api import BaseAPI

    kiara_api: BaseAPI = ctx.obj.base_api

    result = kiara_api.import_archive(source_archive=path, no_aliases=no_aliases)

    render_config = {"add_field_column": False}
    terminal_print_model(
        result,
        format="terminal",
        empty_line_before=None,
        in_panel="Imported values",
        **render_config,
    )


# kiara\kiara\src\kiara\interfaces\cli\archive\__init__.py


# kiara\kiara\src\kiara\interfaces\cli\context\commands.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)
import sys
from typing import TYPE_CHECKING, Tuple, Union

import rich_click as click
from rich import box
from rich.panel import Panel
from rich.table import Table

from kiara.interfaces import BaseAPIWrap, get_console
from kiara.utils import log_exception
from kiara.utils.cli import (
    dict_from_cli_args,
    output_format_option,
    terminal_print,
    terminal_print_model,
)

if TYPE_CHECKING:
    from kiara.interfaces.python_api.base_api import BaseAPI, Kiara, KiaraConfig


@click.group("context")
@click.pass_context
def context(ctx):
    """Kiara context related sub-commands."""


@context.command("list")
@click.pass_context
def list_contexts(ctx) -> None:
    """List existing contexts."""
    kiara_api: BaseAPI = ctx.obj.base_api

    summaries = kiara_api.retrieve_context_infos()

    terminal_print(summaries)


@context.command("explain")
@click.argument("context_name", nargs=-1, required=False)
@click.option("--value-ids", "-i", help="Show value ids.", is_flag=True, default=False)
@click.option(
    "--show-config", "-c", help="Also show kiara config.", is_flag=True, default=False
)
@output_format_option()
@click.pass_context
def explain_context(
    ctx,
    format: str,
    value_ids: bool,
    context_name: Union[Tuple[str], None] = None,
    show_config: bool = False,
):
    """Print details for one or several contexts."""
    kiara_config: KiaraConfig = ctx.obj.kiara_config

    if not context_name:
        cn = ctx.obj.kiara_context_name
        contexts = [cn]
    else:
        contexts = list(context_name)

    from kiara.models.context import ContextInfo

    render_config = {
        "show_lines": False,
        "show_header": False,
        "show_description": False,
    }

    if show_config:
        from rich.table import Table

        config = kiara_config.create_renderable(**render_config)
        table = Table(show_header=False, show_lines=False, box=box.SIMPLE)
        table.add_column("key", style="i")
        table.add_column("value")
        if kiara_config._config_path:
            table.add_row("config file", f"  {kiara_config._config_path}")
        table.add_row("config", config)
        terminal_print(table, in_panel="Kiara config")

    if len(contexts) == 1:

        kcc = kiara_config.get_context_config(contexts[0])
        cs = ContextInfo.create_from_context_config(
            kcc, context_name=contexts[0], runtime_config=kiara_config.runtime_config
        )
        terminal_print_model(
            cs,
            format=format,
            full_details=True,
            show_value_ids=value_ids,
            in_panel=f"Context '{contexts[0]}'",
        )

    else:
        summaries = []
        for c in contexts:
            cc = kiara_config.get_context_config(c)
            cs = ContextInfo.create_from_context_config(
                cc, context_name=c, runtime_config=kiara_config.runtime_config
            )
            summaries.append(cs)
        terminal_print_model(
            *summaries, format=format, full_details=True, show_value_ids=value_ids
        )


@context.command("delete")
@click.argument("context_name", nargs=1, required=False)
@click.option(
    "--force", "-f", help="Delete without prompt.", is_flag=True, default=False
)
@click.option("--all", "-a", help="Delete all contexts.", is_flag=True, default=False)
@click.pass_context
def delete_context(
    ctx,
    context_name: Union[str, None] = None,
    force: bool = False,
    all: bool = False,
):
    """Delete a context and all its stored values."""
    kiara_config: KiaraConfig = ctx.obj.kiara_config

    if not context_name:
        if all:
            _context_name = "ALL_CONTEXTS"
        else:
            _context_name = ctx.obj.kiara_context_name
    else:
        if all:
            if context_name != "ALL_CONTEXTS":
                terminal_print()
                terminal_print(
                    f"Context name '{context_name}' specified, as well as '--all-contexts', this is not valid."
                )
                sys.exit(1)
        _context_name = context_name

    confirmed = False

    if _context_name == "ALL_CONTEXTS":
        if not force:
            from kiara.models.context import ContextInfos

            summaries = ContextInfos.create_context_infos(
                contexts=kiara_config.context_configs
            )
            terminal_print_model(summaries, in_panel="All contexts:")
            user_input = get_console().input(
                r"Deleting all contexts, are you sure? \[yes/no]: "
            )

            if user_input.lower() == "yes":
                confirmed = True
        else:
            confirmed = True

        if not confirmed:
            terminal_print("\nDoing nothing...")
            sys.exit(0)

        terminal_print("Deleting contexts...")
        for _context_name in kiara_config.context_configs.keys():
            terminal_print(f"  - {_context_name}")
            kiara_config.delete(context_name=_context_name, dry_run=False)

        terminal_print("Done.")

    else:

        if not force:

            context_summary = kiara_config.delete(
                context_name=_context_name, dry_run=True
            )
            terminal_print()
            if not context_summary:
                terminal_print(
                    f"Can't determine context details for context '{_context_name}'."
                )
            else:
                terminal_print_model(
                    context_summary,
                    full_details=True,
                    in_panel=f"Context details: {_context_name}",
                )
            terminal_print()
            txt_pre = r"Deleting context '[b i]"
            txt_post = r"[/b i]', are you sure? \[yes/no]: "
            txt_all = f"{txt_pre}{_context_name}{txt_post}"
            user_input = get_console().input(
                txt_all,
            )

            if user_input.lower() == "yes":
                confirmed = True
        else:
            confirmed = True

        if not confirmed:
            terminal_print("\nDoing nothing...")
            sys.exit(0)

        terminal_print("Deleting context...")
        kiara_config.delete(context_name=_context_name, dry_run=False)

        terminal_print("Done.")


@context.group("info")
@click.pass_context
def info(ctx):
    """Information about several aspects of the current/specified kiara context."""


@info.group("config")
@click.pass_context
def config(ctx):
    """Information about the current context configuration."""


@config.command("print")
@output_format_option()
@click.pass_context
def print_config(ctx, format) -> None:
    """Print the (current) kiara context configuration."""
    kiara_obj: Kiara = ctx.obj.kiara

    terminal_print_model(
        kiara_obj.context_config,
        format=format,
        in_panel=f"kiara context config: [b i]{kiara_obj.context_config.context_id}[/b i]",
    )


@config.command("help")
@click.pass_context
def config_help(ctx):
    """Print available configuration options and information about them."""
    from kiara.context import KiaraContextConfig
    from kiara.utils.output import create_table_from_base_model_cls

    table = create_table_from_base_model_cls(model_cls=KiaraContextConfig)
    terminal_print()
    terminal_print(Panel(table))


@info.group(name="environment")
@click.pass_context
def env_group(ctx):
    """Information about whats in the current environment details (Python virtual environment, available metadata models, etc)."""


@env_group.command("print")
@output_format_option()
@click.pass_context
def print_context(ctx, format: str):
    """Print all relevant models within the current (Python virtual) environment."""
    kiara_obj: Kiara = ctx.obj.kiara

    terminal_print_model(
        kiara_obj.context_info,
        format=format,
        in_panel=f"Context info for kiara id: {kiara_obj.id}",
    )


# @info.group(name="runtime")
# @click.pass_context
# def runtime_group(ctx):
#     """Runtime-related sub-commands."""
#
# @runtime_group.command("print")
# @output_format_option()
# @click.pass_context
# def print_runtime(ctx, format: str):
#     """Print runtime information."""
#     kiara_obj: Kiara = ctx.obj.kiara
#
#     terminal_print_model(
#         kiara_obj.runtime_config,
#         format=format,
#         in_panel=f"Runtime info for kiara id: {kiara_obj.id}",
#     )


@env_group.command("list")
@click.pass_context
def list_envs(ctx):
    """List available runtime environment information."""
    from kiara.registries.environment import EnvironmentRegistry

    env_reg = EnvironmentRegistry.instance()

    terminal_print(env_reg)


@env_group.command("explain")
@click.argument("env_type", metavar="ENVIRONMENT_TYPE", nargs=1, required=True)
@output_format_option()
@click.pass_context
def explain_env(ctx, env_type: str, format: str) -> None:

    from kiara.registries.environment import EnvironmentRegistry

    env_reg = EnvironmentRegistry.instance()

    env = env_reg.environments.get(env_type, None)
    if env is None:
        terminal_print()
        terminal_print(
            f"No environment with name '{env_type}' available. Available types: {', '.join(env_reg.environments.keys())}"
        )
        sys.exit()

    terminal_print_model(
        env,
        format=format,
        in_panel=f"Details for environment: [b i]{env_type}[/b i]",
        summary=False,
    )


@env_group.group()
@click.pass_context
def metadata(ctx):
    """Metadata-related sub-commands."""


@metadata.command(name="list")
@output_format_option()
@click.pass_context
def list_metadata(ctx, format) -> None:
    """List available metadata schemas."""
    kiara_obj: Kiara = ctx.obj.kiara
    from kiara.models.values.value_metadata import ValueMetadata

    metadata_types = kiara_obj.kiara_model_registry.get_models_of_type(ValueMetadata)

    terminal_print_model(
        metadata_types, format=format, in_panel="Available metadata types"
    )


@metadata.command(name="explain")
@click.argument("metadata_key", nargs=1, required=True)
# @click.option(
#     "--details",
#     "-d",
#     help="Print more metadata schema details (for 'terminal' format).",
#     is_flag=True,
# )
@output_format_option()
@click.pass_context
def explain_metadata(ctx, metadata_key, format) -> None:
    """Print details for a specific metadata schema."""
    kiara_obj: Kiara = ctx.obj.kiara
    from kiara.models.values.value_metadata import ValueMetadata

    metadata_types = kiara_obj.kiara_model_registry.get_models_of_type(ValueMetadata)

    if metadata_key not in metadata_types.item_infos.keys():
        terminal_print()
        terminal_print(f"No metadata schema for key '{metadata_key}' found...")
        sys.exit(1)

    info_obj = metadata_types.item_infos[metadata_key]

    terminal_print_model(
        info_obj,
        format=format,
        in_panel=f"Details for metadata type: [b i]{metadata_key}[/b i]",
    )


# @env_group.group()
# @click.pass_context
# def api(ctx):
#     """API-related sub-commands."""
#
#
# @api.command()
# @click.argument("filter", nargs=-1, required=False)
# @click.option(
#     "--full-doc",
#     "-d",
#     is_flag=True,
#     help="Display the full doc for all operations (when using 'terminal' as format).",
# )
# @click.pass_context
# def list_endpoints(ctx, filter, full_doc):
#     """List all available API endpoints."""
#     from kiara.interfaces.python_api import KiaraAPI
#     from kiara.interfaces.python_api.proxy import ApiEndpoints
#
#     exclude = ["get_runtime_config", "retrieve_workflow_info"]
#     endpoints = ApiEndpoints(api_cls=KiaraAPI, filters=filter, exclude=exclude)
#
#     terminal_print()
#     terminal_print(endpoints, in_panel="API endpoints", full_doc=full_doc)
#
#
@context.group("service")
@click.pass_context
def service(ctx):
    """Service-related sub-commands."""


@service.command("start")
@click.option("--host", help="The host to bind to.", default="localhost")
@click.option("--port", help="The port to bind to.", required=False, default=0)
@click.option(
    "--monitor",
    help="Monitor the service.",
    required=False,
    default=False,
    is_flag=True,
)
@click.option(
    "--stdout",
    "-o",
    help="Write output logs to this file.",
    required=False,
    default=None,
)
@click.option(
    "--stderr",
    "-e",
    help="Write error logs to this file.",
    required=False,
    default=None,
)
@click.option(
    "--timeout",
    "-t",
    help="If set, the service shuts down if not request happens within the specified timeout (in milliseconds).",
    required=False,
    default=0,
)
@click.pass_context
def start_service(
    ctx,
    host: str,
    port: int,
    monitor: bool = False,
    stdout: Union[str, None] = None,
    stderr: Union[str, None] = None,
    timeout: int = 0,
):
    """Start a kiara zmq service for this context."""

    from kiara.utils.output import create_table_from_model_object
    from kiara.zmq import start_zmq_service

    api_wrap: BaseAPIWrap = ctx.obj

    try:
        details = start_zmq_service(
            api_wrap=api_wrap,
            host=host,
            port=port,
            monitor=monitor,
            stdout=stdout,
            stderr=stderr,
            timeout=timeout,
        )

        if not monitor:
            assert details is not None
            if not details.newly_started:
                terminal_print()
                terminal_print(
                    f"Service for context '{api_wrap.kiara_context_name}' already running, doing nothing..."
                )
            else:
                terminal_print()
                terminal_print("Started service in background process:")
                terminal_print()
                table = create_table_from_model_object(
                    details,
                    render_config={"show_header": False, "show_type_column": False},
                )
                terminal_print(table, in_panel="Service details")

    except Exception as e:
        import traceback

        traceback.print_exc()
        log_exception(e)
        terminal_print()
        terminal_print(f"Error starting service: {e}")
        sys.exit(1)


@service.command("stop")
@click.argument("context_name", nargs=1, required=False)
@click.pass_context
def stop_service(ctx, context_name: Union[None, str]):
    """Stop a running zmq service for this context."""

    from kiara.zmq import get_context_details
    from kiara.zmq.client import KiaraZmqClient

    if not context_name:
        context_name = ctx.obj.kiara_context_name

    context_details = get_context_details(context_name=context_name)  # type: ignore
    if not context_details:
        terminal_print()
        terminal_print(
            f"No service running for context '{context_name}'. Doing nothing..."
        )
        sys.exit(0)

    host = context_details["host"]
    port = context_details["port"]
    zmq_client = KiaraZmqClient(host=host, port=port)
    zmq_client.request(endpoint_name="stop", args={})

    terminal_print()
    terminal_print(f"Stopped context: {context_name}")

    sys.exit(0)


@service.command("list")
@click.option("--details", "-d", help="Show more details.", is_flag=True, default=False)
@click.pass_context
def list_services(ctx, details: bool):
    """List all contexts that have a currently running service."""

    from kiara.zmq import get_context_details, list_registered_contexts
    from kiara.zmq.client import KiaraZmqClient

    contexts = list_registered_contexts()
    if not contexts:
        terminal_print()
        terminal_print("No services running.")
        sys.exit(0)

    terminal_print()

    if not details:
        terminal_print("Running services:")
        for c in contexts:
            terminal_print(f" - {c}")

    else:

        from kiara.context import KiaraContextConfig, KiaraRuntimeConfig
        from kiara.utils.output import create_table_from_model_object

        table = Table(show_header=True, show_lines=False, box=box.SIMPLE)
        table.add_column("context")
        table.add_column("details")

        for c in contexts:
            context_details = get_context_details(c)
            if not context_details:
                table.add_row(c, "Can't get context details, skipping...")
                continue
            zmq_client = KiaraZmqClient(
                host=context_details["host"], port=context_details["port"]
            )
            status = zmq_client.request(endpoint_name="service_status", args={})
            state = status["state"]
            timeout = status["timeout"]
            context_config = KiaraContextConfig(**status["context_config"])
            runtime_config = KiaraRuntimeConfig(**status["runtime_config"])

            config_table = create_table_from_model_object(context_config)
            runtime_table = create_table_from_model_object(runtime_config)

            c_table = Table(show_header=False, show_lines=False, box=box.SIMPLE)
            c_table.add_column("key", style="i")
            c_table.add_column("value")
            c_table.add_row("state", state)
            c_table.add_row("timeout", str(timeout))
            c_table.add_row("context config", config_table)
            c_table.add_row("runtime config", runtime_table)

            table.add_row(c, c_table)

        terminal_print(table, in_panel="Service details")


@service.command("request")
@click.option(
    "--context", "-c", help="The context to use.", required=False, default=None
)
@click.argument("endpoint", required=True, nargs=1)
@click.argument("arguments", required=False, nargs=-1)
@click.pass_context
def request(
    ctx, endpoint: str, arguments: Tuple[str], context: Union[None, str] = None
):
    """Send a request to a kiara zmq service."""

    from kiara.zmq import get_context_details
    from kiara.zmq.client import KiaraZmqClient

    if not context:
        context = ctx.obj.kiara_context_name

    context_details = get_context_details(context_name=context)  # type: ignore
    if not context_details:
        terminal_print()
        terminal_print(f"No service running for context '{context}'. Doing nothing...")
        sys.exit(1)

    zmq_client = KiaraZmqClient(
        host=context_details["host"], port=context_details["port"]
    )

    args = dict_from_cli_args(*arguments)

    try:
        result = zmq_client.request(endpoint_name=endpoint, args=args)
        print(result)  # noqa
    except KeyboardInterrupt:
        terminal_print("\nInterrupted by user, closing connection to service...")
    finally:
        zmq_client.close()


CLI_CLIENT_CLICK_CONTEXT_SETTINGS = {
    "help_option_names": [],
    "ignore_unknown_options": True,
}


@service.command("request-cli", context_settings=CLI_CLIENT_CLICK_CONTEXT_SETTINGS)
@click.option(
    "--context", "-c", help="The context to use.", required=False, default=None
)
@click.argument("arguments", required=False, nargs=-1)
@click.pass_context
def request_cli(ctx, arguments: Tuple[str], context: Union[None, str] = None):
    """Send a request to a kiara zmq service."""
    from kiara.zmq import get_context_details
    from kiara.zmq.client import KiaraZmqClient

    if not context:
        context = ctx.obj.kiara_context_name

    context_details = get_context_details(context_name=context)  # type: ignore
    if not context_details:
        terminal_print()
        terminal_print(f"No service running for context '{context}'. Doing nothing...")
        sys.exit(1)

    zmq_client = KiaraZmqClient(
        host=context_details["host"], port=context_details["port"]
    )

    try:
        zmq_client.request_cli(args=arguments)
    except KeyboardInterrupt:
        terminal_print("\nInterrupted by user, closing connection to service...")
    finally:
        zmq_client.close()


# kiara\kiara\src\kiara\interfaces\cli\context\__init__.py


# kiara\kiara\src\kiara\interfaces\cli\data\commands.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

"""Data-related sub-commands for the cli."""
import os.path
import sys
import uuid
from pathlib import Path
from typing import TYPE_CHECKING, Iterable, Tuple, Union

import rich_click as click
import structlog

from kiara.defaults import DATA_ARCHIVE_DEFAULT_VALUE_MARKER
from kiara.exceptions import InvalidCommandLineInvocation
from kiara.utils import is_develop, log_exception, log_message
from kiara.utils.cli import output_format_option, terminal_print, terminal_print_model
from kiara.utils.cli.exceptions import handle_exception

# from kiara.interfaces.python_api.models.info import RENDER_FIELDS, ValueInfo, ValuesInfo
# from kiara.interfaces.tui.pager import PagerApp
# from kiara.operations.included_core_operations.filter import FilterOperationType


# from kiara.utils.cli.rich_click import rich_format_filter_operation_help
# from kiara.utils.cli.run import (
#     _validate_save_option,
#     calculate_aliases,
#     execute_job,
#     set_and_validate_inputs,
#     validate_operation_in_terminal,
# )
# from kiara.utils.output import OutputDetails


if TYPE_CHECKING:
    from kiara.interfaces.python_api.base_api import BaseAPI, Kiara
    from kiara.operations.included_core_operations.filter import FilterOperationType

logger = structlog.getLogger()


@click.group()
@click.pass_context
def data(ctx):
    """Data-related sub-commands."""


@data.command(name="list")
@click.argument("filter", nargs=-1, required=False)
@click.option(
    "--all-values",
    "-a",
    help="Also list values without aliases.",
    is_flag=True,
    default=False,
)
@click.option(
    "--include-internal",
    "-I",
    help="Also list values that are used mostly internally (e.g. metadata for other values, ...). Implies 'all-ids' "
    "is 'True'.",
    is_flag=True,
)
@click.option(
    "--value_id",
    "-i",
    help="Display value id information for each value.",
    default=False,
    is_flag=True,
)
@click.option(
    "--date-created",
    "-D",
    help="Display the date when the value was created.",
    default=False,
    is_flag=True,
)
@click.option(
    "--type-config",
    "-c",
    help="Display type details for each value.",
    default=False,
    is_flag=True,
)
@click.option(
    "--hash", "-H", help="Display the value hash.", default=False, is_flag=True
)
@click.option(
    "--lineage",
    "-l",
    help="Display lineage information for each value.",
    default=False,
    is_flag=True,
)
@click.option(
    "--pedigree",
    "-P",
    help="Display pedigree information for each value.",
    default=False,
    is_flag=True,
)
@click.option(
    "--data",
    "-d",
    help="Show a preview of the data associated with this value.",
    default=False,
    is_flag=True,
)
@click.option(
    "--serialized",
    "-s",
    help="Display serialization details for this value.",
    is_flag=True,
)
@click.option("--properties", "-p", help="Display the value properties.", is_flag=True)
@click.option(
    "--data-type",
    "-t",
    help="Only display values that match the specified type(s)",
    multiple=True,
    required=False,
)
@output_format_option()
@click.pass_context
@handle_exception()
def list_values(
    ctx,
    filter,
    format,
    all_values,
    hash,
    include_internal,
    value_id,
    date_created,
    pedigree,
    data,
    type_config,
    serialized,
    properties,
    data_type,
    lineage,
) -> None:
    """List all data items that are stored in kiara."""

    from kiara.interfaces.python_api.models.info import RENDER_FIELDS, ValuesInfo

    kiara_api: BaseAPI = ctx.obj.base_api

    if include_internal:
        all_values = True

    matcher_config = {"allow_internal": include_internal, "has_alias": not all_values}
    if data_type:
        matcher_config["data_types"] = data_type

    if filter:
        matcher_config["alias_matchers"] = filter

    values = kiara_api.list_values(**matcher_config)

    list_by_alias = True

    render_fields = [k for k, v in RENDER_FIELDS.items() if v["show_default"]]
    if list_by_alias:
        render_fields[0] = "aliases"
        render_fields[1] = "value_id"

    if not value_id and not all_values:
        render_fields.remove("value_id")

    if date_created:
        render_fields.append("value_created")
    if type_config:
        render_fields.append("data_type_config")
    if hash:
        render_fields.append("hash")
    if data:
        render_fields.append("data")
    if properties:
        render_fields.append("properties")
    if pedigree:
        render_fields.append("pedigree")
    if lineage:
        render_fields.append("lineage")
    if serialized:
        render_fields.append("serialize_details")

    values_info_model = ValuesInfo.create_from_instances(
        kiara=kiara_api.context, instances={str(k): v for k, v in values.items()}
    )

    render_config = {
        "render_type": "terminal",
        "list_by_alias": list_by_alias,
        "show_internal_values": include_internal,
        "render_fields": render_fields,
    }

    if not all_values:
        title = "Available aliases"
    else:
        title = "Available values"

    terminal_print_model(
        values_info_model, format=format, in_panel=title, **render_config
    )


@data.command(name="explain")
@click.argument("value_id", nargs=-1, required=True)
@click.option(
    "--pedigree", "-P", help="Display pedigree information for the value.", is_flag=True
)
@click.option(
    "--lineage", "-l", help="Display lineage information for the value.", is_flag=True
)
@click.option(
    "--serialized",
    "-s",
    help="Display this values' serialization information.",
    is_flag=True,
)
@click.option("--preview-data", "-d", help="Display a data preview.", is_flag=True)
@click.option(
    "--properties",
    "-p",
    help="Resolve and display properties of this value.",
    is_flag=True,
)
@click.option(
    "--destinies",
    "-D",
    help="Resolve and display values destinies for this value.",
    is_flag=True,
)
@click.option(
    "--destiny-backlinks",
    "-B",
    help="Resolve and display values this value is a destiny for.",
    is_flag=True,
)
@click.option(
    "--environment", "-e", help="Show environment hashes and data.", is_flag=True
)
@output_format_option()
@click.pass_context
@handle_exception()
def explain_value(
    ctx,
    value_id: Tuple[str],
    pedigree: bool,
    serialized: bool,
    format: str,
    preview_data: bool,
    properties: bool,
    destinies: bool,
    destiny_backlinks: bool,
    lineage: bool,
    environment: bool,
):
    """
    Print the metadata of a stored value.

    All of the 'show-additional-information' flags are only applied when the 'terminal' output format is selected. This might change in the future.
    """

    kiara_api: BaseAPI = ctx.obj.base_api

    render_config = {
        "show_pedigree": pedigree,
        "show_serialized": serialized,
        "show_data_preview": preview_data,
        "show_properties": properties,
        "show_destinies": destinies,
        "show_destiny_backlinks": destiny_backlinks,
        "show_lineage": lineage,
        "show_environment_hashes": environment,
        "show_environment_data": False,
    }

    all_values = []
    for v_id in value_id:
        try:
            value_info = kiara_api.retrieve_value_info(v_id)
        except Exception as e:
            terminal_print()
            terminal_print(f"[red]Error[/red]: {e}")
            sys.exit(1)
        if not value_info:
            terminal_print(f"[red]Error[/red]: No value found for: {v_id}")
            sys.exit(1)
        all_values.append(value_info)

    if len(all_values) == 1:
        title = f"Value details for: [b i]{value_id[0]}[/b i]"
    else:
        title = "Value details"

    # v_infos = (
    #     ValueInfo.create_from_instance(kiara=kiara_obj, instance=v) for v in all_values
    # )

    terminal_print_model(*all_values, format=format, in_panel=title, **render_config)


@data.command(name="load")
@click.argument("value", nargs=1, required=True)
# @click.option(
#     "--single-page",
#     "-s",
#     help="Only pretty print a single (preview) page, instead of using a pager when available.",
#     is_flag=True,
# )
@click.pass_context
@handle_exception()
def load_value(ctx, value: str):
    """Load a stored value and print it in a format suitable for the terminal."""
    # kiara_obj: Kiara = ctx.obj["kiara"]
    kiara_api: BaseAPI = ctx.obj.base_api

    try:
        _value = kiara_api.get_value(value=value)
    except Exception as e:
        terminal_print()
        terminal_print(f"[red]Error[/red]: {e}")
        sys.exit(1)
    if not _value:
        terminal_print(f"[red]Error[/red]: No value found for: {value}")
        sys.exit(1)

    logger.debug(
        "fallback.render_value",
        solution="use pretty print",
        source_type=_value.data_type_name,
        target_type="terminal_renderable",
        reason="no 'render_value' operation for source/target operation",
    )
    try:
        renderable = kiara_api.context.data_registry.pretty_print_data(
            _value.value_id, target_type="terminal_renderable"
        )
    except Exception as e:
        log_exception(e)
        log_message("error.pretty_print", value=_value.value_id, error=e)
        renderable = [str(_value.data)]

    terminal_print(renderable)
    sys.exit(0)
    # from kiara.interfaces.tui.pager import PagerApp
    #
    # app = PagerApp(api=kiara_api, value=str(_value.value_id))
    # app.run()


@data.command("filter")
@click.argument("value", nargs=1, required=True, default="__no_value__")
@click.argument("filters", nargs=1, default="__no_filters__")
@click.argument("inputs", nargs=-1, required=False)
@click.option(
    "--explain",
    "-e",
    help="Display information about the selected operation and exit.",
    is_flag=True,
)
@click.option(
    "--output", "-o", help="The output format and configuration.", multiple=True
)
@click.option(
    "--save",
    "-s",
    help="Save one or several of the outputs of this run. If the argument contains a '=', the format is [output_name]=[alias], if not, the values will be saved as '[alias]-[output_name]'.",
    required=False,
    multiple=True,
)
@click.option("--help", "-h", help="Show this message and exit.", is_flag=True)
@click.pass_context
def filter_value(
    ctx,
    value: str,
    filters: str,
    inputs: Iterable[str],
    explain: bool,
    output: Iterable[str],
    save: Iterable[str],
    help: bool,
):
    """
    Filter a value, then display it like the 'load' subcommand does.

    Filters must be provided as a single string, where filters are seperated using ":".
    """
    from kiara.utils.cli.rich_click import rich_format_filter_operation_help
    from kiara.utils.cli.run import (
        _validate_save_option,
        calculate_aliases,
        execute_job,
        set_and_validate_inputs,
        validate_operation_in_terminal,
    )
    from kiara.utils.output import OutputDetails

    save_results = _validate_save_option(save)

    output_details = OutputDetails.from_data(output)
    silent = False
    if output_details.format == "silent":
        silent = True

    kiara_obj: Kiara = ctx.obj.kiara
    api: BaseAPI = ctx.obj.base_api

    cmd_help = "[yellow bold]Usage: [/yellow bold][bold]kiara data filter VALUE FILTER_1:FILTER_2 [FILTER ARGS...][/bold]"

    if help and value == "__no_value__":
        rich_format_filter_operation_help(
            api=api,
            obj=ctx.command,
            ctx=ctx,
            cmd_help=cmd_help,
        )
        sys.exit(0)

    if filters == "__no_filters__":
        rich_format_filter_operation_help(
            api=api, obj=ctx.command, ctx=ctx, cmd_help=cmd_help, value=value
        )
        sys.exit(0)

    try:
        _value = kiara_obj.data_registry.get_value(value=value)
    except Exception as e:
        terminal_print()
        terminal_print(f"[red]Error[/red]: {e}")
        sys.exit(1)
    if not _value:
        terminal_print(f"[red]Error[/red]: No value found for: {value}")
        sys.exit(1)

    _filter_names = filters.split(":")
    filter_names = []
    for fn in _filter_names:
        filter_names.extend(fn.split(":"))

    filter_op_type: FilterOperationType = (
        kiara_obj.operation_registry.get_operation_type("filter")
    )
    op = filter_op_type.create_filter_operation(
        data_type=_value.data_type_name, filters=filter_names
    )

    all_inputs = [f"value={value}"]
    all_inputs.extend(inputs)

    try:
        kiara_op = validate_operation_in_terminal(
            api=api, module_or_operation=op.module_config
        )
    except InvalidCommandLineInvocation as e:
        ctx.obj.exit(msg=None, exit_code=e.error_code)
        sys.exit(1)

    final_aliases = calculate_aliases(operation=kiara_op, alias_tokens=save)
    try:
        inputs_value_map = set_and_validate_inputs(
            api=api,
            operation=kiara_op,
            inputs=all_inputs,
            explain=explain,
            print_help=help,
            click_context=ctx,
            cmd_help=cmd_help,
        )
        if inputs_value_map is None:
            ctx.obj.exit(msg=None, exit_code=0)
            return
    except InvalidCommandLineInvocation as e:
        ctx.obj.exit(msg=None, exit_code=e.error_code)
        return

    job_id = execute_job(
        api=api,
        operation=kiara_op,
        inputs=inputs_value_map,
        silent=True,
        save_results=False,
        aliases=final_aliases,
    )

    if not silent:
        result = api.get_job_result(job_id=job_id)
        # result = kiara_op.retrieve_result(job_id=job_id)

        title = f"[b]Value '[i]{value}[/i]'[/b], filtered with: {filters}"
        filtered = result["filtered_value"]
        try:
            renderable = api.context.data_registry.pretty_print_data(
                filtered.value_id, target_type="terminal_renderable"
            )
        except Exception as e:
            log_exception(e)
            log_message("error.pretty_print", value=_value.value_id, error=e)
            renderable = [str(_value.data)]
        terminal_print(
            renderable, in_panel=title, empty_line_before=True, show_data_type=True
        )

    if save_results:
        try:
            result = api.get_job_result(job_id=job_id)
            saved_results = api.store_values(result, alias_map=final_aliases)

            api.context.job_registry.store_job_record(job_id=job_id)

            if len(saved_results) == 1:
                title = "[b]Stored result value[/b]"
            else:
                title = "[b]Stored result values[/b]"
            terminal_print(saved_results, in_panel=title, empty_line_before=True)
        except Exception as e:
            log_exception(e)
            terminal_print(f"[red]Error saving results[/red]: {e}")
            sys.exit(1)


@data.command(name="export")
@click.option(
    "--archive-name",
    "-A",
    help="The name to use for the exported archive. If not provided, the first alias will be used.",
    required=False,
)
@click.option(
    "--path",
    "-p",
    help="The path of the exported archive. If not provided, '<archive_alias>.karchive' will be used.",
    required=False,
)
@click.option(
    "--compression",
    "-c",
    help="The compression inside the archive. If not provided, 'zstd' will be used.",
    type=click.Choice(["zstd", "lz4", "lzma", "none"]),
    default="zstd",
)
@click.option("--append", "-a", help="Append data to existing archive.", is_flag=True)
@click.option("--replace", help="Replace existing archive.", is_flag=True)
# @click.option(
#     "--no-default-value", "-nd", help="Do not set a default value.", is_flag=True
# )
# @click.option("--no-aliases", "-na", help="Do not store aliases.", is_flag=True)
@click.argument("aliases", nargs=-1, required=True)
@click.pass_context
@handle_exception()
def export_data_archive(
    ctx,
    aliases: Tuple[str],
    archive_name: Union[None, str],
    path: Union[str, None],
    compression: str,
    append: bool,
    replace: bool,
    no_default_value: bool = False,
    no_aliases: bool = False,
):
    """Export one or several values into a new (or existing) kiara archive.

    Aliases that already exist in the target archve will be overwritten.
    """

    kiara_api: BaseAPI = ctx.obj.base_api

    values = []
    for idx, alias in enumerate(aliases, start=1):
        if "=" in alias:
            old_alias, new_alias = alias.split("=", maxsplit=1)
        else:
            try:
                uuid.UUID(alias)
                old_alias = alias
                new_alias = None
            except Exception:
                old_alias = alias
                new_alias = alias

        value = kiara_api.get_value(old_alias)
        values.append((value, new_alias))

    if not archive_name:
        archive_name = values[0][1]

    if not archive_name:
        archive_name = str(values[0][0].value_id)

    if not path:
        base_path = "."
        if archive_name.endswith(".kiarchive"):
            file_name = archive_name
        else:
            file_name = f"{archive_name}.kiarchive"
    else:
        base_path = os.path.dirname(path)
        file_name = os.path.basename(path)
        if "." not in file_name:
            file_name = f"{file_name}.kiarchive"

    full_path = Path(base_path) / file_name

    delete = False

    if full_path.exists() and (not append and not replace):
        terminal_print(
            f"[red]Error[/red]: File '{full_path}' already exists and '--append' or '--replace' not specified."
        )
        sys.exit(1)
    elif full_path.exists():
        if append and replace:
            terminal_print(
                "[red]Error[/red]: Can't specify both '--append' and '--replace'."
            )
            sys.exit(1)
        if append:
            terminal_print(f"Appending to existing data_store '{file_name}'...")
        else:
            terminal_print(f"Replacing existing data_store '{file_name}'...")
            delete = True
    else:
        terminal_print(f"Creating new data_store '{file_name}'...")

    terminal_print("Exporting value(s) into new data_store...")

    # no_default_value = False
    #
    # if not no_default_value:
    #     try:
    #         data_store.set_archive_metadata_value(
    #             DATA_ARCHIVE_DEFAULT_VALUE_MARKER, str(values[0][0].value_id)
    #         )
    #     except Exception as e:
    #         data_store.delete_archive(archive_id=data_store.archive_id)
    #         log_exception(e)
    #         terminal_print(f"[red]Error setting value[/red]: {e}")
    #         sys.exit(1)

    values_to_store = {}
    alias_map = {}
    for idx, (value, value_alias) in enumerate(values, start=1):
        key = f"value_{idx}"
        values_to_store[key] = value
        if value_alias:
            alias_map[key] = [value_alias]

    target_store_params = {
        "compression": compression,
    }
    try:
        no_default_value = False
        if not no_default_value:
            metadata_to_add = {
                DATA_ARCHIVE_DEFAULT_VALUE_MARKER: str(values[0][0].value_id)
            }
        else:
            metadata_to_add = None

        if delete:
            os.unlink(full_path)

        store_result = kiara_api.export_values(
            target_archive=full_path,
            values=values_to_store,
            alias_map=alias_map,
            allow_alias_overwrite=True,
            target_registered_name=archive_name,
            append=append,
            target_store_params=target_store_params,
            additional_archive_metadata=metadata_to_add,
        )
        render_config = {"add_field_column": False}
        terminal_print_model(
            store_result,
            format="terminal",
            empty_line_before=None,
            in_panel="Exported values",
            **render_config,
        )

    except Exception as e:
        # TODO: remove archive if it didn't exist before?
        log_exception(e)
        terminal_print(f"[red]Error saving results[/red]: {e}")
        sys.exit(1)


@data.command(name="import")
@click.argument("archive", nargs=1, required=True)
@click.argument("values", nargs=-1, required=True)
@click.option("--no-aliases", "-na", help="Do not store aliases.", is_flag=True)
@click.pass_context
@handle_exception()
def import_data_store(ctx, archive: str, values: Tuple[str], no_aliases: bool = False):
    """Import one or several values from a kiara archive."""

    kiara_api: BaseAPI = ctx.obj.base_api

    archive_path = Path(archive)
    if not archive_path.exists():
        terminal_print()
        terminal_print(f"[red]Error[/red]: Archive '{archive}' does not exist.")
        sys.exit(1)

    result = kiara_api.import_values(
        source_archive=archive, values=values, alias_map=not no_aliases
    )
    terminal_print(result)

    terminal_print("Done.")


if is_develop():

    @data.command(name="write_value")
    @click.argument("value_id_or_alias", nargs=1, required=True)
    @click.option(
        "--directory",
        "-d",
        help="The directory to write the serialized value to.",
        required=False,
    )
    @click.option(
        "--force", "-f", help="Overwrite existing files.", is_flag=True, default=False
    )
    @click.pass_context
    @handle_exception()
    def write_serialized(ctx, value_id_or_alias: str, directory: str, force: bool):
        """Write the serialized form of a value to a directory"""

        kiara_api: BaseAPI = ctx.obj.base_api

        value = kiara_api.get_value(value_id_or_alias)
        serialized = value.serialized_data

        keys = serialized.get_keys()

        if not directory:
            directory = "."

        path = Path(directory)

        for key in keys:
            data = serialized.get_serialized_data(key)

            key_path = path / key
            if key_path.exists() and not force:
                terminal_print(f"Error writing file for '{key}': file already exists.")
                sys.exit(1)

            key_path.parent.mkdir(parents=True, exist_ok=True)

            chunks = data.get_chunks(as_files=False)

            terminal_print(f"- writing file for: {key}")
            with open(key_path, "wb") as f:
                for chunk in chunks:
                    f.write(chunk)  # type: ignore


# kiara\kiara\src\kiara\interfaces\cli\data\__init__.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)


# kiara\kiara\src\kiara\interfaces\cli\info\commands.py
# -*- coding: utf-8 -*-
from typing import TYPE_CHECKING

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)
import rich_click as click

from kiara.utils.cli import output_format_option, terminal_print_model
from kiara.utils.cli.exceptions import handle_exception

if TYPE_CHECKING:
    from kiara.interfaces import BaseAPI, BaseAPIWrap


@click.group("info")
@click.pass_context
def info(ctx):
    """Kiara config related sub-commands."""


@info.group("config")
@click.pass_context
def config(ctx):
    """Kiara config related sub-commands."""


@config.command("print")
@output_format_option()
@click.pass_context
def print_config(ctx, format: str):

    from kiara.context import KiaraConfig

    wrap: "BaseAPIWrap" = ctx.obj
    config: KiaraConfig = wrap.kiara_config
    title = "kiara config"
    if config._config_path:
        title = f"{title} - [i]{config._config_path}[/i]"

    terminal_print_model(config, format=format, in_panel=title)


@info.group("plugin")
@click.pass_context
def plugin(ctx):
    """Kiara plugin related sub-commands."""


@plugin.command("list")
@click.argument("filter-regex", nargs=1, required=False)
@output_format_option()
@click.pass_context
def list_plugins(ctx, filter_regex: str, format):
    """List installed kiara plugins."""

    from kiara.interfaces.python_api.models.info import KiaraPluginInfos

    api: BaseAPI = ctx.obj.base_api

    title = "All available plugins"
    if filter_regex:
        title = "Matching plugins"

    plugin_infos = KiaraPluginInfos.create_group(api.context, title, filter_regex)

    terminal_print_model(plugin_infos, format=format, in_panel=title)


@plugin.command("explain")
@click.argument("plugin_name", nargs=1)
@output_format_option()
@handle_exception()
@click.pass_context
def explain_plugin_info(ctx, plugin_name: str, format: str):

    kiara_api: BaseAPI = ctx.obj.base_api

    plugin_info = kiara_api.retrieve_plugin_info(plugin_name)
    title = f"Info for plugin: [i]{plugin_name}[/i]"

    terminal_print_model(plugin_info, format=format, in_panel=title)


# kiara\kiara\src\kiara\interfaces\cli\info\__init__.py


# kiara\kiara\src\kiara\interfaces\cli\module\commands.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

"""Module related subcommands for the cli."""

from typing import TYPE_CHECKING, Any, Iterable, Union

import rich_click as click

from kiara.utils.cli import (
    dict_from_cli_args,
    output_format_option,
    terminal_print_model,
)

if TYPE_CHECKING:
    from kiara.interfaces.python_api.base_api import BaseAPI


@click.group()
@click.pass_context
def module(ctx):
    """Module-related sub-commands."""


@module.command(name="list")
@click.option(
    "--full-doc",
    "-d",
    is_flag=True,
    help="Display the full documentation for every module type (when using 'terminal' output format).",
)
@output_format_option()
@click.argument("filter", nargs=-1, required=False)
@click.option(
    "--python-package",
    "-p",
    help="Only return modules from this package.",
    required=False,
)
@click.pass_context
def list_modules(
    ctx,
    full_doc: bool,
    filter: Iterable[str],
    format: str,
    python_package: Union[str, None],
):
    """List available module data_types."""
    kiara_api: BaseAPI = ctx.obj.base_api

    module_types_info = kiara_api.retrieve_module_types_info(
        filter=filter, python_package=python_package
    )

    if filter:
        title = f"Filtered modules: {filter}"
    else:
        title = "All modules"

    terminal_print_model(
        module_types_info, format=format, in_panel=title, full_doc=full_doc
    )


@module.command(name="explain")
@click.argument("module_type", nargs=1, required=True)
@output_format_option()
@click.pass_context
def explain_module_type(ctx, module_type: str, format: str):
    """
    Print details of a module type.

    This is different to the 'explain-instance' command, because module data_types need to be
    instantiated with configuration, before we can query all their properties (like
    input/output data_types).
    """
    kiara_api: BaseAPI = ctx.obj.base_api
    info = kiara_api.retrieve_module_type_info(module_type=module_type)

    terminal_print_model(
        info, format=format, in_panel=f"Module type: [b i]{module_type}[/b i]"
    )


@module.command("explain-instance")
@click.argument("module_type", nargs=1)
@click.argument(
    "module_config",
    nargs=-1,
)
@output_format_option()
@click.pass_context
def explain_module(ctx, module_type: str, module_config: Iterable[Any], format: str):
    """
    Describe a module instance.

    This command shows information and metadata about an instantiated *kiara* module.
    """
    if module_config:
        module_config = dict_from_cli_args(*module_config)
    else:
        module_config = {}

    kiara_api: BaseAPI = ctx.obj.base_api

    operation = kiara_api.create_operation(
        module_type=module_type, module_config=module_config
    )

    terminal_print_model(
        operation.create_renderable(),  # type: ignore
        format=format,
        in_panel=f"Module instance of type: [b i]{module_type}[/b i]",
    )


# kiara\kiara\src\kiara\interfaces\cli\module\__init__.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)


# kiara\kiara\src\kiara\interfaces\cli\operation\commands.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import os
import sys
from typing import TYPE_CHECKING, Iterable, Union

import rich_click as click

from kiara.utils.cli import output_format_option, terminal_print, terminal_print_model
from kiara.utils.cli.exceptions import handle_exception

if TYPE_CHECKING:
    from kiara.interfaces.python_api.base_api import BaseAPI, Kiara


@click.group()
@click.pass_context
def operation(ctx):
    """Operation-related sub-commands."""


@operation.command("list-types")
@click.option(
    "--full-doc",
    "-d",
    is_flag=True,
    help="Display the full documentation for every operation type (when using 'terminal' output format).",
)
@click.argument("filter", nargs=-1, required=False)
@output_format_option()
@click.pass_context
def list_types(ctx, full_doc: bool, format: str, filter: Iterable[str]):

    kiara_obj: Kiara = ctx.obj.kiara

    op_mgmt = kiara_obj.operation_registry

    op_types = op_mgmt.operation_type_classes

    title = "Available operation types"
    if filter:
        title = "Filtered data types"
        temp = {}
        for k, v in op_types.items():
            match = True
            for f in filter:
                if f.lower() not in k.lower():
                    match = False
                    break
            if match:
                temp[k] = v
        op_types = temp

    from kiara.interfaces.python_api.models.info import OperationTypeClassesInfo

    operation_types_info = OperationTypeClassesInfo.create_from_type_items(
        kiara=kiara_obj, group_title="all_items", **op_types
    )

    terminal_print_model(operation_types_info, format=format, in_panel=title)


@operation.command()
@click.argument("operation_type", nargs=1, required=True)
@output_format_option()
@click.pass_context
@handle_exception()
def explain_type(ctx, operation_type: str, format: str):

    kiara_api: BaseAPI = ctx.obj.base_api

    op_type = kiara_api.retrieve_operation_type_info(operation_type)

    terminal_print_model(
        op_type, format=format, in_panel=f"Operation type: [b i]{operation_type}[/b i]"
    )


@operation.command(name="list")
@click.option(
    "--by-type",
    "-t",
    is_flag=True,
    help="List the operations by operation type (when using 'terminal' as format).",
)
@click.argument("filter", nargs=-1, required=False)
@click.option(
    "--full-doc",
    "-d",
    is_flag=True,
    help="Display the full doc for all operations (when using 'terminal' as format).",
)
@click.option(
    "--include-internal",
    "-I",
    help="Whether to include operations that are mainly used internally.",
    is_flag=True,
)
@click.option(
    "--python-package",
    "-p",
    help="Only return modules from this package.",
    required=False,
)
@output_format_option()
@click.pass_context
def list_operations(
    ctx,
    by_type: bool,
    filter: Iterable[str],
    full_doc: bool,
    include_internal: bool,
    python_package: Union[str, None],
    format: str,
):

    kiara_obj: Kiara = ctx.obj.kiara
    api: BaseAPI = ctx.obj.base_api

    operations = api.list_operations(
        filter=filter, include_internal=include_internal, python_packages=python_package
    )

    # operations = kiara_obj.operation_registry.operations
    title = "Available operations"
    # if filter:
    #     title = "Filtered operations"
    #     temp = {}
    #     for op_id, op in operations.items():
    #         match = True
    #         for f in filter:
    #             if f.lower() not in op_id.lower():
    #                 match = False
    #                 break
    #         if match:
    #             temp[op_id] = op
    #     operations = temp
    #
    # if not include_internal:
    #     temp = {}
    #     for op_id, op in operations.items():
    #         if not op.operation_details.is_internal_operation:
    #             temp[op_id] = op
    #
    #     operations = temp

    from kiara.interfaces.python_api.models.info import OperationGroupInfo

    ops_info = OperationGroupInfo.create_from_operations(
        kiara=kiara_obj, group_title=title, **operations.root
    )

    terminal_print_model(
        ops_info,
        format=format,
        in_panel=title,
        include_internal_operations=include_internal,
        full_doc=full_doc,
        by_type=by_type,
    )


@operation.command()
@click.argument("operation_id", nargs=1, required=True)
@click.option(
    "--source",
    "-s",
    help="Show module source code (or pipeline configuration).",
    is_flag=True,
)
@click.option(
    "--module-info", "-m", help="Show module type and config information.", is_flag=True
)
@output_format_option()
@click.pass_context
@handle_exception()
def explain(ctx, operation_id: str, source: bool, format: str, module_info: bool):

    kiara_obj: Kiara = ctx.obj.kiara
    api: BaseAPI = ctx.obj.base_api

    if os.path.isfile(os.path.realpath(operation_id)):
        operation = api.get_operation(operation_id)
    else:
        operation = kiara_obj.operation_registry.get_operation(operation_id)

    if not operation:
        terminal_print()
        terminal_print(f"No operation with id '{operation_id}' registered.")
        sys.exit(1)

    terminal_print_model(
        operation,
        format=format,
        in_panel=f"Operation: [b i]{operation_id}[/b i]",
        include_src=source,
        include_module_details=module_info,
    )


# kiara\kiara\src\kiara\interfaces\cli\operation\__init__.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)


# kiara\kiara\src\kiara\interfaces\cli\pipeline\commands.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

"""Pipeline-related subcommands for the cli."""
import typing

import rich_click as click
from rich import box
from rich.table import Table

from kiara.defaults import KIARA_DEFAULT_STAGES_EXTRACTION_TYPE
from kiara.utils.cli import output_format_option, terminal_print_model
from kiara.utils.cli.exceptions import handle_exception

if typing.TYPE_CHECKING:
    from kiara.api import Kiara


@click.group()
@click.pass_context
def pipeline(ctx):
    """Pipeline-related sub-commands."""


@pipeline.command(name="list")
@click.option(
    "--full-doc",
    "-d",
    is_flag=True,
    help="Display the full documentation for every module type.",
)
@click.argument("filter", nargs=-1, required=False)
@output_format_option()
@click.pass_context
def list_pipelines(ctx, full_doc: bool, filter: typing.Iterable[str], format: str):
    """List available module data_types."""
    kiara_obj: Kiara = ctx.obj.kiara

    kiara_obj.operation_registry.get_operation_type("pipeline")

    table = Table(box=box.SIMPLE, show_header=True)
    table.add_column("Id", no_wrap=True)
    table.add_column("Description", no_wrap=False, style="i")

    op_ids = kiara_obj.operation_registry.operations_by_type["pipeline"]

    title = "Available pipelines"
    if filter:
        title = "Filtered pipelines"
        temp = {}
        for op_id in op_ids:
            op = kiara_obj.operation_registry.get_operation(op_id)
            match = True
            for f in filter:
                if f.lower() not in op_id.lower():
                    match = False
                    break
            if match:
                temp[op_id] = op
        operations = temp

    else:
        operations = {
            op_id: kiara_obj.operation_registry.get_operation(op_id) for op_id in op_ids
        }

    from kiara.interfaces.python_api.models.info import OperationGroupInfo

    ops_info = OperationGroupInfo.create_from_operations(
        kiara=kiara_obj, group_title=title, **operations
    )
    terminal_print_model(ops_info, format=format, in_panel=title, full_doc=full_doc)


@pipeline.command()
@click.argument("pipeline-name-or-path", nargs=1)
@click.option(
    "--stages-extraction-type",
    "-s",
    default=KIARA_DEFAULT_STAGES_EXTRACTION_TYPE,
    help="How to extract the stages from the pipeline structure. Available: 'late', 'early', as well as pipeline specific profiles (if in pipeline metadata).",
)
@output_format_option()
@click.pass_context
@handle_exception()
def explain(ctx, pipeline_name_or_path: str, format: str, stages_extraction_type: str):
    """Print details about pipeline inputs, outputs, and overall structure."""
    kiara_obj: Kiara = ctx.obj.kiara

    from kiara.utils.pipelines import get_pipeline_config

    pc = get_pipeline_config(kiara=kiara_obj, pipeline=pipeline_name_or_path)
    terminal_print_model(
        pc,
        format=format,
        in_panel=f"Pipeline: [b i]{pipeline_name_or_path}[/b i]",
        stages_extraction_type=stages_extraction_type,
        show_pipeline_inputs_for_steps=False,
    )


@pipeline.command()
@click.argument("pipeline-name-or-path", nargs=1)
@click.option(
    "--stages-extraction-type",
    "-s",
    default=KIARA_DEFAULT_STAGES_EXTRACTION_TYPE,
    help="How to extract the stages from the pipeline structure. Available: 'late', 'early', as well as pipeline specific profiles (if in pipeline metadata).",
)
@output_format_option()
@click.pass_context
@handle_exception()
def explain_stages(
    ctx, pipeline_name_or_path: str, format: str, stages_extraction_type: str
):
    """Print details about pipeline inputs, outputs, and overall structure."""
    from kiara.models.module.pipeline.stages import PipelineStages
    from kiara.utils.pipelines import get_pipeline_config

    kiara_obj: Kiara = ctx.obj.kiara

    pc = get_pipeline_config(kiara=kiara_obj, pipeline=pipeline_name_or_path)
    structure = pc.structure

    stages = PipelineStages.create(
        structure=structure, stages_extraction_type=stages_extraction_type
    )
    terminal_print_model(
        stages,
        format=format,
        in_panel=f"Stages for pipeline: [b i]{pipeline_name_or_path}[/b i]",
    )


@pipeline.command()
@click.argument("pipeline-name-or-path", nargs=1)
@click.pass_context
def execution_graph(ctx, pipeline_name_or_path: str):
    """Print the execution graph for a pipeline structure."""
    from kiara.utils.graphs import print_ascii_graph
    from kiara.utils.pipelines import get_pipeline_config

    kiara_obj = ctx.obj.kiara

    pc = get_pipeline_config(kiara=kiara_obj, pipeline=pipeline_name_or_path)

    structure = pc.structure

    print_ascii_graph(
        structure.execution_graph, restart_interpreter_if_asciinet_installed=True
    )


@pipeline.command()
@click.argument("pipeline-name-or-path", nargs=1)
@click.option(
    "--full",
    "-f",
    is_flag=True,
    help="Display full data-flow graph, incl. intermediate input/output connections.",
)
@click.pass_context
def data_flow_graph(ctx, pipeline_name_or_path: str, full: bool):
    """Print the data flow graph for a pipeline structure."""

    from kiara.utils.graphs import print_ascii_graph
    from kiara.utils.pipelines import get_pipeline_config

    kiara_obj = ctx.obj.kiara

    pc = get_pipeline_config(kiara=kiara_obj, pipeline=pipeline_name_or_path)

    structure = pc.structure

    if full:
        print_ascii_graph(
            structure.data_flow_graph, restart_interpreter_if_asciinet_installed=True
        )
    else:
        print_ascii_graph(
            structure.data_flow_graph_simple,
            restart_interpreter_if_asciinet_installed=True,
        )


@pipeline.command()
@click.argument("pipeline-name-or-path", nargs=1)
@click.option(
    "--stages-extraction-type",
    "-s",
    default=KIARA_DEFAULT_STAGES_EXTRACTION_TYPE,
    help="How to extract the stages from the pipeline structure. Available: 'late', 'early', as well as pipeline specific profiles (if in pipeline metadata).",
)
@click.pass_context
def stages_graph(ctx, pipeline_name_or_path: str, stages_extraction_type: str):
    """Print the data flow graph for a pipeline structure."""
    from kiara.utils.graphs import print_ascii_graph
    from kiara.utils.pipelines import get_pipeline_config

    kiara_obj = ctx.obj.kiara

    pc = get_pipeline_config(kiara=kiara_obj, pipeline=pipeline_name_or_path)

    structure = pc.structure
    stages_graph = structure.get_stages_graph(
        stages_extraction_type=stages_extraction_type
    )

    print_ascii_graph(
        stages_graph,
        restart_interpreter_if_asciinet_installed=True,
    )


# @pipeline.command()
# @click.argument("pipeline-type", nargs=1)
# @click.pass_context
# def explain_steps(ctx, pipeline_id: str):
#     """List all steps of a pipeline."""
#
#     kiara_obj = ctx.obj["kiara"]
#
#     if os.path.isfile(pipeline_id):
#         pipeline_id = kiara_obj.register_pipeline_description(
#             pipeline_id, raise_exception=True
#         )
#
#     m_cls = kiara_obj.get_module_class(pipeline_id)
#     if not m_cls.is_pipeline():
#         rich_print()
#         rich_print(f"Module '{pipeline_id}' is not a pipeline-type module.")
#         sys.exit(1)
#
#     info = PipelineModuleInfo.from_type_name(
#         module_type_name=pipeline_id, kiara=kiara_obj
#     )
#     print()
#     st_info = info.structure.steps_info
#     rich_print(st_info)
#
#
# @pipeline.command()
# @click.argument("pipeline-type", nargs=1)
# @click.pass_context
# def execution_graph(ctx, pipeline_id: str):
#     """Print the execution graph for a pipeline structure."""
#
#     kiara_obj = ctx.obj["kiara"]
#
#     if os.path.isfile(pipeline_id):
#         pipeline_id = kiara_obj.register_pipeline_description(
#             pipeline_id, raise_exception=True
#         )
#
#     m_cls = kiara_obj.get_module_class(pipeline_id)
#     if not m_cls.is_pipeline():
#         rich_print()
#         rich_print(f"Module '{pipeline_id}' is not a pipeline-type module.")
#         sys.exit(1)
#
#     info = PipelineModuleInfo.from_type_name(pipeline_id, kiara=kiara_obj)
#     info.print_execution_graph()
#
#
# @pipeline.command()
# @click.argument("pipeline-type", nargs=1)
# @click.option(
#     "--full",
#     "-f",
#     is_flag=True,
#     help="Display full data-flow graph, incl. intermediate input/output connections.",
# )
# @click.pass_context
# def data_flow_graph(ctx, pipeline_id: str, full: bool):
#     """Print the data flow graph for a pipeline structure."""
#
#     kiara_obj = ctx.obj["kiara"]
#     if os.path.isfile(pipeline_id):
#         pipeline_id = kiara_obj.register_pipeline_description(
#             pipeline_id, raise_exception=True
#         )
#
#     m_cls = kiara_obj.get_module_class(pipeline_id)
#     if not m_cls.is_pipeline():
#         rich_print()
#         rich_print(f"Module '{pipeline_id}' is not a pipeline-type module.")
#         sys.exit(1)
#
#     info = PipelineModuleInfo.from_type_name(pipeline_id, kiara=kiara_obj)
#
#     info.print_data_flow_graph(simplified=not full)
#
#
# try:
#     pass
#
#     @pipeline.command()
#     @click.argument("pipeline", nargs=1)
#     @click.option(
#         "--template",
#         "-t",
#         help="The template to use. Defaults to 'notebook'.",
#         default="notebook",
#     )
#     @click.pass_context
#     def render(ctx, pipeline, template):
#
#         kiara_obj: Kiara = ctx.obj["kiara"]
#
#         # pipeline = "/home/markus/projects/dharpa/kiara-playground/examples/streamlit/geolocation_prototype/pipelines/geolocation_1.yml"
#         # template = os.path.join(KIARA_RESOURCES_FOLDER, "templates", "python_script.py.j2")
#         rendered = kiara_obj.template_mgmt.render(
#             "pipeline", module=pipeline, template=template
#         )
#
#         print(rendered)
#
#
# except Exception:
#     log_message(
#         "'black' or 'jupytext' not installed, not adding 'pipeline render' subcommand."
#     )


# kiara\kiara\src\kiara\interfaces\cli\pipeline\__init__.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)


# kiara\kiara\src\kiara\interfaces\cli\render\commands.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

"""Pipeline-related subcommands for the cli."""
import sys
import typing
from pathlib import Path
from typing import Tuple, Union

import rich_click as click
from rich.markdown import Markdown

from kiara.utils.cli import (
    dict_from_cli_args,
    output_format_option,
    terminal_print,
    terminal_print_model,
)
from kiara.utils.cli.exceptions import handle_exception

if typing.TYPE_CHECKING:
    from kiara.interfaces import BaseAPIWrap
    from kiara.interfaces.python_api.base_api import BaseAPI


# def list_renderers(ctx, param, value) -> None:
#     """List all available renderers."""
#
#     if not value or ctx.resilient_parsing:
#         return
#     kiara_api: KiaraAPI = ctx.obj.kiara_api
#
#     infos = kiara_api.retrieve_renderer_infos()
#     terminal_print()
#     terminal_print_model(infos, in_panel="Available renderers")
#     sys.exit(0)


@click.group(name="render")
@click.option(
    "--source-type", "-s", required=False, help="Source type of the item to render."
)
@click.option("--target-type", "-t", required=False, help="Target type to render to.")
@click.pass_context
def render(
    ctx, source_type: Union[None, str] = None, target_type: Union[None, str] = None
) -> None:
    """Render-related sub-commands."""

    api_wrap: BaseAPIWrap = ctx.obj
    api_wrap.add_item("source_type", source_type)
    api_wrap.add_item("target_type", target_type)


@render.command("list-renderers")
@output_format_option()
@click.pass_context
@handle_exception()
def list_render_combinations(ctx, format: str):

    api_wrap: BaseAPIWrap = ctx.obj
    kiara_api: BaseAPI = api_wrap.base_api

    source_type = api_wrap.get_item("source_type")
    target_type = api_wrap.get_item("target_type")

    infos = kiara_api.retrieve_renderer_infos(
        source_type=source_type, target_type=target_type
    )
    terminal_print()
    terminal_print_model(infos, in_panel="Available renderers", format=format)
    sys.exit(0)


@render.command(name="item")
# @click.option(
#     "--list",
#     "-l",
#     is_flag=True,
#     help="List all available renderers and exit.",
#     callback=list_renderers,
#     expose_value=False,
#     is_eager=True,
# )
@click.option(
    "--output", "-o", help="Write the rendered output to a file using this path."
)
@click.option("--force", "-f", help="Overwrite existing output file.", is_flag=True)
@click.argument("item_to_render", nargs=1)
@click.argument("render_config", nargs=-1)
@click.pass_context
@handle_exception()
def render_item(
    ctx,
    item_to_render: Union[str, None],
    render_config: Tuple[str],
    output: str,
    force: bool,
) -> None:
    """Render an internal kiara item."""

    api_wrap: BaseAPIWrap = ctx.obj
    kiara_api: BaseAPI = api_wrap.base_api

    source_type = api_wrap.get_item("source_type")
    target_type = api_wrap.get_item("target_type")

    infos = kiara_api.retrieve_renderer_infos()

    available_render_source_types = infos.get_render_source_types()

    if source_type is None:

        if item_to_render in available_render_source_types:
            source_type = item_to_render
        else:
            msg = "No render source type specified, available source types:\n\n"

            for source_type in available_render_source_types:
                msg = f"{msg}  - *{source_type}*\n"

            terminal_print()
            terminal_print(Markdown(msg))
            sys.exit(1)

    elif source_type not in available_render_source_types:
        msg = f"Render source type '{source_type}' not available, available source types:\n\n"

        for source_type in available_render_source_types:
            msg = f"{msg}  - *{source_type}*\n"

        terminal_print()
        terminal_print(Markdown(msg))
        sys.exit(1)

    renderers = kiara_api.retrieve_renderers_for(source_type=source_type)

    all_targets: typing.Set[str] = set()
    for renderer in renderers:
        targets = renderer.retrieve_supported_render_targets()
        if isinstance(targets, str):
            targets = [targets]
        all_targets.update(targets)

    if target_type is None:
        msg = "No render target type specified, available target types:\n\n"

        for target_type in sorted(all_targets):
            msg = f"{msg}  - *{target_type}*\n"

        terminal_print()
        terminal_print(Markdown(msg))
        sys.exit(1)

    if target_type not in all_targets:
        msg = f"Render target type '{target_type}' not available, available target types:\n\n"

        for target_type in sorted(all_targets):
            msg = f"{msg}  - *{target_type}*\n"

        terminal_print()
        terminal_print(Markdown(msg))
        sys.exit(1)

    render_config_dict = dict_from_cli_args(*render_config)

    result = kiara_api.render(
        item=item_to_render,
        source_type=source_type,
        target_type=target_type,
        render_config=render_config_dict,
    )

    # in case we have a rendervalue result, and we want to terminal print, we need to forward some of the render config
    show_render_metadata = render_config_dict.get("include_metadata", False)
    show_render_result = render_config_dict.get("include_data", True)
    terminal_render_config = {
        "show_render_metadata": show_render_metadata,
        "show_render_result": show_render_result,
    }

    if output:
        output_file = Path(output)
        if output_file.exists() and not force:
            terminal_print()
            terminal_print(
                f"Output file '{output_file}' already exists, use '--force' to overwrite.",
            )
        output_file.parent.mkdir(parents=True, exist_ok=True)
        if isinstance(result, str):
            output_file.write_text(result)
        elif isinstance(result, bytes):
            output_file.write_bytes(result)
        else:
            terminal_print()
            terminal_print(
                f"Render output if type '{type(result)}', can't write to file."
            )

    else:

        if isinstance(result, str):
            print(result)  # noqa
        elif isinstance(result, bytes):
            terminal_print()
            terminal_print(
                "Render result is binary data, can't print to terminal. Use the '--output' option to write to a file."
            )
        else:
            if terminal_render_config is None:
                terminal_render_config = {}
            terminal_print(result, **terminal_render_config)


# kiara\kiara\src\kiara\interfaces\cli\render\__init__.py


# kiara\kiara\src\kiara\interfaces\cli\type\commands.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

"""Type-related subcommands for the cli."""
import typing
from typing import Dict, Iterable, Type

import rich_click as click

from kiara.utils.cli import output_format_option, terminal_print, terminal_print_model

if typing.TYPE_CHECKING:
    from kiara.api import Kiara


@click.group(name="data-type")
@click.pass_context
def type_group(ctx):
    """Information about available data types."""


@type_group.command(name="list")
@click.option(
    "--full-doc",
    "-d",
    is_flag=True,
    help="Display the full documentation for every data type (when using 'terminal' output format).",
)
@click.option(
    "--include-internal",
    "-I",
    is_flag=True,
    help="Also list types that are only (or mostly) used internally.",
)
@click.argument("filter", nargs=-1, required=False)
@output_format_option()
@click.pass_context
def list_types(
    ctx, full_doc, include_internal: bool, filter: Iterable[str], format: str
):
    """List available data_types."""
    from kiara.data_types import DataType
    from kiara.interfaces.python_api.models.info import DataTypeClassesInfo

    kiara_obj: Kiara = ctx.obj.kiara

    if not include_internal:
        type_classes: Dict[str, Type[DataType]] = {}
        for name, cls in kiara_obj.data_type_classes.items():
            if not kiara_obj.type_registry.is_internal_type(name):
                type_classes[name] = cls
    else:
        type_classes = dict(kiara_obj.data_type_classes)

    title = "Available data types"
    if filter:
        title = "Filtered data types"
        temp = {}
        for k, v in type_classes.items():
            match = True
            for f in filter:
                if f.lower() not in k.lower():
                    match = False
                    break
            if match:
                temp[k] = v
        type_classes = temp

    data_types_info = DataTypeClassesInfo.create_from_type_items(
        kiara=kiara_obj, group_title=title, **type_classes
    )

    terminal_print_model(
        data_types_info, format=format, in_panel="Available data types"
    )


@type_group.command(name="hierarchy")
@click.option(
    "--include-internal",
    "-i",
    is_flag=True,
    help="Display internally used data types.",
    default=False,
)
@click.pass_context
def hierarchy(ctx, include_internal) -> None:
    """Show the current runtime environments' type hierarchy."""
    from kiara.utils.graphs import print_ascii_graph

    kiara_obj: Kiara = ctx.obj.kiara

    type_mgmt = kiara_obj.type_registry
    terminal_print()

    if include_internal:
        print_ascii_graph(type_mgmt.data_type_hierarchy)
    else:
        sub_graph = type_mgmt.get_sub_hierarchy("any")
        print_ascii_graph(sub_graph)


@type_group.command(name="explain")
@click.argument("type_name", nargs=1, required=True)
@output_format_option()
@click.pass_context
def explain_data_type(ctx, type_name: str, format: str):
    """Print details of a data type."""

    from kiara.interfaces.python_api.models.info import DataTypeClassInfo

    kiara_obj: Kiara = ctx.obj.kiara

    data_type = kiara_obj.type_registry.retrieve_data_type(
        data_type_name=type_name, data_type_config=None
    )

    instance_renderable = data_type.create_renderable(show_type_info=False)
    type_renderable = DataTypeClassInfo.create_from_type_class(
        type_cls=data_type.__class__, kiara=kiara_obj
    )

    terminal_print_model(
        instance_renderable,
        type_renderable,
        format=format,
        in_panel=f"Data type: [b i]{data_type.data_type_name}[/b i]",
    )


# kiara\kiara\src\kiara\interfaces\cli\type\__init__.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)


# kiara\kiara\src\kiara\interfaces\cli\workflow\commands.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

"""Data-related sub-commands for the cli."""
import typing
from typing import Any, Dict, Tuple, Union

import rich_click as click
import structlog

from kiara.utils.cli import dict_from_cli_args, terminal_print, terminal_print_model

if typing.TYPE_CHECKING:
    from kiara.interfaces.python_api.base_api import BaseAPI, Kiara

logger = structlog.getLogger()


@click.group()
@click.pass_context
def workflow(ctx):
    """Workflow-related sub-commands.

    All of those are experiments at the moment, so don't expect them to be available in the future.
    """


@workflow.command()
@click.option(
    "--all", "-a", help="Also displays workflows without alias.", is_flag=True
)
@click.pass_context
def list(ctx, all) -> None:
    """List existing workflows."""
    kiara_api: BaseAPI = ctx.obj.base_api

    if all:
        workflows = kiara_api.retrieve_workflows_info()
    else:
        workflows = kiara_api.retrieve_workflow_aliases_info()

    terminal_print_model(workflows)


@workflow.command()
@click.argument("workflow_alias", nargs=1, required=False)
@click.argument("blueprint", nargs=1, required=False)
@click.argument("inputs", nargs=-1)
@click.option(
    "--force-alias",
    "-f",
    is_flag=True,
    help="Force (replace) an existing alias, if equal to the one provided.",
)
@click.option(
    "--desc", "-d", help="Description string for the workflow.", required=False
)
@click.pass_context
def create(
    ctx,
    workflow_alias: str,
    blueprint: Union[str, None],
    desc: Union[str, None] = None,
    inputs: Tuple[str, ...] = (),
    force_alias: bool = False,
):
    """Create a new workflow."""
    kiara_api: BaseAPI = ctx.obj.base_api

    inputs_dict: Union[None, Dict[str, Any]] = None
    if inputs:
        inputs_dict = dict_from_cli_args(*inputs)

    workflow_obj = kiara_api.create_workflow(
        workflow_alias=workflow_alias,
        initial_pipeline=blueprint,
        initial_inputs=inputs_dict,
    )

    workflow_obj.process_steps()

    if force_alias:
        kiara_api.context.workflow_registry.unregister_alias(workflow_alias)

    workflow_obj.snapshot(save=True)

    terminal_print_model(
        workflow_obj.info,
        in_panel=f"Workflow: [b i]{workflow_alias}[/b i]",
    )


@workflow.command()
@click.argument("workflow", nargs=1)
@click.pass_context
def explain(ctx, workflow: str):
    """Explain the workflow with the specified id/alias."""
    kiara_api: BaseAPI = ctx.obj.base_api
    workflow_info = kiara_api.retrieve_workflow_info(workflow=workflow)
    terminal_print(
        workflow_info.create_renderable(),
        in_panel=f"Workflow: [b i]{workflow}[/b i]",
    )


@workflow.command()
@click.argument("workflow", nargs=1)
@click.argument("inputs", nargs=-1, required=False)
@click.option(
    "--process/--no-process",
    "-a/-n",
    help="Process all possible intermediate and end-results.",
    is_flag=True,
    default=True,
)
@click.pass_context
def set_input(ctx, workflow: str, inputs: Tuple[str], process: bool):
    """Set one or several inputs on the specified workflow."""

    from kiara.interfaces.python_api.workflow import Workflow

    kiara: Kiara = ctx.obj.kiara

    workflow_details = kiara.workflow_registry.get_workflow_metadata(workflow=workflow)
    workflow_obj = Workflow(kiara=kiara, workflow=workflow_details.workflow_id)

    inputs_schema = workflow_obj.current_pipeline_inputs_schema
    list_keys = []
    for name, value_schema in inputs_schema.items():
        if value_schema.type in ["list"]:
            list_keys.append(name)
    inputs_dict = dict_from_cli_args(*inputs, list_keys=list_keys)

    workflow_obj.set_inputs(**inputs_dict)

    if process:
        try:
            workflow_obj.process_steps()
        except Exception as e:
            terminal_print(e)

    workflow_obj.snapshot(save=True)
    terminal_print_model(workflow_obj.info, in_panel=f"Workflow: [b i]{workflow}[/b i]")

    # workflow_obj.save_state()

    # registered = kiara.data_registry.create_valuemap(
    #     data=inputs_dict, schema=inputs_schema
    # )
    # for k, v in registered.items():
    #     kiara.data_registry.store_value(v)
    #
    # value_ids = registered.get_all_value_ids()
    #
    # new_state = state.create_workflow_state(inputs=value_ids)
    # kiara.workflow_registry.add_workflow_state(
    #     workflow_state=new_state, set_current=True
    # )


# kiara\kiara\src\kiara\interfaces\cli\workflow\__init__.py


# kiara\kiara\src\kiara\interfaces\python_api\base_api.py
# -*- coding: utf-8 -*-
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import inspect
import json
import os.path
import sys
import textwrap
import uuid
from functools import cached_property
from pathlib import Path
from typing import (
    TYPE_CHECKING,
    Any,
    Dict,
    Iterable,
    List,
    Literal,
    Mapping,
    MutableMapping,
    Set,
    Type,
    Union,
)

import dpath
import structlog
from ruamel.yaml import YAML

from kiara.defaults import (
    CHUNK_COMPRESSION_TYPE,
    DATA_ARCHIVE_DEFAULT_VALUE_MARKER,
    DEFAULT_STORE_MARKER,
    OFFICIAL_KIARA_PLUGINS,
    VALID_VALUE_QUERY_CATEGORIES,
    VALUE_ATTR_DELIMITER,
)
from kiara.exceptions import (
    DataTypeUnknownException,
    KiaraException,
    NoSuchExecutionTargetException,
    NoSuchWorkflowException,
)
from kiara.interfaces.python_api.models.info import (
    DataTypeClassesInfo,
    DataTypeClassInfo,
    KiaraPluginInfo,
    KiaraPluginInfos,
    ModuleTypeInfo,
    ModuleTypesInfo,
    OperationGroupInfo,
    OperationInfo,
    OperationTypeInfo,
    RendererInfos,
    ValueInfo,
    ValuesInfo,
)
from kiara.interfaces.python_api.models.job import JobDesc
from kiara.interfaces.python_api.value import StoreValueResult, StoreValuesResult
from kiara.models.context import ContextInfo, ContextInfos
from kiara.models.module.manifest import Manifest
from kiara.models.module.operation import Operation
from kiara.models.rendering import RenderValueResult
from kiara.models.runtime_environment.python import PythonRuntimeEnvironment
from kiara.models.values.matchers import ValueMatcher
from kiara.models.values.value import (
    PersistedData,
    Value,
    ValueMapReadOnly,
    ValueSchema,
)
from kiara.models.workflow import WorkflowGroupInfo, WorkflowInfo, WorkflowMetadata
from kiara.operations import OperationType
from kiara.operations.included_core_operations.filter import FilterOperationType
from kiara.operations.included_core_operations.pipeline import PipelineOperationDetails
from kiara.operations.included_core_operations.pretty_print import (
    PrettyPrintOperationType,
)
from kiara.operations.included_core_operations.render_value import (
    RenderValueOperationType,
)
from kiara.registries.environment import EnvironmentRegistry
from kiara.registries.ids import ID_REGISTRY
from kiara.renderers import KiaraRenderer
from kiara.utils import log_exception, log_message
from kiara.utils.downloads import get_data_from_url
from kiara.utils.files import get_data_from_file
from kiara.utils.operations import create_operation
from kiara.utils.string_vars import replace_var_names_in_obj

if TYPE_CHECKING:
    from kiara.context import Kiara, KiaraConfig, KiaraRuntimeConfig
    from kiara.interfaces.python_api.models.archive import KiArchive
    from kiara.interfaces.python_api.models.doc import (
        OperationsMap,
        PipelinesMap,
        WorkflowsMap,
    )
    from kiara.interfaces.python_api.workflow import Workflow
    from kiara.models.archives import KiArchiveInfo
    from kiara.models.module.jobs import ActiveJob, JobRecord
    from kiara.models.module.pipeline import PipelineConfig, PipelineStructure
    from kiara.models.module.pipeline.pipeline import PipelineGroupInfo, PipelineInfo
    from kiara.registries import KiaraArchive
    from kiara.registries.metadata import MetadataStore

logger = structlog.getLogger()
yaml = YAML(typ="safe")


def tag(*tags: str):
    def decorator(func):
        func._tags = tags
        return func

    return decorator


def find_base_api_endpoints(cls, label):
    """Return all endpoints that are tagged with the provided label."""

    # for func in dir(cls):
    #     if not func.startswith("_") and "_tags" not in dir(getattr(cls, func)):
    #         print(dir(getattr(cls, func)))
    return [
        getattr(cls, func)
        for func in dir(cls)
        if "_tags" in dir(getattr(cls, func)) and label in getattr(cls, func)._tags
    ]


class BaseAPI(object):
    """Kiara base API.

    This class wraps a [Kiara][kiara.context.kiara.Kiara] instance, and allows easy a access to tasks that are
    typically done by a frontend. The return types of each method are json seriable in most cases.

    Can be extended for special scenarios and augmented with scenario-specific methdos (Jupyter, web-frontend, ...)

    The naming of the API endpoints follows a (loose-ish) convention:
    - list_*: return a list of ids or items, if items, filtering is supported
    - get_*: get specific instances of a type (operation, value, etc.)
    - retrieve_*: get augmented information about an instance or type of something. This usually implies that there is some overhead,
    so before you use this, make sure that there is not 'get_*' or 'list_*' endpoint that could give you what you need.
    .
    """

    def __init__(self, kiara_config: Union["KiaraConfig", None] = None):

        if kiara_config is None:
            from kiara.context import Kiara, KiaraConfig

            kiara_config = KiaraConfig()

        self._kiara_config: KiaraConfig = kiara_config
        self._contexts: Dict[str, Kiara] = {}
        self._workflow_cache: Dict[uuid.UUID, Workflow] = {}

        self._current_context: Union[None, Kiara] = None
        self._current_context_alias: Union[None, str] = None

    @cached_property
    def doc(self) -> Dict[str, str]:
        """Get the documentation for this API."""

        result = {}
        for method_name in dir(self):
            if method_name.startswith("_"):
                continue

            method = getattr(self.__class__, method_name)
            doc = inspect.getdoc(method)
            if doc is None:
                doc = "-- n/a --"
            else:
                doc = textwrap.dedent(doc)

            result[method_name] = doc

        return result

    @tag("kiara_api")
    def list_available_plugin_names(
        self, regex: str = "^kiara[-_]plugin\\..*"
    ) -> List[str]:
        r"""
        Get a list of all available plugins.

        Arguments:
            regex: an optional regex to indicate the plugin naming scheme (default: /$kiara[_-]plugin\..*/)

        Returns:
            a list of plugin names
        """

        if not regex:
            regex = "^kiara[-_]plugin\\..*"

        return KiaraPluginInfos.get_available_plugin_names(
            kiara=self.context, regex=regex
        )

    @tag("kiara_api")
    def retrieve_plugin_info(self, plugin_name: str) -> KiaraPluginInfo:
        """
        Get information about a plugin.

        This contains information about included data-types, modules, operations, pipelines, as well as metadata
        about author(s), etc.

        Arguments:
            plugin_name: the name of the plugin

        Returns:
            a dictionary with information about the plugin
        """

        info = KiaraPluginInfo.create_from_instance(
            kiara=self.context, instance=plugin_name
        )
        return info

    @tag("kiara_api")
    def retrieve_plugin_infos(
        self, plugin_name_regex: str = "^kiara[-_]plugin\\..*"
    ) -> KiaraPluginInfos:
        """Get information about multiple plugins.

        This is just a convenience method to get information about multiple plugins at once.
        """

        if not plugin_name_regex:
            plugin_name_regex = "^kiara[-_]plugin\\..*"

        plugin_infos = KiaraPluginInfos.create_group(
            self.context, None, plugin_name_regex
        )
        return plugin_infos

    @property
    def context(self) -> "Kiara":
        """
        Return the kiara context.

        DON"T USE THIS! This is going away in the production release.
        """
        if self._current_context is None:
            self._current_context = self._kiara_config.create_context(
                extra_pipelines=None
            )
            self._current_context_alias = self._kiara_config.default_context

        return self._current_context

    def get_runtime_config(self) -> "KiaraRuntimeConfig":
        """Retrieve the current runtime configuration.

        Check the 'KiaraRuntimeConfig' class for more information about the available options.
        """
        return self.context.runtime_config

    @tag("kiara_api")
    def get_context_info(self) -> ContextInfo:
        """Retrieve information about the current kiara context.

        This contains information about the context, like its name/alias, the values & aliases it contains, and which archives are connected to it.

        """
        context_config = self._kiara_config.get_context_config(
            self.get_current_context_name()
        )
        info = ContextInfo.create_from_context_config(
            context_config,
            context_name=self.get_current_context_name(),
            runtime_config=self._kiara_config.runtime_config,
        )

        return info

    def ensure_plugin_packages(
        self, package_names: Union[str, Iterable[str]], update: bool = False
    ) -> Union[bool, None]:
        """
        Ensure that the specified packages are installed.


        NOTE: this is not tested, and it might go away in the future, so don't rely on it being available long-term. Ideally, we'll have other, external ways to manage the environment.

        Arguments:
          package_names: The names of the packages to install.
          update: If True, update the packages if they are already installed

        Returns:
            'None' if run in jupyter, 'True' if any packages were installed, 'False' otherwise.
        """
        if isinstance(package_names, str):
            package_names = [package_names]

        env_reg = EnvironmentRegistry.instance()
        python_env: PythonRuntimeEnvironment = env_reg.environments[  # type: ignore
            "python"
        ]  # type: ignore

        if not package_names:
            package_names = OFFICIAL_KIARA_PLUGINS  # type: ignore

        if not update:
            plugin_packages: List[str] = []
            pkgs = [p.name.replace("_", "-") for p in python_env.packages]
            for package_name in package_names:
                if package_name.startswith("git:"):
                    package_name = package_name.replace("git:", "")
                    git = True
                else:
                    git = False
                package_name = package_name.replace("_", "-")
                if not package_name.startswith("kiara-plugin."):
                    package_name = f"kiara-plugin.{package_name}"

                if git or package_name.replace("_", "-") not in pkgs:
                    if git:
                        package_name = package_name.replace("-", "_")
                        plugin_packages.append(
                            f"git+https://x:x@github.com/DHARPA-project/{package_name}@develop"
                        )
                    else:
                        plugin_packages.append(package_name)
        else:
            plugin_packages = package_names  # type: ignore

        in_jupyter = "google.colab" in sys.modules or "jupyter_client" in sys.modules

        if not plugin_packages:
            if in_jupyter:
                return None
            else:
                # nothing to do
                return False

        class DummyContext(object):
            def __getattribute__(self, item):
                raise Exception(
                    "Currently installing plugins, no other operations are allowed."
                )

        current_context_name = self._current_context_alias
        for k in self._contexts.keys():
            self._contexts[k] = DummyContext()  # type: ignore
        self._current_context = DummyContext()  # type: ignore

        cmd = ["-q", "--isolated", "install"]
        if update:
            cmd.append("--upgrade")
        cmd.extend(plugin_packages)

        if in_jupyter:
            from IPython import get_ipython

            ipython = get_ipython()
            cmd_str = f"sc -l stdout = {sys.executable} -m pip {' '.join(cmd)}"
            ipython.magic(cmd_str)
            exit_code = 100
        else:
            import pip._internal.cli.main as pip

            log_message(
                "install.python_packages", packages=plugin_packages, update=update
            )
            exit_code = pip.main(cmd)

        self._contexts.clear()
        self._current_context = None
        self._current_context_alias = None

        EnvironmentRegistry._instance = None
        if current_context_name:
            self.set_active_context(context_name=current_context_name)

        if exit_code == 100:
            raise SystemExit(
                f"Please manually re-run all cells. Updated or newly installed plugin packages: {', '.join(plugin_packages)}."
            )
        elif exit_code != 0:
            raise Exception(
                f"Failed to install plugin packages: {', '.join(plugin_packages)}"
            )

        return True

    # ==================================================================================================================
    # context-management related functions
    @tag("kiara_api")
    def list_context_names(self) -> List[str]:
        """list the names of all available/registered contexts.

        NOTE: this functionality might be changed in the future, depending on requirements and feedback and
        whether we want to support single-file contexts in the future.
        """
        return list(self._kiara_config.available_context_names)

    @tag("kiara_api")
    def retrieve_context_infos(self) -> ContextInfos:
        """Retrieve information about the available/registered contexts.

        NOTE: this functionality might be changed in the future, depending on requirements and feedback and whether we want to support single-file contexts in the future.
        """
        return ContextInfos.create_context_infos(self._kiara_config.context_configs)

    @tag("kiara_api")
    def get_current_context_name(self) -> str:
        """Retrieve the name of the current context.

        NOTE: this functionality might be changed in the future, depending on requirements and feedback and whether we want to support single-file contexts in the future.
        """
        if self._current_context_alias is None:
            self.context
        return self._current_context_alias  # type: ignore

    def create_new_context(self, context_name: str, set_active: bool = True) -> None:
        """
        Create a new context.

        NOTE: this functionality might be changed in the future, depending on requirements and feedback and whether we want to support single-file contexts in the future. So if you need something like this, please let me know.

        Arguments:
            context_name: the name of the new context
            set_active: set the newly created context as the active one
        """
        if context_name in self.list_context_names():
            raise Exception(
                f"Can't create context with name '{context_name}': context already exists."
            )

        ctx = self._kiara_config.create_context(context_name, extra_pipelines=None)
        if set_active:
            self._current_context = ctx
            self._current_context_alias = context_name

        # return ctx

    @tag("kiara_api")
    def set_active_context(self, context_name: str, create: bool = False) -> None:
        """Set the currently active context for this KiarAPI instance.

        NOTE: this functionality might be changed in the future, depending on requirements and feedback and whether we want to support single-file contexts in the future.
        """

        if not context_name:
            raise Exception("No context name provided.")

        if context_name == self._current_context_alias:
            return
        if context_name not in self.list_context_names():
            if create:
                self._current_context = self._kiara_config.create_context(
                    context=context_name, extra_pipelines=None
                )
                self._current_context_alias = context_name
                return
            else:
                raise Exception(f"No context with name '{context_name}' available.")

        self._current_context = self._kiara_config.create_context(
            context=context_name, extra_pipelines=None
        )
        self._current_context_alias = context_name

    # ==================================================================================================================
    # methods for data_types

    @tag("kiara_api")
    def list_data_type_names(self, include_profiles: bool = False) -> List[str]:
        """Get a list of all registered data types.

        Arguments:
            include_profiles: if True, also include the names of all registered data type profiles
        """

        return self.context.type_registry.get_data_type_names(
            include_profiles=include_profiles
        )

    def is_internal_data_type(self, data_type_name: str) -> bool:
        """Checks if the data type is prepdominantly used internally by kiara, or whether it should be exposed to the user."""

        return self.context.type_registry.is_internal_type(
            data_type_name=data_type_name
        )

    @tag("kiara_api")
    def retrieve_data_types_info(
        self,
        filter: Union[str, Iterable[str], None] = None,
        include_data_type_profiles: bool = False,
        python_package: Union[None, str] = None,
    ) -> DataTypeClassesInfo:
        """
        Retrieve information about all data types.

        A data type is a Python class that inherits from [DataType[kiara.data_types.DataType], and it wraps a specific
        Python class that holds the actual data and provides metadata and convenience methods for managing the data internally. Data types are not directly used by users, but they are exposed in the input/output schemas of moudles and other data-related features.

        Arguments:
            filter: an optional string or (list of strings) the returned datatype ids have to match (all filters in the case of a list)
            include_data_type_profiles: if True, also include the names of all registered data type profiles
            python_package: if provided, only return data types that are defined in the given python package

        Returns:
            an object containing all information about all data types
        """

        kiara = self.context

        if python_package:
            data_type_info = kiara.type_registry.get_context_metadata(
                only_for_package=python_package
            )

            if filter:
                title = f"Filtered data types in package '{python_package}'"

                if isinstance(filter, str):
                    filter = [filter]

                filtered_types: Dict[str, DataTypeClassInfo] = {}

                for dt in data_type_info.item_infos.keys():
                    match = True

                    for f in filter:
                        if f.lower() not in dt.lower():
                            match = False
                            break
                    if match:
                        filtered_types[dt] = data_type_info.item_infos[dt]

                data_types_info = DataTypeClassesInfo(
                    group_title=title, item_infos=filtered_types
                )
                # data_types_info._kiara = kiara

            else:
                title = f"All data types in package '{python_package}'"
                data_types_info = data_type_info
                data_types_info.group_title = title
        else:
            if filter:
                if isinstance(filter, str):
                    filter = [filter]

                title = f"Filtered data_types: {filter}"
                data_type_names: Iterable[str] = []

                for m in kiara.type_registry.get_data_type_names(
                    include_profiles=include_data_type_profiles
                ):
                    match = True

                    for f in filter:

                        if f.lower() not in m.lower():
                            match = False
                            break

                    if match:
                        data_type_names.append(m)  # type: ignore
            else:
                title = "All data types"
                data_type_names = kiara.type_registry.get_data_type_names(
                    include_profiles=include_data_type_profiles
                )

            data_types = {
                d: kiara.type_registry.get_data_type_cls(d) for d in data_type_names
            }
            data_types_info = DataTypeClassesInfo.create_from_type_items(  # type: ignore
                kiara=kiara, group_title=title, **data_types
            )

        return data_types_info  # type: ignore

    @tag("kiara_api")
    def retrieve_data_type_info(self, data_type_name: str) -> DataTypeClassInfo:
        """
        Retrieve information about a specific data type.

        Arguments:
            data_type: the registered name of the data type

        Returns:
            an object containing all information about a data type
        """
        dt_cls = self.context.type_registry.get_data_type_cls(data_type_name)
        info = DataTypeClassInfo.create_from_type_class(
            kiara=self.context, type_cls=dt_cls
        )
        return info

    # ==================================================================================================================
    # methods for module and operations info

    @tag("kiara_api")
    def list_module_type_names(self) -> List[str]:
        """Get a list of all registered module types."""
        return list(self.context.module_registry.get_module_type_names())

    @tag("kiara_api")
    def retrieve_module_types_info(
        self,
        filter: Union[None, str, Iterable[str]] = None,
        python_package: Union[str, None] = None,
    ) -> ModuleTypesInfo:
        """
        Retrieve information for all available module types (or a filtered subset thereof).

        A module type is Python class that inherits from [KiaraModule][kiara.modules.KiaraModule], and is the basic
        building block for processing pipelines. Module types are not used directly by users, Operations are. Operations
         are instantiated modules (meaning: the module & some (optional) configuration).

        Arguments:
            filter: an optional string (or list of string) the returned module names have to match (all filters in case of list)
            python_package: an optional string, if provided, only modules from the specified python package are returned

        Returns:
            a mapping object containing module names as keys, and information about the modules as values
        """

        if python_package:

            modules_type_info = self.context.module_registry.get_context_metadata(
                only_for_package=python_package
            )

            if filter:
                title = f"Filtered modules: {filter} (in package '{python_package}')"
                if isinstance(filter, str):
                    filter = [filter]

                filtered_types: Dict[str, ModuleTypeInfo] = {}

                for m in modules_type_info.item_infos.keys():
                    match = True

                    for f in filter:

                        if f.lower() not in m.lower():
                            match = False
                            break

                    if match:
                        filtered_types[m] = modules_type_info.item_infos[m]

                module_types_info = ModuleTypesInfo(
                    group_title=title, item_infos=filtered_types
                )
                module_types_info._kiara = self.context
            else:
                title = f"All modules in package '{python_package}'"
                module_types_info = modules_type_info
                module_types_info.group_title = title

        else:

            if filter:

                if isinstance(filter, str):
                    filter = [filter]
                title = f"Filtered modules: {filter}"
                module_types_names: Iterable[str] = []

                for m in self.context.module_registry.get_module_type_names():
                    match = True

                    for f in filter:

                        if f.lower() not in m.lower():
                            match = False
                            break

                    if match:
                        module_types_names.append(m)  # type: ignore
            else:
                title = "All modules"
                module_types_names = (
                    self.context.module_registry.get_module_type_names()
                )

            module_types = {
                n: self.context.module_registry.get_module_class(n)
                for n in module_types_names
            }

            module_types_info = ModuleTypesInfo.create_from_type_items(  # type: ignore
                kiara=self.context, group_title=title, **module_types
            )

        return module_types_info  # type: ignore

    @tag("kiara_api")
    def retrieve_module_type_info(self, module_type: str) -> ModuleTypeInfo:
        """
        Retrieve information about a specific module type.

        This can be used to retrieve information like module documentation and configuration options.

        Arguments:
            module_type: the registered name of the module

        Returns:
            an object containing all information about a module type
        """
        m_cls = self.context.module_registry.get_module_class(module_type)
        info = ModuleTypeInfo.create_from_type_class(kiara=self.context, type_cls=m_cls)
        return info

    def create_operation(
        self,
        module_type: str,
        module_config: Union[Mapping[str, Any], str, None] = None,
    ) -> Operation:
        """
        Create an [Operation][kiara.models.module.operation.Operation] instance for the specified module type and (optional) config.

        An operation is defined as a specific module type, and a specific configuration.

        This endpoint can be used to get information about the operation itself, it's inputs & outputs schemas, documentation etc.

        Arguments:
            module_type: the registered name of the module
            module_config: (Optional) configuration for the module instance.

        Returns:
            an Operation instance (which contains all the available information about an instantiated module)
        """
        if module_config is None:
            module_config = {}
        elif isinstance(module_config, str):
            try:
                module_config = json.load(module_config)  # type: ignore
            except Exception:
                try:
                    module_config = yaml.load(module_config)  # type: ignore
                except Exception:
                    raise Exception(
                        f"Can't parse module config string: {module_config}."
                    )

        if module_type == "pipeline":
            if not module_config:
                raise Exception("Pipeline configuration can't be empty.")
            assert module_config is None or isinstance(module_config, Mapping)
            operation = create_operation(
                "pipeline", operation_config=module_config, kiara=self.context
            )
            return operation
        else:
            mc = Manifest(module_type=module_type, module_config=module_config)
            module_obj = self.context.module_registry.create_module(mc)

            return module_obj.operation

    @tag("kiara_api")
    def list_operation_ids(
        self,
        filter: Union[str, None, Iterable[str]] = None,
        input_types: Union[str, Iterable[str], None] = None,
        output_types: Union[str, Iterable[str], None] = None,
        operation_types: Union[str, Iterable[str], None] = None,
        include_internal: bool = False,
        python_packages: Union[str, None, Iterable[str]] = None,
    ) -> List[str]:
        """
        Get a list of all operation ids that match the specified filter.

        Arguments:
            filter: the (optional) filter string(s), an operation must match all of them to be included in the result
            input_types: each operation must have at least one input that matches one of the specified types
            output_types: each operation must have at least one output that matches one of the specified types
            operation_types: only include operations of the specified type(s)
            include_internal: whether to include operations that are predominantly used internally in kiara.
            python_packages: only include operations that are contained in one of the provided python packages
        """
        if not filter and include_internal and not python_packages:
            return sorted(self.context.operation_registry.operation_ids)

        else:
            return sorted(
                self.list_operations(
                    filter=filter,
                    input_types=input_types,
                    output_types=output_types,
                    operation_types=operation_types,
                    include_internal=include_internal,
                    python_packages=python_packages,
                ).keys()
            )

    @tag("kiara_api")
    def get_operation(
        self,
        operation: Union[Mapping[str, Any], str, Path],
        allow_external: Union[bool, None] = None,
    ) -> Operation:
        """
        Return the operation instance with the specified id.

        The difference to the 'create_operation' endpoint is slight, in most cases you could use either of them, but this one is a bit more convenient in most cases, as it tries to do the right thing with whatever 'operation' argument you use it. The 'create_opearation' endpoint will always create a new 'Operation' instance, while this may or may not return a re-used one.

        This endpoint can be used to get information about a specific operation, like inputs/outputs scheman, documentation, etc.

        The order in which the operation argument is resolved:
        - if it's a string, and an existing, registered operation_id, the associated operation is returned
        - if it's a path to an existing file, the content of the file is loaded into a dict and depending on the content a pipeline module will be created, or a 'normal' manifest (if module_type is a key in the dict)

        Arguments:
            operation: the operation id, module_type_name, path to a file, or url
            allow_external: if True, allow loading operations from external sources (e.g. a URL), if 'None' is provided, the configured value in the runtime configuration is used.

        Returns:
            operation instance data
        """
        _module_type = None
        _module_config: Any = None

        if allow_external is None:
            allow_external = self.get_runtime_config().allow_external

        if isinstance(operation, Path):
            operation = operation.as_posix()

        if (
            isinstance(operation, Mapping)
            and "module_type" in operation.keys()
            and "module_config" in operation.keys()
            and not operation["module_config"]
        ):
            operation = operation["module_type"]

        if isinstance(operation, str):

            if operation in self.list_operation_ids(include_internal=True):
                _operation = self.context.operation_registry.get_operation(operation)
                return _operation

            if not allow_external:
                raise NoSuchExecutionTargetException(
                    selected_target=operation,
                    available_targets=self.context.operation_registry.operation_ids,
                    msg=f"Can't find operation with id '{operation}', and external operations are not allowed.",
                )

            if os.path.isfile(operation):
                try:
                    from kiara.models.module.pipeline import PipelineConfig

                    # we use the 'from_file' here, because that will resolve any relative paths in the pipeline
                    # if this doesn't work, we just assume the file is not a pipeline configuration but
                    # a manifest file with 'module_type' and optional 'module_config' keys
                    pipeline_conf = PipelineConfig.from_file(
                        path=operation, kiara=self.context
                    )
                    _module_config = pipeline_conf.model_dump()
                except Exception as e:
                    log_exception(e)
                    _module_config = get_data_from_file(operation)
            elif operation.startswith("http"):
                _module_config = get_data_from_url(operation)
            else:
                try:
                    _module_config = json.load(operation)  # type: ignore
                except Exception:
                    try:
                        _module_config = yaml.load(operation)  # type: ignore
                    except Exception:
                        raise Exception(
                            f"Can't parse configuration string: {operation}."
                        )
            if not isinstance(_module_config, Mapping):
                raise NoSuchExecutionTargetException(
                    selected_target=operation,
                    available_targets=self.context.operation_registry.operation_ids,
                    msg=f"Can't find operation or execution target for string '{operation}'.",
                )

        else:
            _module_config = dict(operation)  # type: ignore

        if "module_type" in _module_config.keys():
            _module_type = _module_config["module_type"]
            _module_config = _module_config.get("module_config", {})
        else:
            _module_type = "pipeline"

        op = self.create_operation(
            module_type=_module_type, module_config=_module_config
        )
        return op

    @tag("kiara_api")
    def list_operations(
        self,
        filter: Union[str, None, Iterable[str]] = None,
        input_types: Union[str, Iterable[str], None] = None,
        output_types: Union[str, Iterable[str], None] = None,
        operation_types: Union[str, Iterable[str], None] = None,
        python_packages: Union[str, Iterable[str], None] = None,
        include_internal: bool = False,
    ) -> "OperationsMap":
        """
        List all available operations, optionally filter.

        Arguments:
            filter: the (optional) filter string(s), an operation must match all of them to be included in the result
            input_types: each operation must have at least one input that matches one of the specified types
            output_types: each operation must have at least one output that matches one of the specified types
            operation_types: only include operations of the specified type(s)
            include_internal: whether to include operations that are predominantly used internally in kiara.
            python_packages: only include operations that are contained in one of the provided python packages

        Returns:
            a dictionary with the operation id as key, and [kiara.models.module.operation.Operation] instance data as value
        """
        if operation_types:
            if isinstance(operation_types, str):
                operation_types = [operation_types]
            temp: Dict[str, Operation] = {}
            for op_type_name in operation_types:
                op_type = self.context.operation_registry.operation_types.get(
                    op_type_name, None
                )
                if op_type is None:
                    raise Exception(f"Operation type not registered: {op_type_name}")

                temp.update(op_type.operations)

            operations: Mapping[str, Operation] = temp
        else:
            operations = self.context.operation_registry.operations

        if filter:
            if isinstance(filter, str):
                filter = [filter]
            temp = {}
            for op_id, op in operations.items():
                match = True
                for f in filter:
                    if not f:
                        continue
                    if f.lower() not in op_id.lower():
                        match = False
                        break
                if match:
                    temp[op_id] = op
            operations = temp

        if not include_internal:
            temp = {}
            for op_id, op in operations.items():
                if not op.operation_details.is_internal_operation:
                    temp[op_id] = op

            operations = temp

        if input_types:
            if isinstance(input_types, str):
                input_types = [input_types]
            temp = {}
            for op_id, op in operations.items():
                for input_type in input_types:
                    match = False
                    for schema in op.inputs_schema.values():
                        if schema.type == input_type:
                            temp[op_id] = op
                            match = True
                            break
                    if match:
                        break

            operations = temp

        if output_types:
            if isinstance(output_types, str):
                output_types = [output_types]
            temp = {}
            for op_id, op in operations.items():
                for output_type in output_types:
                    match = False
                    for schema in op.outputs_schema.values():
                        if schema.type == output_type:
                            temp[op_id] = op
                            match = True
                            break
                    if match:
                        break

            operations = temp

        if python_packages:
            temp = {}
            if isinstance(python_packages, str):
                python_packages = [python_packages]
            for op_id, op in operations.items():
                info = OperationInfo.create_from_instance(
                    kiara=self.context, instance=op
                )
                pkg = info.context.labels.get("package", None)
                if pkg in python_packages:
                    temp[op_id] = op
            operations = temp

        from kiara.interfaces.python_api.models.doc import OperationsMap

        return OperationsMap.model_construct(root=operations)  # type: ignore

    @tag("kiara_api")
    def retrieve_operation_info(
        self, operation: str, allow_external: bool = False
    ) -> OperationInfo:
        """
        Return the full information for the specified operation id.

        This is similar to the 'get_operation' method, but returns additional information. Only use this instead of
        'get_operation' if you need the additional info, as it's more expensive to get.

        Arguments:
            operation: the operation id

        Returns:
            augmented operation instance data
        """
        if not allow_external:
            op = self.context.operation_registry.get_operation(operation_id=operation)
        else:
            op = create_operation(module_or_operation=operation)
        op_info = OperationInfo.create_from_operation(kiara=self.context, operation=op)
        return op_info

    @tag("kiara_api")
    def retrieve_operations_info(
        self,
        *filters: str,
        input_types: Union[str, Iterable[str], None] = None,
        output_types: Union[str, Iterable[str], None] = None,
        operation_types: Union[str, Iterable[str], None] = None,
        python_packages: Union[str, Iterable[str], None] = None,
        include_internal: bool = False,
    ) -> OperationGroupInfo:
        """
        Retrieve information about the matching operations.

        This retrieves the same list of operations as [list_operations][kiara.interfaces.python_api.KiaraAPI.list_operations],
        but augments each result instance with additional information that might be useful in frontends.

        'OperationInfo' objects contains augmented information on top of what 'normal' [Operation][kiara.models.module.operation.Operation] objects
        hold, but they can take longer to create/resolve. If you don't need any
        of the augmented information, just use the [list_operations][kiara.interfaces.python_api.KiaraAPI.list_operations] method
        instead.

        Arguments:
            filters: the (optional) filter strings, an operation must match all of them to be included in the result
            include_internal: whether to include operations that are predominantly used internally in kiara.
            input_types: each operation must have at least one input that matches one of the specified types
            output_types: each operation must have at least one output that matches one of the specified types
            operation_types: only include operations of the specified type(s)
            include_internal: whether to include operations that are predominantly used internally in kiara.
            python_packages: only include operations that are contained in one of the provided python packages
        Returns:
            a wrapper object containing a dictionary of items with value_id as key, and [kiara.interfaces.python_api.models.info.OperationInfo] as value
        """
        title = "Available operations"
        if filters:
            title = "Filtered operations"

        operations = self.list_operations(
            filters,
            input_types=input_types,
            output_types=output_types,
            include_internal=include_internal,
            operation_types=operation_types,
            python_packages=python_packages,
        )

        ops_info = OperationGroupInfo.create_from_operations(
            kiara=self.context, group_title=title, **operations
        )
        return ops_info

    # ==================================================================================================================
    # methods relating to pipelines

    def list_pipeline_ids(
        self,
        filter: Union[str, None, Iterable[str]] = None,
        input_types: Union[str, Iterable[str], None] = None,
        output_types: Union[str, Iterable[str], None] = None,
        include_internal: bool = False,
        python_packages: Union[str, None, Iterable[str]] = None,
    ) -> List[str]:
        """
        Get a list of all pipeline (operation) ids that match the specified filter.

        Arguments:
            filter: an optional single or list of filters (all filters must match the operation id for the operation to be included)
            include_internal: also return internal pipelines
        """

        result: List[str] = self.list_operation_ids(
            filter=filter,
            input_types=input_types,
            output_types=output_types,
            operation_types=["pipeline"],
            include_internal=include_internal,
            python_packages=python_packages,
        )
        return result

    def list_pipelines(
        self,
        filter: Union[str, None, Iterable[str]] = None,
        input_types: Union[str, Iterable[str], None] = None,
        output_types: Union[str, Iterable[str], None] = None,
        python_packages: Union[str, Iterable[str], None] = None,
        include_internal: bool = False,
    ) -> "PipelinesMap":
        """List all available pipelines, optionally filter.

        Arguments:
            filter: the (optional) filter string(s), an operation must match all of them to be included in the result
            input_types: each operation must have at least one input that matches one of the specified types
            output_types: each operation must have at least one output that matches one of the specified types
            operation_types: only include operations of the specified type(s)
            include_internal: whether to include operations that are predominantly used internally in kiara.
            python_packages: only include operations that are contained in one of the provided python packages

        Returns:
            a dictionary with the operation id as key, and [kiara.models.module.operation.Operation] instance data as value
        """
        from kiara.interfaces.python_api.models.doc import PipelinesMap

        ops = self.list_operations(
            filter=filter,
            input_types=input_types,
            output_types=output_types,
            operation_types=["pipeline"],
            python_packages=python_packages,
            include_internal=include_internal,
        )

        result: Dict[str, PipelineStructure] = {}
        for op in ops.values():
            details: PipelineOperationDetails = op.operation_details
            config: "PipelineConfig" = details.pipeline_config
            structure = config.structure
            result[op.operation_id] = structure

        return PipelinesMap.model_construct(root=result)

    def get_pipeline_structure(
        self,
        pipeline: Union[Mapping[str, Any], str, Path],
        allow_external: Union[bool, None] = None,
    ) -> "PipelineStructure":
        """
        Return the pipeline (Structure) instance with the specified id.

        This can be used to get information about a pipeline, like inputs/outputs scheman, documentation, included steps, stages, etc.

        The order in which the operation argument is resolved:
        - if it's a string, and an existing, registered operation_id, the associated operation is returned
        - if it's a path to an existing file, the content of the file is loaded into a dict and a pipeline operation will be created

        Arguments:
            pipeline: the pipeline id, module_type_name, path to a file, or url
            allow_external: if True, allow loading operations from external sources (e.g. a URL), if 'None' is provided, the configured value in the runtime configuration is used.

        Returns:
            pipeline structure data
        """

        op = self.get_operation(operation=pipeline, allow_external=allow_external)
        if op.module_type != "pipeline":
            raise KiaraException(
                f"Operation '{op.operation_id}' is not a pipeline, but a '{op.module_type}'"
            )
        details: PipelineOperationDetails = op.operation_details  # type: ignore
        config: "PipelineConfig" = details.pipeline_config

        return config.structure

    def retrieve_pipeline_info(
        self, pipeline: str, allow_external: bool = False
    ) -> "PipelineInfo":
        """
        Return the full information for the specified pipeline id.

        This is similar to the 'get_pipeline' method, but returns additional information. Only use this instead of
        'get_pipeline' if you need the additional info, as it's more expensive to get.

        Arguments:
            pipeline: the pipeline (operation) id

        Returns:
            augmented pipeline instance data
        """
        if not allow_external:
            op = self.context.operation_registry.get_operation(operation_id=pipeline)
        else:
            op = create_operation(module_or_operation=pipeline)

        if op.module_type != "pipeline":
            raise KiaraException(
                f"Operation '{op.operation_id}' is not a pipeline, but a '{op.module_type}'"
            )

        from kiara.models.module.pipeline.pipeline import Pipeline, PipelineInfo

        details: PipelineOperationDetails = op.operation_details  # type: ignore
        config: "PipelineConfig" = details.pipeline_config
        pipeline_instance = Pipeline(structure=config.structure, kiara=self.context)

        p_info: PipelineInfo = PipelineInfo.create_from_instance(
            kiara=self.context, instance=pipeline_instance
        )
        return p_info

    def retrieve_pipelines_info(
        self,
        *filters,
        input_types: Union[str, Iterable[str], None] = None,
        output_types: Union[str, Iterable[str], None] = None,
        python_packages: Union[str, Iterable[str], None] = None,
        include_internal: bool = False,
    ) -> "PipelineGroupInfo":
        """
        Retrieve information about the matching pipelines.

        This retrieves the same list of pipelines as [list_pipelines][kiara.interfaces.python_api.KiaraAPI.list_pipelines],
        but augments each result instance with additional information that might be useful in frontends.

        'PipelineInfo' objects contains augmented information on top of what 'normal' [PipelineStructure][kiara.models.module.pipeline.PipelineStructure] objects
        hold, but they can take longer to create/resolve. If you don't need any
        of the augmented information, just use the [list_pipelines][kiara.interfaces.python_api.KiaraAPI.list_pipelines] method
        instead.

        Arguments:
            filters: the (optional) filter strings, an operation must match all of them to be included in the result
            include_internal: whether to include operations that are predominantly used internally in kiara.
            input_types: each operation must have at least one input that matches one of the specified types
            output_types: each operation must have at least one output that matches one of the specified types
            include_internal: whether to include operations that are predominantly used internally in kiara.
            python_packages: only include operations that are contained in one of the provided python packages
        Returns:
            a wrapper object containing a dictionary of items with value_id as key, and [kiara.interfaces.python_api.models.info.OperationInfo] as value
        """

        title = "Available pipelines"
        if filters:
            title = "Filtered pipelines"

        operations = self.list_operations(
            filters,
            input_types=input_types,
            output_types=output_types,
            include_internal=include_internal,
            operation_types=["pipeline"],
            python_packages=python_packages,
        )

        from kiara.models.module.pipeline.pipeline import Pipeline, PipelineGroupInfo

        pipelines = {}
        for op_id, op in operations.items():
            details: PipelineOperationDetails = op.operation_details  # type: ignore
            config: "PipelineConfig" = details.pipeline_config
            pipeline = Pipeline(structure=config.structure, kiara=self.context)
            pipelines[op_id] = pipeline

        ps_info = PipelineGroupInfo.create_from_pipelines(
            kiara=self.context, group_title=title, **pipelines
        )
        return ps_info

    def register_pipeline(
        self,
        data: Union[Path, str, Mapping[str, Any]],
        operation_id: Union[str, None] = None,
    ) -> Operation:
        """
        Register a pipelne as new operation into this context.

        If 'operation_id' is not provided, the id will be auto-determined (in most cases using the pipeline name).

        Arguments:
            data: a dict or a path to a json/yaml file containing the definition
            operation_id: the id to use for the operation (if not specified, the id will be auto-determined)

        Returns:
            the assembled operation
        """
        return self.context.operation_registry.register_pipeline(
            data=data, operation_id=operation_id
        )

    def register_pipelines(
        self, *pipeline_paths: Union[str, Path]
    ) -> Dict[str, Operation]:
        """Register all pipelines found in the specified paths."""
        return self.context.operation_registry.register_pipelines(*pipeline_paths)

    # ==================================================================================================================
    # methods relating to values and data

    def register_data(
        self,
        data: Any,
        data_type: Union[None, str, ValueSchema, Mapping[str, Any]] = None,
        reuse_existing: bool = False,
    ) -> Value:
        """
        Register data with kiara.

        This will create a new value instance from the data and return it. The data/value itself won't be stored
        in a store, you have to use the 'store_value' function for that.

        Arguments:
            data: the data to register
            data_type: (optional) the data type of the data. If not provided, kiara will try to infer the data type.
            reuse_existing: whether to re-use an existing value that is already registered and has the same hash.

        Returns:
            a [kiara.models.values.value.Value] instance
        """
        if data_type is None:
            raise NotImplementedError(
                "Infering data types not implemented yet. Please provide one manually."
            )

        value = self.context.data_registry.register_data(
            data=data, schema=data_type, reuse_existing=reuse_existing
        )
        return value

    @tag("kiara_api")
    def list_all_value_ids(self) -> List[uuid.UUID]:
        """List all value ids in the current context.

        This returns everything, even internal values. It should be faster than using
        `list_value_ids` with equivalent parameters, because no filtering has to happen.

        Returns:
            all value_ids in the current context, using every registered store
        """

        _values = self.context.data_registry.retrieve_all_available_value_ids()
        return sorted(_values)

    @tag("kiara_api")
    def list_value_ids(self, **matcher_params: Any) -> List[uuid.UUID]:
        """
        List all available value ids for this kiara context.

        By default, this also includes internal values.

        This method exists mainly so frontends can retrieve a list of all value_ids that exists on the backend without
        having to look up the details of each value (like [list_values][kiara.interfaces.python_api.KiaraAPI.list_values]
        does). This method can also be used with a matcher, but in this case the [list_values][kiara.interfaces.python_api.KiaraAPI.list_values]
        would be preferable in most cases, because it is called under the hood, and the performance advantage of not
        having to look up value details is gone.

        Arguments:
            matcher_params: the (optional) filter parameters, check the [ValueMatcher][kiara.models.values.matchers.ValueMatcher] class for available parameters and defaults

        Returns:
            a list of value ids
        """

        values = self.list_values(**matcher_params)
        return sorted((v.value_id for v in values.values()))

    @tag("kiara_api")
    def list_all_values(self) -> ValueMapReadOnly:
        """List all values in the current context, incl. internal ones.

        This should be faster than `list_values` with equivalent matcher params, because no
        filtering has to happen.
        """

        # TODO: make that parallel?
        values = {
            k: self.context.data_registry.get_value(k)
            for k in self.context.data_registry.retrieve_all_available_value_ids()
        }
        result = ValueMapReadOnly.create_from_values(
            **{str(k): v for k, v in values.items()}
        )
        return result

    @tag("kiara_api")
    def list_values(self, **matcher_params: Any) -> ValueMapReadOnly:
        """
        List all available (relevant) values, optionally filter.

        Retrieve information about all values that are available in the current kiara context session (both stored and non-stored).

        Check the `ValueMatcher` class for available parameters and defaults, for example this excludes
        internal values by default.

        Arguments:
            matcher_params: the (optional) filter parameters, check the [ValueMatcher][kiara.models.values.matchers.ValueMatcher] class for available parameters

        Returns:
            a dictionary with value_id as key, and [kiara.models.values.value.Value] as value
        """

        matcher = ValueMatcher.create_matcher(**matcher_params)
        values = self.context.data_registry.find_values(matcher=matcher)

        result = ValueMapReadOnly.create_from_values(
            **{str(k): v for k, v in values.items()}
        )
        return result

    @tag("kiara_api")
    def get_value(self, value: Union[str, Value, uuid.UUID, Path]) -> Value:
        """
        Retrieve a value instance with the specified id or alias.

        Basically a convenience method to convert any possible Python type into
        a 'Value' instance. Raises an exception if no value could be found.

        Arguments:
            value: a value id, alias or object that has a 'value_id' attribute.

        Returns:
            the Value instance
        """

        return self.context.data_registry.get_value(value=value)

    @tag("kiara_api")
    def get_values(self, **values: Union[str, Value, uuid.UUID]) -> ValueMapReadOnly:
        """Retrieve Value instances for the specified value ids or aliases.

        This is a convenience method to get fully 'hydrated' `Value` objects from references to them.

        Arguments:
            values: a dictionary with value ids or aliases as keys, and value instances as values

        Returns:
            a mapping with value_id as key, and [kiara.models.values.value.Value] as value
        """

        return self.context.data_registry.load_values(values=values)

    def query_value(
        self,
        value_or_path: Union[str, Value, uuid.UUID],
        query_path: Union[str, None] = None,
    ) -> Any:
        """
        Retrieve a value attribute with the specified id or alias.

        NOTE: This is a provisional endpoint, don't use for now, if you have a requirement that would
        be covered by this, please let me know.

        A query path is delimited by "::", and has the following format:

        ```
        <value_id_or_alias>::[<category_name>]::[<attribute_name>]::[...]
        ```

        Currently supported categories:
        - "data": the data of the value
        - "properties: the properties of the value

        If no category is specified, the value instance itself is returned.

        Raises an exception if no value could be found.

        Arguments:
            value_or_path: a value or value reference, or a query path containing the value id or alias as first token
            query_path: a query path which will be appended a potential query path computed from the first argument

        Returns:
            the attribute value
        """

        if isinstance(value_or_path, str):
            tokens = value_or_path.split(VALUE_ATTR_DELIMITER)
            value_id = tokens.pop(0)
            _value = self.get_value(value=value_id)
        else:
            tokens = []
            _value = self.get_value(value=value_or_path)

        if query_path:
            tokens.extend(query_path.split(VALUE_ATTR_DELIMITER))

        if not tokens:
            return _value

        current_result: Any = _value
        category = tokens.pop(0)
        if category == "properties":
            current_result = current_result.get_all_property_data(flatten_models=True)
        elif category == "data":
            current_result = current_result.data
        else:
            raise KiaraException(
                f"Invalid query path category: {category}. Valid categories are: {', '.join(VALID_VALUE_QUERY_CATEGORIES)}"
            )

        if tokens:
            try:
                path = VALUE_ATTR_DELIMITER.join(tokens)
                current_result = dpath.get(
                    current_result, path, separator=VALUE_ATTR_DELIMITER
                )

            except Exception:

                def dict_path(path, my_dict, all_paths):
                    for k, v in my_dict.items():
                        if isinstance(v, dict):
                            dict_path(path + "::" + k, v, all_paths)
                        else:
                            all_paths.append(path[2:] + "::" + k)

                valid_base_keys = list(current_result.keys())
                details = "Valid (base) sub-keys are:\n\n"
                for k in valid_base_keys:
                    details += f"  - {k}\n"

                all_paths: List[str] = []
                dict_path("", current_result, all_paths)

                details += "\nValid (full) sub-paths are:\n\n"
                for k in all_paths:
                    details += f"  - {k}\n"

                raise KiaraException(
                    msg=f"Failed to retrieve value attribute using query sub-path: {path}",
                    details=details,
                )

        return current_result

    @tag("kiara_api")
    def retrieve_value_info(
        self, value: Union[str, uuid.UUID, Value, Path]
    ) -> ValueInfo:
        """
        Retrieve an info object for a value.

        Companion method to 'get_value', 'ValueInfo' objects contains augmented information on top of what 'normal' [Value][kiara.models.values.value.Value] objects
        hold (like resolved properties for example), but they can take longer to create/resolve. If you don't need any
        of the augmented information, just use the [get_value][kiara.interfaces.python_api.KiaraAPI.get_value] method
        instead.

        Arguments:
            value: a value id, alias or object that has a 'value_id' attribute.

        Returns:
            the ValueInfo instance

        """
        _value = self.get_value(value=value)
        return ValueInfo.create_from_instance(kiara=self.context, instance=_value)

    @tag("kiara_api")
    def retrieve_values_info(self, **matcher_params: Any) -> ValuesInfo:
        """
        Retrieve information about the matching values.

        This retrieves the same list of values as [list_values][kiara.interfaces.python_api.KiaraAPI.list_values],
        but augments each result value instance with additional information that might be useful in frontends.

        'ValueInfo' objects contains augmented information on top of what 'normal' [Value][kiara.models.values.value.Value] objects
        hold (like resolved properties for example), but they can take longer to create/resolve. If you don't need any
        of the augmented information, just use the [list_values][kiara.interfaces.python_api.KiaraAPI.list_values] method
        instead.

        Arguments:
            matcher_params: the (optional) filter parameters, check the [ValueMatcher][kiara.models.values.matchers.ValueMatcher] class for available parameters

        Returns:
            a wrapper object containing the items as dictionary with value_id as key, and [kiara.interfaces.python_api.models.values.ValueInfo] as value
        """
        values: MutableMapping[str, Value] = self.list_values(**matcher_params)

        infos = ValuesInfo.create_from_instances(
            kiara=self.context, instances={str(k): v for k, v in values.items()}
        )
        return infos  # type: ignore

    @tag("kiara_api")
    def list_alias_names(self, **matcher_params: Any) -> List[str]:
        """
        List all available alias keys.

        This method exists mainly so frontend can retrieve a list of all value_ids that exists on the backend without
        having to look up the details of each value (like [list_aliases][kiara.interfaces.python_api.KiaraAPI.list_aliases]
        does). This method can also be used with a matcher, but in this case the [list_aliases][kiara.interfaces.python_api.KiaraAPI.list_aliases]
        would be preferrable in most cases, because it is called under the hood, and the performance advantage of not
        having to look up value details is gone.

        Arguments:
            matcher_params: the (optional) filter parameters, check the [ValueMatcher][kiara.models.values.matchers.ValueMatcher] class for available parameters

        Returns:
            a list of value ids
        """
        if matcher_params:
            values = self.list_aliases(**matcher_params)
            return list(values.keys())
        else:
            _values = self.context.alias_registry.all_aliases
            return list(_values)

    @tag("kiara_api")
    def list_aliases(self, **matcher_params: Any) -> ValueMapReadOnly:
        """
        List all available values that have an alias assigned, optionally filter.

        Arguments:
            matcher_params: the (optional) filter parameters, check the [ValueMatcher][kiara.models.values.matchers.ValueMatcher] class for available parameters

        Returns:
            a dictionary with value_id as key, and [kiara.models.values.value.Value] as value
        """
        if matcher_params:
            matcher_params["has_alias"] = True
            all_values = self.list_values(**matcher_params)

            result: Dict[str, Value] = {}
            for value in all_values.values():
                aliases = self.context.alias_registry.find_aliases_for_value_id(
                    value_id=value.value_id
                )
                for a in aliases:
                    if a in result.keys():
                        raise Exception(
                            f"Duplicate value alias '{a}': this is most likely a bug."
                        )
                    result[a] = value

            result = {k: result[k] for k in sorted(result.keys())}
        else:
            # faster if not other matcher params
            all_aliases = self.context.alias_registry.all_aliases
            result = {
                k: self.context.data_registry.get_value(f"alias:{k}")
                for k in all_aliases
            }

        return ValueMapReadOnly.create_from_values(**result)

    @tag("kiara_api")
    def retrieve_aliases_info(self, **matcher_params: Any) -> ValuesInfo:
        """
        Retrieve information about the matching values.

        This retrieves the same list of values as [list_values][kiara.interfaces.python_api.KiaraAPI.list_values],
        but augments each result value instance with additional information that might be useful in frontends.

        'ValueInfo' objects contains augmented information on top of what 'normal' [Value][kiara.models.values.value.Value] objects
        hold (like resolved properties for example), but they can take longer to create/resolve. If you don't need any
        of the augmented information, just use the [get_value][kiara.interfaces.python_api.KiaraAPI.list_aliases] method
        instead.

        Arguments:
            matcher_params: the (optional) filter parameters, check the [ValueMatcher][kiara.models.values.matchers.ValueMatcher] class for available parameters

        Returns:
            a dictionary with a value alias as key, and [kiara.interfaces.python_api.models.values.ValueInfo] as value
        """
        values = self.list_aliases(**matcher_params)

        infos = ValuesInfo.create_from_instances(
            kiara=self.context, instances={str(k): v for k, v in values.items()}
        )
        return infos  # type: ignore

    def register_value_alias(
        self,
        value: Union[str, Value, uuid.UUID],
        alias: Union[str, Iterable[str]],
        allow_overwrite: bool = False,
        alias_store: Union[str, None] = None,
    ) -> None:

        self.context.alias_registry.register_aliases(
            value_id=value,
            aliases=alias,
            allow_overwrite=allow_overwrite,
            alias_store=alias_store,
        )

    def assemble_value_map(
        self,
        values: Mapping[str, Union[uuid.UUID, None, str, Value, Any]],
        values_schema: Union[None, Mapping[str, ValueSchema]] = None,
        register_data: bool = False,
        reuse_existing_data: bool = False,
    ) -> ValueMapReadOnly:
        """
        Retrive a [ValueMap][kiara.models.values.value.ValueMap] object from the provided value ids or value links.

        In most cases, this endpoint won't be used by front-ends, it's a fairly low-level method that is
        mainly used for internal purposes. If you have a use-case, let me know and I'll improve the docs
        if insufficient.

        By default, this method can only use values/datasets that are already registered in *kiara*. If you want to
        auto-register 'raw' data, you need to set the 'register_data' flag to 'True', and provide a schema for each of the fields that are not yet registered.

        Arguments:
            values: a dictionary with the values in question
            values_schema: an optional dictionary with the schema for each of the values that are not yet registered
            register_data: whether to allow auto-registration of 'raw' data
            reuse_existing_data: whether to reuse existing data with the same hash as the 'raw' data that is being registered

        Returns:
            a value map instance
        """

        if register_data:
            temp: Dict[str, Union[str, Value, uuid.UUID, None]] = {}
            for k, v in values.items():

                if isinstance(v, (Value, uuid.UUID)):
                    temp[k] = v
                    continue

                if not values_schema:
                    details = "No schema provided."
                    raise KiaraException(
                        f"Invalid field name: '{k}' (value: {v}).", details=details
                    )

                if k not in values_schema.keys():
                    details = "Valid field names: " + ", ".join(values_schema.keys())
                    raise KiaraException(
                        f"Invalid field name: '{k}' (value: {v}).", details=details
                    )

                if isinstance(v, str):

                    if v.startswith("alias:"):
                        temp[k] = v
                        continue
                    elif v.startswith("archive:"):
                        temp[k] = v
                        continue

                    try:
                        v = uuid.UUID(v)
                        temp[k] = v
                        continue
                    except Exception:
                        if v.startswith("alias:"):  # type: ignore
                            _v = v.replace("alias:", "")  # type: ignore
                        else:
                            _v = v

                        data_type = values_schema[k].type
                        if data_type != "string" and _v in self.list_aliases():
                            temp[k] = f"alias:{_v}"
                            continue

                if v is None:
                    temp[k] = None
                else:
                    _v = self.register_data(
                        data=v,
                        # data_type=values_schema[k].type,
                        data_type=values_schema[k],
                        reuse_existing=reuse_existing_data,
                    )
                    temp[k] = _v
            values = temp
        return self.context.data_registry.load_values(
            values=values, values_schema=values_schema
        )

    @tag("kiara_api")
    def store_value(
        self,
        value: Union[str, uuid.UUID, Value],
        alias: Union[str, Iterable[str], None],
        allow_overwrite: bool = True,
        store: Union[str, None] = None,
        store_related_metadata: bool = True,
        set_as_store_default: bool = False,
    ) -> StoreValueResult:
        """
        Store the specified value in a value store.

        If you provide values for the 'data_store' and/or 'alias_store' other than 'default', you need
        to make sure those stores are registered with the current context. In most cases, the 'export' endpoint (to be done) will probably be an easier way to export values, which I suspect will
        be the main use-case for this endpoint if any of the 'store' arguments where needed. Otherwise, this endpoint is useful to persist values for use in later seperate sessions.

        This method does not raise an error if the storing of the value fails, so you have to investigate the
        'StoreValueResult' instance that is returned to see if the storing was successful.

        Arguments:
            value: the value (or a reference to it)
            alias: (Optional) one or several aliases for the value
            allow_overwrite: whether to allow overwriting existing aliases
            store: in case data and alias store names are the same, you can use this, if you specify one or both of the others, this will be overwritten
            store_related_metadata: whether to store related metadata (comments, etc.) in the same store as the data
            set_as_store_default: whether to set the specified store as the default store for the value
        """
        # if isinstance(alias, str):
        #     alias = [alias]

        value_obj = self.get_value(value)
        persisted_data: Union[None, PersistedData] = None

        try:
            persisted_data = self.context.data_registry.store_value(
                value=value_obj, data_store=store
            )
            if alias:
                self.context.alias_registry.register_aliases(
                    value_obj,
                    alias,
                    allow_overwrite=allow_overwrite,
                    alias_store=store,
                )

            if store_related_metadata:

                from kiara.registries.metadata import MetadataMatcher

                matcher = MetadataMatcher.create_matcher(
                    reference_item_ids=[value_obj.job_id, value_obj.value_id]
                )

                target_store: MetadataStore = self.context.metadata_registry.get_archive(store)  # type: ignore
                matching_metadata = self.context.metadata_registry.find_metadata_items(
                    matcher=matcher
                )
                target_store.store_metadata_and_ref_items(matching_metadata)

            if set_as_store_default:
                store_instance = self.context.data_registry.get_archive(store)
                store_instance.set_archive_metadata_value(
                    DATA_ARCHIVE_DEFAULT_VALUE_MARKER, str(value_obj.value_id)
                )
            result = StoreValueResult(
                value=value_obj,
                aliases=sorted(alias) if alias else [],
                error=None,
                persisted_data=persisted_data,
            )
        except Exception as e:
            log_exception(e)
            result = StoreValueResult(
                value=value_obj,
                aliases=sorted(alias) if alias else [],
                error=(
                    str(e) if str(e) else f"Unknown error (type '{type(e).__name__}')."
                ),
                persisted_data=persisted_data,
            )

        return result

    @tag("kiara_api")
    def store_values(
        self,
        values: Union[
            str,
            Value,
            uuid.UUID,
            Mapping[str, Union[str, uuid.UUID, Value]],
            Iterable[Union[str, uuid.UUID, Value]],
        ],
        alias_map: Union[Mapping[str, Iterable[str]], bool, str] = False,
        allow_alias_overwrite: bool = True,
        store: Union[str, None] = None,
        store_related_metadata: bool = True,
    ) -> StoreValuesResult:
        """
        Store multiple values into the (default) kiara value store.

        Convenience method to store multiple values. In a lot of cases you can be more flexible if you
        loop over the values on the frontend side, and call the 'store_value' method for each value. But this might be meaningfully slower. This method has the potential to be optimized in the future.

        You have several options to provide the values and aliases you want to store:

        - as a string, in which case the item will be wrapped in a list (see non-mapping iterable below)

        - as a (non-mapping) iterable of value items, those can either be:

          - a value id (as string or uuid)
          - a value alias (as string)
          - a value instance

        If you do that, then the 'alias_map' argument can either be:

          - 'False', in which case no aliases will be registered
          - 'True', in which case all items in the 'values' iterable must be a valid alias, and the alias will be copied without change to the new store
          - a 'string', in which case all items in the 'values' iterable also must be a valid alias, and the alias that will be registered in the new store will use the string value as prefix (e.g. 'alias_map' = 'experiment1' and 'values' = ['a', 'b'] will result in the aliases 'experiment1.a' and 'experiment1.b')
          - a map that uses the stringi-fied uuid of the value that should get one or several aliases as key, and a list of aliases as values

        You can also use a mapping type (like a dict) for the 'values' argument. In this case, the key is a string, and the value can be:

          - a value id (as string or uuid)
          - a value alias (as string)
          - a value instance

        In this case, the meaning of the 'alias_map' is as follows:

          - 'False': no aliases will be registered
          - 'True': the key in the 'values' argument will be used as alias
          - a string: all keys from the 'values' map will be used as alias, prefixed with the value of 'alias_map'
          - another map, with a string referring to the key in the 'values' argument as key, and a list of aliases (strings) as value

        Sorry, this is all a bit convoluted, but it's the only way I could think of to make this work for all the requirements I had. In most keases, you'll only have to use 'True' or 'False' here, hopefully.

        This method does not raise an error if the storing of the value fails, so you have to investigate the
        'StoreValuesResult' instance that is returned to see if the storing was successful.

        Arguments:
            values: an iterable/map of value keys/values
            alias_map: a map of value keys aliases
            allow_alias_overwrite: whether to allow overwriting existing aliases
            store: in case data and alias store names are the same, you can use this, if you specify one or both of the others, this will be overwritten
            data_store: the registered name (or archive id as string) of the store to write the data
            alias_store: the registered name (or archive id as string) of the store to persist the alias(es)/value_id mapping

        Returns:
            an object outlining which values (identified by the specified value key or an enumerated index) where stored and how

        """

        if isinstance(values, (str, uuid.UUID, Value)):
            values = [values]

        result = {}
        if not isinstance(values, Mapping):
            if not alias_map:
                use_aliases = False
            elif alias_map and (alias_map is True or isinstance(alias_map, str)):

                invalid: List[Union[str, uuid.UUID, Value]] = []
                valid: Dict[str, List[str]] = {}
                for value in values:
                    if not isinstance(value, str):
                        invalid.append(value)
                        continue
                    value_id = self.context.alias_registry.find_value_id_for_alias(
                        alias=value
                    )
                    if value_id is None:
                        invalid.append(value)
                    else:
                        if alias_map is True:
                            if "#" in value:
                                new_alias = value.split("#")[1]
                            else:
                                new_alias = value
                            valid.setdefault(str(value_id), []).append(new_alias)
                        else:
                            if "#" in value:
                                new_alias = value.split("#")[1]
                            else:
                                new_alias = value
                            new_alias = f"{alias_map}{new_alias}"
                            valid.setdefault(str(value_id), []).append(new_alias)
                if invalid:
                    invalid_str = ", ".join((str(x) for x in invalid))
                    raise KiaraException(
                        msg=f"Cannot use auto-aliases with non-mapping iterable, some items are not valid aliases: {invalid_str}"
                    )
                else:
                    alias_map = valid
                    use_aliases = True
            else:
                use_aliases = True

            for value in values:

                aliases: Set[str] = set()

                value_obj = self.get_value(value)
                if use_aliases:
                    alias_key = str(value_obj.value_id)
                    alias: Union[str, None] = alias_map.get(alias_key, None)  # type: ignore
                    if alias:
                        aliases.update(alias)

                store_result = self.store_value(
                    value=value_obj,
                    alias=aliases,
                    allow_overwrite=allow_alias_overwrite,
                    store=store,
                    store_related_metadata=store_related_metadata,
                )
                result[str(value_obj.value_id)] = store_result
        else:

            for field_name, value in values.items():
                if alias_map is False:
                    aliases_map: Union[None, Iterable[str]] = None
                elif alias_map is True:
                    aliases_map = [field_name]
                elif isinstance(alias_map, str):
                    aliases_map = [f"{alias_map}.{field_name}"]
                else:
                    # means it's a mapping
                    _aliases = alias_map.get(field_name)
                    if _aliases:
                        aliases_map = list(_aliases)
                    else:
                        aliases_map = None

                value_obj = self.get_value(value)
                store_result = self.store_value(
                    value=value_obj,
                    alias=aliases_map,
                    allow_overwrite=allow_alias_overwrite,
                    store=store,
                    store_related_metadata=store_related_metadata,
                )
                result[field_name] = store_result

        return StoreValuesResult(root=result)

    # ------------------------------------------------------------------------------------------------------------------
    # archive-related methods
    @tag("kiara_api")
    def import_values(
        self,
        source_archive: Union[str, Path],
        values: Union[
            str,
            Mapping[str, Union[str, uuid.UUID, Value]],
            Iterable[Union[str, uuid.UUID, Value]],
        ],
        alias_map: Union[Mapping[str, Iterable[str]], bool, str] = False,
        allow_alias_overwrite: bool = True,
        source_registered_name: Union[str, None] = None,
    ) -> StoreValuesResult:
        """Import one or several values from an external kiara archive, along with their aliases (optional).

        For the 'values' & 'alias_map' arguments, see the 'store_values' endpoint, as they will be forwarded to that endpoint as is,
        and there are several ways to use them which is information I don't want to duplicate.

        If you provide aliases in the 'values' parameter, the aliases must be available in the external archive.

        Currently, this only works with an external archive file, not with an archive that is registered into the context.
        This will probably be added later on, let me know if there is demand, then I'll prioritize.

        This method does not raise an error if the storing of the value fails, so you have to investigate the
        'StoreValuesResult' instance that is returned to see if the storing was successful.

        # NOTE: this is a preliminary endpoint, and might be changed in the future. If you have a use-case for this, please let me know.

        Arguments:
            source_archive: the name of the archive to store the values into
            values: an iterable/map of value keys/values
            alias_map: a map of value keys aliases
            allow_alias_overwrite: whether to allow overwriting existing aliases
            source_registered_name: the name to register the archive under in the context
        """

        if source_archive in [None, DEFAULT_STORE_MARKER]:
            raise KiaraException(
                "You cannot use the default store as source for this operation."
            )

        if alias_map is True:
            pass
        elif alias_map is False:
            pass
        elif isinstance(alias_map, str):
            pass
        elif isinstance(alias_map, Mapping):
            pass
        else:
            raise KiaraException(
                f"Invalid type for 'alias_map' argument: {type(alias_map)}."
            )

        source_archive_ref = self.register_archive(
            archive=source_archive,  # type: ignore
            registered_name=source_registered_name,
            create_if_not_exists=False,
            allow_write_access=False,
            existing_ok=True,
        )

        value_ids: Set[uuid.UUID] = set()
        aliases: Set[str] = set()

        if isinstance(values, str):
            values = [values]

        if not isinstance(values, Mapping):
            # means we have a list of value ids/aliases
            for value in values:
                if isinstance(value, uuid.UUID):
                    value_ids.add(value)
                elif isinstance(value, str):
                    try:
                        _value = uuid.UUID(value)
                        value_ids.add(_value)
                    except Exception:
                        aliases.add(value)
        else:
            raise NotImplementedError("Not implemented yet.")

        new_values: Dict[str, Union[uuid.UUID, str]] = {}
        idx = 0
        for value_id in value_ids:
            field = f"field_{idx}"
            idx += 1
            new_values[field] = value_id

        new_alias_map = {}
        for alias in aliases:
            field = f"field_{idx}"
            idx += 1
            new_values[field] = f"{source_archive_ref}#{alias}"
            if alias_map is False:
                pass
            elif alias_map is True:
                new_alias_map[field] = [f"{alias}"]
            elif isinstance(alias_map, str):
                new_alias_map[field] = [f"{alias_map}{alias}"]
            else:
                # means its a dict
                if alias in alias_map.keys():
                    for a in alias_map[alias]:
                        new_alias_map.setdefault(field, []).append(a)

        result: StoreValuesResult = self.store_values(
            values=new_values,
            alias_map=new_alias_map,
            allow_alias_overwrite=allow_alias_overwrite,
        )
        return result

    @tag("kiara_api")
    def export_values(
        self,
        target_archive: Union[str, Path],
        values: Union[
            str,
            Value,
            uuid.UUID,
            Mapping[str, Union[str, uuid.UUID, Value]],
            Iterable[Union[str, uuid.UUID, Value]],
        ],
        alias_map: Union[Mapping[str, Iterable[str]], bool, str] = False,
        allow_alias_overwrite: bool = True,
        target_registered_name: Union[str, None] = None,
        append: bool = False,
        target_store_params: Union[None, Mapping[str, Any]] = None,
        export_related_metadata: bool = True,
        additional_archive_metadata: Union[None, Mapping[str, Any]] = None,
    ) -> StoreValuesResult:
        """Store one or several values along with (optional) aliases into a kiara archive.

        For the 'values' & 'alias_map' arguments, see the 'store_values' endpoint, as they will be forwarded to that endpoint as is,
        and there are several ways to use them which is information I don't want to duplicate.

        Currently, this only works with an external archive file, not with an archive that is registered into the context.
        This will probably be added later on, let me know if there is demand, then I'll prioritize.

        'target_store_params' is used if the archive does not exist yet. The one supported value for the 'target_store_params' argument currently is 'compression', which can be one of:

        - zstd: zstd compression (default) -- fairly fast, and good compression
        - none: no compression
        - LZMA: LZMA compression -- very slow, but very good compression
        - LZ4: LZ4 compression -- very fast, but not as good compression as zstd

        This method does not raise an error if the storing of the value fails, so you have to investigate the
        'StoreValuesResult' instance that is returned to see if the storing was successful.

        # NOTE: this is a preliminary endpoint, and might be changed in the future. If you have a use-case for this, please let me know.

        Arguments:
            target_store: the name of the archive to store the values into
            values: an iterable/map of value keys/values
            alias_map: a map of value keys aliases
            allow_alias_overwrite: whether to allow overwriting existing aliases
            target_registered_name: the name to register the archive under in the context
            append: whether to append to an existing archive
            target_store_params: additional parameters to pass to the 'create_kiarchive' method if the file does not exist yet
            export_related_metadata: whether to export related metadata (e.g. job info, comments, ..) to the new archive or not
            additional_archive_metadata: (optional) additional metadata to add to the archive

        """

        if target_archive in [None, DEFAULT_STORE_MARKER]:
            raise KiaraException(
                "You cannot use the default store as target for this operation."
            )

        if target_store_params is None:
            target_store_params = {}

        target_archive_ref = self.register_archive(
            archive=target_archive,  # type: ignore
            registered_name=target_registered_name,
            create_if_not_exists=True,
            allow_write_access=True,
            existing_ok=True if append else False,
            **target_store_params,
        )

        result: StoreValuesResult = self.store_values(
            values=values,
            alias_map=alias_map,
            allow_alias_overwrite=allow_alias_overwrite,
            store=target_archive_ref,
            store_related_metadata=export_related_metadata,
        )

        if additional_archive_metadata:
            for k, v in additional_archive_metadata.items():
                self.set_archive_metadata_value(target_archive_ref, k, v)

        return result

    def register_archive(
        self,
        archive: Union[str, Path, "KiArchive"],
        allow_write_access: bool = False,
        registered_name: Union[str, None] = None,
        create_if_not_exists: bool = True,
        existing_ok: bool = True,
        **create_params: Any,
    ) -> str:
        """Register a kiarchive with the current context.

        In most cases, this will be used to 'load' an existing kiarchive file and attach it to the current context.
        If the file does not exist, one will be created, with the filename (without '.kiarchive' suffix) as the archive name if not specified.

        In the future this might also take a URL, but for now only local files are supported.

        # NOTE: this is a preliminary endpoint, and might be changed in the future. If you have a use-case for this, please let me know.

        Arguments:
            archive: the uri of the archive (file path), or a [Kiarchive][kiara.interfaces.python_api.models.archive.Kiarchive] instance
            allow_write_access: whether to allow write access to the archive
            registered_name: the name/alias that the archive is registered in the context, and which can be used in the 'store_value(s)' endpoint, if not provided, it will be auto-determined from the file name
            create_if_not_exists: if the file does not exist, create it. If this is 'False', an exception will be raised if the file does not exist.
            existing_ok: whether the file is allowed to exist already, if 'False', an exception will be raised if the file exists
            create_params: additional parameters to pass to the 'create_kiarchive' method if the file does not exist yet

        Returns:
            the name/alias that the archive is registered in the context, and which can be used in the 'store_value(s)' endpoint
        """
        from kiara.interfaces.python_api.models.archive import KiArchive

        if not existing_ok and not create_if_not_exists:
            raise KiaraException(
                "Both 'existing_ok' and 'create_if_not_exists' cannot be 'False' at the same time."
            )

        if isinstance(archive, str):
            archive = Path(archive)

        if isinstance(archive, Path):

            if not archive.name.endswith(".kiarchive"):
                archive = archive.parent / f"{archive.name}.kiarchive"

            if archive.exists():
                if not existing_ok:
                    raise KiaraException(
                        f"Archive file '{archive.as_posix()}' already exists."
                    )
                archive = KiArchive.load_kiarchive(
                    kiara=self.context,
                    path=archive,
                    archive_name=registered_name,
                    allow_write_access=allow_write_access,
                )
                log_message("archive.loaded", archive_name=archive.archive_name)
            else:
                if not create_if_not_exists:
                    raise KiaraException(
                        f"Archive file '{archive.as_posix()}' does not exist."
                    )
                kiarchive_alias = archive.name
                if kiarchive_alias.endswith(".kiarchive"):
                    kiarchive_alias = kiarchive_alias[:-10]

                compression: Union[None, CHUNK_COMPRESSION_TYPE, str] = None
                for k, v in create_params.items():
                    if k == "compression":
                        compression = v
                    else:
                        raise KiaraException(
                            msg=f"Invalid archive creation parameter: '{k}'."
                        )

                archive = KiArchive.create_kiarchive(
                    kiara=self.context,
                    kiarchive_uri=archive.as_posix(),
                    allow_existing=False,
                    archive_name=kiarchive_alias,
                    allow_write_access=allow_write_access,
                    compression=compression,
                )
                log_message("archive.created", archive_name=archive.archive_name)

        else:
            raise NotImplementedError("Only local files are supported for now.")

        data_archive = archive.data_archive
        assert data_archive is not None
        data_alias = self.context.register_external_archive(
            data_archive,
            allow_write_access=allow_write_access,
        )

        alias_archive = archive.alias_archive
        assert alias_archive is not None
        alias_alias = self.context.register_external_archive(
            alias_archive, allow_write_access=allow_write_access
        )

        job_archive = archive.job_archive
        assert job_archive is not None
        job_alias = self.context.register_external_archive(
            job_archive, allow_write_access=allow_write_access
        )

        metadata_archive = archive.metadata_archive
        assert metadata_archive is not None
        metadata_alias = self.context.register_external_archive(
            metadata_archive, allow_write_access=allow_write_access
        )
        assert data_alias["data"] == alias_alias["alias"]
        assert data_alias["data"] == job_alias["job_record"]
        assert data_alias["data"] == metadata_alias["metadata"]
        assert archive.archive_name == data_alias["data"]

        return archive.archive_name

    def set_archive_metadata_value(
        self,
        archive: Union[str, uuid.UUID],
        key: str,
        value: Any,
        archive_type: Literal["data", "alias", "job_record", "metadata"] = "data",
    ) -> None:
        """Add metadata to an archive.

        Note that this is different to adding metadata to a context, since it is attached directly
        to a special section of the archive itself.
        """

        if archive_type == "data":
            _archive: Union[None, KiaraArchive] = (
                self.context.data_registry.get_archive(archive)
            )
            if _archive is None:
                raise KiaraException(f"Archive '{archive}' does not exist.")
            _archive.set_archive_metadata_value(key, value)
        elif archive_type == "alias":
            _archive = self.context.alias_registry.get_archive(archive)
            if _archive is None:
                raise KiaraException(f"Archive '{archive}' does not exist.")
            _archive.set_archive_metadata_value(key, value)
        elif archive_type == "metadata":
            _archive = self.context.metadata_registry.get_archive(archive)
            if _archive is None:
                raise KiaraException(f"Archive '{archive}' does not exist.")
            _archive.set_archive_metadata_value(key, value)
        elif archive_type == "job_record":
            _archive = self.context.job_registry.get_archive(archive)
            if _archive is None:
                raise KiaraException(f"Archive '{archive}' does not exist.")
            _archive.set_archive_metadata_value(key, value)
        else:
            raise KiaraException(
                f"Invalid archive type: {archive_type}. Valid types are: 'data', 'alias'."
            )

    @tag("kiara_api")
    def retrieve_archive_info(
        self, archive: Union[str, "KiArchive"]
    ) -> "KiArchiveInfo":
        """Retrieve information about an archive at the specified local path

        Currently, this only works with an external archive file, not with an archive that is registered into the context.
        This will probably be added later on, let me know if there is demand, then I'll prioritize.

        # NOTE: this is a preliminary endpoint, and might be changed in the future. If you have a use-case for this, please let me know.

        Arguments:
            archive: the uri of the archive (file path)

        Returns:
            a [KiarchiveInfo][kiara.interfaces.python_api.models.archive.KiarchiveInfo] instance, containing details about the archive
        """

        from kiara.interfaces.python_api.models.archive import KiArchive
        from kiara.models.archives import KiArchiveInfo

        if not isinstance(archive, KiArchive):
            archive = KiArchive.load_kiarchive(kiara=self.context, path=archive)

        kiarchive_info = KiArchiveInfo.create_from_instance(
            kiara=self.context, instance=archive
        )
        return kiarchive_info

    @tag("kiara_api")
    def export_archive(
        self,
        target_archive: Union[str, Path],
        target_registered_name: Union[str, None] = None,
        append: bool = False,
        no_aliases: bool = False,
        target_store_params: Union[None, Mapping[str, Any]] = None,
    ) -> StoreValuesResult:
        """Export all data from the default store in your context into the specfied archive path.

        The target archives will be registered into the context, either using the provided registered_name, or the name
        will be auto-determined from the archive metadata.

        Currently, this only works with an external archive file, not with an archive that is already registered into the context.
        This will be added later on.

        Also, currently you can only export all data from the default store, there is no way to select only a sub-set. This will
        also be supported later on.

        The one supported value for the 'target_store_params' argument currently is 'compression', which can be one of:

        - zstd: zstd compression (default) -- fairly fast, and good compression
        - none: no compression
        - LZMA: LZMA compression -- very slow, but very good compression
        - LZ4: LZ4 compression -- very fast, but not as good compression as zstd

        This method does not raise an error if the storing of the value fails, so you have to investigate the
        'StoreValuesResult' instance that is returned to see if the storing was successful

        Arguments:
            target_archive: the registered_name or uri of the target archive
            target_registered_name: the name/alias that the archive should be registered in the context (if necessary)
            append: whether to append to an existing archive or error out if the target already exists
            no_aliases: whether to skip importing aliases
            target_store_params: additional parameters to pass to the 'create_kiarchive' method if the target file does not exist yet

        Returns:
            an object outlining which values (identified by the specified value key or an enumerated index) where stored and how
        """

        result = self.copy_archive(
            source_archive=DEFAULT_STORE_MARKER,
            target_archive=target_archive,
            target_registered_name=target_registered_name,
            append=append,
            target_store_params=target_store_params,
            no_aliases=no_aliases,
        )
        return result

    @tag("kiara_api")
    def import_archive(
        self,
        source_archive: Union[str, Path],
        source_registered_name: Union[str, None] = None,
        no_aliases: bool = False,
    ) -> StoreValuesResult:
        """Import all data from the specified archive into the current contexts default data & alias store.

        The source target will be registered into the context, either using the provided registered_name, otherwise the name
        will be auto-determined from the archive metadata.

        Currently, this only works with an external archive file, not with an archive that is registered into the context.
        This will be added later on.

        Also, currently you can only import all data into the default store, there is no way to select only a sub-set. This will
        also be supported later on.

        This method does not raise an error if the storing of the value fails, so you have to investigate the
        'StoreValuesResult' instance that is returned to see if the storing was successful

        Arguments:
            source_archive: the registered_name or uri of the source archive
            source_registered_name: the name/alias that the archive should be registered in the context (if necessary)
            no_aliases: whether to skip importing aliases

        Returns:
            an object outlining which values (identified by the specified value key or an enumerated index) where stored and how

        """

        result = self.copy_archive(
            source_archive=source_archive,
            target_archive=DEFAULT_STORE_MARKER,
            source_registered_name=source_registered_name,
            no_aliases=no_aliases,
        )
        return result

    def copy_archive(
        self,
        source_archive: Union[None, str, Path],
        target_archive: Union[None, str, Path] = None,
        source_registered_name: Union[str, None] = None,
        target_registered_name: Union[str, None] = None,
        append: bool = False,
        no_aliases: bool = False,
        target_store_params: Union[None, Mapping[str, Any]] = None,
    ) -> StoreValuesResult:
        """Import all data from the specified archive into the current context.

        The archives will be registered into the context, either using the provided registered_name, otherwise the name
        will be auto-determined from the archive metadata.

        Currently, this only works with an external archive file, not with an archive that is registered into the context.
        This will be added later on.

        The one supported value for the 'target_store_params' argument currently is 'compression', which can be one of:

        - zstd: zstd compression (default) -- fairly fast, and good compression
        - none: no compression
        - LZMA: LZMA compression -- very slow, but very good compression
        - LZ4: LZ4 compression -- very fast, but not as good compression as zstd

        This method does not raise an error if the storing of the value fails, so you have to investigate the
        'StoreValuesResult' instance that is returned to see if the storing was successful

        Arguments:
            source_archive: the registered_name or uri of the source archive, if None, the context default data/alias store will be used
            target_archive: the registered_name or uri of the target archive, defaults to the context default data/alias store
            source_registered_name: the name/alias that the archive should be registered in the context (if necessary)
            target_registered_name: the name/alias that the archive should be registered in the context (if necessary)
            append: whether to append to an existing archive or error out if the target already exists
            no_aliases: whether to skip importing aliases
            target_store_params: additional parameters to pass to the 'create_kiarchive' method if the target file does not exist yet

        Returns:
            an object outlining which values (identified by the specified value key or an enumerated index) where stored and how

        """

        if source_archive in [None, DEFAULT_STORE_MARKER]:
            source_archive_ref = DEFAULT_STORE_MARKER
        else:
            source_archive_ref = self.register_archive(
                archive=source_archive,  # type: ignore
                registered_name=source_registered_name,
                create_if_not_exists=False,
                existing_ok=True,
            )

        if target_archive in [None, DEFAULT_STORE_MARKER]:
            target_archive_ref = DEFAULT_STORE_MARKER
        else:
            if target_store_params is None:
                target_store_params = {}
            target_archive_ref = self.register_archive(
                archive=target_archive,  # type: ignore
                registered_name=target_registered_name,
                create_if_not_exists=True,
                allow_write_access=True,
                existing_ok=True if append else False,
                **target_store_params,
            )

        if source_archive_ref == target_archive_ref:
            raise KiaraException(
                f"Source and target archive cannot be the same: {source_archive_ref} != {target_archive_ref}"
            )

        source_values = self.list_values(
            in_data_archives=[source_archive_ref], allow_internal=True, has_alias=False
        ).values()

        if not no_aliases:
            aliases = self.list_aliases(in_data_archives=[source_archive_ref])
            alias_map: Union[bool, Dict[str, List[str]]] = {}
            for alias, value in aliases.items():

                if source_archive_ref != DEFAULT_STORE_MARKER:
                    # TODO: maybe add a matcher arg to the list_aliases endpoint
                    if not alias.startswith(f"{source_archive_ref}#"):
                        continue
                    alias_map.setdefault(str(value.value_id), []).append(  # type: ignore
                        alias[len(source_archive_ref) + 1 :]
                    )
                else:
                    if "#" in alias:
                        continue
                    alias_map.setdefault(str(value.value_id), []).append(alias)  # type: ignore
        else:
            alias_map = False

        result: StoreValuesResult = self.store_values(
            source_values, alias_map=alias_map, store=target_archive_ref
        )
        return result

    # ------------------------------------------------------------------------------------------------------------------
    # operation-related methods

    def get_operation_type(
        self, op_type: Union[str, Type[OperationType]]
    ) -> OperationType:
        """Get the management object for the specified operation type."""
        return self.context.operation_registry.get_operation_type(op_type=op_type)

    def retrieve_operation_type_info(
        self, op_type: Union[str, Type[OperationType]]
    ) -> OperationTypeInfo:
        """Get an info object for the specified operation type."""
        _op_type = self.get_operation_type(op_type=op_type)
        return OperationTypeInfo.create_from_type_class(
            kiara=self.context, type_cls=_op_type.__class__
        )

    def find_operation_id(
        self, module_type: str, module_config: Union[None, Mapping[str, Any]] = None
    ) -> Union[None, str]:
        """
        Try to find the registered operation id for the specified module type and configuration.

        Arguments:
            module_type: the module type
            module_config: the module configuration

        Returns:
            the registered operation id, if found, or None
        """
        manifest = self.context.create_manifest(
            module_or_operation=module_type, config=module_config
        )
        return self.context.operation_registry.find_operation_id(manifest=manifest)

    def assemble_filter_pipeline_config(
        self,
        data_type: str,
        filters: Union[str, Iterable[str], Mapping[str, str]],
        endpoint: Union[None, Manifest, str] = None,
        endpoint_input_field: Union[str, None] = None,
        endpoint_step_id: Union[str, None] = None,
        extra_input_aliases: Union[None, Mapping[str, str]] = None,
        extra_output_aliases: Union[None, Mapping[str, str]] = None,
    ) -> "PipelineConfig":
        """
        Assemble a (pipeline) module config to filter values of a specific data type.

        NOTE: this is a preliminary endpoint, and might go away in the future. If you have a need for this
        functionality, please let me know your requirements and we can work on fleshing this out.

        Optionally, a module that uses the filtered dataset as input can be specified.

        # TODO: document filter names
        For the 'filters' argument, the accepted inputs are:
        - a string, in which case a single-step pipeline will be created, with the string referencing the operation id or filter
        - a list of strings: in which case a multi-step pipeline will be created, the step_ids will be calculated automatically
        - a map of string pairs: the keys are step ids, the values operation ids or filter names

        Arguments:
            data_type: the type of the data to filter
            filters: a list of operation ids or filter names (and potentiall step_ids if type is a mapping)
            endpoint: optional module to put as last step in the created pipeline
            endpoing_input_field: field name of the input that will receive the filtered value
            endpoint_step_id: id to use for the endpoint step (module type name will be used if not provided)
            extra_input_aliases: extra output aliases to add to the pipeline config
            extra_output_aliases: extra output aliases to add to the pipeline config

        Returns:
            the (pipeline) module configuration of the filter pipeline
        """
        filter_op_type: FilterOperationType = self.context.operation_registry.get_operation_type("filter")  # type: ignore
        pipeline_config = filter_op_type.assemble_filter_pipeline_config(
            data_type=data_type,
            filters=filters,
            endpoint=endpoint,
            endpoint_input_field=endpoint_input_field,
            endpoint_step_id=endpoint_step_id,
            extra_input_aliases=extra_input_aliases,
            extra_output_aliases=extra_output_aliases,
        )

        return pipeline_config

    # ------------------------------------------------------------------------------------------------------------------
    # metadata-related methods

    def register_metadata_item(
        self, key: str, value: str, store: Union[str, None] = None
    ) -> uuid.UUID:
        """Register a metadata item into the specified metadata store.

        Currently, this allows you to store comments within the default kiara context. You can use any string,
        as key, for example a stringified `job_id`, or `value_id`, or any other string that makes sense in
        the context you are using this in.

        If you use the store argument, the store needs to be mounted into the current *kiara* context. For now,
        you can ignore this and not provide any value here, since this area is still in flux. If you need
        to store a metadata item into an external context, and you can't figure out how to do it,
        let me know.

        Note: this is preliminary and subject to change based on your input, so please provide your thoughts

        Arguments:
            key: the key under which to store the metadata (can be anything you can think of)
            value: the comment you want to store
            store: the store to use, by default the context default is used

        Returns:
            a globally unique identifier for the metadata item
        """

        if not value:
            raise KiaraException("Cannot store empty metadata item.")

        from kiara.models.metadata import CommentMetadata

        item = CommentMetadata(comment=value)

        return self.context.metadata_registry.register_metadata_item(
            key=key, item=item, store=store
        )

    def find_metadata_items(self, **matcher_params: Any):

        from kiara.registries.metadata import MetadataMatcher

        matcher = MetadataMatcher.create_matcher(**matcher_params)

        return self.context.metadata_registry.find_metadata_items(matcher=matcher)

    # ------------------------------------------------------------------------------------------------------------------
    # render-related methods

    def retrieve_renderer_infos(
        self, source_type: Union[str, None] = None, target_type: Union[str, None] = None
    ) -> RendererInfos:
        """Retrieve information about the available renderers.

        Note: this is preliminary and mainly used in the cli, if another use-case comes up let me know and I'll make this more generic, and an 'official' endpoint.

        Arguments:
            source_type: the type of the item to render (optional filter)
            target_type: the type/profile of the rendered result (optional filter)

        Returns:
            a wrapper object containing the items as dictionary with renderer alias as key, and [kiara.interfaces.python_api.models.info.RendererInfo] as value

        """

        if not source_type and not target_type:
            renderers = self.context.render_registry.registered_renderers
        elif source_type and not target_type:
            renderers = self.context.render_registry.retrieve_renderers_for_source_type(
                source_type=source_type
            )
        elif target_type and not source_type:
            raise KiaraException(msg="Cannot retrieve renderers for target type only.")
        else:
            renderers = self.context.render_registry.retrieve_renderers_for_source_target_combination(
                source_type=source_type, target_type=target_type  # type: ignore
            )

        group = {k.get_renderer_alias(): k for k in renderers}
        infos = RendererInfos.create_from_instances(kiara=self.context, instances=group)
        return infos  # type: ignore

    def retrieve_renderers_for(self, source_type: str) -> List[KiaraRenderer]:
        """Retrieve available renderer instances for a specific data type.

        Note: this is not preliminary, and, mainly used in the cli, if another use-case comes up let me know and I'll make this more generic, and an 'official' endpoint.
        """

        return self.context.render_registry.retrieve_renderers_for_source_type(
            source_type=source_type
        )

    def render(
        self,
        item: Any,
        source_type: str,
        target_type: str,
        render_config: Union[Mapping[str, Any], None] = None,
    ) -> Any:
        """Render an internal instance of a supported source type into one of the supported target types.

        Note: this is not preliminary, and, mainly used in the cli, if another use-case comes up let me know and I'll make this more generic, and an 'official' endpoint.

        To find out the supported source/target combinations, you can use the kiara cli:

        ```
        kiara render list-renderers
        ```
        or, for a filtered list:
        ````
        kiara render --source-type pipeline list-renderers
        ```

        What Python types are actually supported for the 'item' argument depends on the source_type of the renderer you are calling, for example if that is a pipeline, most of the ways to specify a pipeline would be supported (operation_id, pipeline file, etc.). This might need more documentation, let me know what exactly is needed in a support ticket and I'll add that information.

        Arguments:
            item: the item to render
            source_type: the type of the item to render
            target_type: the type/profile of the rendered result
            render_config: optional configuration, depends on the renderer that is called

        """

        registry = self.context.render_registry
        result = registry.render(
            item=item,
            source_type=source_type,
            target_type=target_type,
            render_config=render_config,
        )
        return result

    def assemble_render_pipeline(
        self,
        data_type: str,
        target_format: Union[str, Iterable[str]] = "string",
        filters: Union[None, str, Iterable[str], Mapping[str, str]] = None,
        use_pretty_print: bool = False,
    ) -> Operation:
        """
        Create a manifest describing a transformation that renders a value of the specified data type in the target format.

        NOTE: this is a preliminary endpoint, don't use in anger yet.

        If a list is provided as value for 'target_format', all items are tried until a 'render_value' operation is found that matches
        the value type of the source value, and the provided target format.

        Arguments:
            value: the value (or value id)
            target_format: the format into which to render the value
            filters: a list of filters to apply to the value before rendering it
            use_pretty_print: if True, use a 'pretty_print' operation instead of 'render_value'

        Returns:
            the manifest for the transformation
        """
        if data_type not in self.context.data_type_names:
            raise DataTypeUnknownException(data_type=data_type)

        if use_pretty_print:
            pretty_print_op_type: PrettyPrintOperationType = (
                self.context.operation_registry.get_operation_type("pretty_print")
            )  # type: ignore
            ops = pretty_print_op_type.get_target_types_for(data_type)
        else:
            render_op_type: (
                RenderValueOperationType
            ) = self.context.operation_registry.get_operation_type(
                # type: ignore
                "render_value"
            )  # type: ignore
            ops = render_op_type.get_render_operations_for_source_type(data_type)

        if isinstance(target_format, str):
            target_format = [target_format]

        match = None
        for _target_type in target_format:
            if _target_type not in ops.keys():
                continue
            match = ops[_target_type]
            break

        if not match:
            if not ops:
                msg = f"No render operations registered for source type '{data_type}'."
            else:
                msg = f"Registered target types for source type '{data_type}': {', '.join(ops.keys())}."
            raise Exception(
                f"No render operation for source type '{data_type}' to target type(s) registered: '{', '.join(target_format)}'. {msg}"
            )

        if filters:
            # filter_op_type: FilterOperationType = self._kiara.operation_registry.get_operation_type("filter")  # type: ignore
            endpoint = Manifest(
                module_type=match.module_type, module_config=match.module_config
            )
            extra_input_aliases = {"render_value.render_config": "render_config"}
            extra_output_aliases = {
                "render_value.render_value_result": "render_value_result"
            }
            pipeline_config = self.assemble_filter_pipeline_config(
                data_type=data_type,
                filters=filters,
                endpoint=endpoint,
                endpoint_input_field="value",
                endpoint_step_id="render_value",
                extra_input_aliases=extra_input_aliases,
                extra_output_aliases=extra_output_aliases,
            )
            manifest = Manifest(
                module_type="pipeline", module_config=pipeline_config.model_dump()
            )
            module = self.context.module_registry.create_module(manifest=manifest)
            operation = Operation.create_from_module(module, doc=pipeline_config.doc)
        else:
            operation = match

        return operation

    # ------------------------------------------------------------------------------------------------------------------
    # job-related methods
    def queue_manifest(
        self,
        manifest: Manifest,
        inputs: Union[None, Mapping[str, Any]] = None,
        **job_metadata: Any,
    ) -> uuid.UUID:
        """
        Queue a job using the provided manifest to describe the module and config that should be executed.

        You probably want to use 'queue_job' instead.

        Arguments:
            manifest: the manifest
            inputs: the job inputs (can be either references to values, or raw inputs

        Returns:
            a result value map instance
        """

        if self.context.runtime_config.runtime_profile == "dharpa":
            if not job_metadata:
                raise Exception(
                    "No job metadata provided. You need to provide a 'comment' argument when running your job."
                )

            if "comment" not in job_metadata.keys():
                raise KiaraException(msg="You need to provide a 'comment' for the job.")

            save_values = True
        else:
            save_values = False

        if inputs is None:
            inputs = {}

        job_config = self.context.job_registry.prepare_job_config(
            manifest=manifest, inputs=inputs
        )

        job_id = self.context.job_registry.execute_job(
            job_config=job_config, wait=False, auto_save_result=save_values
        )

        if job_metadata:
            self.context.metadata_registry.register_job_metadata_items(
                job_id=job_id, items=job_metadata
            )

        return job_id

    def run_manifest(
        self,
        manifest: Manifest,
        inputs: Union[None, Mapping[str, Any]] = None,
        **job_metadata: Any,
    ) -> ValueMapReadOnly:
        """
        Run a job using the provided manifest to describe the module and config that should be executed.

        You probably want to use 'run_job' instead.

        Arguments:
            manifest: the manifest
            inputs: the job inputs (can be either references to values, or raw inputs
            job_metadata: additional metadata to store with the job

        Returns:
            a result value map instance
        """
        job_id = self.queue_manifest(manifest=manifest, inputs=inputs, **job_metadata)
        return self.context.job_registry.retrieve_result(job_id=job_id)

    def queue_job(
        self,
        operation: Union[str, Path, Manifest, OperationInfo, JobDesc],
        inputs: Union[Mapping[str, Any], None],
        operation_config: Union[None, Mapping[str, Any]] = None,
        **job_metadata: Any,
    ) -> uuid.UUID:
        """
        Queue a job from a operation id, module_name (and config), or pipeline file, wait for the job to finish and retrieve the result.

        This is a convenience method that auto-detects what is meant by the 'operation' string input argument.

        If the 'operation' is a JobDesc instance, and that JobDesc instance has the 'save' attribute
        set, it will be ignored, so you'll have to store any results manually.

        Arguments:
            operation: a module name, operation id, or a path to a pipeline file (resolved in this order, until a match is found)..
            inputs: the operation inputs
            operation_config: the (optional) module config in case 'operation' is a module name
            job_metadata: additional metadata to store with the job

        Returns:
            the queued job id
        """

        if inputs is None:
            inputs = {}

        if isinstance(operation, str):
            if os.path.isfile(operation):
                job_path = Path(operation)
                if not job_path.is_file():
                    raise Exception(
                        f"Can't queue job from file '{job_path.as_posix()}': file does not exist/not a file."
                    )

                op_data = get_data_from_file(job_path)
                if isinstance(op_data, Mapping) and "operation" in op_data.keys():
                    try:
                        repl_dict: Dict[str, Any] = {
                            "this_dir": job_path.parent.as_posix()
                        }
                        job_data = replace_var_names_in_obj(
                            op_data, repl_dict=repl_dict
                        )
                        job_data["job_alias"] = job_path.stem
                        job_desc = JobDesc(**job_data)
                        _operation: Union[Manifest, str] = job_desc.get_operation(
                            kiara_api=self
                        )
                        if job_desc.inputs:
                            _inputs = dict(job_desc.inputs)
                            _inputs.update(inputs)
                            inputs = _inputs
                    except Exception as e:
                        raise KiaraException(
                            f"Failed to parse job description file: {operation}",
                            parent=e,
                        )
                else:
                    _operation = job_path.as_posix()
            else:
                _operation = operation
        elif isinstance(operation, Path):
            if not operation.is_file():
                raise Exception(
                    f"Can't queue job from file '{operation.as_posix()}': file does not exist/not a file."
                )
            _operation = operation.as_posix()
        elif isinstance(operation, OperationInfo):
            _operation = operation.operation
        elif isinstance(operation, JobDesc):
            if operation_config:
                raise KiaraException(
                    "Specifying 'operation_config' when operation is a job_desc is invalid."
                )
            _operation = operation.get_operation(kiara_api=self)
            if operation.inputs:
                _inputs = dict(operation.inputs)
                _inputs.update(inputs)
                inputs = _inputs
        else:
            _operation = operation

        if not isinstance(_operation, Manifest):
            manifest: Manifest = create_operation(
                module_or_operation=_operation,
                operation_config=operation_config,
                kiara=self.context,
            )
        else:
            manifest = _operation

        job_id = self.queue_manifest(manifest=manifest, inputs=inputs, **job_metadata)

        return job_id

    def run_job(
        self,
        operation: Union[str, Path, Manifest, OperationInfo, JobDesc],
        inputs: Union[None, Mapping[str, Any]] = None,
        operation_config: Union[None, Mapping[str, Any]] = None,
        **job_metadata: Any,
    ) -> ValueMapReadOnly:
        """
        Run a job from a operation id, module_name (and config), or pipeline file, wait for the job to finish and retrieve the result.

        This is a convenience method that auto-detects what is meant by the 'operation' string input argument.

        In general, try to avoid this method and use 'queue_job', 'get_job' and 'retrieve_job_result' manually instead,
        since this is a blocking operation.

        If the 'operation' is a JobDesc instance, and that JobDesc instance has the 'save' attribute
        set, it will be ignored, so you'll have to store any results manually.

        Arguments:
            operation: a module name, operation id, or a path to a pipeline file (resolved in this order, until a match is found)..
            inputs: the operation inputs
            operation_config: the (optional) module config in case 'operation' is a module name
            **job_metadata: additional metadata to store with the job

        Returns:
            the job result value map

        """
        if inputs is None:
            inputs = {}

        job_id = self.queue_job(
            operation=operation,
            inputs=inputs,
            operation_config=operation_config,
            **job_metadata,
        )
        return self.context.job_registry.retrieve_result(job_id=job_id)

    @tag("kiara_api")
    def get_job(self, job_id: Union[str, uuid.UUID]) -> "ActiveJob":
        """Retrieve the status of the job with the provided id."""
        if isinstance(job_id, str):
            job_id = uuid.UUID(job_id)

        job_status = self.context.job_registry.get_job(job_id=job_id)
        return job_status

    @tag("kiara_api")
    def get_job_result(self, job_id: Union[str, uuid.UUID]) -> ValueMapReadOnly:
        """Retrieve the result(s) of the specified job."""
        if isinstance(job_id, str):
            job_id = uuid.UUID(job_id)

        result = self.context.job_registry.retrieve_result(job_id=job_id)
        return result

    @tag("kiara_api")
    def list_all_job_record_ids(self) -> List[uuid.UUID]:
        """List all available job ids in this kiara context, ordered from newest to oldest, including internal jobs.

        This should be faster than `list_job_record_ids` with equivalent parameters, because no filtering
        needs to be done.
        """

        job_ids = self.context.job_registry.retrieve_all_job_record_ids()
        return job_ids

    @tag("kiara_api")
    def list_job_record_ids(self, **matcher_params: Any) -> List[uuid.UUID]:
        """List all available job ids in this kiara context, ordered from newest to oldest.

        You can look up the supported matcher parameter arguments via the [JobMatcher][kiara.models.module.jobs.JobMatcher] class. By default, this method for example
        does not return jobs marked as 'internal'.

        Arguments:
            matcher_params: additional parameters to pass to the job matcher

        Returns:
            a list of job ids, ordered from latest to earliest
        """

        job_ids = list(self.list_job_records(**matcher_params).keys())
        return job_ids

    @tag("kiara_api")
    def list_all_job_records(self) -> Mapping[uuid.UUID, "JobRecord"]:
        """List all available job records in this kiara context, ordered from newest to oldest, including internal jobs.

        This should be faster than `list_job_records` with equivalent parameters, because no filtering
        needs to be done.
        """

        job_records = self.context.job_registry.retrieve_all_job_records()
        return job_records

    @tag("kiara_api")
    def list_job_records(
        self, **matcher_params: Any
    ) -> Mapping[uuid.UUID, "JobRecord"]:
        """List all available job ids in this kiara context, ordered from newest to oldest.

        You can look up the supported matcher parameter arguments via the [JobMatcher][kiara.models.module.jobs.JobMatcher] class. By default, this method for example
        does not return jobs marked as 'internal'.

        You can look up the supported matcher parameter arguments via the [JobMatcher][kiara.models.module.jobs.JobMatcher] class.

        Arguments:
            matcher_params: additional parameters to pass to the job matcher

        Returns:
            a list of job details, ordered from latest to earliest

        """

        from kiara.models.module.jobs import JobMatcher

        matcher = JobMatcher(**matcher_params)
        job_records = self.context.job_registry.find_job_records(matcher=matcher)

        return job_records

    @tag("kiara_api")
    def get_job_record(self, job_id: Union[str, uuid.UUID]) -> Union["JobRecord", None]:
        """Retrieve the detailed job record for the specified job id.

        If no job can be found, 'None' is returned.
        """

        if isinstance(job_id, str):
            job_id = uuid.UUID(job_id)

        job_record = self.context.job_registry.get_job_record(job_id=job_id)
        return job_record

    def render_value(
        self,
        value: Union[str, uuid.UUID, Value],
        target_format: Union[str, Iterable[str]] = "string",
        filters: Union[None, Iterable[str], Mapping[str, str]] = None,
        render_config: Union[Mapping[str, str], None] = None,
        add_root_scenes: bool = True,
        use_pretty_print: bool = False,
    ) -> RenderValueResult:
        """
        Render a value in the specified target format.

        NOTE: this is a preliminary endpoint, don't use in anger yet.

        If a list is provided as value for 'target_format', all items are tried until a 'render_value' operation is found that matches
        the value type of the source value, and the provided target format.

        Arguments:
            value: the value (or value id)
            target_format: the format into which to render the value
            filters: an (optional) list of filters
            render_config: manifest specific render configuration
            add_root_scenes: add root scenes to the result
            use_pretty_print: use 'pretty_print' operation instead of 'render_value'

        Returns:
            the rendered value data, and any related scenes, if applicable
        """
        _value = self.get_value(value)
        try:
            render_operation: Union[None, Operation] = self.assemble_render_pipeline(
                data_type=_value.data_type_name,
                target_format=target_format,
                filters=filters,
                use_pretty_print=use_pretty_print,
            )

        except Exception as e:

            log_message(
                "create_render_pipeline.failure",
                source_type=_value.data_type_name,
                target_format=target_format,
                error=e,
            )

            if use_pretty_print:
                pretty_print_ops: PrettyPrintOperationType = self.context.operation_registry.get_operation_type("pretty_print")  # type: ignore
                if not isinstance(target_format, str):
                    raise NotImplementedError(
                        "Can't handle multiple target formats for 'render_value' yet."
                    )
                render_operation = (
                    pretty_print_ops.get_operation_for_render_combination(
                        source_type="any", target_type=target_format
                    )
                )
            else:
                render_ops: RenderValueOperationType = self.context.operation_registry.get_operation_type("render_value")  # type: ignore
                if not isinstance(target_format, str):
                    raise NotImplementedError(
                        "Can't handle multiple target formats for 'render_value' yet."
                    )
                render_operation = render_ops.get_render_operation(
                    source_type="any", target_type=target_format
                )

        if render_operation is None:
            raise Exception(
                f"Could not find render operation for value '{_value.value_id}', type: {_value.value_schema.type}"
            )

        if render_config and "render_config" in render_config.keys():
            # raise NotImplementedError()
            # TODO: is this necessary?
            render_config = render_config["render_config"]  # type: ignore
            # manifest_hash = render_config["manifest_hash"]
            # if manifest_hash != render_operation.manifest_hash:
            #     raise NotImplementedError(
            #         "Using a non-default render operation is not supported (yet)."
            #     )
            # render_config = render_config["render_config"]

        if render_config is None:
            render_config = {}
        else:
            render_config = dict(render_config)

        # render_type = render_config.pop("render_type", None)
        # if not render_type or render_type == "data":
        #     pass
        # elif render_type == "metadata":
        #     pass
        # elif render_type == "properties":
        #     pass
        # elif render_type == "lineage":
        #     pass

        result = render_operation.run(
            kiara=self.context,
            inputs={"value": _value, "render_config": render_config},
        )

        if use_pretty_print:
            render_result: Value = result["rendered_value"]
            value_render_data: RenderValueResult = render_result.data
        else:
            render_result = result["render_value_result"]

            if render_result.data_type_name != "render_value_result":
                raise Exception(
                    f"Invalid result type for render operation: {render_result.data_type_name}"
                )

            value_render_data = render_result.data  # type: ignore

        return value_render_data

    # ------------------------------------------------------------------------------------------------------------------
    # workflow-related methods
    # all of the workflow-related methods are provisional experiments, so don't rely on them to be availale long term

    def list_workflow_ids(self) -> List[uuid.UUID]:
        """List all available workflow ids.

        NOTE: this is a provisional endpoint, don't use in anger yet
        """
        return list(self.context.workflow_registry.all_workflow_ids)

    def list_workflow_alias_names(self) -> List[str]:
        """ "List all available workflow aliases.

        NOTE: this is a provisional endpoint, don't use in anger yet
        """
        return list(self.context.workflow_registry.workflow_aliases.keys())

    def get_workflow(
        self, workflow: Union[str, uuid.UUID], create_if_necessary: bool = True
    ) -> "Workflow":
        """Retrieve the workflow instance with the specified id or alias.

        NOTE: this is a provisional endpoint, don't use in anger yet
        """
        no_such_alias: bool = False
        workflow_id: Union[uuid.UUID, None] = None
        workflow_alias: Union[str, None] = None

        if isinstance(workflow, str):
            try:
                workflow_id = uuid.UUID(workflow)
            except Exception:
                workflow_alias = workflow
                try:
                    workflow_id = self.context.workflow_registry.get_workflow_id(
                        workflow_alias=workflow
                    )
                except NoSuchWorkflowException:
                    no_such_alias = True
        else:
            workflow_id = workflow

        if workflow_id is None:
            raise Exception(f"Can't retrieve workflow for: {workflow}")

        if workflow_id in self._workflow_cache.keys():
            return self._workflow_cache[workflow_id]

        if workflow_id is None and not create_if_necessary:
            if not no_such_alias:
                msg = f"No workflow with id '{workflow}' registered."
            else:
                msg = f"No workflow with alias '{workflow}' registered."

            raise NoSuchWorkflowException(workflow=workflow, msg=msg)

        if workflow_id:
            # workflow_metadata = self.context.workflow_registry.get_workflow_metadata(
            #     workflow=workflow_id
            # )
            workflow_obj = Workflow(kiara=self.context, workflow=workflow_id)
            self._workflow_cache[workflow_obj.workflow_id] = workflow_obj
        else:
            # means we need to create it
            workflow_obj = self.create_workflow(workflow_alias=workflow_alias)

        return workflow_obj

    def retrieve_workflow_info(
        self, workflow: Union[str, uuid.UUID, "Workflow"]
    ) -> WorkflowInfo:
        """Retrieve information about the specified workflow.

        NOTE: this is a provisional endpoint, don't use in anger yet
        """

        from kiara.interfaces.python_api.workflow import Workflow

        if isinstance(workflow, Workflow):
            _workflow: Workflow = workflow
        else:
            _workflow = self.get_workflow(workflow)

        return WorkflowInfo.create_from_workflow(workflow=_workflow)

    def list_workflows(self, **matcher_params) -> "WorkflowsMap":
        """List all available workflow sessions, indexed by their unique id."""

        from kiara.interfaces.python_api.models.doc import WorkflowsMap
        from kiara.interfaces.python_api.models.workflow import WorkflowMatcher

        workflows = {}

        matcher = WorkflowMatcher(**matcher_params)
        if matcher.has_alias:
            for (
                alias,
                workflow_id,
            ) in self.context.workflow_registry.workflow_aliases.items():

                workflow = self.get_workflow(workflow=workflow_id)
                workflows[workflow.workflow_id] = workflow
            return WorkflowsMap(root={str(k): v for k, v in workflows.items()})
        else:
            for workflow_id in self.context.workflow_registry.all_workflow_ids:
                workflow = self.get_workflow(workflow=workflow_id)
                workflows[workflow_id] = workflow
            return WorkflowsMap(root={str(k): v for k, v in workflows.items()})

    def list_workflow_aliases(self, **matcher_params) -> "WorkflowsMap":
        """List all available workflow sessions that have an alias, indexed by alias.

        NOTE: this is a provisional endpoint, don't use in anger yet
        """

        from kiara.interfaces.python_api.models.doc import WorkflowsMap
        from kiara.interfaces.python_api.workflow import Workflow

        if matcher_params:
            matcher_params["has_alias"] = True
            workflows = self.list_workflows(**matcher_params)
            result: Dict[str, Workflow] = {}
            for workflow in workflows.values():
                aliases = self.context.workflow_registry.get_aliases(
                    workflow_id=workflow.workflow_id
                )
                for a in aliases:
                    if a in result.keys():
                        raise Exception(
                            f"Duplicate workflow alias '{a}': this is most likely a bug."
                        )
                    result[a] = workflow
            result = {k: result[k] for k in sorted(result.keys())}
        else:
            # faster if not other matcher params
            all_aliases = self.context.workflow_registry.workflow_aliases
            result = {
                a: self.get_workflow(workflow=all_aliases[a])
                for a in sorted(all_aliases.keys())
            }

        return WorkflowsMap(root=result)

    def retrieve_workflows_info(self, **matcher_params: Any) -> WorkflowGroupInfo:
        """Get a map info instances for all available workflows, indexed by (stringified) workflow-id.

        NOTE: this is a provisional endpoint, don't use in anger yet
        """
        workflows = self.list_workflows(**matcher_params)

        workflow_infos = WorkflowGroupInfo.create_from_workflows(
            *workflows.values(),
            group_title=None,
            alias_map=self.context.workflow_registry.workflow_aliases,
        )
        return workflow_infos

    def retrieve_workflow_aliases_info(
        self, **matcher_params: Any
    ) -> WorkflowGroupInfo:
        """Get a map info instances for all available workflows, indexed by alias.

        NOTE: this is a provisional endpoint, don't use in anger yet
        """
        workflows = self.list_workflow_aliases(**matcher_params)
        workflow_infos = WorkflowGroupInfo.create_from_workflows(
            *workflows.values(),
            group_title=None,
            alias_map=self.context.workflow_registry.workflow_aliases,
        )
        return workflow_infos

    def create_workflow(
        self,
        workflow_alias: Union[None, str] = None,
        initial_pipeline: Union[None, Path, str, Mapping[str, Any]] = None,
        initial_inputs: Union[None, Mapping[str, Any]] = None,
        documentation: Union[Any, None] = None,
        save: bool = False,
        force_alias: bool = False,
    ) -> "Workflow":
        """Create a workflow instance.

        NOTE: this is a provisional endpoint, don't use in anger yet
        """

        from kiara.interfaces.python_api.workflow import Workflow

        if workflow_alias is not None:
            try:
                uuid.UUID(workflow_alias)
                raise Exception(
                    f"Can't create workflow, provided alias can't be a uuid: {workflow_alias}."
                )
            except Exception:
                pass

        workflow_id = ID_REGISTRY.generate()
        metadata = WorkflowMetadata(
            workflow_id=workflow_id, documentation=documentation
        )

        workflow_obj = Workflow(kiara=self.context, workflow=metadata)
        if workflow_alias:
            workflow_obj._pending_aliases.add(workflow_alias)

        if initial_pipeline:
            operation = self.get_operation(operation=initial_pipeline)
            if operation.module_type == "pipeline":
                pipeline_details: PipelineOperationDetails = operation.operation_details  # type: ignore
                workflow_obj.add_steps(*pipeline_details.pipeline_config.steps)
                input_aliases = pipeline_details.pipeline_config.input_aliases
                for k, v in input_aliases.items():
                    workflow_obj.set_input_alias(input_field=k, alias=v)
                output_aliases = pipeline_details.pipeline_config.output_aliases
                for k, v in output_aliases.items():
                    workflow_obj.set_output_alias(output_field=k, alias=v)
            else:
                raise NotImplementedError()

            workflow_obj.set_inputs(**operation.module.config.defaults)

        if initial_inputs:
            workflow_obj.set_inputs(**initial_inputs)

        self._workflow_cache[workflow_obj.workflow_id] = workflow_obj

        if save:
            if force_alias and workflow_alias:
                self.context.workflow_registry.unregister_alias(workflow_alias)
            workflow_obj.save()

        return workflow_obj

    def _repr_html_(self):

        info = self.get_context_info()
        r = info.create_renderable()
        mime_bundle = r._repr_mimebundle_(include=[], exclude=[])  # type: ignore
        return mime_bundle["text/html"]


# kiara\kiara\src\kiara\interfaces\python_api\batch.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)
import os
import uuid
from typing import TYPE_CHECKING, Any, Dict, List, Mapping, Union

from pydantic import (
    BaseModel,
    ConfigDict,
    Field,
    PrivateAttr,
    field_validator,
    model_validator,
)
from pydantic_core.core_schema import ValidationInfo

from kiara.context import Kiara
from kiara.interfaces.python_api.utils import create_save_config
from kiara.models.module.pipeline import PipelineConfig
from kiara.models.module.pipeline.controller import SinglePipelineBatchController
from kiara.models.module.pipeline.pipeline import Pipeline
from kiara.models.values.value import ValueMap
from kiara.utils.files import get_data_from_file

if TYPE_CHECKING:
    pass


class BatchOperation(BaseModel):
    model_config = ConfigDict(validate_assignment=True)

    @classmethod
    def from_file(
        cls,
        path: str,
        kiara: Union["Kiara", None] = None,
    ):

        data = get_data_from_file(path)
        pipeline_id = data.get("pipeline_name", None)
        if pipeline_id is None:
            name = os.path.basename(path)
            if name.endswith(".json"):
                name = name[0:-5]
            elif name.endswith(".yaml"):
                name = name[0:-5]
            data["pipeline_name"] = name

        alias = os.path.basename(path)

        return cls.from_config(alias=alias, data=data, kiara=kiara)

    @classmethod
    def from_config(
        cls,
        alias: str,
        data: Mapping[str, Any],
        kiara: Union["Kiara", None],
    ):

        data = dict(data)
        inputs = data.pop("inputs", {})
        save = data.pop("save", False)
        pipeline_id = data.pop("pipeline_name", None)
        if pipeline_id is None:
            pipeline_id = str(uuid.uuid4())

        if kiara is None:
            kiara = Kiara.instance()

        pipeline_config = PipelineConfig.from_config(
            pipeline_name=pipeline_id, data=data, kiara=kiara
        )

        result = cls(
            alias=alias,
            pipeline_config=pipeline_config,
            inputs=inputs,
            save_defaults=save,
        )
        result._kiara = kiara
        return result

    alias: str = Field(description="The batch name/alias.")
    pipeline_config: PipelineConfig = Field(
        description="The configuration of the underlying pipeline."
    )
    inputs: Dict[str, Any] = Field(
        description="The (base) inputs to use. Can be augmented before running the operation."
    )

    save_defaults: Dict[str, List[str]] = Field(
        description="Configuration which values to save, under which alias(es).",
        default_factory=dict,
        validate_default=True,
    )

    _kiara: Kiara = PrivateAttr(default=None)

    @model_validator(mode="before")
    @classmethod
    def add_alias(cls, values):

        if not values.get("alias", None):
            pc = values.get("pipeline_config", None)
            if not pc:
                raise ValueError("No pipeline config provided.")
            if isinstance(pc, PipelineConfig):
                alias = pc.pipeline_name
            else:
                alias = pc.get("pipeline_name", None)
            values["alias"] = alias

        return values

    @field_validator("save_defaults", mode="before")
    @classmethod
    def validate_save(cls, save_defaults: Dict[str, List[str]], info: ValidationInfo):

        alias = info.data["alias"]
        pipeline_config = info.data["pipeline_config"]
        return cls.create_save_aliases(
            save=save_defaults, alias=alias, pipeline_config=pipeline_config
        )

    @classmethod
    def create_save_aliases(
        cls,
        save: Union[bool, None, str, Mapping],
        alias: str,
        pipeline_config: PipelineConfig,
    ) -> Mapping[str, Any]:

        assert isinstance(pipeline_config, PipelineConfig)

        if save in [False, None]:
            save_new: Dict[str, Any] = {}
        elif save is True:
            field_names = pipeline_config.structure.pipeline_outputs_schema.keys()
            save_new = create_save_config(field_names=field_names, aliases=alias)
        elif isinstance(save, str):
            field_names = pipeline_config.structure.pipeline_outputs_schema.keys()
            save_new = create_save_config(field_names=field_names, aliases=save)
        elif isinstance(save, Mapping):
            save_new = dict(save)
        else:
            raise ValueError(
                f"Invalid type '{type(save)}' for 'save' attribute: must be None, bool, string or Mapping."
            )

        return save_new

    def run(
        self,
        inputs: Union[Mapping[str, Any], None] = None,
        save: Union[None, bool, str, Mapping[str, Any]] = None,
    ) -> ValueMap:

        pipeline = Pipeline(
            structure=self.pipeline_config.structure,
            kiara=self._kiara,
        )
        pipeline_controller = SinglePipelineBatchController(
            pipeline=pipeline, job_registry=self._kiara.job_registry
        )

        run_inputs = dict(self.inputs)
        if inputs:
            run_inputs.update(inputs)

        pipeline.set_pipeline_inputs(inputs=run_inputs)
        pipeline_controller.process_pipeline()

        result = self._kiara.data_registry.load_values(
            pipeline.get_current_pipeline_outputs()
        )

        if save is not None:
            if save is True:
                save = self.save_defaults
            else:
                save = self.__class__.create_save_aliases(
                    save=save, alias=self.alias, pipeline_config=self.pipeline_config
                )

            self._kiara.save_values(values=result, alias_map=save)

        return result


# kiara\kiara\src\kiara\interfaces\python_api\kiara_api.py
# -*- coding: utf-8 -*-
import uuid
from pathlib import Path

# BEGIN AUTO-GENERATED-IMPORTS
from typing import TYPE_CHECKING, Any, ClassVar, Iterable, List, Mapping, Union
from uuid import UUID

if TYPE_CHECKING:
    from kiara.interfaces.python_api.models.info import (
        DataTypeClassesInfo,
        DataTypeClassInfo,
        KiaraPluginInfo,
        KiaraPluginInfos,
        ModuleTypeInfo,
        ModuleTypesInfo,
        OperationGroupInfo,
        OperationInfo,
        ValueInfo,
        ValuesInfo,
    )
    from kiara.interfaces.python_api.value import StoreValueResult, StoreValuesResult
    from kiara.models.context import ContextInfo, ContextInfos
    from kiara.models.module.operation import Operation
    from kiara.models.values.value import Value, ValueMapReadOnly

# END AUTO-GENERATED-IMPORTS

if TYPE_CHECKING:
    from kiara.context import KiaraConfig
    from kiara.interfaces.python_api.models.archive import KiArchive
    from kiara.interfaces.python_api.models.doc import OperationsMap
    from kiara.interfaces.python_api.models.info import (
        KiaraPluginInfo,
        KiaraPluginInfos,
    )
    from kiara.interfaces.python_api.models.job import JobDesc
    from kiara.models.archives import KiArchiveInfo
    from kiara.models.metadata import KiaraMetadata
    from kiara.models.module.jobs import ActiveJob, JobRecord
    from kiara.models.module.manifest import Manifest


class KiaraAPI(object):
    """Kiara API for clients.

    This class wraps a [Kiara][kiara.context.kiara.Kiara] instance, and allows easy a access to tasks that are
    typically done by a frontend. The return types of each method are json seriable in most cases.

    The naming of the API endpoints follows a (loose-ish) convention:
    - list_*: return a list of ids or items, if items, filtering is supported
    - get_*: get specific instances of a type (operation, value, etc.)
    - retrieve_*: get augmented information about an instance or type of something. This usually implies that there is some overhead,
    so before you use this, make sure that there is not 'get_*' or 'list_*' endpoint that could give you what you need.

    Some methods of this class are copied (automatically) from the [BaseAPI][kiara.interfaces.python_api.base_api.BaseAPI] class, which is the actual implementation of the API.
    This is done for different reasons:
    - to keep the 'BaseAPI' class flexible, as it is used internally, so the `KiaraAPI` class can serve as a sort of 'stable' frontend, even if the underlying BaseAPI changes
    - to avoid having to write the same documentation / code twice
    - to be able to postpone the imports that are in the `base_api` module
    - to be able to add instrumentation, logging, etc. to the API calls later on

    Re-generating those copied methods can be done like so:

    ```
     kiara render --source-type base_api --target-type kiara_api item kiara_api template_file=kiara/src/kiara/interfaces/python_api/kiara_api.py target_file=kiara/src/kiara/interfaces/python_api/kiara_api.py
    ```

    All endpoints that have the 'tag' annotation `kiara_api` will then be copied.

    """

    _default_instance: ClassVar[Union["KiaraAPI", None]] = None

    @classmethod
    def instance(cls) -> "KiaraAPI":
        """Retrieve the default KiaraAPI instance.

        This is a convenience method to get a singleton KiaraAPI instance. If this is the first time this method is called, it loads the default *kiara* context. If this is called subsequently, it will return
        the same instance, so if you or some-one (or -thing) switched that context, this might not be the case.

        So make sure you understand the implications, and if in doubt, it might be safer to create your own `KiaraAPI` instance manually.
        """

        if cls._default_instance is not None:
            return cls._default_instance

        from kiara.utils.config import assemble_kiara_config

        config = assemble_kiara_config()

        api = KiaraAPI(kiara_config=config)
        cls._default_instance = api
        return api

    def __init__(self, kiara_config: Union["KiaraConfig", None] = None):

        from kiara.interfaces.python_api.base_api import BaseAPI

        self._api: BaseAPI = BaseAPI(kiara_config=kiara_config)

    def run_job(
        self,
        operation: Union[str, Path, "Manifest", "OperationInfo", "JobDesc"],
        inputs: Union[Mapping[str, Any], None] = None,
        comment: Union[str, None] = None,
        operation_config: Union[None, Mapping[str, Any]] = None,
    ) -> "ValueMapReadOnly":
        """
        Run a job from a operation id, module_name (and config), or pipeline file, wait for the job to finish and retrieve the result.

        This is a convenience method that auto-detects what is meant by the 'operation' string input argument.

        In general, try to avoid this method and use 'queue_job', 'get_job' and 'retrieve_job_result' manually instead,
        since this is a blocking operation.

        If the 'operation' is a JobDesc instance, and that JobDesc instance has the 'save' attribute
        set, it will be ignored, so you'll have to store any results manually.

        Arguments:
            operation: a module name, operation id, or a path to a pipeline file (resolved in this order, until a match is found)..
            inputs: the operation inputs
            comment: a (required) comment to attach to the job
            operation_config: the (optional) module config in case 'operation' is a module name

        Returns:
            the job result value map

        """

        if not comment and comment != "":
            from kiara.exceptions import KiaraException

            raise KiaraException(msg="Can't submit job: no comment provided.")

        if inputs is None:
            inputs = {}

        return self._api.run_job(
            operation=operation,
            inputs=inputs,
            operation_config=operation_config,
            comment=comment,
        )

    def queue_job(
        self,
        operation: Union[str, Path, "Manifest", "OperationInfo", "JobDesc"],
        inputs: Mapping[str, Any],
        comment: str,
        operation_config: Union[None, Mapping[str, Any]] = None,
    ) -> uuid.UUID:
        """
        Queue a job from a operation id, module_name (and config), or pipeline file, wait for the job to finish and retrieve the result.

        This is a convenience method that auto-detects what is meant by the 'operation' string input argument.

        If the 'operation' is a JobDesc instance, and that JobDesc instance has the 'save' attribute
        set, it will be ignored, so you'll have to store any results manually.

        Arguments:
            operation: a module name, operation id, or a path to a pipeline file (resolved in this order, until a match is found)..
            inputs: the operation inputs
            comment: a (required) comment to attach to the job
            operation_config: the (optional) module config in case 'operation' is a module name

        Returns:
            the queued job id
        """

        if not comment and comment != "":
            from kiara.exceptions import KiaraException

            raise KiaraException(msg="Can't submit job: no comment provided.")

        return self._api.queue_job(
            operation=operation,
            inputs=inputs,
            operation_config=operation_config,
            comment=comment,
        )

    def set_job_comment(
        self, job_id: Union[str, uuid.UUID], comment: str, force: bool = True
    ):
        """Set a comment for the specified job.

        Arguments:
            job_id: the job id
            comment: the comment to set
            force: whether to overwrite an existing comment
        """

        from kiara.models.metadata import CommentMetadata

        if isinstance(job_id, str):
            job_id = uuid.UUID(job_id)

        comment_metadata = CommentMetadata(comment=comment)
        items = {"comment": comment_metadata}

        self._api.context.metadata_registry.register_job_metadata_items(
            job_id=job_id, items=items
        )

    def get_job_comment(self, job_id: Union[str, uuid.UUID]) -> Union[str, None]:
        """Retrieve the comment for the specified job.

        Returns 'None' if the job_id does not exist, or the job does not have a comment attached to it.

        Arguments:
            job_id: the job id

        Returns:
            the comment as string, or None
        """

        from kiara.models.metadata import CommentMetadata

        if isinstance(job_id, str):
            job_id = uuid.UUID(job_id)

        metadata: Union[None, "KiaraMetadata"] = (
            self._api.context.metadata_registry.retrieve_job_metadata_item(
                job_id=job_id, key="comment"
            )
        )

        if not metadata:
            return None

        if not isinstance(metadata, CommentMetadata):
            from kiara.exceptions import KiaraException

            raise KiaraException(
                msg=f"Metadata item 'comment' for job '{job_id}' is not a comment."
            )
        return metadata.comment

    # BEGIN IMPORTED-ENDPOINTS
    def list_available_plugin_names(
        self, regex: str = r"^kiara[-_]plugin\..*"
    ) -> List[str]:
        r"""Get a list of all available plugins.

        Arguments:
            regex: an optional regex to indicate the plugin naming scheme (default: /$kiara[_-]plugin\..*/)

        Returns:
            a list of plugin names
        """

        result: List[str] = self._api.list_available_plugin_names(regex=regex)
        return result

    def retrieve_plugin_info(self, plugin_name: str) -> "KiaraPluginInfo":
        """Get information about a plugin.

        This contains information about included data-types, modules, operations, pipelines, as well as metadata
        about author(s), etc.

        Arguments:
            plugin_name: the name of the plugin

        Returns:
            a dictionary with information about the plugin
        """

        result: "KiaraPluginInfo" = self._api.retrieve_plugin_info(
            plugin_name=plugin_name
        )
        return result

    def retrieve_plugin_infos(
        self, plugin_name_regex: str = r"^kiara[-_]plugin\..*"
    ) -> "KiaraPluginInfos":
        """Get information about multiple plugins.

        This is just a convenience method to get information about multiple plugins at once.
        """

        result: "KiaraPluginInfos" = self._api.retrieve_plugin_infos(
            plugin_name_regex=plugin_name_regex
        )
        return result

    def get_context_info(self) -> "ContextInfo":
        """Retrieve information about the current kiara context.

        This contains information about the context, like its name/alias, the values & aliases it contains, and which archives are connected to it.
        """

        result: "ContextInfo" = self._api.get_context_info()
        return result

    def list_context_names(self) -> List[str]:
        """list the names of all available/registered contexts.

        NOTE: this functionality might be changed in the future, depending on requirements and feedback and
        whether we want to support single-file contexts in the future.
        """

        result: List[str] = self._api.list_context_names()
        return result

    def retrieve_context_infos(self) -> "ContextInfos":
        """Retrieve information about the available/registered contexts.

        NOTE: this functionality might be changed in the future, depending on requirements and feedback and whether we want to support single-file contexts in the future.
        """

        result: "ContextInfos" = self._api.retrieve_context_infos()
        return result

    def get_current_context_name(self) -> str:
        """Retrieve the name of the current context.

        NOTE: this functionality might be changed in the future, depending on requirements and feedback and whether we want to support single-file contexts in the future.
        """

        result: str = self._api.get_current_context_name()
        return result

    def set_active_context(self, context_name: str, create: bool = False):
        """Set the currently active context for this KiarAPI instance.

        NOTE: this functionality might be changed in the future, depending on requirements and feedback and whether we want to support single-file contexts in the future.
        """

        self._api.set_active_context(context_name=context_name, create=create)

    def list_data_type_names(self, include_profiles: bool = False) -> List[str]:
        """Get a list of all registered data types.

        Arguments:
            include_profiles: if True, also include the names of all registered data type profiles
        """

        result: List[str] = self._api.list_data_type_names(
            include_profiles=include_profiles
        )
        return result

    def retrieve_data_types_info(
        self,
        filter: Union[str, Iterable[str], None] = None,
        include_data_type_profiles: bool = False,
        python_package: Union[None, str] = None,
    ) -> "DataTypeClassesInfo":
        """Retrieve information about all data types.

        A data type is a Python class that inherits from [DataType[kiara.data_types.DataType], and it wraps a specific
        Python class that holds the actual data and provides metadata and convenience methods for managing the data internally. Data types are not directly used by users, but they are exposed in the input/output schemas of moudles and other data-related features.

        Arguments:
            filter: an optional string or (list of strings) the returned datatype ids have to match (all filters in the case of a list)
            include_data_type_profiles: if True, also include the names of all registered data type profiles
            python_package: if provided, only return data types that are defined in the given python package

        Returns:
            an object containing all information about all data types
        """

        result: "DataTypeClassesInfo" = self._api.retrieve_data_types_info(
            filter=filter,
            include_data_type_profiles=include_data_type_profiles,
            python_package=python_package,
        )
        return result

    def retrieve_data_type_info(self, data_type_name: str) -> "DataTypeClassInfo":
        """Retrieve information about a specific data type.

        Arguments:
            data_type: the registered name of the data type

        Returns:
            an object containing all information about a data type
        """

        result: "DataTypeClassInfo" = self._api.retrieve_data_type_info(
            data_type_name=data_type_name
        )
        return result

    def list_module_type_names(self) -> List[str]:
        """Get a list of all registered module types."""

        result: List[str] = self._api.list_module_type_names()
        return result

    def retrieve_module_types_info(
        self,
        filter: Union[None, str, Iterable[str]] = None,
        python_package: Union[str, None] = None,
    ) -> "ModuleTypesInfo":
        """Retrieve information for all available module types (or a filtered subset thereof).

        A module type is Python class that inherits from [KiaraModule][kiara.modules.KiaraModule], and is the basic
        building block for processing pipelines. Module types are not used directly by users, Operations are. Operations
         are instantiated modules (meaning: the module & some (optional) configuration).

        Arguments:
            filter: an optional string (or list of string) the returned module names have to match (all filters in case of list)
            python_package: an optional string, if provided, only modules from the specified python package are returned

        Returns:
            a mapping object containing module names as keys, and information about the modules as values
        """

        result: "ModuleTypesInfo" = self._api.retrieve_module_types_info(
            filter=filter, python_package=python_package
        )
        return result

    def retrieve_module_type_info(self, module_type: str) -> "ModuleTypeInfo":
        """Retrieve information about a specific module type.

        This can be used to retrieve information like module documentation and configuration options.

        Arguments:
            module_type: the registered name of the module

        Returns:
            an object containing all information about a module type
        """

        result: "ModuleTypeInfo" = self._api.retrieve_module_type_info(
            module_type=module_type
        )
        return result

    def list_operation_ids(
        self,
        filter: Union[str, None, Iterable[str]] = None,
        input_types: Union[str, Iterable[str], None] = None,
        output_types: Union[str, Iterable[str], None] = None,
        operation_types: Union[str, Iterable[str], None] = None,
        include_internal: bool = False,
        python_packages: Union[str, None, Iterable[str]] = None,
    ) -> List[str]:
        """Get a list of all operation ids that match the specified filter.

        Arguments:
            filter: the (optional) filter string(s), an operation must match all of them to be included in the result
            input_types: each operation must have at least one input that matches one of the specified types
            output_types: each operation must have at least one output that matches one of the specified types
            operation_types: only include operations of the specified type(s)
            include_internal: whether to include operations that are predominantly used internally in kiara.
            python_packages: only include operations that are contained in one of the provided python packages
        """

        result: List[str] = self._api.list_operation_ids(
            filter=filter,
            input_types=input_types,
            output_types=output_types,
            operation_types=operation_types,
            include_internal=include_internal,
            python_packages=python_packages,
        )
        return result

    def get_operation(
        self,
        operation: Union[Mapping[str, Any], str, "Path"],
        allow_external: Union[bool, None] = None,
    ) -> "Operation":
        """Return the operation instance with the specified id.

        The difference to the 'create_operation' endpoint is slight, in most cases you could use either of them, but this one is a bit more convenient in most cases, as it tries to do the right thing with whatever 'operation' argument you use it. The 'create_opearation' endpoint will always create a new 'Operation' instance, while this may or may not return a re-used one.

        This endpoint can be used to get information about a specific operation, like inputs/outputs scheman, documentation, etc.

        The order in which the operation argument is resolved:
        - if it's a string, and an existing, registered operation_id, the associated operation is returned
        - if it's a path to an existing file, the content of the file is loaded into a dict and depending on the content a pipeline module will be created, or a 'normal' manifest (if module_type is a key in the dict)

        Arguments:
            operation: the operation id, module_type_name, path to a file, or url
            allow_external: if True, allow loading operations from external sources (e.g. a URL), if 'None' is provided, the configured value in the runtime configuration is used.

        Returns:
            operation instance data
        """

        result: "Operation" = self._api.get_operation(
            operation=operation, allow_external=allow_external
        )
        return result

    def list_operations(
        self,
        filter: Union[str, None, Iterable[str]] = None,
        input_types: Union[str, Iterable[str], None] = None,
        output_types: Union[str, Iterable[str], None] = None,
        operation_types: Union[str, Iterable[str], None] = None,
        python_packages: Union[str, Iterable[str], None] = None,
        include_internal: bool = False,
    ) -> "OperationsMap":
        """List all available operations, optionally filter.

        Arguments:
            filter: the (optional) filter string(s), an operation must match all of them to be included in the result
            input_types: each operation must have at least one input that matches one of the specified types
            output_types: each operation must have at least one output that matches one of the specified types
            operation_types: only include operations of the specified type(s)
            include_internal: whether to include operations that are predominantly used internally in kiara.
            python_packages: only include operations that are contained in one of the provided python packages

        Returns:
            a dictionary with the operation id as key, and [kiara.models.module.operation.Operation] instance data as value
        """

        result: "OperationsMap" = self._api.list_operations(
            filter=filter,
            input_types=input_types,
            output_types=output_types,
            operation_types=operation_types,
            python_packages=python_packages,
            include_internal=include_internal,
        )
        return result

    def retrieve_operation_info(
        self, operation: str, allow_external: bool = False
    ) -> "OperationInfo":
        """Return the full information for the specified operation id.

        This is similar to the 'get_operation' method, but returns additional information. Only use this instead of
        'get_operation' if you need the additional info, as it's more expensive to get.

        Arguments:
            operation: the operation id

        Returns:
            augmented operation instance data
        """

        result: "OperationInfo" = self._api.retrieve_operation_info(
            operation=operation, allow_external=allow_external
        )
        return result

    def retrieve_operations_info(
        self,
        *filters: str,
        input_types: Union[str, Iterable[str], None] = None,
        output_types: Union[str, Iterable[str], None] = None,
        operation_types: Union[str, Iterable[str], None] = None,
        python_packages: Union[str, Iterable[str], None] = None,
        include_internal: bool = False,
    ) -> "OperationGroupInfo":
        """Retrieve information about the matching operations.

        This retrieves the same list of operations as [list_operations][kiara.interfaces.python_api.KiaraAPI.list_operations],
        but augments each result instance with additional information that might be useful in frontends.

        'OperationInfo' objects contains augmented information on top of what 'normal' [Operation][kiara.models.module.operation.Operation] objects
        hold, but they can take longer to create/resolve. If you don't need any
        of the augmented information, just use the [list_operations][kiara.interfaces.python_api.KiaraAPI.list_operations] method
        instead.

        Arguments:
            filters: the (optional) filter strings, an operation must match all of them to be included in the result
            include_internal: whether to include operations that are predominantly used internally in kiara.
            input_types: each operation must have at least one input that matches one of the specified types
            output_types: each operation must have at least one output that matches one of the specified types
            operation_types: only include operations of the specified type(s)
            include_internal: whether to include operations that are predominantly used internally in kiara.
            python_packages: only include operations that are contained in one of the provided python packages
        Returns:
            a wrapper object containing a dictionary of items with value_id as key, and [kiara.interfaces.python_api.models.info.OperationInfo] as value
        """

        result: "OperationGroupInfo" = self._api.retrieve_operations_info(
            *filters,
            input_types=input_types,
            output_types=output_types,
            operation_types=operation_types,
            python_packages=python_packages,
            include_internal=include_internal,
        )
        return result

    def list_all_value_ids(self) -> List["UUID"]:
        """List all value ids in the current context.

        This returns everything, even internal values. It should be faster than using
        `list_value_ids` with equivalent parameters, because no filtering has to happen.

        Returns:
            all value_ids in the current context, using every registered store
        """

        result: List["UUID"] = self._api.list_all_value_ids()
        return result

    def list_value_ids(self, **matcher_params: Any) -> List["UUID"]:
        """List all available value ids for this kiara context.

        By default, this also includes internal values.

        This method exists mainly so frontends can retrieve a list of all value_ids that exists on the backend without
        having to look up the details of each value (like [list_values][kiara.interfaces.python_api.KiaraAPI.list_values]
        does). This method can also be used with a matcher, but in this case the [list_values][kiara.interfaces.python_api.KiaraAPI.list_values]
        would be preferable in most cases, because it is called under the hood, and the performance advantage of not
        having to look up value details is gone.

        Arguments:
            matcher_params: the (optional) filter parameters, check the [ValueMatcher][kiara.models.values.matchers.ValueMatcher] class for available parameters and defaults

        Returns:
            a list of value ids
        """

        result: List["UUID"] = self._api.list_value_ids(**matcher_params)
        return result

    def list_all_values(self) -> "ValueMapReadOnly":
        """List all values in the current context, incl. internal ones.

        This should be faster than `list_values` with equivalent matcher params, because no
        filtering has to happen.
        """

        result: "ValueMapReadOnly" = self._api.list_all_values()
        return result

    def list_values(self, **matcher_params: Any) -> "ValueMapReadOnly":
        """List all available (relevant) values, optionally filter.

        Retrieve information about all values that are available in the current kiara context session (both stored and non-stored).

        Check the `ValueMatcher` class for available parameters and defaults, for example this excludes
        internal values by default.

        Arguments:
            matcher_params: the (optional) filter parameters, check the [ValueMatcher][kiara.models.values.matchers.ValueMatcher] class for available parameters

        Returns:
            a dictionary with value_id as key, and [kiara.models.values.value.Value] as value
        """

        result: "ValueMapReadOnly" = self._api.list_values(**matcher_params)
        return result

    def get_value(self, value: Union[str, "Value", "UUID", "Path"]) -> "Value":
        """Retrieve a value instance with the specified id or alias.

        Basically a convenience method to convert any possible Python type into
        a 'Value' instance. Raises an exception if no value could be found.

        Arguments:
            value: a value id, alias or object that has a 'value_id' attribute.

        Returns:
            the Value instance
        """

        result: "Value" = self._api.get_value(value=value)
        return result

    def get_values(self, **values: Union[str, "Value", "UUID"]) -> "ValueMapReadOnly":
        """Retrieve Value instances for the specified value ids or aliases.

        This is a convenience method to get fully 'hydrated' `Value` objects from references to them.

        Arguments:
            values: a dictionary with value ids or aliases as keys, and value instances as values

        Returns:
            a mapping with value_id as key, and [kiara.models.values.value.Value] as value
        """

        result: "ValueMapReadOnly" = self._api.get_values(**values)
        return result

    def retrieve_value_info(
        self, value: Union[str, "UUID", "Value", "Path"]
    ) -> "ValueInfo":
        """Retrieve an info object for a value.

        Companion method to 'get_value', 'ValueInfo' objects contains augmented information on top of what 'normal' [Value][kiara.models.values.value.Value] objects
        hold (like resolved properties for example), but they can take longer to create/resolve. If you don't need any
        of the augmented information, just use the [get_value][kiara.interfaces.python_api.KiaraAPI.get_value] method
        instead.

        Arguments:
            value: a value id, alias or object that has a 'value_id' attribute.

        Returns:
            the ValueInfo instance
        """

        result: "ValueInfo" = self._api.retrieve_value_info(value=value)
        return result

    def retrieve_values_info(self, **matcher_params: Any) -> "ValuesInfo":
        """Retrieve information about the matching values.

        This retrieves the same list of values as [list_values][kiara.interfaces.python_api.KiaraAPI.list_values],
        but augments each result value instance with additional information that might be useful in frontends.

        'ValueInfo' objects contains augmented information on top of what 'normal' [Value][kiara.models.values.value.Value] objects
        hold (like resolved properties for example), but they can take longer to create/resolve. If you don't need any
        of the augmented information, just use the [list_values][kiara.interfaces.python_api.KiaraAPI.list_values] method
        instead.

        Arguments:
            matcher_params: the (optional) filter parameters, check the [ValueMatcher][kiara.models.values.matchers.ValueMatcher] class for available parameters

        Returns:
            a wrapper object containing the items as dictionary with value_id as key, and [kiara.interfaces.python_api.models.values.ValueInfo] as value
        """

        result: "ValuesInfo" = self._api.retrieve_values_info(**matcher_params)
        return result

    def list_alias_names(self, **matcher_params: Any) -> List[str]:
        """List all available alias keys.

        This method exists mainly so frontend can retrieve a list of all value_ids that exists on the backend without
        having to look up the details of each value (like [list_aliases][kiara.interfaces.python_api.KiaraAPI.list_aliases]
        does). This method can also be used with a matcher, but in this case the [list_aliases][kiara.interfaces.python_api.KiaraAPI.list_aliases]
        would be preferrable in most cases, because it is called under the hood, and the performance advantage of not
        having to look up value details is gone.

        Arguments:
            matcher_params: the (optional) filter parameters, check the [ValueMatcher][kiara.models.values.matchers.ValueMatcher] class for available parameters

        Returns:
            a list of value ids
        """

        result: List[str] = self._api.list_alias_names(**matcher_params)
        return result

    def list_aliases(self, **matcher_params: Any) -> "ValueMapReadOnly":
        """List all available values that have an alias assigned, optionally filter.

        Arguments:
            matcher_params: the (optional) filter parameters, check the [ValueMatcher][kiara.models.values.matchers.ValueMatcher] class for available parameters

        Returns:
            a dictionary with value_id as key, and [kiara.models.values.value.Value] as value
        """

        result: "ValueMapReadOnly" = self._api.list_aliases(**matcher_params)
        return result

    def retrieve_aliases_info(self, **matcher_params: Any) -> "ValuesInfo":
        """Retrieve information about the matching values.

        This retrieves the same list of values as [list_values][kiara.interfaces.python_api.KiaraAPI.list_values],
        but augments each result value instance with additional information that might be useful in frontends.

        'ValueInfo' objects contains augmented information on top of what 'normal' [Value][kiara.models.values.value.Value] objects
        hold (like resolved properties for example), but they can take longer to create/resolve. If you don't need any
        of the augmented information, just use the [get_value][kiara.interfaces.python_api.KiaraAPI.list_aliases] method
        instead.

        Arguments:
            matcher_params: the (optional) filter parameters, check the [ValueMatcher][kiara.models.values.matchers.ValueMatcher] class for available parameters

        Returns:
            a dictionary with a value alias as key, and [kiara.interfaces.python_api.models.values.ValueInfo] as value
        """

        result: "ValuesInfo" = self._api.retrieve_aliases_info(**matcher_params)
        return result

    def store_value(
        self,
        value: Union[str, "UUID", "Value"],
        alias: Union[str, Iterable[str], None],
        allow_overwrite: bool = True,
        store: Union[str, None] = None,
        store_related_metadata: bool = True,
        set_as_store_default: bool = False,
    ) -> "StoreValueResult":
        """Store the specified value in a value store.

        If you provide values for the 'data_store' and/or 'alias_store' other than 'default', you need
        to make sure those stores are registered with the current context. In most cases, the 'export' endpoint (to be done) will probably be an easier way to export values, which I suspect will
        be the main use-case for this endpoint if any of the 'store' arguments where needed. Otherwise, this endpoint is useful to persist values for use in later seperate sessions.

        This method does not raise an error if the storing of the value fails, so you have to investigate the
        'StoreValueResult' instance that is returned to see if the storing was successful.

        Arguments:
            value: the value (or a reference to it)
            alias: (Optional) one or several aliases for the value
            allow_overwrite: whether to allow overwriting existing aliases
            store: in case data and alias store names are the same, you can use this, if you specify one or both of the others, this will be overwritten
            store_related_metadata: whether to store related metadata (comments, etc.) in the same store as the data
            set_as_store_default: whether to set the specified store as the default store for the value
        """

        result: "StoreValueResult" = self._api.store_value(
            value=value,
            alias=alias,
            allow_overwrite=allow_overwrite,
            store=store,
            store_related_metadata=store_related_metadata,
            set_as_store_default=set_as_store_default,
        )
        return result

    def store_values(
        self,
        values: Union[
            str,
            "Value",
            "UUID",
            Mapping[str, Union[str, "UUID", "Value"]],
            Iterable[Union[str, "UUID", "Value"]],
        ],
        alias_map: Union[Mapping[str, Iterable[str]], bool, str] = False,
        allow_alias_overwrite: bool = True,
        store: Union[str, None] = None,
        store_related_metadata: bool = True,
    ) -> "StoreValuesResult":
        """Store multiple values into the (default) kiara value store.

        Convenience method to store multiple values. In a lot of cases you can be more flexible if you
        loop over the values on the frontend side, and call the 'store_value' method for each value. But this might be meaningfully slower. This method has the potential to be optimized in the future.

        You have several options to provide the values and aliases you want to store:

        - as a string, in which case the item will be wrapped in a list (see non-mapping iterable below)

        - as a (non-mapping) iterable of value items, those can either be:

          - a value id (as string or uuid)
          - a value alias (as string)
          - a value instance

        If you do that, then the 'alias_map' argument can either be:

          - 'False', in which case no aliases will be registered
          - 'True', in which case all items in the 'values' iterable must be a valid alias, and the alias will be copied without change to the new store
          - a 'string', in which case all items in the 'values' iterable also must be a valid alias, and the alias that will be registered in the new store will use the string value as prefix (e.g. 'alias_map' = 'experiment1' and 'values' = ['a', 'b'] will result in the aliases 'experiment1.a' and 'experiment1.b')
          - a map that uses the stringi-fied uuid of the value that should get one or several aliases as key, and a list of aliases as values

        You can also use a mapping type (like a dict) for the 'values' argument. In this case, the key is a string, and the value can be:

          - a value id (as string or uuid)
          - a value alias (as string)
          - a value instance

        In this case, the meaning of the 'alias_map' is as follows:

          - 'False': no aliases will be registered
          - 'True': the key in the 'values' argument will be used as alias
          - a string: all keys from the 'values' map will be used as alias, prefixed with the value of 'alias_map'
          - another map, with a string referring to the key in the 'values' argument as key, and a list of aliases (strings) as value

        Sorry, this is all a bit convoluted, but it's the only way I could think of to make this work for all the requirements I had. In most keases, you'll only have to use 'True' or 'False' here, hopefully.

        This method does not raise an error if the storing of the value fails, so you have to investigate the
        'StoreValuesResult' instance that is returned to see if the storing was successful.

        Arguments:
            values: an iterable/map of value keys/values
            alias_map: a map of value keys aliases
            allow_alias_overwrite: whether to allow overwriting existing aliases
            store: in case data and alias store names are the same, you can use this, if you specify one or both of the others, this will be overwritten
            data_store: the registered name (or archive id as string) of the store to write the data
            alias_store: the registered name (or archive id as string) of the store to persist the alias(es)/value_id mapping

        Returns:
            an object outlining which values (identified by the specified value key or an enumerated index) where stored and how
        """

        result: "StoreValuesResult" = self._api.store_values(
            values=values,
            alias_map=alias_map,
            allow_alias_overwrite=allow_alias_overwrite,
            store=store,
            store_related_metadata=store_related_metadata,
        )
        return result

    def import_values(
        self,
        source_archive: Union[str, "Path"],
        values: Union[
            str,
            Mapping[str, Union[str, "UUID", "Value"]],
            Iterable[Union[str, "UUID", "Value"]],
        ],
        alias_map: Union[Mapping[str, Iterable[str]], bool, str] = False,
        allow_alias_overwrite: bool = True,
        source_registered_name: Union[str, None] = None,
    ) -> "StoreValuesResult":
        """Import one or several values from an external kiara archive, along with their aliases (optional).

        For the 'values' & 'alias_map' arguments, see the 'store_values' endpoint, as they will be forwarded to that endpoint as is,
        and there are several ways to use them which is information I don't want to duplicate.

        If you provide aliases in the 'values' parameter, the aliases must be available in the external archive.

        Currently, this only works with an external archive file, not with an archive that is registered into the context.
        This will probably be added later on, let me know if there is demand, then I'll prioritize.

        This method does not raise an error if the storing of the value fails, so you have to investigate the
        'StoreValuesResult' instance that is returned to see if the storing was successful.

        # NOTE: this is a preliminary endpoint, and might be changed in the future. If you have a use-case for this, please let me know.

        Arguments:
            source_archive: the name of the archive to store the values into
            values: an iterable/map of value keys/values
            alias_map: a map of value keys aliases
            allow_alias_overwrite: whether to allow overwriting existing aliases
            source_registered_name: the name to register the archive under in the context
        """

        result: "StoreValuesResult" = self._api.import_values(
            source_archive=source_archive,
            values=values,
            alias_map=alias_map,
            allow_alias_overwrite=allow_alias_overwrite,
            source_registered_name=source_registered_name,
        )
        return result

    def export_values(
        self,
        target_archive: Union[str, "Path"],
        values: Union[
            str,
            Mapping[str, Union[str, "UUID", "Value"]],
            Iterable[Union[str, "UUID", "Value"]],
        ],
        alias_map: Union[Mapping[str, Iterable[str]], bool, str] = False,
        allow_alias_overwrite: bool = True,
        target_registered_name: Union[str, None] = None,
        append: bool = False,
        target_store_params: Union[None, Mapping[str, Any]] = None,
        export_related_metadata: bool = True,
        additional_archive_metadata: Union[None, Mapping[str, Any]] = None,
    ) -> "StoreValuesResult":
        """Store one or several values along with (optional) aliases into a kiara archive.

        For the 'values' & 'alias_map' arguments, see the 'store_values' endpoint, as they will be forwarded to that endpoint as is,
        and there are several ways to use them which is information I don't want to duplicate.

        Currently, this only works with an external archive file, not with an archive that is registered into the context.
        This will probably be added later on, let me know if there is demand, then I'll prioritize.

        'target_store_params' is used if the archive does not exist yet. The one supported value for the 'target_store_params' argument currently is 'compression', which can be one of:

        - zstd: zstd compression (default) -- fairly fast, and good compression
        - none: no compression
        - LZMA: LZMA compression -- very slow, but very good compression
        - LZ4: LZ4 compression -- very fast, but not as good compression as zstd

        This method does not raise an error if the storing of the value fails, so you have to investigate the
        'StoreValuesResult' instance that is returned to see if the storing was successful.

        # NOTE: this is a preliminary endpoint, and might be changed in the future. If you have a use-case for this, please let me know.

        Arguments:
            target_store: the name of the archive to store the values into
            values: an iterable/map of value keys/values
            alias_map: a map of value keys aliases
            allow_alias_overwrite: whether to allow overwriting existing aliases
            target_registered_name: the name to register the archive under in the context
            append: whether to append to an existing archive
            target_store_params: additional parameters to pass to the 'create_kiarchive' method if the file does not exist yet
            export_related_metadata: whether to export related metadata (e.g. job info, comments, ..) to the new archive or not
            additional_archive_metadata: (optional) additional metadata to add to the archive
        """

        result: "StoreValuesResult" = self._api.export_values(
            target_archive=target_archive,
            values=values,
            alias_map=alias_map,
            allow_alias_overwrite=allow_alias_overwrite,
            target_registered_name=target_registered_name,
            append=append,
            target_store_params=target_store_params,
            export_related_metadata=export_related_metadata,
            additional_archive_metadata=additional_archive_metadata,
        )
        return result

    def retrieve_archive_info(
        self, archive: Union[str, "KiArchive"]
    ) -> "KiArchiveInfo":
        """Retrieve information about an archive at the specified local path

        Currently, this only works with an external archive file, not with an archive that is registered into the context.
        This will probably be added later on, let me know if there is demand, then I'll prioritize.

        # NOTE: this is a preliminary endpoint, and might be changed in the future. If you have a use-case for this, please let me know.

        Arguments:
            archive: the uri of the archive (file path)

        Returns:
            a [KiarchiveInfo][kiara.interfaces.python_api.models.archive.KiarchiveInfo] instance, containing details about the archive
        """

        result: "KiArchiveInfo" = self._api.retrieve_archive_info(archive=archive)
        return result

    def export_archive(
        self,
        target_archive: Union[str, "Path"],
        target_registered_name: Union[str, None] = None,
        append: bool = False,
        no_aliases: bool = False,
        target_store_params: Union[None, Mapping[str, Any]] = None,
    ) -> "StoreValuesResult":
        """Export all data from the default store in your context into the specfied archive path.

        The target archives will be registered into the context, either using the provided registered_name, or the name
        will be auto-determined from the archive metadata.

        Currently, this only works with an external archive file, not with an archive that is already registered into the context.
        This will be added later on.

        Also, currently you can only export all data from the default store, there is no way to select only a sub-set. This will
        also be supported later on.

        The one supported value for the 'target_store_params' argument currently is 'compression', which can be one of:

        - zstd: zstd compression (default) -- fairly fast, and good compression
        - none: no compression
        - LZMA: LZMA compression -- very slow, but very good compression
        - LZ4: LZ4 compression -- very fast, but not as good compression as zstd

        This method does not raise an error if the storing of the value fails, so you have to investigate the
        'StoreValuesResult' instance that is returned to see if the storing was successful

        Arguments:
            target_archive: the registered_name or uri of the target archive
            target_registered_name: the name/alias that the archive should be registered in the context (if necessary)
            append: whether to append to an existing archive or error out if the target already exists
            no_aliases: whether to skip importing aliases
            target_store_params: additional parameters to pass to the 'create_kiarchive' method if the target file does not exist yet

        Returns:
            an object outlining which values (identified by the specified value key or an enumerated index) where stored and how
        """

        result: "StoreValuesResult" = self._api.export_archive(
            target_archive=target_archive,
            target_registered_name=target_registered_name,
            append=append,
            no_aliases=no_aliases,
            target_store_params=target_store_params,
        )
        return result

    def import_archive(
        self,
        source_archive: Union[str, "Path"],
        source_registered_name: Union[str, None] = None,
        no_aliases: bool = False,
    ) -> "StoreValuesResult":
        """Import all data from the specified archive into the current contexts default data & alias store.

        The source target will be registered into the context, either using the provided registered_name, otherwise the name
        will be auto-determined from the archive metadata.

        Currently, this only works with an external archive file, not with an archive that is registered into the context.
        This will be added later on.

        Also, currently you can only import all data into the default store, there is no way to select only a sub-set. This will
        also be supported later on.

        This method does not raise an error if the storing of the value fails, so you have to investigate the
        'StoreValuesResult' instance that is returned to see if the storing was successful

        Arguments:
            source_archive: the registered_name or uri of the source archive
            source_registered_name: the name/alias that the archive should be registered in the context (if necessary)
            no_aliases: whether to skip importing aliases

        Returns:
            an object outlining which values (identified by the specified value key or an enumerated index) where stored and how
        """

        result: "StoreValuesResult" = self._api.import_archive(
            source_archive=source_archive,
            source_registered_name=source_registered_name,
            no_aliases=no_aliases,
        )
        return result

    def get_job(self, job_id: Union[str, "UUID"]) -> "ActiveJob":
        """Retrieve the status of the job with the provided id."""

        result: "ActiveJob" = self._api.get_job(job_id=job_id)
        return result

    def get_job_result(self, job_id: Union[str, "UUID"]) -> "ValueMapReadOnly":
        """Retrieve the result(s) of the specified job."""

        result: "ValueMapReadOnly" = self._api.get_job_result(job_id=job_id)
        return result

    def list_all_job_record_ids(self) -> List["UUID"]:
        """List all available job ids in this kiara context, ordered from newest to oldest, including internal jobs.

        This should be faster than `list_job_record_ids` with equivalent parameters, because no filtering
        needs to be done.
        """

        result: List["UUID"] = self._api.list_all_job_record_ids()
        return result

    def list_job_record_ids(self, **matcher_params: Any) -> List["UUID"]:
        """List all available job ids in this kiara context, ordered from newest to oldest.

        You can look up the supported matcher parameter arguments via the [JobMatcher][kiara.models.module.jobs.JobMatcher] class. By default, this method for example
        does not return jobs marked as 'internal'.

        Arguments:
            matcher_params: additional parameters to pass to the job matcher

        Returns:
            a list of job ids, ordered from latest to earliest
        """

        result: List["UUID"] = self._api.list_job_record_ids(**matcher_params)
        return result

    def list_all_job_records(self) -> Mapping["UUID", "JobRecord"]:
        """List all available job records in this kiara context, ordered from newest to oldest, including internal jobs.

        This should be faster than `list_job_records` with equivalent parameters, because no filtering
        needs to be done.
        """

        result: Mapping["UUID", "JobRecord"] = self._api.list_all_job_records()
        return result

    def list_job_records(self, **matcher_params: Any) -> Mapping["UUID", "JobRecord"]:
        """List all available job ids in this kiara context, ordered from newest to oldest.

        You can look up the supported matcher parameter arguments via the [JobMatcher][kiara.models.module.jobs.JobMatcher] class. By default, this method for example
        does not return jobs marked as 'internal'.

        You can look up the supported matcher parameter arguments via the [JobMatcher][kiara.models.module.jobs.JobMatcher] class.

        Arguments:
            matcher_params: additional parameters to pass to the job matcher

        Returns:
            a list of job details, ordered from latest to earliest
        """

        result: Mapping["UUID", "JobRecord"] = self._api.list_job_records(
            **matcher_params
        )
        return result

    def get_job_record(self, job_id: Union[str, "UUID"]) -> Union["JobRecord", None]:
        """Retrieve the detailed job record for the specified job id.

        If no job can be found, 'None' is returned.
        """

        result: Union["JobRecord", None] = self._api.get_job_record(job_id=job_id)
        return result

    # END IMPORTED-ENDPOINTS


# kiara\kiara\src\kiara\interfaces\python_api\operation.py
# -*- coding: utf-8 -*-
# import os
# import uuid
# from rich.console import Group, RenderableType
# from rich.markdown import Markdown
# from typing import TYPE_CHECKING, Any, Dict, List, Mapping, Union
#
# from kiara.exceptions import FailedJobException, InvalidValuesException
# from kiara.interfaces.python_api.utils import create_save_config
#
# # from kiara.interfaces.python_api import KiaraContext
# from kiara.interfaces.python_api.value import StoreValuesResult
# from kiara.models.module.jobs import JobConfig, JobStatus
# from kiara.models.module.operation import Operation
# from kiara.models.values.value import ValueMap
# from kiara.utils.files import get_data_from_file
# from kiara.utils.operations import create_operation
# from kiara.utils.output import (
#     create_table_from_field_schemas,
#     create_value_map_status_renderable,
# )
#
# if TYPE_CHECKING:
#     from kiara.context import Kiara
#
#
# class KiaraOperation(object):
#     """A class to provide a convenience API around executing a specific operation."""
#
#     def __init__(
#         self,
#         kiara: "Kiara",
#         operation_name: str,
#         operation_config: Union[Mapping[str, Any], None] = None,
#     ):
#
#         self._kiara: Kiara = kiara
#         self._operation_name: str = operation_name
#         if operation_config is None:
#             operation_config = {}
#         else:
#             operation_config = dict(operation_config)
#         self._operation_config: Dict[str, Any] = operation_config
#
#         self._inputs_raw: Dict[str, Any] = {}
#
#         self._operation: Union[Operation, None] = None
#         self._inputs: Union[ValueMap, None] = None
#
#         self._job_config: Union[JobConfig, None] = None
#
#         self._queued_jobs: Dict[uuid.UUID, Dict[str, Any]] = {}
#         self._last_job: Union[uuid.UUID, None] = None
#         self._results: Dict[uuid.UUID, ValueMap] = {}
#
#         self._defaults: Union[Dict[str, Any], None] = None
#
#     def validate(self):
#
#         self.job_config
#
#     def _invalidate(self):
#
#         self._job_config = None
#         # self._defaults = None
#
#     @property
#     def operation_inputs(self) -> ValueMap:
#
#         if self._inputs is not None:
#             return self._inputs
#
#         self._invalidate()
#         if self._defaults is not None:
#             data = dict(self._defaults)
#         else:
#             data = {}
#         data.update(self._inputs_raw)
#
#         self._inputs = self._kiara.data_registry.create_valuemap(
#             data, self.operation.inputs_schema
#         )
#         return self._inputs
#
#     def set_input(self, field: Union[str, None], value: Union[Any, None] = None):
#
#         if field is None:
#             if value is None:
#                 self._inputs_raw.clear()
#                 self._invalidate()
#                 return
#             else:
#                 if not isinstance(value, Mapping):
#                     raise Exception(
#                         "Can't set inputs dictionary (if no key is provided, value must be 'None' or of type 'Mapping')."
#                     )
#
#                 self._inputs_raw.clear()
#                 self.set_inputs(**value)
#                 self._invalidate()
#                 return
#         else:
#             old = self._inputs_raw.get(field, None)
#             self._inputs_raw[field] = value
#             if old != value:
#                 self._invalidate()
#             return
#
#     def set_inputs(self, **inputs: Any):
#
#         changed = False
#         for k, v in inputs.items():
#             old = self._inputs_raw.get(k, None)
#             self._inputs_raw[k] = v
#             if old != v:
#                 changed = True
#
#         if changed:
#             self._invalidate()
#
#         return
#
#     def run(self, **inputs: Any) -> ValueMap:
#
#         job_id = self.queue_job(**inputs)
#         results = self.retrieve_result(job_id=job_id)
#         return results
#
#     @property
#     def operation_name(self) -> str:
#         return self._operation_name
#
#     @operation_name.setter
#     def operation_name(self, operation_name: str):
#         self._operation_name = operation_name
#         self._operation = None
#
#     @property
#     def operation_config(self) -> Mapping[str, Any]:
#         return self._operation_config
#
#     def set_operation_config_value(
#         self, key: Union[str, None], value: Union[Any, None] = None
#     ) -> Mapping[str, Any]:
#
#         if key is None:
#             if value is None:
#                 old = bool(self._operation_config)
#                 self._operation_config.clear()
#                 if old:
#                     self._operation = None
#                 return self._operation_config
#             else:
#                 try:
#                     old_conf = self._operation_config
#                     self._operation_config = dict(value)
#                     if old_conf != self._operation_config:
#                         self._operation = None
#                     return self._operation_config
#                 except Exception as e:
#                     raise Exception(
#                         f"Can't set configuration value dictionary (if no key is provided, value must be 'None' or of type 'Mapping'): {e}"
#                     )
#
#         self._operation_config[key] = value
#         self._invalidate()
#         return self._operation_config
#
#     @property
#     def operation(self) -> "Operation":
#
#         if self._operation is not None:
#             return self._operation
#
#         self._invalidate()
#         self._defaults = None
#
#         operation = create_operation(
#             module_or_operation=self._operation_name,
#             operation_config=self.operation_config,
#             kiara=self._kiara,
#         )
#
#         if os.path.isfile(self._operation_name):
#             data = get_data_from_file(self._operation_name)
#             self._defaults = data.get("inputs", {})
#
#         self._operation = operation
#         return self._operation
#
#     @property
#     def job_config(self) -> JobConfig:
#
#         if self._job_config is not None:
#             return self._job_config
#
#         self._job_config = self.operation.prepare_job_config(
#             kiara=self._kiara, inputs=self.operation_inputs
#         )
#         return self._job_config
#
#     def queue_job(self, **inputs) -> uuid.UUID:
#
#         if inputs:
#             self.set_inputs(**inputs)
#
#         job_config = self.job_config
#         operation = self.operation
#         op_inputs = self.operation_inputs
#
#         job_id = self._kiara.job_registry.execute_job(job_config=job_config, wait=False)
#
#         self._queued_jobs[job_id] = {
#             "job_config": job_config,
#             "operation": operation,
#             "inputs": op_inputs,
#         }
#         self._last_job = job_id
#         return job_id
#
#     def retrieve_result(self, job_id: Union[uuid.UUID, None] = None) -> ValueMap:
#
#         if job_id in self._results.keys():
#             assert job_id is not None
#             return self._results[job_id]
#
#         if job_id is None:
#             job_id = self._last_job
#
#         if job_id is None:
#             raise Exception("No job queued (yet).")
#
#         operation: Operation = self._queued_jobs[job_id]["operation"]  # type: ignore
#
#         status = self._kiara.job_registry.get_job_status(job_id=job_id)
#
#         if status == JobStatus.FAILED:
#             job = self._kiara.job_registry.get_active_job(job_id=job_id)
#             raise FailedJobException(job=job)
#
#         outputs = self._kiara.job_registry.retrieve_result(job_id)
#         outputs = operation.process_job_outputs(outputs=outputs)
#         self._results[job_id] = outputs
#         return outputs
#
#     def save_result(
#         self,
#         job_id: Union[uuid.UUID, None] = None,
#         aliases: Union[None, str, Mapping] = None,
#     ) -> StoreValuesResult:
#
#         if job_id is None:
#             job_id = self._last_job
#
#         if job_id is None:
#             raise Exception("No job queued (yet).")
#
#         result = self.retrieve_result(job_id=job_id)
#         alias_map = create_save_config(field_names=result.field_names, aliases=aliases)
#
#         store_result = self._kiara.save_values(values=result, alias_map=alias_map)
#         # if self.operation.module.characteristics.is_idempotent:
#         self._kiara.job_registry.store_job_record(job_id=job_id)
#
#         return store_result
#
#     def create_renderable(self, **config: Any) -> RenderableType:
#
#         show_operation_name = config.get("show_operation_name", True)
#         show_operation_doc = config.get("show_operation_doc", True)
#         show_inputs = config.get("show_inputs", False)
#         show_outputs_schema = config.get("show_outputs_schema", False)
#         show_headers = config.get("show_headers", True)
#
#         items: List[Any] = []
#
#         if show_operation_name:
#             items.append(f"Operation: [bold]{self.operation_name}[/bold]")
#         if show_operation_doc and self.operation.doc.is_set:
#             items.append("")
#             items.append(Markdown(self.operation.doc.full_doc, style="i"))
#
#         if show_inputs:
#             if show_headers:
#                 items.append("\nInputs:")
#             try:
#                 op_inputs = self.operation_inputs
#                 inputs: Any = create_value_map_status_renderable(op_inputs)
#             except InvalidValuesException as ive:
#                 inputs = ive.create_renderable(**config)
#             except Exception as e:
#                 inputs = f"[red bold]{e}[/red bold]"
#             items.append(inputs)
#         if show_outputs_schema:
#             if show_headers:
#                 items.append("\nOutputs:")
#             outputs_schema = create_table_from_field_schemas(
#                 _add_default=False,
#                 _add_required=False,
#                 _show_header=True,
#                 _constants=None,
#                 fields=self.operation.outputs_schema,
#             )
#             items.append(outputs_schema)
#
#         return Group(*items)


# kiara\kiara\src\kiara\interfaces\python_api\proxy.py
# -*- coding: utf-8 -*-
import inspect
from typing import Any, Callable, Dict, Iterable, List, Mapping, Type, Union

from docstring_parser import Docstring, parse
from pydantic.v1.decorator import ValidatedFunction
from pydantic.v1.main import BaseModel as BaseModel1
from rich import box
from rich.console import Group, RenderableType
from rich.markdown import Markdown
from rich.markup import escape
from rich.table import Table

from kiara.defaults import DEFAULT_NO_DESC_VALUE
from kiara.exceptions import KiaraException
from kiara.models.documentation import DocumentationMetadataModel
from kiara.utils.reflection import extract_signature_metadata

EXCLUDED_KEYS = ["self", "v__duplicate_kwargs", "args", "kwargs"]


class ApiEndpoint(object):
    def __init__(self, func: Callable):

        self._func = func
        self._wrapped: Union[None, ValidatedFunction] = None
        self._arg_names: Union[None, List[str]] = None
        self._param_details: Union[None, Dict[str, Any]] = None
        self._raw_doc: Union[None, str] = None
        self._doc_string: Union[None, str] = None
        self._parsed_doc: Union[Docstring, None] = None
        self._doc: Union[DocumentationMetadataModel, None] = None
        self._result_type: Union[Type, None] = None
        self._signature_metadata: Union[None, Mapping[str, Any]] = None

    @property
    def doc_string(self):

        if self._doc_string is not None:
            return self._doc_string

        _doc_string = self.raw_doc
        self._doc_string = inspect.cleandoc(_doc_string)
        return self._doc_string

    @property
    def func(self) -> Callable:
        return self._func

    @property
    def raw_doc(self) -> str:

        if self._raw_doc is not None:
            return self._raw_doc

        _doc_string = self._func.__doc__
        if _doc_string is None:
            _doc_string = ""
        self._raw_doc = _doc_string
        return self._raw_doc

    @property
    def doc(self) -> DocumentationMetadataModel:

        if self._doc is not None:
            return self._doc

        desc = self.parsed_doc.short_description
        if desc is None:
            desc = DEFAULT_NO_DESC_VALUE
        self._doc = DocumentationMetadataModel(
            description=desc,
            doc=self.parsed_doc.long_description,
        )
        return self._doc

    @property
    def parsed_doc(self) -> Docstring:

        if self._parsed_doc is not None:
            return self._parsed_doc

        parsed = parse(self.doc_string)
        self._parsed_doc = parsed
        return self._parsed_doc

    def get_arg_doc(self, arg_name: str) -> str:

        for p in self.parsed_doc.params:
            if p.arg_name == arg_name:
                desc: Union[str, None] = p.description
                return desc if desc else ""

        return ""

    @property
    def validated_func(self) -> ValidatedFunction:

        if self._wrapped is not None:
            return self._wrapped

        self._wrapped = ValidatedFunction(self._func, None)
        return self._wrapped

    @property
    def arg_model(self) -> Type[BaseModel1]:

        # TODO: pydantic refactoring, find a different way to do this in version 2
        result: Type[BaseModel1] = self.validated_func.model
        return result

    @property
    def argument_names(self) -> List[str]:

        if self._arg_names is not None:
            return self._arg_names

        self._arg_names = [
            x for x in self.validated_func.model.__fields__ if x not in EXCLUDED_KEYS
        ]
        return self._arg_names

    @property
    def arg_schema(self) -> Dict[str, Mapping[str, Any]]:

        if self._param_details is not None:
            return self._param_details

        param_details = {
            arg_name: self.signature_metadata["parameters"][arg_name]
            for arg_name in self.argument_names
        }
        for arg_name, details in param_details.items():
            details["doc"] = self.get_arg_doc(arg_name)

        self._param_details = param_details
        return self._param_details

    @property
    def signature_metadata(self) -> Mapping[str, Any]:

        if self._signature_metadata is not None:
            return self._signature_metadata

        self._signature_metadata = extract_signature_metadata(self._func)
        return self._signature_metadata

    @property
    def result_type(self) -> Type:

        result: Type = self.signature_metadata["return_type"]
        return result

    @property
    def result_doc(self) -> str:
        if self.parsed_doc.returns:
            desc: Union[None, str] = self.parsed_doc.returns.description
            return desc if desc else DEFAULT_NO_DESC_VALUE
        else:
            return DEFAULT_NO_DESC_VALUE

    def execute(self, instance: Any, **kwargs: Any) -> Any:

        result = self.validated_func.call(instance, **kwargs)
        return result

    def validate_and_assemble_args(self, **kwargs) -> BaseModel1:

        kwargs.pop("self", None)
        return self.validated_func.init_model_instance(None, **kwargs)

    def create_arg_schema_renderable(self, **config: Any) -> RenderableType:

        table = Table(box=box.SIMPLE, show_lines=False)
        table.add_column("Field name", style="i")
        table.add_column("Type", max_width=40)
        table.add_column("Description")
        table.add_column("Required")
        table.add_column("Default", justify="right", max_width=30)

        for arg_name in self.argument_names:
            row: List[RenderableType] = [f"[b]{arg_name}[/b]"]
            arg_type = self.arg_schema[arg_name]["type"]
            arg_str = str(arg_type)
            if arg_str.startswith("<class"):
                arg_str = arg_type.__name__
            arg_str = escape(str(arg_str))
            arg_str = arg_str.replace("typing.", "")
            row.append(arg_str)
            row.append(self.arg_schema[arg_name]["doc"])

            row.append(
                "[red]yes[/red]"
                if self.arg_schema[arg_name]["required"]
                else "[green]no[/green]"
            )

            default = self.arg_schema[arg_name]["default"]
            if default is not None:
                row.append(str(self.arg_schema[arg_name]["default"]))

            table.add_row(*row)

        return table

    def create_renderable(self, **config: Any) -> RenderableType:

        full_doc = config.get("full_doc", False)

        items: List[RenderableType] = []
        if full_doc:
            items.append(Markdown(self.doc.full_doc))
        else:
            items.append(Markdown(self.doc.description))

        items.append("")
        items.append("[b]Inputs[/b]")
        items.append(self.create_arg_schema_renderable(**config))

        if self.result_type is not None:
            items.append("")
            items.append("[b]Output[/b]")
            table = Table(box=box.SIMPLE, show_lines=False)
            table.add_column("Type", style="i")
            table.add_column("Description")
            result_type_name = str(self.result_type)
            if hasattr(self.result_type, "__name__"):
                result_type_name = self.result_type.__name__
            table.add_row(result_type_name, self.result_doc)
            items.append(table)

        return Group(*items)


class ApiEndpoints(object):
    def __init__(
        self,
        api_cls: Type,
        filters: Union[None, Iterable[str], str] = None,
        exclude: Union[None, Iterable[str], str] = None,
        include_tags: Union[None, Iterable[str], str] = None,
    ):

        if filters is None:
            filters = []
        elif isinstance(filters, str):
            filters = [filters]

        if exclude is None:
            exclude = []
        elif isinstance(exclude, str):
            exclude = [exclude]

        if include_tags is None:
            include_tags = []
        elif isinstance(include_tags, str):
            include_tags = [include_tags]

        self._api_cls = api_cls
        self._filters: Iterable[str] = filters
        self._exclude: Iterable[str] = exclude
        self._include_tags: Iterable[str] = include_tags

        self._api_endpoint_names: Union[None, List[str]] = None
        self._endpoint_details: Dict[str, ApiEndpoint] = {}

    @property
    def api_endpint_names(self) -> List[str]:

        if self._api_endpoint_names is not None:
            return self._api_endpoint_names

        temp = []

        avail_methods = list(
            inspect.getmembers(self._api_cls, predicate=inspect.isfunction)
        )

        avail_methods.sort(key=lambda x: inspect.getsourcelines(x[1])[1])

        method_names = [x[0] for x in avail_methods]
        for func_name in method_names:
            if func_name.startswith("_"):
                continue

            if func_name in self._exclude:
                continue

            func = getattr(self._api_cls, func_name)
            if not callable(func):
                continue

            if self._include_tags:
                if not hasattr(func, "_tags"):
                    continue
                tags = getattr(func, "_tags")
                match = False
                for t in tags:
                    if t in self._include_tags:
                        match = True
                        break
                if not match:
                    continue

            if self._filters:
                match = True
                for f in self._filters:
                    if f not in func_name:
                        match = False
                        break
                if match:
                    temp.append(func_name)

            else:
                temp.append(func_name)

        self._api_endpoint_names = temp
        return self._api_endpoint_names

    def get_api_endpoint(self, endpoint_name: str) -> ApiEndpoint:

        if endpoint_name in self._endpoint_details:
            return self._endpoint_details[endpoint_name]

        if not hasattr(self._api_cls, endpoint_name):
            details = "Available endpoints:\n"
            for n in self.api_endpint_names:
                details += f" - {n}"
            raise KiaraException(
                f"Endpoint '{endpoint_name}' not available.", details=details
            )

        func = getattr(self._api_cls, endpoint_name)
        result = ApiEndpoint(func)
        self._endpoint_details[endpoint_name] = result
        return result

    def create_renderable(self, **config: Any) -> RenderableType:

        from rich.table import Table

        if len(self.api_endpint_names) == 1:
            table = Table(box=box.SIMPLE, show_lines=False)
        else:
            table = Table(box=box.MINIMAL, show_lines=True)
        table.add_column("Endpoint", style="i b")
        table.add_column("Documentation")

        for endpoint_name in self.api_endpint_names:
            endpoint = self.get_api_endpoint(endpoint_name)
            table.add_row(endpoint_name, endpoint.create_renderable(**config))

        return table


# kiara\kiara\src\kiara\interfaces\python_api\utils.py
# -*- coding: utf-8 -*-
from typing import Any, Dict, Iterable, List, Mapping, Union

import structlog

logger = structlog.getLogger()


def create_save_config(
    field_names: Union[str, Iterable[str]],
    aliases: Union[None, str, Iterable[str], Mapping[str, Any]],
) -> Dict[str, List[str]]:

    if isinstance(field_names, str):
        field_names = [field_names]

    if aliases is None:
        alias_map: Dict[str, List[str]] = {}
    elif isinstance(aliases, str):
        alias_map = {}
        for field_name in field_names:
            alias_map[field_name] = [f"{aliases}.{field_name}"]
    elif isinstance(aliases, Mapping):
        alias_map = {}
        for field_name in aliases.keys():
            if field_name in field_names:
                if isinstance(aliases[field_name], str):
                    alias_map[field_name] = [aliases[field_name]]
                else:
                    alias_map[field_name] = sorted(aliases[field_name])
            else:
                logger.warning(
                    "ignore.field_alias",
                    ignored_field_name=field_name,
                    reason="field name not in results",
                    available_field_names=sorted(field_names),
                )
                continue
    else:
        raise Exception(
            f"Invalid type '{type(aliases)}' for aliases parameter, must be string or mapping."
        )

    return alias_map


# kiara\kiara\src\kiara\interfaces\python_api\value.py
# -*- coding: utf-8 -*-
from typing import Any, Dict, List, Union

import humanfriendly
import structlog
from pydantic import BaseModel, Field, RootModel
from rich import box
from rich.console import Console, ConsoleOptions, RenderableType, RenderResult
from rich.panel import Panel
from rich.table import Table

from kiara.models.values.value import PersistedData, Value

logger = structlog.getLogger()


class StoreValueResult(BaseModel):

    value: Value = Field(description="The stored value.")
    aliases: List[str] = Field(
        description="The aliases that where assigned to the value when stored."
    )
    persisted_data: Union[None, PersistedData] = Field(
        None,
        description="The structure describing the data that was persisted, 'None' if the data was already stored before (or storing failed).",
    )
    error: Union[str, None] = Field(
        None, description="An error that occured while trying to store."
    )

    def _repr_html_(self):

        r = self.create_renderable()
        mime_bundle = r._repr_mimebundle_(include=[], exclude=[])  # type: ignore
        return mime_bundle["text/html"]

    def __rich_console__(
        self, console: Console, options: ConsoleOptions
    ) -> RenderResult:

        yield self.create_renderable()

    def create_renderable(self, **config) -> RenderableType:

        table = Table(show_header=False, box=box.SIMPLE)
        table.add_column("key", "i")
        table.add_column("value")

        table.add_row("value_id", str(self.value.value_id))
        if self.aliases:
            if len(self.aliases) > 1:
                a = "aliases"
            else:
                a = "alias"
            table.add_row(a, ", ".join(self.aliases))
        else:
            table.add_row("aliases", "-- no aliases --")
        table.add_row("data type", self.value.data_type_name)
        table.add_row("size", humanfriendly.format_size(self.value.value_size))
        table.add_row("success", "yes" if not self.error else "no")
        if self.error:
            table.add_row("[red]error[/red]", f"{self.error}")

        return Panel(table, title="Store operation result", title_align="left")


class StoreValuesResult(RootModel):

    root: Dict[str, StoreValueResult]

    def create_renderable(self, **config: Any) -> RenderableType:

        add_field_column = config.get("add_field_column", True)

        errors = {}
        for field_name, value_result in self.root.items():
            if value_result.error:
                errors[field_name] = value_result.error

        table = Table(show_header=True, show_lines=False, box=box.SIMPLE)
        if add_field_column:
            table.add_column("field", style="b")
        table.add_column("stored id", style="i")
        table.add_column("data type", style="i")
        table.add_column("alias(es)")
        if errors:
            table.add_column("error", style="red")

        for field_name, value_result in self.root.items():
            if add_field_column:
                row = [
                    field_name,
                    str(value_result.value.value_id),
                    str(value_result.value.value_schema.type),
                ]
            else:
                row = [
                    str(value_result.value.value_id),
                    str(value_result.value.value_schema.type),
                ]

            if value_result.aliases:
                row.append(", ".join(value_result.aliases))
            else:
                row.append("")

            if errors.get(field_name) is not None:
                row.append(errors[field_name])
            table.add_row(*row)

        return table

    def keys(self):
        return self.root.keys()

    def values(self):
        return self.root.values()

    @property
    def errors(self) -> Dict[str, str]:

        result = {}
        for field_name, value_result in self.root.items():
            if value_result.error:
                result[field_name] = value_result.error

        return result

    def __iter__(self):
        return iter(self.root)

    def __len__(self):
        return len(self.root)


# kiara\kiara\src\kiara\interfaces\python_api\workflow.py
# -*- coding: utf-8 -*-
import uuid
from datetime import datetime
from typing import TYPE_CHECKING, Any, Dict, Hashable, List, Mapping, Set, Tuple, Union

import structlog
from boltons.strutils import slugify

from kiara.defaults import (
    KIARA_DEFAULT_STAGES_EXTRACTION_TYPE,
    NONE_VALUE_ID,
    NOT_SET_VALUE_ID,
)
from kiara.exceptions import NoSuchWorkflowException
from kiara.models import KiaraModel
from kiara.models.documentation import DocumentationMetadataModel
from kiara.models.events.pipeline import ChangedValue, PipelineEvent
from kiara.models.module.jobs import ActiveJob, ExecutionContext, JobConfig, JobStatus
from kiara.models.module.manifest import Manifest
from kiara.models.module.pipeline import (
    PipelineConfig,
    PipelineStep,
    StepStatus,
    StepValueAddress,
    generate_pipeline_endpoint_name,
)
from kiara.models.module.pipeline.controller import SinglePipelineController
from kiara.models.module.pipeline.pipeline import Pipeline, PipelineInfo
from kiara.models.python_class import KiaraModuleInstance
from kiara.models.values.value import Value, ValueMap
from kiara.models.values.value_schema import ValueSchema
from kiara.models.workflow import WorkflowInfo, WorkflowMetadata, WorkflowState
from kiara.registries.ids import ID_REGISTRY
from kiara.utils import find_free_id, log_exception
from kiara.utils.dates import get_current_time_incl_timezone

if TYPE_CHECKING:
    from kiara.context import Kiara

logger = structlog.getLogger()


class WorkflowPipelineController(SinglePipelineController):
    """
    A [PipelineController][kiara.models.modules.pipeline.controller.PipelineController] that executes all pipeline steps non-interactively.

    This is the default implementation of a ``PipelineController``, and probably the most simple implementation of one.
    It waits until all inputs are set, after which it executes all pipeline steps in the required order.

    """

    def __init__(
        self,
        kiara: "Kiara",
    ):

        self._is_running: bool = False
        super().__init__(job_registry=kiara.job_registry)

    def _pipeline_event_occurred(self, event: PipelineEvent):

        if event.pipeline_id != self.pipeline.pipeline_id:
            return

        self._pipeline_details = None

    def process_pipeline(
        self,
    ) -> Tuple[Mapping[uuid.UUID, uuid.UUID], Mapping[str, ActiveJob]]:

        log = logger.bind(pipeline_id=self.pipeline.pipeline_id)
        if self._is_running:
            log.debug(
                "ignore.pipeline_process",
                reason="Pipeline already running.",
            )
            raise Exception("Pipeline already running.")

        log.debug("execute.pipeline")
        self._is_running = True

        result: Dict[uuid.UUID, uuid.UUID] = {}
        errors: Dict[str, ActiveJob] = {}

        try:
            stages = self.pipeline.structure.extract_processing_stages(
                stages_extraction_type=KIARA_DEFAULT_STAGES_EXTRACTION_TYPE
            )
            for idx, stage in enumerate(stages, start=1):

                log.debug(
                    "execute.pipeline.stage",
                    stage=idx,
                )

                job_ids = {}
                stage_failed = False
                for step_id in stage:

                    log.debug(
                        "execute.pipeline.step",
                        step_id=step_id,
                    )

                    try:
                        job_id = self.process_step(step_id)
                        job_ids[step_id] = job_id
                    except Exception as e:
                        # TODO: cancel running jobs?
                        log_exception(e)
                        log.error(
                            "error.processing.workflow_pipeline",
                            step_id=step_id,
                            error=e,
                        )
                        stage_failed = True

                self._job_registry.wait_for(*job_ids.values())
                for step_id, job_id in job_ids.items():
                    j = self._job_registry.get_job(job_id)
                    if j.status != JobStatus.SUCCESS:
                        errors[step_id] = j
                output_job_map = self.set_processing_results(job_ids=job_ids)
                result.update(output_job_map)
                if not stage_failed:
                    log.debug(
                        "execute_finished.pipeline.stage",
                        stage=idx,
                    )
                else:
                    log.debug(
                        "execute_failed.pipeline.stage",
                        stage=idx,
                    )
                    break
        except Exception as e:
            log_exception(e)
        finally:
            self._is_running = False

        log.debug("execute_finished.pipeline")
        return result, errors


class WorkflowStatus(KiaraModel):
    pass


class Workflow(object):
    """
    A wrapper object to make working with workflows easier for frontend code.

    This is the type of class everyone advises you against creating in Object-Oriented code, all it contains is basically
    internal state. In this case, I believe it's warranted because otherwise frontend code would spin out of control,
    complexity-wise. I'm happy to be proven wrong, though.

    None of this is thread-safe (yet).

    This object can be initialized in different ways, depending on circumstances:

     - if you want to create a new workflow object: use 'None' or a WorkflowMetadata object you createed earlier
         (ensure you don't re-use an existing workflow_id, otherwise it might get overwrittern in the backend)
     - if you want to create the wrapper object from an existing workflow: provide its id or alias string

    Arguments:
    ---------
        kiara: the kiara context in which this workflow lives
        workflow: the workflow metadata (or reference to it)
        load_existing: if set to 'False', a workflow with the provided id/alias can't already exist, if 'True', it must, if set to 'None', kiara tries to be smart and loads if exists, otherwise creates a new one
    """

    @classmethod
    def load(
        cls,
        workflow: Union[uuid.UUID, str],
        kiara: Union["Kiara", None] = None,
        create: bool = False,
    ):
        """Load an existing workflow using a workflow id or alias."""
        try:
            workflow_obj = Workflow(workflow=workflow, kiara=kiara, load_existing=True)
        except NoSuchWorkflowException as nswe:
            if create:

                if isinstance(workflow, uuid.UUID):
                    raise nswe
                temp = None
                try:
                    temp = uuid.UUID(workflow)
                except Exception:
                    pass
                if temp is not None:
                    raise nswe

                if kiara is None:
                    from kiara.context import Kiara

                    kiara = Kiara.instance()

                workflow_metadata = kiara.workflow_registry.register_workflow(
                    workflow_aliases=[workflow]
                )
                workflow_obj = Workflow(workflow=workflow_metadata.workflow_id)

        return workflow_obj

    @classmethod
    def create(
        cls,
        alias: Union[None, str] = None,
        replace_existing_alias: bool = False,
        doc: Union[Any, None] = None,
        kiara: Union["Kiara", None] = None,
    ):
        """Create a new workflow object."""
        if replace_existing_alias and alias is not None:
            if kiara is None:
                from kiara.context import Kiara

                kiara = Kiara.instance()

            kiara.workflow_registry.unregister_alias(alias=alias)

        workflow = Workflow(workflow=alias, kiara=kiara, load_existing=False)
        if doc:
            workflow.documentation = doc
        return workflow

    def __init__(
        self,
        workflow: Union[None, WorkflowMetadata, uuid.UUID, str] = None,
        kiara: Union["Kiara", None] = None,
        load_existing: Union[bool, None] = None,
    ):

        if kiara is None:
            from kiara.context import Kiara

            kiara = Kiara.instance()

        self._kiara: "Kiara" = kiara
        self._metadata_is_stored: bool = False
        self._metadata_is_synced: bool = False

        self._pending_aliases: Set[str] = set()

        _workflow_id: Union[None, uuid.UUID] = None
        _workflow_alias: Union[None, str] = None
        _workflow_metadata: Union[None, WorkflowMetadata] = None

        if workflow is None:
            if load_existing is True:
                raise Exception(
                    "Can't create workflow: no workflow reference provided, but 'load_existing' forced to 'True'."
                )
            _w_id = ID_REGISTRY.generate(comment="New workflow object.")
            _workflow_metadata = WorkflowMetadata(workflow_id=_w_id)
        elif isinstance(workflow, str):
            try:
                _workflow_id = uuid.UUID(workflow)
                _workflow_metadata = (
                    self._kiara.workflow_registry.get_workflow_metadata(
                        workflow=_workflow_id
                    )
                )
                if load_existing is False:
                    raise Exception(
                        f"Can't create workflow for id '{_workflow_id}': workflow with this id already registered and 'load_existing' set to 'False'."
                    )
                self._metadata_is_stored = True
                self._metadata_is_synced = True
            except NoSuchWorkflowException as nswe:
                # means an uuid was provided
                raise nswe
            except Exception:
                # means it's an alias
                _workflow_alias = workflow
                try:
                    _workflow_id = self._kiara.workflow_registry.get_workflow_id(
                        workflow
                    )
                    if load_existing is False:
                        raise Exception(
                            f"Can't create workflow with alias '{_workflow_alias}': alias already registered, and 'load_existing' set to 'False'."
                        )
                    _workflow_metadata = (
                        self._kiara.workflow_registry.get_workflow_metadata(
                            workflow=_workflow_id
                        )
                    )
                    self._metadata_is_stored = True
                    self._metadata_is_synced = True
                except NoSuchWorkflowException:
                    # does not exist yet
                    if load_existing is True:
                        raise NoSuchWorkflowException(
                            msg=f"Can't load workflow with alias '{_workflow_alias}': no workflow with this alias registered.",
                            workflow=_workflow_alias,
                        )
                    self._pending_aliases.add(_workflow_alias)
                    _workflow_id = ID_REGISTRY.generate(comment="New workflow object.")
                    _workflow_metadata = WorkflowMetadata(workflow_id=_workflow_id)
        elif isinstance(workflow, uuid.UUID):
            _workflow_id = workflow
            if load_existing is False:
                raise Exception(
                    f"Can't create workflow for id '{_workflow_id}': 'load_existing' set to 'False'."
                )
            _workflow_metadata = self._kiara.workflow_registry.get_workflow_metadata(
                workflow=_workflow_id
            )
            self._metadata_is_stored = True
            self._metadata_is_synced = True
        elif isinstance(workflow, WorkflowMetadata):
            if load_existing is True:
                raise Exception(
                    f"Can't create workflow with id '{workflow.workflow_id}': 'load_existing' forced to 'True'."
                )
            temp = None
            _workflow_metadata = None
            try:
                temp = self._kiara.workflow_registry.get_workflow_metadata(
                    workflow.workflow_id
                )
            except Exception:
                _workflow_metadata = workflow
                _workflow_id = _workflow_metadata.workflow_id

            if temp is not None:
                raise Exception(
                    f"Can't create new workflow with id '{workflow.workflow_id}': id already registered."
                )

        else:
            raise Exception(
                f"Can't find workflow metadata for '{workflow}: invalid type '{type(workflow)}'."
            )

        assert _workflow_id is not None
        assert _workflow_metadata is not None

        self._workflow_metadata: WorkflowMetadata = _workflow_metadata
        self._workflow_id: uuid.UUID = self.workflow_metadata.workflow_id

        self._execution_context: ExecutionContext = ExecutionContext()
        self._pipeline_controller: WorkflowPipelineController = (
            WorkflowPipelineController(kiara=self._kiara)
        )

        self._all_inputs: Dict[str, Union[None, uuid.UUID]] = {}
        self._all_inputs_optimistic_lookup: Dict[str, Dict[Hashable, uuid.UUID]] = {}
        self._current_pipeline_inputs: Union[Dict[str, uuid.UUID], None] = None
        self._current_pipeline_outputs: Union[Dict[str, uuid.UUID], None] = None

        self._steps: Dict[str, PipelineStep] = {}

        self._current_workflow_inputs_schema: Union[Dict[str, ValueSchema], None] = None
        self._current_workflow_outputs_schema: Union[Dict[str, ValueSchema], None] = (
            None
        )
        self._workflow_input_aliases: Dict[str, str] = dict(
            _workflow_metadata.input_aliases
        )
        self._workflow_output_aliases: Dict[str, str] = dict(
            _workflow_metadata.output_aliases
        )

        self._current_workflow_inputs: Union[Dict[str, uuid.UUID], None] = None
        self._current_workflow_outputs: Union[Dict[str, uuid.UUID], None] = None

        # self._current_state_cid: Union[None, CID] = None
        # self._state_history: Dict[datetime, str] = {}
        self._state_cache: Dict[str, WorkflowState] = {}
        self._state_output_cache: Dict[str, Set[uuid.UUID]] = {}
        self._state_jobrecord_cache: Dict[str, Set[uuid.UUID]] = {}

        self._job_id_cache: Dict[uuid.UUID, uuid.UUID] = {}
        """Cache to save job ids per output value(s), in order to save jobs if output values are saved."""

        self._pipeline: Union[Pipeline, None] = None
        self._pipeline_info: Union[PipelineInfo, None] = None
        self._current_info: Union[WorkflowInfo, None] = None
        self._current_state: Union[WorkflowState, None] = None

        if self._workflow_metadata.workflow_history:
            self.load_state()

    def _sync_workflow_metadata(self):
        """Store/update the metadata of this workflow."""
        self._workflow_metadata = self._kiara.workflow_registry.register_workflow(
            workflow_metadata=self._workflow_metadata,
            workflow_aliases=self._pending_aliases,
        )
        self._pending_aliases.clear()
        self._metadata_is_stored = True
        self._metadata_is_synced = True

    @property
    def workflow_id(self) -> uuid.UUID:
        """Retrieve the globally unique id for this workflow."""
        return self._workflow_metadata.workflow_id

    @property
    def workflow_metadata(self) -> WorkflowMetadata:
        """Retrieve an object that contains metadata for this workflow."""
        return self._workflow_metadata

    @property
    def documentation(self) -> DocumentationMetadataModel:
        return self.workflow_metadata.documentation

    @documentation.setter
    def documentation(self, documentation: Any):

        doc = DocumentationMetadataModel.create(documentation)
        self.workflow_metadata.documentation = doc

    @property
    def is_persisted(self) -> bool:
        """Check whether this workflow is persisted in it's current state."""
        return self.workflow_metadata.is_persisted

    @property
    def current_pipeline_inputs_schema(self) -> Mapping[str, ValueSchema]:
        return self.pipeline.structure.pipeline_inputs_schema

    @property
    def current_pipeline_inputs(self) -> Mapping[str, uuid.UUID]:

        if self._current_pipeline_inputs is None:
            self._apply_inputs()
        assert self._current_pipeline_inputs is not None
        return self._current_pipeline_inputs

    @property
    def current_pipeline_input_values(self) -> ValueMap:
        return self._kiara.data_registry.load_values(
            values=self.current_pipeline_inputs
        )

    @property
    def current_pipeline_outputs_schema(self) -> Mapping[str, ValueSchema]:
        return self.pipeline.structure.pipeline_outputs_schema

    @property
    def current_pipeline_outputs(self) -> Mapping[str, uuid.UUID]:

        if self._current_pipeline_outputs is None:
            try:
                self.process_steps()
            except Exception:
                self._current_pipeline_outputs = (
                    self.pipeline.get_current_pipeline_outputs()
                )

        assert self._current_pipeline_outputs is not None
        return self._current_pipeline_outputs

    @property
    def current_pipeline_output_values(self) -> ValueMap:
        return self._kiara.data_registry.load_values(
            values=self.current_pipeline_outputs
        )

    @property
    def input_aliases(self) -> Mapping[str, str]:
        return self._workflow_input_aliases

    @property
    def output_aliases(self) -> Mapping[str, str]:
        return self._workflow_output_aliases

    def clear_current_inputs_for_step(self, step_id):

        fields = self.get_current_inputs_schema_for_step(step_id)
        for field in fields.keys():
            self.set_inputs(**{k: None for k in fields.keys()})

    @property
    def current_inputs_schema(self) -> Mapping[str, ValueSchema]:

        if self._current_workflow_inputs_schema is not None:
            return self._current_workflow_inputs_schema

        temp = {}
        # TODO; check correctness when an alias refers to two different inputs
        for k, v in self.current_pipeline_inputs_schema.items():
            if k in self._workflow_input_aliases.keys():
                temp[self._workflow_input_aliases[k]] = v
            else:
                temp[k] = v
        self._current_workflow_inputs_schema = temp
        return self._current_workflow_inputs_schema

    def get_current_inputs_schema_for_step(
        self, step_id: str
    ) -> Mapping[str, ValueSchema]:
        return self.pipeline.structure.get_pipeline_inputs_schema_for_step(
            step_id=step_id
        )

    def get_current_outputs_schema_for_step(
        self, step_id: str
    ) -> Mapping[str, ValueSchema]:
        return self.pipeline.structure.get_pipeline_outputs_schema_for_step(
            step_id=step_id
        )

    @property
    def current_input_names(self) -> List[str]:
        return sorted(self.current_inputs_schema.keys())

    @property
    def current_inputs(self) -> Mapping[str, uuid.UUID]:

        if self._current_workflow_inputs is not None:
            return self._current_workflow_inputs

        temp = {}
        for k, v in self.current_pipeline_inputs.items():
            if k in self._workflow_input_aliases.keys():
                temp[self._workflow_input_aliases[k]] = v
            else:
                temp[k] = v
        self._current_workflow_inputs = temp
        return self._current_workflow_inputs

    @property
    def current_input_values(self) -> ValueMap:
        return self._kiara.data_registry.load_values(values=self.current_inputs)

    @property
    def current_outputs_schema(self) -> Mapping[str, ValueSchema]:

        if self._current_workflow_outputs_schema is not None:
            return self._current_workflow_outputs_schema

        if not self._workflow_output_aliases:
            self._current_workflow_outputs_schema = dict(
                self.current_pipeline_outputs_schema
            )
        else:
            temp = {}
            for k, v in self._workflow_output_aliases.items():
                temp[v] = self.current_pipeline_outputs_schema[k]
            self._current_workflow_outputs_schema = temp

        return self._current_workflow_outputs_schema

    @property
    def current_output_names(self) -> List[str]:
        return sorted(self.current_outputs_schema.keys())

    @property
    def current_outputs(self) -> Mapping[str, uuid.UUID]:

        if self._current_workflow_outputs is not None:
            return self._current_workflow_outputs

        if not self._workflow_output_aliases:
            self._current_workflow_outputs = dict(self.current_pipeline_outputs)
        else:
            temp: Dict[str, uuid.UUID] = {}
            for k, v in self._workflow_output_aliases.items():
                temp[v] = self.current_pipeline_outputs[k]
            self._current_workflow_outputs = temp

        return self._current_workflow_outputs

    @property
    def current_output_values(self) -> ValueMap:
        return self._kiara.data_registry.load_values(values=self.current_outputs)

    @property
    def current_state(self) -> WorkflowState:

        if self._current_state is not None:
            return self._current_state

        self._current_state = WorkflowState.create_from_workflow(self)
        self._state_cache[self._current_state.instance_id] = self._current_state
        return self._current_state

    @property
    def pipeline(self) -> Pipeline:

        if self._pipeline is not None:
            return self._pipeline

        self._invalidate_pipeline()

        steps = list(self._steps.values())
        # input_aliases_temp = create_input_alias_map(steps=steps)
        # input_aliases = {}
        # for k, v in input_aliases_temp.items():
        #     if k in self._pipeline_input_aliases.keys():
        #         input_aliases[k] = self._pipeline_input_aliases[k]
        #     else:
        #         input_aliases[k] = v
        #
        # if not self._pipeline_output_aliasess:
        #     output_aliases = create_output_alias_map(steps=steps)
        # else:
        #     output_aliases = self._pipeline_output_aliasess

        pipeline_config = PipelineConfig.from_config(
            pipeline_name="__workflow__",
            data={
                "steps": steps,
                "doc": self.workflow_metadata.documentation,
                # "input_aliases": input_aliases,
                # "output_aliases": output_aliases,
            },
        )
        structure = pipeline_config.structure
        self._pipeline = Pipeline(structure=structure, kiara=self._kiara)
        self._pipeline_controller.pipeline = self._pipeline
        return self._pipeline

    def _apply_inputs(self) -> Mapping[str, Mapping[str, Mapping[str, ChangedValue]]]:

        pipeline = self.pipeline

        inputs_to_set = {}
        for field_name, value in self._all_inputs.items():
            # if value in [None, NONE_VALUE_ID, NOT_SET_VALUE_ID]:
            #     continue
            if field_name in pipeline.structure.pipeline_inputs_schema.keys():
                inputs_to_set[field_name] = value

        logger.debug(
            "workflow.apply_inputs",
            workflow_id=str(self.workflow_id),
            keys=", ".join(inputs_to_set.keys()),
        )

        changed: Mapping[str, Mapping[str, Mapping[str, ChangedValue]]] = (
            pipeline.set_pipeline_inputs(inputs=inputs_to_set)
        )

        self._current_pipeline_inputs = pipeline.get_current_pipeline_inputs()

        for field_name, value_id in self._current_pipeline_inputs.items():
            self._all_inputs[field_name] = value_id
        self._current_pipeline_outputs = None

        for stage, steps in pipeline.get_steps_by_stage().items():
            stage_valid = True
            cached_steps = []
            for step_id in steps.keys():
                step_details = pipeline.get_step_details(step_id=step_id)
                if step_details.status == StepStatus.INPUTS_INVALID:
                    stage_valid = False
                    break
                elif step_details.status == StepStatus.INPUTS_READY:
                    job_config = JobConfig(
                        module_type=step_details.step.module.manifest.module_type,
                        module_config=step_details.step.module.manifest.module_config,
                        is_resolved=step_details.step.module.manifest.is_resolved,
                        inputs=step_details.inputs,
                    )
                    match = self._kiara.job_registry.find_job_record_for_manifest(
                        inputs_manifest=job_config
                    )
                    if match:
                        cached_steps.append(step_id)
            if cached_steps:
                self.process_steps(*cached_steps)
            if not stage_valid:
                break

        self._current_state = None
        self._current_info = None
        self._pipeline_info = None
        return changed

    def process_steps(
        self, *step_ids: str
    ) -> Tuple[Mapping[uuid.UUID, uuid.UUID], Mapping[str, ActiveJob]]:

        self.pipeline

        if not step_ids:
            output_job_map, errors = self._pipeline_controller.process_pipeline()
        else:
            job_ids = {}
            for step_id in step_ids:
                job_id = self._pipeline_controller.process_step(
                    step_id=step_id, wait=True
                )
                job_ids[step_id] = job_id

            self._pipeline_controller._job_registry.wait_for(*job_ids.values())
            errors = {}
            for step_id, job_id in job_ids.items():
                j = self._pipeline_controller._job_registry.get_job(job_id)
                if j.status != JobStatus.SUCCESS:
                    errors[step_id] = j
            output_job_map = self._pipeline_controller.set_processing_results(
                job_ids=job_ids
            )

        self._job_id_cache.update(output_job_map)

        self._current_pipeline_outputs = self.pipeline.get_current_pipeline_outputs()
        self._current_state = None
        self._pipeline_info = None
        self._current_info = None

        return output_job_map, errors

    def _invalidate_pipeline(self):

        self._pipeline_controller.pipeline = None
        self._pipeline = None
        self._pipeline_info = None
        self._current_info = None
        self._current_state = None
        self._current_workflow_inputs_schema = None
        self._current_workflow_outputs_schema = None
        self._current_pipeline_inputs = None
        self._current_pipeline_outputs = None

    def set_input(self, field_name: str, value: Any) -> Union[None, uuid.UUID]:
        """
        Set a single pipeline input.

        Arguments:
        ---------
            field_name: The name of the input field.
            value: The value to set.

        Returns:
        -------
            None if the value for that field in the pipeline didn't change, otherwise the value_id of the new (registered) value.

        """
        diff = self.set_inputs(**{field_name: value})
        return diff.get(field_name, None)

    def set_inputs(self, **inputs: Any) -> Dict[str, Union[uuid.UUID, None]]:
        """
        Set multiple pipeline inputs, at once.

        Arguments:
        ---------
            inputs: The inputs to set.

        Returns:
        -------
            a dict containing only the newly set or changed values with field name as keys, and value_id (or None) as values.
        """
        _inputs = {}
        for k, v in inputs.items():
            # translate aliases
            match = False
            for field, alias in self._workflow_input_aliases.items():
                if k == alias:
                    match = True
                    _inputs[field] = v

            if not match and k in self.current_pipeline_inputs_schema.keys():
                _inputs[k] = v

        inputs = _inputs

        invalid = []
        for k, v in inputs.items():
            if k not in self.pipeline.structure.pipeline_inputs_schema.keys():
                invalid.append(k)
        if invalid:
            raise Exception(
                f"Can't set pipeline inputs, invalid field(s): {', '.join(invalid)}. Available inputs: '{', '.join(self.pipeline.structure.pipeline_inputs_schema.keys())}'"
            )

        diff: Dict[str, Union[None, uuid.UUID]] = {}
        for k, val_new in inputs.items():

            val_old = self._all_inputs.get(k, None)
            if val_old is None and val_new is None:
                continue

            if val_new is None:
                self._all_inputs[k] = None
                diff[k] = None
                continue

            if isinstance(val_new, uuid.UUID):
                if val_new == val_old:
                    continue
                else:
                    self._all_inputs[k] = val_new
                    diff[k] = val_new
                    continue

            if isinstance(val_new, Value):
                if val_new.value_id == val_old:
                    continue
                else:
                    self._all_inputs[k] = val_new.value_id
                    diff[k] = val_new.value_id
                    continue

            # TODO: check for aliases?
            try:
                _new_item_hash = hash(val_new)

                _match: Union[None, uuid.UUID] = None
                for _item_hash, _value_id in self._all_inputs_optimistic_lookup.get(
                    k, {}
                ).items():
                    if _item_hash == _new_item_hash:
                        if _value_id == val_old:
                            _match = val_old
                            break

                if not _match:
                    _schema = self.current_pipeline_inputs_schema[k]
                    val = self._kiara.data_registry.register_data(
                        data=val_new, schema=_schema, reuse_existing=True
                    )
                    _match = val.value_id
                    self._all_inputs_optimistic_lookup.setdefault(k, {})[
                        _new_item_hash
                    ] = _match

                if _match == val_old:
                    continue
                else:
                    self._all_inputs[k] = _match
                    diff[k] = _match
                    continue

            except Exception:
                # value can't be hashed, so we have to accept we can not re-use an existing value for this
                _schema = self.current_pipeline_inputs_schema[k]
                val = self._kiara.data_registry.register_data(
                    data=val_new, schema=_schema, reuse_existing=True
                )
                new_value_id = val.value_id
                if new_value_id == val_old:
                    continue
                else:
                    self._all_inputs[k] = new_value_id
                    diff[k] = new_value_id

        self._all_inputs.update(diff)

        if diff:
            self._current_info = None
            self._current_state = None
            self._current_pipeline_inputs = None
            self._current_pipeline_outputs = None
            self._current_workflow_inputs = None
            self._current_workflow_outputs = None
            self._pipeline_info = None
            self._apply_inputs()

        return diff

    def add_steps(
        self,
        *pipeline_steps: Union[PipelineStep, Mapping[str, Any]],
        replace_existing: bool = False,
        clear_existing: bool = False,
    ):

        if clear_existing:
            self.clear_steps()

        duplicates = []
        for step in pipeline_steps:
            if isinstance(step, PipelineStep):
                step_id = step.step_id
            else:
                step_id = step["step_id"]
            if step_id in self._steps.keys() and not replace_existing:
                duplicates.append(step_id)

        if duplicates:
            raise Exception(
                f"Can't add steps, step id(s) already taken: {', '.join(duplicates)}."
            )

        for step in pipeline_steps:
            if isinstance(step, PipelineStep):
                input_connections = {}
                for input_field, links in step.input_links.items():
                    if len(links) != 1:
                        raise NotImplementedError()
                    input_connections[input_field] = links[0].alias
                data: Mapping[str, Any] = {
                    "operation": step.manifest_src.module_type,
                    "step_id": step.step_id,
                    "module_config": step.manifest_src.module_config,
                    "input_connections": input_connections,
                    "doc": step.doc,
                    "replace_existing": replace_existing,
                }
            else:
                data = step

            self.add_step(**data)

        self._invalidate_pipeline()

    def clear_steps(self, *step_ids: str):

        if not step_ids:
            self._steps.clear()
        else:
            for step_id in step_ids:
                self._steps.pop(step_id, None)

        self._invalidate_pipeline()

    def set_input_alias(self, input_field: str, alias: str):

        if "." in input_field:
            tokens = input_field.split(".")
            if len(tokens) != 2:
                raise Exception(
                    f"Invalid input field specification '{input_field}': can only contain a single (or no) '.' character."
                )
            input_field = generate_pipeline_endpoint_name(tokens[0], tokens[1])

        self._workflow_input_aliases[input_field] = alias
        self._current_workflow_inputs = None
        self._current_workflow_inputs_schema = None
        self.workflow_metadata.input_aliases[input_field] = alias
        self._metadata_is_synced = False

    def set_output_alias(self, output_field: str, alias: str):

        if "." in output_field:
            tokens = output_field.split(".")
            if len(tokens) != 2:
                raise Exception(
                    f"Invalid output field specification '{output_field}': can only contain a single (or no) '.' character."
                )
            output_field = generate_pipeline_endpoint_name(tokens[0], tokens[1])

        self._workflow_output_aliases[output_field] = alias
        self._current_workflow_outputs = None
        self._current_workflow_outputs_schema = None
        self.workflow_metadata.output_aliases[output_field] = alias
        self._metadata_is_synced = False

    # def remove_step(self, step_id: str):
    #
    #     if step_id not in self._steps.keys():
    #         raise Exception(f"Can't remove step, no step with id '{step_id}'.")
    #
    #     del_step = self._steps[step_id]
    #     for step_id, step in self._steps.items():
    #         for input_field, links in step.input_links.items():
    #             for link in links:
    #                 if link.step_id == del_step.step_id:
    #                     links.remove(link)
    #
    #     self._invalidate_pipeline()

    def add_step(
        self,
        operation: str,
        step_id: Union[str, None] = None,
        module_config: Union[None, Mapping[str, Any]] = None,
        input_connections: Union[None, Mapping[str, str]] = None,
        doc: Union[str, DocumentationMetadataModel, None] = None,
        replace_existing: bool = False,
    ) -> PipelineStep:
        """
        Add a step to the workflows current pipeline structure.

        If no 'step_id' is provided, a unque one will automatically be generated based on the 'module_type' argument.

        Arguments:
        ---------
            operation: the module or operation name
            step_id: the id of the new step
            module_config: (optional) configuration for the kiara module this step uses
            input_connections: a map with this steps input field name(s) as keys and output field links (format: <step_id>.<output_field_name>) as value(s).
            replace_existing: if set to 'True', this replaces a step with the same id that already exists, otherwise an exception will be thrown
        """
        if step_id is None:
            step_id = find_free_id(
                slugify(operation, delim="_"), current_ids=self._steps.keys()
            )

        if "." in step_id:
            raise Exception(f"Invalid step id '{step_id}': id can't contain '.'.")

        if step_id in self._steps.keys() and not replace_existing:
            raise Exception(
                f"Can't add step with id '{step_id}': step already exists and 'replace_existing' not set."
            )
        elif step_id in self._steps.keys():
            raise NotImplementedError()

        manifest = self._kiara.create_manifest(
            module_or_operation=operation, config=module_config
        )
        module = self._kiara.module_registry.create_module(manifest=manifest)
        manifest_src = Manifest(
            module_type=manifest.module_type, module_config=manifest.module_config
        )
        step = PipelineStep(
            step_id=step_id,
            module_type=module.module_type_name,
            module_config=module.config.model_dump(),
            module_details=KiaraModuleInstance.from_module(module=module),
            doc=doc,
            manifest_src=manifest_src,
        )
        step._module = module
        self._steps[step_id] = step

        if input_connections:
            for k, v in input_connections.items():
                self.connect_to_inputs(v, f"{step_id}.{k}")

        self._invalidate_pipeline()

        return step

    def connect_fields(self, *fields: Union[Tuple[str, str], str]):

        pairs = []
        current_pair = None
        for field in fields:
            if isinstance(field, str):
                tokens = field.split(".")
                if not len(tokens) == 2:
                    raise Exception(
                        f"Can't connect field '{field}', field name must be in format: <step_id>.<field_name>."
                    )
                if not current_pair:
                    current_pair = [tokens]
                else:
                    if not len(current_pair) == 1:
                        raise Exception(
                            f"Can't connect fields, invalid input(s): {fields}"
                        )
                    current_pair.append(tokens)
                    pairs.append(current_pair)
                    current_pair = None
            else:
                if not len(field) == 2:
                    raise Exception(
                        f"Can't connect fields, field tuples must have length 2: {field}"
                    )
                if current_pair:
                    raise Exception(
                        f"Can't connect fields, dangling single field: {current_pair}"
                    )
                pair = []
                for f in field:
                    tokens = f.split(".")
                    if not len(tokens) == 2:
                        raise Exception(
                            f"Can't connect field '{f}', field name must be in format: <step_id>.<field_name>."
                        )
                    pair.append(tokens)
                pairs.append(pair)

        for pair in pairs:
            self.connect_steps(pair[0][0], pair[0][1], pair[1][0], pair[1][1])

    def connect_steps(
        self,
        source_step: Union[PipelineStep, str],
        source_field: str,
        target_step: Union[PipelineStep, str],
        target_field: str,
    ):

        if isinstance(source_step, str):
            source_step_obj = self.get_step(source_step)
        else:
            source_step_obj = source_step
        if isinstance(target_step, str):
            target_step_obj = self.get_step(target_step)
        else:
            target_step_obj = target_step

        source_step_id = source_step_obj.step_id
        target_step_id = target_step_obj.step_id

        reversed = False

        if source_field not in source_step_obj.module.outputs_schema.keys():
            reversed = True
        if target_field not in target_step_obj.module.inputs_schema.keys():
            reversed = True

        if reversed:
            if target_field not in target_step_obj.module.outputs_schema.keys():
                raise Exception(
                    f"Can't connect steps '{source_step_id}.{source_field}' -> '{target_step_id}.{target_field}': invalid field name(s)."
                )
            if source_field not in source_step_obj.module.inputs_schema.keys():
                raise Exception(
                    f"Can't connect steps '{source_step_id}.{source_field}' -> '{target_step_id}.{target_field}': invalid field name(s)."
                )
        else:
            if target_field not in target_step_obj.module.inputs_schema.keys():
                raise Exception(
                    f"Can't connect steps '{source_step_id}.{source_field}' -> '{target_step_id}.{target_field}': invalid field name(s)."
                )
            if source_field not in source_step_obj.module.outputs_schema.keys():
                raise Exception(
                    f"Can't connect steps '{source_step_id}.{source_field}' -> '{target_step_id}.{target_field}': invalid field name(s)."
                )

        # we rely on the value of input links to always be a dict here
        if not reversed:
            source_addr = StepValueAddress(
                step_id=source_step_id, value_name=source_field
            )
            target_step_obj.input_links.setdefault(target_field, []).append(source_addr)  # type: ignore
        else:
            source_addr = StepValueAddress(
                step_id=target_step_id, value_name=target_field
            )
            source_step_obj.input_links.setdefault(source_field, []).append(source_addr)  # type: ignore

        self._invalidate_pipeline()

    def connect_to_inputs(self, source_field: str, *input_fields: str):

        source_tokens = source_field.split(".")
        if len(source_tokens) != 2:
            raise Exception(
                f"Can't add input link(s): invalid format for provided source '{source_field}', must be string with a single '.' to delimit step-id and output field name."
            )

        source_step = self.get_step(source_tokens[0])
        if source_step is None:
            raise Exception(
                f"Can't add input link(s)': no source step with id '{source_tokens[0]}' exists."
            )

        if source_tokens[1] not in source_step.module.outputs_schema.keys():
            av_fields = ", ".join(source_step.module.outputs_schema.keys())
            raise Exception(
                f"Can't add input link(s): source step with id '{source_step.step_id}' does not have output field '{source_tokens[1]}'. Available field names: {av_fields}."
            )

        source_addr = StepValueAddress(
            step_id=source_step.step_id, value_name=source_tokens[1]
        )

        steps = []
        for input_field in input_fields:
            input_tokens = input_field.split(".")
            if len(input_tokens) != 2:
                raise Exception(
                    f"Can't add input link '{input_field}': invalid format, must be string with a single '.' to delimit step-id and field name."
                )

            step = self.get_step(input_tokens[0])
            if step is None:
                raise Exception(
                    f"Can't add input link '{input_field}': no step with id '{input_tokens[0]}' exists."
                )

            if input_tokens[1] not in step.module.inputs_schema.keys():
                av_fields = ", ".join(step.module.inputs_schema.keys())
                raise Exception(
                    f"Can't add input link '{input_field}': step with id '{input_tokens[0]}' does not have input field '{input_tokens[1]}'. Available field names: {av_fields}."
                )
            steps.append((step, input_tokens[1]))

        for s in steps:
            step, field_name = s
            # we rely on the value of input links to always be a dict here
            step.input_links.setdefault(field_name, []).append(source_addr)  # type: ignore

        self._invalidate_pipeline()

    def get_step(self, step_id: str) -> PipelineStep:

        step = self._steps.get(step_id, None)
        if step is None:
            if self._steps:
                msg = f"Available step ids: {', '.join(self._steps.keys())}"
            else:
                msg = "Workflow does not have any steps (yet)."
            raise Exception(f"No step with id '{step_id}' registered. {msg}")
        return step

    def load_state(
        self, workflow_state_id: Union[str, None] = None
    ) -> Union[None, WorkflowState]:
        """
        Load a past state.

        If no state id is specified, the latest one that was saved will be used.

        Returns:
        -------
            'None' if no state was loaded, otherwise the relevant 'WorkflowState' instance
        """
        if workflow_state_id is None:
            if not self._workflow_metadata.workflow_history:
                return None
            else:
                workflow_state_id = self._workflow_metadata.last_state_id

        if workflow_state_id is None:
            raise Exception(
                f"Can't load current state for workflow '{self.workflow_id}': no state available."
            )

        state = self._state_cache.get(workflow_state_id, None)
        if state is not None:
            return state

        state = self._kiara.workflow_registry.get_workflow_state(
            workflow=self.workflow_id, workflow_state_id=workflow_state_id
        )
        assert workflow_state_id == state.instance_id

        self._state_cache[workflow_state_id] = state

        self._all_inputs.clear()
        self._current_pipeline_inputs = None
        self.clear_steps()
        self._invalidate_pipeline()

        self.add_steps(*state.steps)
        # self._workflow_input_aliases = dict(state.input_aliases)
        # self._workflow_output_aliases = dict(state.output_aliases)

        self.set_inputs(**state.inputs)
        assert {k: v for k, v in self._current_pipeline_inputs.items() if v not in [NONE_VALUE_ID, NOT_SET_VALUE_ID]} == {k: v for k, v in state.inputs.items() if v not in [NONE_VALUE_ID, NOT_SET_VALUE_ID]}  # type: ignore
        self._current_pipeline_outputs = (
            state.pipeline_info.pipeline_state.pipeline_outputs
        )
        self._pipeline_info = state.pipeline_info
        self._current_state = state
        self._current_info = None

        return state

    @property
    def all_state_ids(self) -> List[str]:

        hashes = set(self._workflow_metadata.workflow_history.values())
        return sorted(hashes)

    @property
    def all_states(self) -> Mapping[str, WorkflowState]:
        """Return a list of all states this workflow had in the past, indexed by the hash of each state."""
        missing = []
        for state_id in self.workflow_metadata.workflow_history.values():
            if state_id not in self._state_cache.keys():
                missing.append(state_id)

        if missing:
            # TODO: only request missing ones?
            all_states = self._kiara.workflow_registry.get_all_states_for_workflow(
                workflow=self.workflow_id
            )
            self._state_cache.update(all_states)

        return self._state_cache

    @property
    def info(self) -> WorkflowInfo:

        if self._current_info is not None:
            return self._current_info

        self._current_info = WorkflowInfo.create_from_workflow(workflow=self)
        return self._current_info

    @property
    def pipeline_info(self) -> PipelineInfo:

        if self._pipeline_info is not None:
            return self._pipeline_info

        self._pipeline_info = PipelineInfo.create_from_pipeline(
            kiara=self._kiara, pipeline=self.pipeline
        )
        return self._pipeline_info

    def save(self, *aliases: str):

        self._pending_aliases.update(aliases)

        self._workflow_metadata = self._kiara.workflow_registry.register_workflow(
            workflow_metadata=self.workflow_metadata,
            workflow_aliases=self._pending_aliases,
        )
        self._pending_aliases.clear()
        self._metadata_is_stored = True
        self._metadata_is_synced = True

    def snapshot(self, save: bool = False) -> WorkflowState:

        state = self.current_state

        if state.instance_id not in self._state_cache.keys():
            self._state_cache[state.instance_id] = state

        now = get_current_time_incl_timezone()

        for field_name, value in self.current_pipeline_outputs.items():
            if value in [NOT_SET_VALUE_ID, NONE_VALUE_ID]:
                continue

            self._state_output_cache.setdefault(state.instance_id, set()).add(value)
            self._state_jobrecord_cache.setdefault(state.instance_id, set()).add(
                self._job_id_cache[value]
            )

        self.workflow_metadata.workflow_history[now] = state.instance_id
        self._metadata_is_synced = False

        if save:
            self.register_snapshot(snapshot=state.instance_id)
        return state

    def register_snapshot(self, snapshot: Union[datetime, str]):

        timestamps: List[datetime]
        if isinstance(snapshot, str):
            if snapshot not in self._state_cache.keys():
                raise Exception(
                    f"Can't register snapshot with hash '{snapshot}': no state with this hash available."
                )
            state: WorkflowState = self._state_cache[snapshot]
            timestamps = [
                _timestamp
                for _timestamp, _hash in self.workflow_metadata.workflow_history.items()
                if _hash == snapshot
            ]
        elif isinstance(snapshot, datetime):
            if snapshot not in self.workflow_metadata.workflow_history.keys():
                raise Exception(
                    f"Can't register snapshot with timestamp '{snapshot}': no state with this timestamp available."
                )
            state = self._state_cache[self.workflow_metadata.workflow_history[snapshot]]
            timestamps = [snapshot]
        else:
            raise Exception(
                f"Can't register snapshot '{snapshot}': invalid type '{type(snapshot)}'."
            )

        # input values are stored in the add_workflow_state method on the backend

        if state.instance_id in self._state_output_cache.keys():
            for value_id in self._state_output_cache[state.instance_id]:
                self._kiara.data_registry.store_value(value=value_id)

        if state.instance_id in self._state_jobrecord_cache.keys():
            for job_id in self._state_jobrecord_cache[state.instance_id]:
                try:
                    self._kiara.job_registry.store_job_record(job_id=job_id)
                except Exception as e:
                    log_exception(e)

        if not self._metadata_is_stored:
            self._sync_workflow_metadata()
            self._metadata_is_synced = True

        if not self._metadata_is_synced:
            self._kiara.workflow_registry.update_workflow_metadata(
                self.workflow_metadata
            )

        for timestamp in timestamps:
            self._workflow_metadata = self._kiara.workflow_registry.add_workflow_state(
                workflow=self._workflow_metadata.workflow_id,
                workflow_state=state,
                timestamp=timestamp,
            )
            self._metadata_is_synced = True

        return state

    def create_renderable(self, **config: Any):

        if not self._steps:
            return "Invalid workflow: no steps set yet."

        return self.info.create_renderable(**config)


# kiara\kiara\src\kiara\interfaces\python_api\__init__.py
# -*- coding: utf-8 -*-
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

# If you are looking for the `KiaraAPI` class, this has moved into the `kiara_api` sub-module.


# kiara\kiara\src\kiara\interfaces\python_api\models\archive.py
# -*- coding: utf-8 -*-
import uuid
from pathlib import Path
from typing import TYPE_CHECKING, Any, ClassVar, Mapping, Union

from pydantic import Field, PrivateAttr

from kiara.defaults import CHUNK_COMPRESSION_TYPE, DEFAULT_CHUNK_COMPRESSION
from kiara.models import KiaraModel

if TYPE_CHECKING:
    from kiara.context import Kiara
    from kiara.registries.aliases import AliasArchive, AliasStore
    from kiara.registries.data import DataArchive, DataStore
    from kiara.registries.jobs import JobArchive, JobStore
    from kiara.registries.metadata import MetadataArchive, MetadataStore


class KiArchive(KiaraModel):
    @classmethod
    def load_kiarchive(
        cls,
        kiara: "Kiara",
        path: Union[str, Path],
        allow_write_access: bool = False,
        archive_name: Union[str, None] = None,
    ) -> "KiArchive":

        if isinstance(path, Path):
            path = path.as_posix()

        archive_path = Path(path).absolute()

        if not archive_path.is_file():
            raise FileNotFoundError(f"Archive file '{path}' does not exist.")

        from kiara.utils.stores import check_external_archive

        archives = check_external_archive(
            archive=archive_path.as_posix(),
            allow_write_access=allow_write_access,
            archive_name=archive_name,
        )

        if "metadata" in archives.keys():
            metadata_archive: Union[MetadataArchive, None] = archives["metadata"]  # type: ignore
            metadata_archive_config: Union[Mapping[str, Any], None] = metadata_archive.config.model_dump()  # type: ignore
        else:
            metadata_archive_config = None
            metadata_archive = None

        if "data" in archives.keys():
            data_archive: Union[DataArchive, None] = archives["data"]  # type: ignore
            data_archive_config: Union[Mapping[str, Any], None] = data_archive.config.model_dump()  # type: ignore
        else:
            data_archive_config = None
            data_archive = None

        if "alias" in archives.keys():
            alias_archive: Union[AliasArchive, None] = archives["alias"]  # type: ignore
            alias_archive_config: Union[Mapping[str, Any], None] = alias_archive.config.model_dump()  # type: ignore
        else:
            alias_archive_config = None
            alias_archive = None

        if "job_record" in archives.keys():
            jobs_archive: Union[JobArchive, None] = archives["job_record"]  # type: ignore
            jobs_archive_config: Union[Mapping[str, Any], None] = jobs_archive.config.model_dump()  # type: ignore
        else:
            jobs_archive_config = None
            jobs_archive = None

        _archives = [
            x
            for x in (data_archive, alias_archive, metadata_archive, jobs_archive)
            if x is not None
        ]
        if not _archives:
            raise Exception(f"No archive found in file: {path}")
        else:
            archive_id = _archives[0].archive_id
            archive_alias = _archives[0].archive_name
            for archive in _archives:
                if archive.archive_id != archive_id:
                    raise Exception(
                        f"Multiple different archive ids found in file: {path}"
                    )
                if archive.archive_name != archive_alias:
                    raise Exception(
                        f"Multiple different archive aliases found in file: {path}"
                    )

        kiarchive = KiArchive(
            archive_id=archive_id,
            archive_name=archive_alias,
            metadata_archive_config=metadata_archive_config,
            data_archive_config=data_archive_config,
            alias_archive_config=alias_archive_config,
            job_archive_config=jobs_archive_config,
            archive_base_path=archive_path.parent.as_posix(),
            archive_file_name=archive_path.name,
            allow_write_access=allow_write_access,
        )

        kiarchive._metadata_archive = metadata_archive
        kiarchive._data_archive = data_archive
        kiarchive._alias_archive = alias_archive
        kiarchive._jobs_archive = jobs_archive
        kiarchive._kiara = kiara

        return kiarchive

    @classmethod
    def create_kiarchive(
        cls,
        kiara: "Kiara",
        kiarchive_uri: Union[str, Path],
        archive_name: Union[str, None] = None,
        compression: Union[None, CHUNK_COMPRESSION_TYPE, str] = None,
        allow_write_access: bool = True,
        allow_existing: bool = False,
    ) -> "KiArchive":

        if compression is None:
            compression = DEFAULT_CHUNK_COMPRESSION

        if isinstance(kiarchive_uri, str):
            kiarchive_uri = Path(kiarchive_uri)

        if not archive_name:
            archive_name = kiarchive_uri.name
            if archive_name.endswith(".kiarchive"):
                archive_name = archive_name[:-10]

        if kiarchive_uri.exists():
            if not allow_existing:
                raise FileExistsError(f"Archive file '{kiarchive_uri}' already exists.")
            kiarchive = cls.load_kiarchive(
                kiara=kiara, path=kiarchive_uri, allow_write_access=allow_write_access
            )
        else:
            from kiara.utils.stores import create_new_archive

            archive_base_path = kiarchive_uri.parent.as_posix()
            archive_file_name = kiarchive_uri.name

            if isinstance(compression, str):
                compression = CHUNK_COMPRESSION_TYPE[compression.upper()]

            data_store: DataStore = create_new_archive(  # type: ignore
                archive_name=archive_name,
                store_base_path=archive_base_path,
                store_type="sqlite_data_store",
                file_name=archive_file_name,
                default_chunk_compression=str(compression.name),
                allow_write_access=allow_write_access,
            )
            data_store_config = data_store.config

            metadata_store: MetadataStore = create_new_archive(  # type: ignore
                archive_name=archive_name,
                store_base_path=archive_base_path,
                store_type="sqlite_metadata_store",
                file_name=archive_file_name,
                allow_write_access=True,
                set_archive_name_metadata=False,
            )
            metadata_store_config = metadata_store.config

            alias_store: AliasStore = create_new_archive(  # type: ignore
                archive_name=archive_name,
                store_base_path=archive_base_path,
                store_type="sqlite_alias_store",
                file_name=archive_file_name,
                allow_write_access=allow_write_access,
                set_archive_name_metadata=False,
            )
            alias_store_config = alias_store.config

            job_store: JobStore = create_new_archive(  # type: ignore
                archive_name=archive_name,
                store_base_path=archive_base_path,
                store_type="sqlite_job_store",
                file_name=archive_file_name,
                allow_write_access=allow_write_access,
                set_archive_name_metadata=False,
            )
            job_store_config = job_store.config

            kiarchive_id = data_store.archive_id
            assert alias_store.archive_id == kiarchive_id
            assert metadata_store.archive_id == kiarchive_id
            assert job_store.archive_id == kiarchive_id

            kiarchive = KiArchive(
                archive_id=kiarchive_id,
                archive_name=archive_name,
                archive_base_path=archive_base_path,
                archive_file_name=archive_file_name,
                metadata_archive_config=metadata_store_config.model_dump(),
                data_archive_config=data_store_config.model_dump(),
                alias_archive_config=alias_store_config.model_dump(),
                job_archive_config=job_store_config.model_dump(),
                allow_write_access=allow_write_access,
            )
            kiarchive._metadata_archive = metadata_store
            kiarchive._data_archive = data_store
            kiarchive._alias_archive = alias_store
            kiarchive._jobs_archive = job_store
            kiarchive._kiara = kiara

        return kiarchive

    _kiara_model_id: ClassVar = "instance.kiarchive"

    archive_id: uuid.UUID = Field(description="The unique identifier of the archive.")
    archive_name: str = Field(description="The alias of the archive.")
    archive_base_path: str = Field(description="The base path/uri of the store.")
    archive_file_name: str = Field(description="The (file-)name of the store.")
    allow_write_access: bool = Field(
        description="Whether the store allows write access.", default=False
    )
    metadata_archive_config: Union[Mapping[str, Any], None] = Field(
        description="The archive to store metadata in.", default=None
    )
    data_archive_config: Union[Mapping[str, Any], None] = Field(
        description="The archive to store the data in.", default=None
    )
    alias_archive_config: Union[Mapping[str, Any], None] = Field(
        description="The archive to store aliases in.", default=None
    )
    job_archive_config: Union[Mapping[str, Any], None] = Field(
        description="The archive to store jobs in.", default=None
    )

    _metadata_archive: Union["MetadataArchive", None] = PrivateAttr(default=None)
    _data_archive: Union["DataArchive", None] = PrivateAttr(default=None)
    _alias_archive: Union["AliasArchive", None] = PrivateAttr(default=None)
    _jobs_archive: Union["JobArchive", None] = PrivateAttr(default=None)

    _kiara: Union["Kiara", None] = PrivateAttr(default=None)

    @property
    def metadata_archive(self) -> Union["MetadataArchive", None]:

        if self._metadata_archive:
            return self._metadata_archive

        if self.metadata_archive_config is None:
            return None

        from kiara.utils.stores import create_new_archive

        metadata_archive: MetadataArchive = create_new_archive(  # type: ignore
            archive_name=self.archive_name,
            store_base_path=self.archive_base_path,
            store_type="sqlite_metadata_store",
            file_name=self.archive_file_name,
            allow_write_access=True,
            **self.metadata_archive_config,
        )
        self._metadata_archive = metadata_archive
        return self._metadata_archive

    @property
    def data_archive(self) -> Union["DataArchive", None]:

        if self._data_archive:
            return self._data_archive

        if self.data_archive_config is None:
            return None

        from kiara.utils.stores import create_new_archive

        data_archive: DataArchive = create_new_archive(  # type: ignore
            archive_name=self.archive_name,
            store_base_path=self.archive_base_path,
            store_type="sqlite_data_store",
            file_name=self.archive_file_name,
            allow_write_access=True,
            **self.data_archive_config,
        )
        self._data_archive = data_archive
        return self._data_archive

    @property
    def alias_archive(self) -> Union["AliasArchive", None]:

        if self._alias_archive is not None:
            return self._alias_archive

        if self.alias_archive_config is None:
            return None

        from kiara.utils.stores import create_new_archive

        alias_archive: AliasStore = create_new_archive(  # type: ignore
            archive_name=self.archive_name,
            store_base_path=self.archive_base_path,
            store_type="sqlite_alias_store",
            file_name=self.archive_file_name,
            allow_write_access=True,
        )
        self._alias_archive = alias_archive
        return self._alias_archive

    @property
    def job_archive(self) -> Union["JobArchive", None]:

        if self._jobs_archive is not None:
            return self._jobs_archive

        if self.job_archive_config is None:
            return None

        from kiara.utils.stores import create_new_archive

        jobs_archive: JobStore = create_new_archive(  # type: ignore
            archive_name=self.archive_name,
            store_base_path=self.archive_base_path,
            store_type="sqlite_job_store",
            file_name=self.archive_file_name,
            allow_write_access=True,
        )
        self._jobs_archive = jobs_archive
        return self._jobs_archive


# kiara\kiara\src\kiara\interfaces\python_api\models\doc.py
# -*- coding: utf-8 -*-
import collections.abc
from typing import TYPE_CHECKING, Dict

from pydantic import RootModel

if TYPE_CHECKING:
    # we don't want those imports (yet), since they take a while to load
    from kiara.interfaces.python_api.workflow import Workflow
    from kiara.models.module.operation import Operation
    from kiara.models.module.pipeline import PipelineStructure


#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)


class OperationsMap(RootModel, collections.abc.Mapping):
    """A list of available context names."""

    root: Dict[str, "Operation"]

    def __getitem__(self, key):
        return self.root.__getitem__(key)

    def __iter__(self):
        return self.root.__iter__()

    def __len__(self):
        return self.root.__len__()


class PipelinesMap(RootModel, collections.abc.Mapping):
    """A list of available context names."""

    root: Dict[str, "PipelineStructure"]

    def __getitem__(self, key):
        return self.root.__getitem__(key)

    def __iter__(self):
        return self.root.__iter__()

    def __len__(self):
        return self.root.__len__()


class WorkflowsMap(RootModel, collections.abc.Mapping):
    """A list of available context names."""

    root: Dict[str, "Workflow"]

    def __getitem__(self, key):
        return self.root.__getitem__(key)

    def __iter__(self):
        return self.root.__iter__()

    def __len__(self):
        return self.root.__len__()


# kiara\kiara\src\kiara\interfaces\python_api\models\info.py
# -*- coding: utf-8 -*-

import abc
import importlib
import inspect
import re
import textwrap
import uuid
from datetime import datetime
from typing import (
    TYPE_CHECKING,
    Any,
    Callable,
    ClassVar,
    Dict,
    Generic,
    Iterable,
    List,
    Literal,
    Mapping,
    Type,
    TypeVar,
    Union,
)

import humanfriendly
import orjson
from pydantic import BaseModel, Field, PrivateAttr, field_validator
from rich import box
from rich.console import RenderableType
from rich.markdown import Markdown
from rich.panel import Panel
from rich.syntax import Syntax
from rich.table import Table
from rich.tree import Tree

from kiara.defaults import DEFAULT_NO_DESC_VALUE
from kiara.exceptions import KiaraException
from kiara.models import KiaraModel
from kiara.models.documentation import (
    AuthorModel,
    AuthorsMetadataModel,
    ContextMetadataModel,
    DocumentationMetadataModel,
)
from kiara.models.module import KiaraModuleConfig
from kiara.models.module.jobs import JobRecord
from kiara.models.module.operation import Operation
from kiara.models.module.pipeline import PipelineConfig, PipelineStep
from kiara.models.module.pipeline.structure import (
    PipelineStructure,
    StepInfo,
)
from kiara.models.module.pipeline.value_refs import PipelineInputRef, PipelineOutputRef
from kiara.models.python_class import PythonClass
from kiara.models.values import DataTypeCharacteristics, ValueStatus
from kiara.models.values.lineage import ValueLineage
from kiara.models.values.value import (
    ORPHAN,
    DataTypeInfo,
    PersistedData,
    Value,
    ValueMap,
    ValuePedigree,
)
from kiara.models.values.value_schema import ValueSchema
from kiara.modules import KiaraModule
from kiara.renderers import KiaraRenderer
from kiara.utils import log_exception, log_message
from kiara.utils.class_loading import find_all_kiara_model_classes
from kiara.utils.cli import HORIZONTALS_NO_TO_AND_BOTTOM
from kiara.utils.dates import to_human_readable_date_string
from kiara.utils.json import orjson_dumps
from kiara.utils.output import extract_renderable

try:
    from typing import Self  # type: ignore
except ImportError:
    from typing_extensions import Self  # type: ignore


if TYPE_CHECKING:
    from kiara.context import Kiara
    from kiara.data_types import DataType
    from kiara.models.runtime_environment.python import PythonRuntimeEnvironment
    from kiara.models.values.value_metadata import ValueMetadata
    from kiara.operations import OperationType
    from kiara.registries.aliases import AliasRegistry
    from kiara.registries.data import DataRegistry

INFO_BASE_INSTANCE_TYPE = TypeVar("INFO_BASE_INSTANCE_TYPE")
INFO_BASE_CLASS = TypeVar("INFO_BASE_CLASS", bound=type)


def pretty_print_value_data_terminal(value: "ValueInfo"):

    try:
        renderable = value._value._data_registry.pretty_print_data(
            value.value_id, target_type="terminal_renderable"
        )
    except Exception as e:
        log_exception(e)
        log_message("error.pretty_print", value=value.value_id, error=e)
        renderable = [str(value._value.data)]

    return renderable


RENDER_FIELDS: Dict[str, Dict[str, Any]] = {
    "value_id": {
        "show_default": True,
        "render": {"terminal": lambda v: str(v.value_id)},
    },
    "aliases": {
        "show_default": True,
        "render": {"terminal": lambda v: ", ".join(v.aliases)},
    },
    "type": {
        "show_default": True,
        "render": {"terminal": lambda x: x.value_schema.type},
    },
    "value_schema": {"show_default": False},
    "value_created": {
        "show_default": False,
        "render": {
            "terminal": lambda v: f"{to_human_readable_date_string(v.value_created)} ago"
        },
    },
    "is_persisted": {
        "show_default": False,
        "render": {"terminal": lambda v: "yes" if v.is_persisted else "no"},
    },
    "hash": {"show_default": False, "render": {"terminal": lambda v: v.value_hash}},
    "data": {
        "show_default": False,
        "render": {"terminal": pretty_print_value_data_terminal},
    },
    "pedigree": {
        "show_default": False,
        "render": {"terminal": lambda v: "-- external data -- " if v == ORPHAN else v},
    },
    "lineage": {"show_default": False},
    "load_config": {"show_default": False},
    "data_type_config": {
        "show_default": False,
        "render": {
            "terminal": lambda v: Syntax(
                orjson_dumps(v.value_schema.type_config, option=orjson.OPT_INDENT_2),
                "json",
                background_color="default",
            )
        },
    },
    "serialize_details": {
        "show_default": False,
        "render": {"terminal": lambda v: v.serialized.create_renderable()},
    },
    "properties": {
        "show_default": False,
        "render": {
            "terminal": lambda v: v.property_values.create_renderable(show_header=False)
        },
    },
    "size": {
        "show_default": True,
        "render": {"terminal": lambda v: humanfriendly.format_size(v.value_size)},
    },
}


class ValueTypeAndDescription(BaseModel):

    description: str = Field(description="The description for the value.")
    type: str = Field(description="The value type.")
    value_default: Any = Field(description="Default for the value.", default=None)
    required: bool = Field(description="Whether this value is required")


class ItemInfo(KiaraModel):
    """Base class that holds/manages information about an item within kiara."""

    @classmethod
    @abc.abstractmethod
    def base_instance_class(cls) -> Type:
        pass

    @classmethod
    @abc.abstractmethod
    def create_from_instance(cls, kiara: "Kiara", instance: Any, **kwargs):
        pass

    @field_validator("documentation", mode="before")
    @classmethod
    def validate_doc(cls, value):

        return DocumentationMetadataModel.create(value)

    type_name: str = Field(description="The registered name for this item type.")
    documentation: DocumentationMetadataModel = Field(
        description="Documentation for the item."
    )
    authors: AuthorsMetadataModel = Field(
        description="Information about authorship for the item."
    )
    context: ContextMetadataModel = Field(
        description="Generic properties of this item (description, tags, labels, references, ...)."
    )

    def _retrieve_id(self) -> str:
        return self.type_name

    def _retrieve_data_to_hash(self) -> Any:
        return self.type_name

    def create_renderable(self, **config: Any) -> RenderableType:

        include_doc = config.get("include_doc", True)

        table = Table(box=box.SIMPLE, show_header=False, padding=(0, 0, 0, 0))
        table.add_column("property", style="i")
        table.add_column("value")

        if include_doc:

            table.add_row(
                "Documentation",
                Panel(self.documentation.create_renderable(), box=box.SIMPLE),
            )
        table.add_row("Author(s)", self.authors.create_renderable())
        table.add_row("Context", self.context.create_renderable())

        if hasattr(self, "python_class"):
            table.add_row("Python class", self.python_class.create_renderable())  # type: ignore

        return table


class TypeInfo(ItemInfo, Generic[INFO_BASE_CLASS]):
    @classmethod
    def create_from_instance(cls, kiara: "Kiara", instance: INFO_BASE_CLASS, **kwargs):

        return cls.create_from_type_class(type_cls=instance, kiara=kiara)

    @classmethod
    @abc.abstractmethod
    def create_from_type_class(
        self, type_cls: INFO_BASE_CLASS, kiara: "Kiara"
    ) -> "TypeInfo":
        pass

    @classmethod
    def base_instance_class(self) -> INFO_BASE_CLASS:
        return type  # type: ignore

    python_class: PythonClass = Field(
        description="The python class that implements this module type."
    )


INFO_ITEM_TYPE = TypeVar("INFO_ITEM_TYPE", bound=ItemInfo)


class InfoItemGroup(KiaraModel, Generic[INFO_ITEM_TYPE]):
    @classmethod
    @abc.abstractmethod
    def base_info_class(cls) -> Type[INFO_ITEM_TYPE]:
        pass

    @classmethod
    def create_from_instances(
        cls,
        kiara: "Kiara",
        instances: Mapping[str, Any],
        **kwargs: Any,
    ) -> "InfoItemGroup[INFO_ITEM_TYPE]":

        info_cls = cls.base_info_class()
        items = {}
        for k in sorted(instances.keys()):
            v = instances[k]
            items[k] = info_cls.create_from_instance(kiara=kiara, instance=v, **kwargs)

        group_title = kwargs.pop("group_title", None)
        result = cls(group_title=group_title, item_infos=items)
        result._kiara = kiara
        return result

    group_title: Union[str, None] = Field(description="The group alias.", default=None)
    item_infos: Mapping[str, INFO_ITEM_TYPE] = Field(description="The info items.")
    _kiara: Union["Kiara", None] = PrivateAttr(default=None)

    def _retrieve_subcomponent_keys(self) -> Iterable[str]:
        return self.item_infos.keys()

    def _retrieve_data_to_hash(self) -> Any:
        return {"type_name": self.__class__._kiara_model_name, "included_types": list(self.item_infos.keys())}  # type: ignore

    def create_renderable(self, **config: Any) -> RenderableType:

        full_doc = config.get("full_doc", False)

        table = Table(show_header=True, box=box.SIMPLE, show_lines=full_doc)
        table.add_column("Name", style="i")
        table.add_column("Description")

        for type_name in sorted(self.item_infos.keys()):
            t_md = self.item_infos[type_name]
            if full_doc:
                md = Markdown(t_md.documentation.full_doc)
            else:
                md = Markdown(t_md.documentation.description)
            table.add_row(type_name, md)

        return table

    def __getitem__(self, item: str) -> INFO_ITEM_TYPE:

        return self.item_infos[item]

    # def __iter__(self):
    #     return self.item_infos.__iter__()

    def __len__(self):
        return len(self.item_infos)


class TypeInfoItemGroup(InfoItemGroup[TypeInfo]):
    @classmethod
    @abc.abstractmethod
    def base_info_class(cls) -> Type[TypeInfo]:
        pass

    @classmethod
    def create_from_type_items(
        cls, kiara: "Kiara", group_title: Union[str, None] = None, **items: Type
    ) -> Self:

        type_infos: Mapping[str, TypeInfo[Any]] = {
            k: cls.base_info_class().create_from_type_class(type_cls=v, kiara=kiara)
            for k, v in items.items()
        }
        data_types_info = cls(group_title=group_title, item_infos=type_infos)
        return data_types_info


class KiaraModelTypeInfo(TypeInfo[Type[KiaraModel]]):

    _kiara_model_id: ClassVar = "info.kiara_model"

    @classmethod
    def create_from_type_class(
        self, type_cls: Type[KiaraModel], kiara: "Kiara"
    ) -> "KiaraModelTypeInfo":

        authors_md = AuthorsMetadataModel.from_class(type_cls)
        doc = DocumentationMetadataModel.from_class_doc(type_cls)
        python_class = PythonClass.from_class(type_cls)
        properties_md = ContextMetadataModel.from_class(type_cls)
        type_name = type_cls._kiara_model_id  # type: ignore
        schema = type_cls.model_json_schema()

        return KiaraModelTypeInfo(
            type_name=type_name,
            documentation=doc,
            authors=authors_md,
            context=properties_md,
            python_class=python_class,
            metadata_schema=schema,
        )

    metadata_schema: Dict[str, Any] = Field(
        description="The (json) schema for this model data."
    )

    def create_renderable(self, **config: Any) -> RenderableType:

        include_doc = config.get("include_doc", True)
        include_schema = config.get("include_schema", False)

        table = Table(box=box.SIMPLE, show_header=False, padding=(0, 0, 0, 0))
        table.add_column("property", style="i")
        table.add_column("value")

        if include_doc:
            table.add_row(
                "Documentation",
                Panel(self.documentation.create_renderable(), box=box.SIMPLE),
            )
        table.add_row("Author(s)", self.authors.create_renderable())
        table.add_row("Context", self.context.create_renderable())

        if hasattr(self, "python_class"):
            table.add_row("Python class", self.python_class.create_renderable())

        if include_schema:
            schema = Syntax(
                orjson_dumps(self.metadata_schema, option=orjson.OPT_INDENT_2),
                "json",
                background_color="default",
            )
            table.add_row("metadata_schema", schema)

        return table


class KiaraModelClassesInfo(TypeInfoItemGroup):

    _kiara_model_id: ClassVar = "info.kiara_models"

    @classmethod
    def find_kiara_models(
        cls, alias: Union[str, None] = None, only_for_package: Union[str, None] = None
    ) -> "KiaraModelClassesInfo":

        models = find_all_kiara_model_classes()

        # we don't need the kiara instance, this is just to satisfy mypy
        kiara: Kiara = None  # type: ignore
        group: KiaraModelClassesInfo = KiaraModelClassesInfo.create_from_type_items(kiara=kiara, group_title=alias, **models)  # type: ignore

        if only_for_package:
            temp = {}
            for key, info in group.item_infos.items():
                if info.context.labels.get("package") == only_for_package:
                    temp[key] = info

            group = KiaraModelClassesInfo(
                group_id=group.instance_id, group_title=group.group_alias, item_infos=temp  # type: ignore
            )

        return group

    @classmethod
    def base_info_class(cls) -> Type[KiaraModelTypeInfo]:
        return KiaraModelTypeInfo  # type: ignore

    type_name: Literal["kiara_model"] = "kiara_model"
    item_infos: Mapping[str, KiaraModelTypeInfo] = Field(  # type: ignore
        description="The value metadata info instances for each type."
    )


class ValueInfo(ItemInfo):

    _kiara_model_id: ClassVar = "info.value"

    @classmethod
    def base_instance_class(cls) -> Type[Value]:
        return Value

    @classmethod
    def create_from_instance(
        cls, kiara: "Kiara", instance: Value, **kwargs: Any
    ) -> "ValueInfo":

        resolve_aliases = kwargs.get("resolve_aliases", True)
        resolve_destinies = kwargs.get("resolve_destinies", True)
        resolve_properties = kwargs.get("resolve_properties", True)

        if resolve_aliases:
            aliases = sorted(
                kiara.alias_registry.find_aliases_for_value_id(instance.value_id)
            )
        else:
            aliases = None

        if instance.is_stored:
            persisted_details = kiara.data_registry.retrieve_persisted_value_details(
                value_id=instance.value_id
            )
        else:
            persisted_details = None

        if instance.data_type_name in kiara.type_registry.data_type_profiles:
            is_internal = "internal" in kiara.type_registry.get_type_lineage(
                instance.data_type_name
            )
        else:
            is_internal = False

        if resolve_destinies:
            destiny_links = (
                kiara.data_registry.retrieve_destinies_for_value_from_archives(
                    value_id=instance.value_id
                )
            )
            filtered_destinies = {}
            for alias, value_id in destiny_links.items():
                if (
                    alias in instance.property_links.keys()
                    and value_id == instance.property_links[alias]
                ):
                    continue
                filtered_destinies[alias] = value_id
        else:
            filtered_destinies = None

        if resolve_properties:
            properties = instance.get_all_property_data()
        else:
            properties = None

        authors = AuthorsMetadataModel()
        context = ContextMetadataModel()
        doc = DocumentationMetadataModel()

        model = ValueInfo(
            type_name=str(instance.value_id),
            documentation=doc,
            authors=authors,
            context=context,
            value_id=instance.value_id,
            kiara_id=instance.kiara_id,
            value_schema=instance.value_schema,
            value_created=instance.value_created,
            value_status=instance.value_status,
            environment_hashes=instance.environment_hashes,
            value_size=instance.value_size,
            value_hash=instance.value_hash,
            pedigree=instance.pedigree,
            pedigree_output_name=instance.pedigree_output_name,
            data_type_info=instance.data_type_info,
            # data_type_config=instance.data_type_config,
            # data_type_class=instance.data_type_class,
            property_links=instance.property_links,
            destiny_links=filtered_destinies,
            destiny_backlinks=instance.destiny_backlinks,
            aliases=aliases,
            serialized=persisted_details,
            properties=properties,
            is_internal=is_internal,
            is_persisted=instance._is_stored,
        )
        model._value = instance
        model._alias_registry = kiara.alias_registry  # type: ignore
        model._data_registry = instance._data_registry
        return model

    value_id: uuid.UUID = Field(description="The id of the value.")

    kiara_id: uuid.UUID = Field(
        description="The id of the kiara context this value belongs to."
    )

    value_schema: ValueSchema = Field(
        description="The schema that was used for this Value."
    )
    value_created: datetime = Field(description="The time this value was created.")
    value_status: ValueStatus = Field(description="The set/unset status of this value.")
    value_size: int = Field(description="The size of this value, in bytes.")
    value_hash: str = Field(description="The hash of this value.")
    pedigree: ValuePedigree = Field(
        description="Information about the module and inputs that went into creating this value."
    )
    pedigree_output_name: str = Field(
        description="The output name that produced this value (using the manifest inside the pedigree)."
    )
    data_type_info: DataTypeInfo = Field(
        description="Information about the underlying data type and it's configuration."
    )
    aliases: Union[List[str], None] = Field(
        description="The aliases that are registered for this value."
    )
    serialized: Union[PersistedData, None] = Field(
        description="Details for the serialization process that was used for this value."
    )
    properties: Union[Mapping[str, Any], None] = Field(
        description="Property data for this value.", default=None
    )
    destiny_links: Union[Mapping[str, uuid.UUID], None] = Field(
        description="References to all the values that act as destiny for this value in this context."
    )
    environment_hashes: Mapping[str, Mapping[str, str]] = Field(
        description="Hashes for the environments this value was created in."
    )
    enviroments: Union[Mapping[str, Mapping[str, Any]], None] = Field(
        description="Information about the environments this value was created in.",
        default=None,
    )
    property_links: Mapping[str, uuid.UUID] = Field(
        description="Links to values that are properties of this value.",
        default_factory=dict,
    )
    destiny_backlinks: Mapping[uuid.UUID, str] = Field(
        description="Backlinks to values that this value acts as destiny/or property for.",
        default_factory=dict,
    )
    is_internal: bool = Field(
        description="Whether this value is only used internally in kiara.",
        default=False,
    )
    is_persisted: bool = Field(
        description="Whether this value is stored in at least one data store."
    )
    _alias_registry: "AliasRegistry" = PrivateAttr(default=None)
    _data_registry: "DataRegistry" = PrivateAttr(default=None)
    _value: Value = PrivateAttr(default=None)

    def _retrieve_id(self) -> str:
        return str(self.value_id)

    def _retrieve_data_to_hash(self) -> Any:
        return self.value_id.bytes

    @property
    def property_values(self) -> "ValueMap":
        return self._value.property_values

    @property
    def lineage(self) -> "ValueLineage":
        return self._value.lineage

    def resolve_aliases(self):
        if self.aliases is None:
            aliases = self._alias_registry.find_aliases_for_value_id(self.value_id)
            if aliases:
                aliases = sorted(aliases)
            self.aliases = aliases

    def resolve_destinies(self):
        if self.destiny_links is None:
            destiny_links = (
                self._value._data_registry.retrieve_destinies_for_value_from_archives(
                    value_id=self.value_id
                )
            )
            filtered_destinies = {}
            for alias, value_id in destiny_links.items():
                if (
                    alias in self.property_links.keys()
                    and value_id == self.property_links[alias]
                ):
                    continue
                filtered_destinies[alias] = value_id
            self.destiny_links = filtered_destinies

    def create_info_data(self, **config: Any) -> Mapping[str, Any]:

        return self._value.create_info_data(**config)

    def create_renderable(self, **render_config: Any) -> RenderableType:

        return self._value.create_renderable(**render_config)


class ValuesInfo(InfoItemGroup[ValueInfo]):
    @classmethod
    def base_info_class(cls) -> Type[ValueInfo]:
        return ValueInfo

    def create_render_map(
        self, render_type: str, default_render_func: Callable, **render_config
    ):

        list_by_alias = render_config.get("list_by_alias", True)
        show_internal = render_config.get("show_internal_values", False)

        render_fields = render_config.get("render_fields", None)
        if not render_fields:
            render_fields = [k for k, v in RENDER_FIELDS.items() if v["show_default"]]
            if list_by_alias:
                render_fields[0] = "aliases"
                render_fields[1] = "value_id"

        render_map: Dict[uuid.UUID, Dict[str, Any]] = {}

        lookup = {}
        value_info: ValueInfo
        for value_info in self.item_infos.values():  # type: ignore
            if not show_internal and value_info.is_internal:
                continue
            lookup[value_info.value_id] = value_info

            details = {}
            for property in render_fields:

                render_func = (
                    RENDER_FIELDS.get(property, {})
                    .get("render", {})
                    .get(render_type, None)
                )
                if render_func is None:
                    if hasattr(value_info, property):
                        attr = getattr(value_info, property)
                        rendered = default_render_func(attr)
                    else:
                        raise Exception(
                            f"Can't render property '{property}': no render function registered and not a property."
                        )
                else:
                    rendered = render_func(value_info)
                details[property] = rendered
            render_map[value_info.value_id] = details

        if not list_by_alias:
            return {str(k): v for k, v in render_map.items()}
        else:
            result: Dict[str, Dict[str, Any]] = {}
            for value_id, render_details in render_map.items():
                value_aliases = lookup[value_id].aliases
                if value_aliases:
                    for alias in value_aliases:
                        assert alias not in result.keys()
                        render_details = dict(render_details)
                        render_details["alias"] = alias
                        result[alias] = render_details
                else:
                    render_details["alias"] = ""
                    result[f"no_aliases_{value_id}"] = render_details
            return result

    def create_renderable(self, render_type: str = "terminal", **render_config: Any):

        render_map = self.create_render_map(
            render_type=render_type,
            default_render_func=extract_renderable,
            **render_config,
        )

        list_by_alias = render_config.get("list_by_alias", True)
        render_fields = render_config.get("render_fields", None)
        if not render_fields:
            render_fields = [k for k, v in RENDER_FIELDS.items() if v["show_default"]]
        if list_by_alias:
            render_fields.insert(0, "alias")
            render_fields.remove("aliases")

        table = Table(box=box.SIMPLE)
        for property in render_fields:
            if property == "aliases" and list_by_alias:
                table.add_column("alias")
            elif property == "size":
                table.add_column("size", justify="right")
            else:
                table.add_column(property)

        for item_id, details in render_map.items():
            row = []
            for field in render_fields:
                value = details[field]
                row.append(value)
            table.add_row(*row)

        return table


class KiaraModuleConfigMetadata(KiaraModel):

    _kiara_model_id: ClassVar = "metadata.module_config"

    @classmethod
    def from_config_class(
        cls,
        config_cls: Type[KiaraModuleConfig],
    ):

        schema = config_cls.model_json_schema()
        fields = schema["properties"]

        config_values = {}
        for field_name, details in fields.items():

            type_str = "unknown"
            if "type" in details.keys():
                type_str = details["type"]
            elif "anyOf" in details.keys():
                type_str = f"anyOf: {details['anyOf']}"
            elif "allOf" in details.keys():
                type_str = f"allOf: {details['allOf']}"

            desc = details.get("description", DEFAULT_NO_DESC_VALUE)
            default = config_cls.model_fields[field_name].default
            if default is None:
                if callable(config_cls.model_fields[field_name].default_factory):
                    default = config_cls.model_fields[field_name].default_factory()  # type: ignore

            req = config_cls.model_fields[field_name].is_required()

            config_values[field_name] = ValueTypeAndDescription(
                description=desc, type=type_str, value_default=default, required=req
            )

        python_cls = PythonClass.from_class(config_cls)
        return KiaraModuleConfigMetadata(
            python_class=python_cls, config_values=config_values
        )

    python_class: PythonClass = Field(description="The config model python class.")
    config_values: Dict[str, ValueTypeAndDescription] = Field(
        description="The available configuration values."
    )

    def _retrieve_id(self) -> str:
        return self.python_class.full_name

    def _retrieve_data_to_hash(self) -> Any:
        return self.python_class.full_name


class MetadataTypeInfo(TypeInfo):

    _kiara_model_id: ClassVar = "info.metadata_type"

    @classmethod
    def create_from_type_class(
        self, type_cls: Type["ValueMetadata"], kiara: "Kiara"
    ) -> "MetadataTypeInfo":

        authors_md = AuthorsMetadataModel.from_class(type_cls)
        doc = DocumentationMetadataModel.from_class_doc(type_cls)
        python_class = PythonClass.from_class(type_cls)
        properties_md = ContextMetadataModel.from_class(type_cls)
        type_name = type_cls._metadata_key  # type: ignore
        schema = type_cls.model_json_schema()

        return MetadataTypeInfo(
            type_name=type_name,
            documentation=doc,
            authors=authors_md,
            context=properties_md,
            python_class=python_class,
            metadata_schema=schema,
        )

    @classmethod
    def base_class(self) -> Type["ValueMetadata"]:
        from kiara.models.values.value_metadata import ValueMetadata

        return ValueMetadata

    @classmethod
    def category_name(cls) -> str:
        return "value_metadata"

    metadata_schema: Dict[str, Any] = Field(
        description="The (json) schema for this metadata value."
    )

    def create_renderable(self, **config: Any) -> RenderableType:

        include_doc = config.get("include_doc", True)
        include_schema = config.get("include_schema", True)

        table = Table(box=box.SIMPLE, show_header=False, padding=(0, 0, 0, 0))
        table.add_column("property", style="i")
        table.add_column("value")

        if include_doc:
            table.add_row(
                "Documentation",
                Panel(self.documentation.create_renderable(), box=box.SIMPLE),
            )
        table.add_row("Author(s)", self.authors.create_renderable())
        table.add_row("Context", self.context.create_renderable())

        if hasattr(self, "python_class"):
            table.add_row("Python class", self.python_class.create_renderable())

        if include_schema:
            schema = Syntax(
                orjson_dumps(self.metadata_schema, option=orjson.OPT_INDENT_2),
                "json",
                background_color="default",
            )
            table.add_row("metadata_schema", schema)

        return table


class MetadataTypeClassesInfo(TypeInfoItemGroup):

    _kiara_model_id: ClassVar = "info.metadata_types"

    @classmethod
    def base_info_class(cls) -> Type[TypeInfo]:
        return MetadataTypeInfo

    type_name: Literal["value_metadata"] = "value_metadata"
    item_infos: Mapping[str, MetadataTypeInfo] = Field(  # type: ignore
        description="The value metadata info instances for each type."
    )


class DataTypeClassInfo(TypeInfo[Type["DataType"]]):

    _kiara_model_id: ClassVar = "info.data_type"

    @classmethod
    def create_from_type_class(
        self, type_cls: Type["DataType"], kiara: Union["Kiara", None] = None
    ) -> "DataTypeClassInfo":

        from kiara.utils.metadata import get_metadata_model_for_data_type

        authors = AuthorsMetadataModel.from_class(type_cls)
        doc = DocumentationMetadataModel.from_class_doc(type_cls)
        properties_md = ContextMetadataModel.from_class(type_cls)

        if kiara is None:
            raise NotImplementedError(
                "Kiara instance is required to create DataTypeClassInfo."
            )
        else:
            data_type_name = getattr(type_cls, "_data_type_name", None)
            if not data_type_name:
                raise KiaraException(
                    f"Data type class '{type_cls.__name__}' does not have a '_data_type_name' attribute."
                )
            metadata_models = get_metadata_model_for_data_type(
                kiara=kiara, data_type=data_type_name
            )

        if kiara is not None:
            qual_profiles = kiara.type_registry.get_associated_profiles(type_cls._data_type_name)  # type: ignore
            lineage = kiara.type_registry.get_type_lineage(type_cls._data_type_name)  # type: ignore
        else:
            qual_profiles = None
            lineage = None

        try:
            result = DataTypeClassInfo(
                type_name=type_cls._data_type_name,  # type: ignore
                python_class=PythonClass.from_class(type_cls),
                value_cls=PythonClass.from_class(type_cls.python_class()),
                data_type_config_cls=PythonClass.from_class(
                    type_cls.data_type_config_class()
                ),
                lineage=lineage,  # type: ignore
                qualifier_profiles=qual_profiles,
                documentation=doc,
                authors=authors,
                context=properties_md,
                supported_properties=metadata_models,
            )
        except Exception as e:
            if isinstance(
                e, TypeError
            ) and "missing 1 required positional argument: 'cls'" in str(e):
                raise Exception(
                    f"Invalid implementation of TypeValue subclass '{type_cls.__name__}': 'python_class' method must be marked as a '@classmethod'. This is a bug."
                )
            raise e

        result._kiara = kiara
        return result

    @classmethod
    def base_class(self) -> Type["DataType"]:
        from kiara.data_types import DataType

        return DataType

    @classmethod
    def category_name(cls) -> str:
        return "data_type"

    value_cls: PythonClass = Field(description="The python class of the value itself.")
    data_type_config_cls: PythonClass = Field(
        description="The python class holding the schema for configuring this type."
    )
    lineage: Union[List[str], None] = Field(description="This types lineage.")
    qualifier_profiles: Union[Mapping[str, Mapping[str, Any]], None] = Field(
        description="A map of qualifier profiles for this data types."
    )
    supported_properties: MetadataTypeClassesInfo = Field(
        description="The supported property types for this data type."
    )
    _kiara: Union["Kiara", None] = PrivateAttr(default=None)

    def _retrieve_id(self) -> str:
        return self.type_name

    def _retrieve_data_to_hash(self) -> Any:
        return self.type_name

    def create_renderable(self, **config: Any) -> RenderableType:

        include_doc = config.get("include_doc", True)
        include_lineage = config.get("include_lineage", True)
        include_qualifer_profiles = config.get("include_qualifier_profiles", True)

        table = Table(box=box.SIMPLE, show_header=False, padding=(0, 0, 0, 0))
        table.add_column("property", style="i")
        table.add_column("value")

        if include_lineage:
            if self.lineage:
                table.add_row("lineage", "\n".join(self.lineage[0:]))
            else:
                table.add_row("lineage", "-- n/a --")

        if include_qualifer_profiles:
            if self.qualifier_profiles:
                qual_table = Table(show_header=False, box=box.SIMPLE)
                qual_table.add_column("name")
                qual_table.add_column("config")
                for name, details in self.qualifier_profiles.items():
                    json_details = orjson_dumps(details, option=orjson.OPT_INDENT_2)
                    qual_table.add_row(
                        name, Syntax(json_details, "json", background_color="default")
                    )
                table.add_row("qualifier profile(s)", qual_table)
            else:
                table.add_row("qualifier profile(s)", "-- n/a --")

        if include_doc:
            table.add_row(
                "Documentation",
                Panel(self.documentation.create_renderable(), box=box.SIMPLE),
            )

        table.add_row("Author(s)", self.authors.create_renderable())
        table.add_row("Context", self.context.create_renderable())

        table.add_row("Python class", self.python_class.create_renderable())
        table.add_row("Config class", self.data_type_config_cls.create_renderable())
        table.add_row("Value class", self.value_cls.create_renderable())

        return table


class DataTypeClassesInfo(TypeInfoItemGroup):

    _kiara_model_id: ClassVar = "info.data_types"

    # @classmethod
    # def create_from_type_items(
    #     cls,
    #     group_title: Union[str, None] = None,
    #     **items: Type,
    # ) -> "TypeInfoModelGroup":
    #
    #     type_infos = {
    #         k: cls.base_info_class().create_from_type_class(v) for k, v in items.items()  # type: ignore
    #     }
    #     data_types_info = cls(group_alias=group_title, item_infos=type_infos)  # type: ignore
    #     return data_types_info
    #
    # @classmethod
    # def create_augmented_from_type_items(
    #     cls,
    #     kiara: Union["Kiara", None] = None,
    #     group_alias: Union[str, None] = None,
    #     **items: Type,
    # ) -> "TypeInfoModelGroup":
    #
    #     type_infos = {
    #         k: cls.base_info_class().create_from_type_class(v, kiara=kiara) for k, v in items.items()  # type: ignore
    #     }
    #     data_types_info = cls(group_alias=group_alias, item_infos=type_infos)  # type: ignore
    #     data_types_info._kiara = kiara
    #     return data_types_info

    @classmethod
    def base_info_class(cls) -> Type[DataTypeClassInfo]:
        return DataTypeClassInfo

    type_name: Literal["data_type"] = "data_type"
    item_infos: Mapping[str, DataTypeClassInfo] = Field(  # type: ignore
        description="The data_type info instances for each type."
    )
    # _kiara: Union["Kiara", None] = PrivateAttr(default=None)

    def create_renderable(self, **config: Any) -> RenderableType:

        full_doc = config.get("full_doc", False)
        show_subtypes_inline = config.get("show_qualifier_profiles_inline", False)
        show_lineage = config.get("show_type_lineage", True)

        show_lines = full_doc or show_subtypes_inline or show_lineage

        table = Table(show_header=True, box=box.SIMPLE, show_lines=show_lines)
        table.add_column("type name", style="i")

        if show_lineage:
            table.add_column("type lineage")

        if show_subtypes_inline:
            table.add_column("(qualifier) profiles")

        if full_doc:
            table.add_column("documentation")
        else:
            table.add_column("description")

        all_types = self.item_infos.keys()

        for type_name in sorted(all_types):  # type: ignore

            t_md = self.item_infos[type_name]  # type: ignore
            row: List[Any] = [type_name]
            if show_lineage:
                lineage = t_md.lineage
                if lineage is None:
                    lineage_str = "-- n/a --"
                else:
                    lineage_str = ", ".join(reversed(lineage[1:]))
                row.append(lineage_str)
            if show_subtypes_inline:
                qual_profiles = t_md.qualifier_profiles
                if not qual_profiles:
                    qual_profiles_str = "-- n/a --"
                else:
                    qual_profiles_str = "\n".join(qual_profiles)
                row.append(qual_profiles_str)

            if full_doc:
                md = Markdown(t_md.documentation.full_doc)
            else:
                md = Markdown(t_md.documentation.description)
            row.append(md)
            table.add_row(*row)

        return table


class ModuleTypeInfo(TypeInfo[Type["KiaraModule"]]):

    _kiara_model_id: ClassVar = "info.kiara_module_type"

    @classmethod
    def create_from_type_class(cls, type_cls: Type["KiaraModule"], kiara: "Kiara") -> "ModuleTypeInfo":  # type: ignore

        module_attrs = cls.extract_module_attributes(module_cls=type_cls)
        return cls(**module_attrs)

    @classmethod
    def base_class(self) -> Type["KiaraModule"]:

        from kiara.modules import KiaraModule

        return KiaraModule

    @classmethod
    def category_name(cls) -> str:
        return "module"

    @classmethod
    def extract_module_attributes(
        self, module_cls: Type["KiaraModule"]
    ) -> Dict[str, Any]:

        if not hasattr(module_cls, "process"):
            raise Exception(f"Module class '{module_cls}' misses 'process' method.")

        module_src = textwrap.dedent(inspect.getsource(module_cls))  # type: ignore

        authors_md = AuthorsMetadataModel.from_class(module_cls)
        doc = DocumentationMetadataModel.from_class_doc(module_cls)
        python_class = PythonClass.from_class(module_cls)
        properties_md = ContextMetadataModel.from_class(module_cls)
        config = KiaraModuleConfigMetadata.from_config_class(module_cls._config_cls)

        return {
            "type_name": module_cls._module_type_name,  # type: ignore
            "documentation": doc,
            "authors": authors_md,
            "context": properties_md,
            "python_class": python_class,
            "module_config": config,
            "module_src": module_src,
        }

    module_config: KiaraModuleConfigMetadata = Field(
        description="The module config metadata."
    )
    module_src: str = Field(
        description="The source code of the process method of the module."
    )

    def create_renderable(self, **config: Any) -> RenderableType:

        include_config_schema = config.get("include_config_schema", True)
        include_src = config.get("include_src", False)
        include_doc = config.get("include_doc", True)

        table = Table(box=box.SIMPLE, show_header=False, padding=(0, 0, 0, 0))
        table.add_column("property", style="i")
        table.add_column("value")

        if include_doc:
            table.add_row(
                "Documentation",
                Panel(self.documentation.create_renderable(), box=box.SIMPLE),
            )
        table.add_row("Author(s)", self.authors.create_renderable())
        table.add_row("Context", self.context.create_renderable())

        if include_config_schema:
            config_cls = self.python_class.get_class()._config_cls  # type: ignore
            from kiara.utils.output import create_table_from_base_model_cls

            table.add_row(
                "Module config schema", create_table_from_base_model_cls(config_cls)
            )

        table.add_row("Python class", self.python_class.create_renderable())

        if include_src:
            from kiara.context.config import KIARA_SETTINGS

            _config = Syntax(
                self.module_src,
                "python",
                background_color=KIARA_SETTINGS.syntax_highlight_background,
            )
            table.add_row("Processing source code", Panel(_config, box=box.HORIZONTALS))

        return table


class ModuleTypesInfo(TypeInfoItemGroup):

    _kiara_model_id: ClassVar = "info.module_types"

    @classmethod
    def base_info_class(cls) -> Type[TypeInfo]:
        return ModuleTypeInfo

    type_name: Literal["module_type"] = "module_type"
    item_infos: Mapping[str, ModuleTypeInfo] = Field(  # type: ignore
        description="The module type info instances for each type."
    )


class OperationTypeInfo(TypeInfo[Type["OperationType"]]):

    _kiara_model_id: ClassVar = "info.operation_type"

    @classmethod
    def create_from_type_class(  # type: ignore
        cls, kiara: "Kiara", type_cls: Type["OperationType"]  # type: ignore
    ) -> "OperationTypeInfo":

        authors_md = AuthorsMetadataModel.from_class(type_cls)
        doc = DocumentationMetadataModel.from_class_doc(type_cls)
        python_class = PythonClass.from_class(type_cls)
        properties_md = ContextMetadataModel.from_class(type_cls)

        return OperationTypeInfo(
            type_name=type_cls._operation_type_name,  # type: ignore
            documentation=doc,
            authors=authors_md,
            context=properties_md,
            python_class=python_class,
        )

    @classmethod
    def base_class(self) -> Type["OperationType"]:
        from kiara.operations import OperationType

        return OperationType

    @classmethod
    def category_name(cls) -> str:
        return "operation_type"

    def _retrieve_id(self) -> str:
        return self.type_name

    def _retrieve_data_to_hash(self) -> Any:
        return self.type_name


class OperationTypeClassesInfo(TypeInfoItemGroup):

    _kiara_model_id: ClassVar = "info.operation_types"

    @classmethod
    def base_info_class(cls) -> Type[OperationTypeInfo]:  # type: ignore
        return OperationTypeInfo

    type_name: Literal["operation_type"] = "operation_type"
    item_infos: Mapping[str, OperationTypeInfo] = Field(  # type: ignore
        description="The operation info instances for each type."
    )


class FieldInfo(BaseModel):

    field_name: str = Field(description="The field name.")
    field_schema: ValueSchema = Field(description="The schema of the field.")
    data_type_info: DataTypeInfo = Field(
        description="Information about the data type instance of the associated value."
    )
    value_required: bool = Field(
        description="Whether user input is required (meaning: 'optional' is False, and no default set)."
    )


class PipelineStructureInfo(ItemInfo):

    _kiara_model_id: ClassVar = "info.pipeline_structure"

    @classmethod
    def base_instance_class(cls) -> Type[PipelineStructure]:
        return PipelineStructure

    @classmethod
    def create_from_instance(
        cls, kiara: "Kiara", instance: PipelineStructure, **kwargs
    ):

        authors = AuthorsMetadataModel()
        context = ContextMetadataModel()

        execution_graph: Dict[str, Any] = {}
        data_flow_graph: Dict[str, Any] = {}
        data_flow_graph_simple: Dict[str, Any] = {}

        input_fields = {}
        for field_name, schema in instance.pipeline_inputs_schema.items():
            dt = kiara.type_registry.get_data_type_instance(
                type_name=schema.type, type_config=schema.type_config
            )
            dt_info = FieldInfo(
                field_name=field_name,
                field_schema=schema,
                data_type_info=dt.info,
                value_required=schema.is_required(),
            )
            input_fields[field_name] = dt_info

        output_fields = {}
        for field_name, schema in instance.pipeline_outputs_schema.items():
            dt = kiara.type_registry.get_data_type_instance(
                type_name=schema.type, type_config=schema.type_config
            )
            dt_info = FieldInfo(
                field_name=field_name,
                field_schema=schema,
                data_type_info=dt.info,
                value_required=schema.is_required(),
            )
            output_fields[field_name] = dt_info

        return cls(
            type_name=instance.instance_id,
            documentation=instance.pipeline_config.doc,
            authors=authors,
            context=context,
            pipeline_config=instance.pipeline_config,
            pipeline_config_orig=instance.pipeline_config.get_raw_config(),
            # steps={step.step_id: step for step in instance.steps},
            step_details=instance.steps_details,
            input_aliases=instance.input_aliases,
            output_aliases=instance.output_aliases,
            constants=instance.constants,
            defaults=instance.defaults,
            pipeline_input_fields=input_fields,
            pipeline_output_fields=output_fields,
            pipeline_input_refs=instance.pipeline_input_refs,
            pipeline_output_refs=instance.pipeline_output_refs,
            execution_graph=execution_graph,
            data_flow_graph=data_flow_graph,
            data_flow_graph_simple=data_flow_graph_simple,
            processing_stages=instance.processing_stages,
            # processing_stages_info=instance.processing_stages_info,
        )

    pipeline_config: PipelineConfig = Field(
        description="The underlying pipeline config."
    )
    pipeline_config_orig: Dict[str, Any] = Field(
        description="The original, user-provided pipeline config."
    )
    # steps: Mapping[str, PipelineStep] = Field(
    #     description="All steps for this pipeline, indexed by their step_id.", exclude=True
    # )
    step_details: Mapping[str, StepInfo] = Field(
        description="Additional information for each step."
    )
    input_aliases: Dict[str, str] = Field(description="The input aliases.")
    output_aliases: Dict[str, str] = Field(description="The output aliases.")
    constants: Mapping[str, Any] = Field(
        description="The input constants for this pipeline."
    )
    defaults: Mapping[str, Any] = Field(
        description="The default inputs for this pipeline."
    )

    pipeline_input_fields: Mapping[str, FieldInfo] = Field(
        description="The pipeline inputs schema."
    )
    pipeline_output_fields: Mapping[str, FieldInfo] = Field(
        description="The pipeline outputs schema."
    )

    pipeline_input_refs: Mapping[str, PipelineInputRef] = Field(
        description="References to the step inputs that are linked to pipeline inputs."
    )
    pipeline_output_refs: Mapping[str, PipelineOutputRef] = Field(
        description="References to the step outputs that are linked to pipeline outputs."
    )

    execution_graph: Dict[str, Any] = Field(
        description="Data describing the execution graph of this pipeline."
    )
    data_flow_graph: Dict[str, Any] = Field(
        description="Data describing the data flow of this pipeline."
    )
    data_flow_graph_simple: Dict[str, Any] = Field(
        description="Data describing the (simplified) data flow of this pipeline."
    )

    processing_stages: List[List[str]] = Field(
        description="A list of lists, containing all the step_ids per stage, in the order of execution."
    )
    # processing_stages_info: Mapping[int, PipelineStage] = Field(
    #     description="More detailed information about each step of this pipelines execution graph."
    # )

    def get_step(self, step_id) -> PipelineStep:
        return self.step_details[step_id].step

    def get_step_details(self, step_id: str) -> StepInfo:
        return self.step_details[step_id]

    def create_renderable(self, **config: Any) -> RenderableType:

        tree = Tree("pipeline")
        inputs = tree.add("inputs")
        for field_name, field_info in self.pipeline_input_fields.items():
            inputs.add(f"[i]{field_name}[i] (type: {field_info.field_schema.type})")

        steps = tree.add("steps")
        for idx, stage in enumerate(self.processing_stages, start=1):
            stage_node = steps.add(f"stage {idx}")
            for step_id in stage:
                step_node = stage_node.add(f"step: {step_id}")
                step = self.get_step(step_id=step_id)
                if step.doc.is_set:
                    step_node.add(f"desc: {step.doc.description}")
                step_node.add(f"operation: {step.manifest_src.module_type}")

        outputs = tree.add("outputs")
        for field_name, field_info in self.pipeline_output_fields.items():
            outputs.add(f"[i]{field_name}[i] (type: {field_info.field_schema.type})")

        return tree


class OperationInfo(ItemInfo):

    _kiara_model_id: ClassVar = "info.operation"

    @classmethod
    def base_instance_class(cls) -> Type[Operation]:
        return Operation

    @classmethod
    def create_from_instance(cls, kiara: "Kiara", instance: Operation, **kwargs):

        return cls.create_from_operation(kiara=kiara, operation=instance)

    @classmethod
    def create_from_operation(
        cls, kiara: "Kiara", operation: Operation
    ) -> "OperationInfo":

        module = operation.module
        module_cls = module.__class__

        authors_md = AuthorsMetadataModel.from_class(module_cls)
        properties_md = ContextMetadataModel.from_class(module_cls)

        op_types = kiara.operation_registry.find_all_operation_types(
            operation_id=operation.operation_id
        )

        input_fields = {}
        for field_name, schema in operation.inputs_schema.items():

            try:
                dt = kiara.type_registry.get_data_type_instance(
                    type_name=schema.type, type_config=schema.type_config
                )
                dt_info = FieldInfo(
                    field_name=field_name,
                    field_schema=schema,
                    data_type_info=dt.info,
                    value_required=schema.is_required(),
                )
            except Exception:
                dtc = PythonClass.from_class(object)
                dti = DataTypeInfo(
                    data_type_name=f"{schema.type} (invalid)",
                    data_type_config=schema.type_config,
                    characteristics=DataTypeCharacteristics(),
                    data_type_class=dtc,
                )
                dt_info = FieldInfo(
                    field_name=field_name,
                    field_schema=schema,
                    data_type_info=dti,
                    value_required=schema.is_required(),
                )

            input_fields[field_name] = dt_info

        output_fields = {}
        for field_name, schema in operation.outputs_schema.items():
            dt = kiara.type_registry.get_data_type_instance(
                type_name=schema.type, type_config=schema.type_config
            )
            dt_info = FieldInfo(
                field_name=field_name,
                field_schema=schema,
                data_type_info=dt.info,
                value_required=schema.is_required(),
            )
            output_fields[field_name] = dt_info

        op_info = OperationInfo(
            type_name=operation.operation_id,
            operation_types=list(op_types),
            input_fields=input_fields,
            output_fields=output_fields,
            operation=operation,
            documentation=operation.doc,
            authors=authors_md,
            context=properties_md,
        )

        return op_info

    @classmethod
    def category_name(cls) -> str:
        return "operation"

    operation: Operation = Field(description="The operation instance.")
    operation_types: List[str] = Field(
        description="The operation types this operation belongs to."
    )
    input_fields: Mapping[str, FieldInfo] = Field(
        description="The inputs schema for this operation."
    )
    output_fields: Mapping[str, FieldInfo] = Field(
        description="The outputs schema for this operation."
    )

    def create_renderable(self, **config: Any) -> RenderableType:

        include_doc = config.get("include_doc", False)
        include_module_details = config.get("include_module_details", True)
        include_op_details = config.get("include_op_details", True)

        table = Table(box=box.SIMPLE, show_header=False, padding=(0, 0, 0, 0))
        table.add_column("property", style="i")
        table.add_column("value")

        if include_doc:
            table.add_row(
                "Documentation",
                Panel(self.documentation.create_renderable(), box=box.SIMPLE),
            )
        table.add_row("Author(s)", self.authors.create_renderable(**config))
        table.add_row("Context", self.context.create_renderable(**config))

        if include_module_details:
            table.add_row("Module type", self.operation.module_type)
            if self.operation.module_config:
                table.add_row(
                    "Module config",
                    Syntax(
                        orjson_dumps(
                            self.operation.module_config, option=orjson.OPT_INDENT_2
                        ),
                        "json",
                        background_color="default",
                    ),
                )

        if include_op_details:
            table.add_row(
                "Operation details", self.operation.create_renderable(**config)
            )
        return table


class OperationGroupInfo(InfoItemGroup):

    _kiara_model_id: ClassVar = "info.operations"

    @classmethod
    def base_info_class(cls) -> Type[ItemInfo]:
        return OperationInfo

    @classmethod
    def create_from_operations(
        cls, kiara: "Kiara", group_title: Union[str, None] = None, **items: Operation
    ) -> "OperationGroupInfo":

        op_infos = {
            k: OperationInfo.create_from_operation(kiara=kiara, operation=v)
            for k, v in items.items()
        }

        op_group_info = cls(group_title=group_title, item_infos=op_infos)
        return op_group_info

    # type_name: Literal["operation_type"] = "operation_type"
    item_infos: Mapping[str, OperationInfo] = Field(
        description="The operation info instances for each type."
    )

    def create_renderable(self, **config: Any) -> RenderableType:

        by_type = config.get("by_type", False)

        if by_type:
            return self._create_renderable_by_type(**config)
        else:
            return self._create_renderable_list(**config)

    def _create_renderable_list(self, **config) -> RenderableType:

        include_internal_operations = config.get("include_internal_operations", True)
        full_doc = config.get("full_doc", False)
        filter = config.get("filter", [])
        show_internal_column = config.get("show_internal_column", False)

        table = Table(box=box.SIMPLE, show_header=True)
        table.add_column("Id", no_wrap=True, style="i")
        table.add_column("Type(s)", style="green")
        if show_internal_column:
            table.add_column("Internal", justify="center")
        table.add_column("Description")

        for op_id, op_info in self.item_infos.items():

            if (
                not include_internal_operations
                and op_info.operation.operation_details.is_internal_operation
            ):
                continue

            types = op_info.operation_types

            if "custom_module" in types:
                types.remove("custom_module")

            desc_str = op_info.documentation.description
            if full_doc:
                desc = Markdown(op_info.documentation.full_doc)
            else:
                desc = Markdown(op_info.documentation.description)

            is_internal = op_info.operation.module.characteristics.is_internal
            is_internal_str = "\u2714" if is_internal else ""

            if filter:
                match = True
                for f in filter:
                    if (
                        f.lower() not in op_id.lower()
                        and f.lower() not in desc_str.lower()
                    ):
                        match = False
                        break
                if match:
                    if not show_internal_column:
                        table.add_row(op_id, ", ".join(types), desc)
                    else:
                        table.add_row(op_id, ", ".join(types), is_internal_str, desc)

            else:
                if not show_internal_column:
                    table.add_row(op_id, ", ".join(types), desc)
                else:
                    table.add_row(op_id, ", ".join(types), is_internal_str, desc)

        return table

    def _create_renderable_by_type(self, **config) -> Table:

        include_internal_operations = config.get("include_internal_operations", True)
        full_doc = config.get("full_doc", False)
        filter = config.get("filter", [])

        by_type: Dict[str, Dict[str, OperationInfo]] = {}
        for op_id, op in self.item_infos.items():
            if filter:
                match = True
                for f in filter:
                    if (
                        f.lower() not in op_id.lower()
                        and f.lower() not in op.documentation.description.lower()
                    ):
                        match = False
                        break
                if not match:
                    continue
            for op_type in op.operation_types:
                by_type.setdefault(op_type, {})[op_id] = op

        table = Table(box=box.SIMPLE, show_header=True)
        table.add_column("Type", no_wrap=True, style="b green")
        table.add_column("Id", no_wrap=True, style="i")
        if full_doc:
            table.add_column("Documentation", no_wrap=False, style="i")
        else:
            table.add_column("Description", no_wrap=False, style="i")

        for operation_name in sorted(by_type.keys()):

            # if operation_name == "custom_module":
            #     continue

            first_line_value = True
            op_infos = by_type[operation_name]

            for op_id in sorted(op_infos.keys()):
                op_info: OperationInfo = op_infos[op_id]

                if (
                    not include_internal_operations
                    and op_info.operation.operation_details.is_internal_operation
                ):
                    continue

                if full_doc:
                    desc = Markdown(op_info.documentation.full_doc)
                else:
                    desc = Markdown(op_info.documentation.description)

                row: List[RenderableType] = []
                if first_line_value:
                    row.append(operation_name)
                else:
                    row.append("")

                row.append(op_id)
                row.append(desc)

                table.add_row(*row)
                first_line_value = False

        return table


class RendererInfo(ItemInfo):

    renderer_config: Mapping[str, Any] = Field(description="The renderer config.")
    renderer_cls: PythonClass = Field(
        description="The Python class that implements the renderer."
    )
    supported_inputs: List[str] = Field(
        description="Descriptions of the supported inputs."
    )
    supported_source_types: List[str] = Field(
        description="Descriptions of the supported source types."
    )
    supported_target_types: List[str] = Field(
        description="Descriptions of the supported target types."
    )
    supported_python_classes: List[PythonClass] = Field(
        description="A list of supported Python types that are acceptable as inputs."
    )

    @classmethod
    def base_instance_class(cls) -> Type[KiaraRenderer]:
        return KiaraRenderer

    @classmethod
    def create_from_instance(cls, kiara: "Kiara", instance: KiaraRenderer, **kwargs):

        doc = instance.doc
        authors = AuthorsMetadataModel.from_class(instance.__class__)
        properties_md = ContextMetadataModel.from_class(instance.__class__)

        renderer_name = instance._renderer_name  # type: ignore
        renderer_config = instance.renderer_config.model_dump()
        renderer_cls = PythonClass.from_class(item_cls=instance.__class__)
        supported_inputs = list(instance.supported_inputs_descs)
        supported_python_classes = [
            PythonClass.from_class(x)
            for x in instance.retrieve_supported_python_classes()
        ]

        supported_input_types = instance.retrieve_supported_render_sources()
        if isinstance(supported_input_types, str):
            supported_input_types = [supported_input_types]
        supported_target_types = instance.retrieve_supported_render_targets()
        if isinstance(supported_target_types, str):
            supported_target_types = [supported_target_types]

        return cls(
            type_name=renderer_name,
            documentation=doc,
            authors=authors,
            context=properties_md,
            renderer_config=renderer_config,
            renderer_cls=renderer_cls,
            supported_inputs=supported_inputs,
            supported_python_classes=supported_python_classes,
            supported_source_types=supported_input_types,
            supported_target_types=supported_target_types,
        )

    def create_renderable(self, **config: Any) -> RenderableType:

        show_metadata = config.get("show_metadata", False)

        table = Table(box=box.SIMPLE, show_header=False)
        table.add_column("key", style="i")
        table.add_column("value")

        table.add_row("Documentation", self.documentation.create_renderable(**config))
        inputs_md = ""
        for inp in self.supported_inputs:
            inputs_md += f"- {inp}\n"
        table.add_row("Supported inputs", Markdown(inputs_md))

        if show_metadata:
            table.add_row("Renderer name", self.type_name)
            if self.renderer_config:
                json = orjson_dumps(self.renderer_config, option=orjson.OPT_INDENT_2)
                table.add_row(
                    "Renderer config", Syntax(json, "json", background_color="default")
                )

            table.add_row(
                "Renderer class", self.renderer_cls.create_renderable(**config)
            )
            table.add_row("Author(s)", self.authors.create_renderable(**config))
            table.add_row("Context", self.context.create_renderable(**config))

        python_cls_md = ""
        for inp_cls in self.supported_python_classes:
            python_cls_md += f"- {inp_cls.full_name}\n"
        table.add_row(python_cls_md)

        return table


class RendererInfos(InfoItemGroup[RendererInfo]):
    @classmethod
    def base_info_class(cls) -> Type[RendererInfo]:
        return RendererInfo

    def get_render_source_types(self) -> List[str]:

        all_source_types = set()
        item: RendererInfo
        for item in self.item_infos.values():  # type: ignore
            all_source_types.update(item.supported_source_types)

        return sorted(all_source_types)

    def create_renderable(self, **config: Any) -> RenderableType:

        table = Table(
            show_header=True, box=HORIZONTALS_NO_TO_AND_BOTTOM, show_lines=True
        )
        table.add_column("Source type(s)")
        table.add_column("Target type(s)")
        table.add_column("Description")

        rows: Dict[str, Dict[str, List[RenderableType]]] = {}
        info: RendererInfo
        for info in self.item_infos.values():  # type: ignore
            row: List[RenderableType] = []
            source_types = "\n".join(info.supported_source_types)
            target_types = "\n".join(info.supported_target_types)
            row.append(source_types)  # type: ignore
            row.append(target_types)  # type: ignore
            row.append(info.documentation.create_renderable(**config))

            rows.setdefault(source_types, {})[target_types] = row

        for source in sorted(rows.keys()):
            for target in sorted(rows[source].keys()):
                row = rows[source][target]
                table.add_row(*row)

        return table


class KiaraPluginInfo(ItemInfo):
    @classmethod
    def base_instance_class(cls) -> Type[str]:
        return str

    @classmethod
    def create_from_instance(
        cls, kiara: "Kiara", instance: str, **kwargs
    ) -> "KiaraPluginInfo":

        registry = kiara.environment_registry
        python_env: PythonRuntimeEnvironment = registry.environments["python"]  # type: ignore

        match: Union[str, None] = None
        for pkg in python_env.packages:
            pkg_name = pkg.name
            if pkg_name == instance:
                match = pkg.name
            elif pkg_name.startswith("kiara-plugin") or pkg_name.startswith(
                "kiara_plugin"
            ):
                underscored = pkg_name.replace("-", "_")
                if underscored == instance:
                    match = underscored
                    break

        if not match:
            raise KiaraException(
                msg=f"Can't provide information for plugin '{instance}'.",
                reason="Plugin not installed.",
            )

        match = match.replace("kiara-plugin", "kiara_plugin")

        from kiara.utils.operations import filter_operations

        data_types = kiara.type_registry.get_context_metadata(only_for_package=match)
        modules = kiara.module_registry.get_context_metadata(only_for_package=match)
        operation_types = kiara.operation_registry.get_context_metadata(
            only_for_package=match
        )
        operations = filter_operations(
            kiara=kiara, pkg_name=match, **kiara.operation_registry.operations
        )

        model_registry = kiara.kiara_model_registry
        kiara_models = model_registry.get_models_for_package(package_name=match)

        final_pkg_name = match.replace("-", "_")
        base_module = importlib.import_module(final_pkg_name)
        from importlib.metadata import metadata

        pkg_metadata = metadata(final_pkg_name)

        summary = pkg_metadata.get("Summary", None)
        desc = pkg_metadata.get("Description", None)

        if not summary and not desc:
            doc = DocumentationMetadataModel.create()
        elif not summary:
            doc = DocumentationMetadataModel.create(desc)
        elif not desc:
            doc = DocumentationMetadataModel.create(summary)
        else:
            doc = DocumentationMetadataModel.create(f"{summary}\n\n{desc}")

        def parse_name_email(s):
            match = re.match(r"(.*)\s*<(.+)>", s)
            if match:
                name = match.group(1).strip()
                email = match.group(2).strip()
                return name, email
            else:
                return None, None

        version = pkg_metadata["Version"]
        authors: List[AuthorModel] = []
        for key in pkg_metadata.keys():
            if key == "Author-email":
                author, email = parse_name_email(pkg_metadata[key])
                if not author:
                    author = email.split("@")[0]
            elif key == "Author":
                author = pkg_metadata[key]
                email = None
            else:
                continue

            author_obj = AuthorModel(name=author, email=email)
            authors.append(author_obj)

        author_md = AuthorsMetadataModel(authors=authors)

        context = ContextMetadataModel.from_class(base_module)  # type: ignore

        info = KiaraPluginInfo(
            type_name=instance,
            version=version,
            documentation=doc,
            authors=author_md,
            context=context,
            data_types=data_types,
            module_types=modules,
            kiara_model_types=kiara_models,
            operation_types=operation_types,
            operations=operations,
        )
        return info

    version: str = Field(description="The version of the plugin.")
    data_types: DataTypeClassesInfo = Field(description="The included data types.")
    module_types: ModuleTypesInfo = Field(
        description="The included kiara module types."
    )
    kiara_model_types: KiaraModelClassesInfo = Field(
        description="The included model classes."
    )
    # metadata_types: MetadataTypeClassesInfo = Field(
    #     description="The included value metadata types."
    # )
    operation_types: OperationTypeClassesInfo = Field(
        description="The included operation types."
    )
    operations: OperationGroupInfo = Field(description="The included operations.")

    def create_renderable(self, **config: Any) -> RenderableType:

        include_doc = config.get("include_doc", True)
        include_full_doc = config.get("include_full_doc", False)
        include_data_types = config.get("include_data_types", True)
        include_module_types = config.get("include_module_types", True)
        include_operations = config.get("include_operations", True)
        include_operation_types = config.get("include_operation_types", False)
        include_model_types = config.get("include_model_types", False)

        table = Table(box=box.SIMPLE, show_header=False, padding=(0, 0, 0, 0))
        table.add_column("property", style="i")
        table.add_column("value")

        if include_doc:
            if include_full_doc:
                title = "Documentation"
                doc_str = self.documentation.full_doc
            else:
                title = "Description"
                doc_str = self.documentation.description
            table.add_row(
                title,
                Panel(doc_str, box=box.SIMPLE),
            )

        table.add_row("Version", Panel(self.version, box.SIMPLE))
        table.add_row("Author(s)", self.authors.create_renderable())
        table.add_row("Context", self.context.create_renderable())

        if include_data_types:
            table.add_row("data_types", self.data_types.create_renderable(**config))
        if include_module_types:
            table.add_row("module_types", self.module_types.create_renderable(**config))
        if include_operations:
            config_ops = config.copy()
            config_ops["show_internal_column"] = True
            table.add_row("operations", self.operations.create_renderable(**config_ops))
        if include_operation_types:
            table.add_row(
                "operation_types", self.operation_types.create_renderable(**config)
            )
        if include_model_types:
            table.add_row(
                "kiara_model_types", self.kiara_model_types.create_renderable(**config)
            )

        return table


class KiaraPluginInfos(InfoItemGroup[KiaraPluginInfo]):
    @classmethod
    def get_available_plugin_names(
        cls, kiara: "Kiara", regex: Union[str, None] = None
    ) -> List[str]:
        """
        Get a list of all available plugins.

        Arguments:
            regex: an optional regex to indicate the plugin naming scheme (default: /$kiara[_-]plugin\\..*/)

        Returns:
            a list of plugin names
        """

        if regex is None:
            regex = r"^kiara[-_]plugin\..*"

        registry = kiara.environment_registry
        python_env: PythonRuntimeEnvironment = registry.environments["python"]  # type: ignore

        if not regex:
            regex = "^kiara[-_]plugin\\..*"
        regex_c = re.compile(regex)

        result = []
        for pkg in python_env.packages:
            pkg_name = pkg.name
            if pkg_name == "kiara":
                continue

            # check if the package is a kiara plugin
            match = regex_c.search(pkg_name)
            if match:
                result.append(pkg_name)

        return result

    @classmethod
    def base_info_class(cls) -> Type[KiaraPluginInfo]:
        return KiaraPluginInfo

    @classmethod
    def create_group(
        cls,
        kiara: "Kiara",
        group_title: Union[str, None] = None,
        plugin_name_regex: str = "^kiara[-_]plugin\\..*",
    ) -> "KiaraPluginInfos":

        names = cls.get_available_plugin_names(kiara=kiara, regex=plugin_name_regex)
        result = cls.create_from_plugin_names(kiara, group_title, *names)
        return result

    @classmethod
    def create_from_plugin_names(
        cls, kiara: "Kiara", group_title: Union[str, None] = None, *items: str
    ) -> "KiaraPluginInfos":
        """Create the info group from a list of plugin names."""

        plugin_infos = {
            k: KiaraPluginInfo.create_from_instance(kiara=kiara, instance=k)
            for k in items
        }

        op_group_info = cls(group_title=group_title, item_infos=plugin_infos)
        return op_group_info

    def create_renderable(self, **config: Any) -> RenderableType:

        full_doc = config.get("full_doc", False)

        table = Table(show_header=True, box=box.SIMPLE, show_lines=full_doc)
        table.add_column("Name", style="i")
        table.add_column("Version")
        table.add_column("Description")

        for type_name in sorted(self.item_infos.keys()):
            t_md = self.item_infos[type_name]
            version = t_md.version
            if full_doc:
                md = Markdown(t_md.documentation.full_doc)
            else:
                md = Markdown(t_md.documentation.description)
            table.add_row(type_name, version, md)

        return table


class JobInfo(ItemInfo):

    job_record: JobRecord = Field(description="The job record instance.")
    operation: OperationInfo = Field(description="The operation info instance.")
    inputs: Mapping[str, ValueInfo] = Field(description="The inputs.")
    outputs: Mapping[str, ValueInfo] = Field(description="The result(s).")

    @classmethod
    def base_instance_class(cls) -> Type[JobRecord]:
        return JobRecord

    @classmethod
    def create_from_instance(cls, kiara: "Kiara", instance: JobRecord, **kwargs):

        type_name = str(instance.job_id)

        module = kiara.module_registry.create_module(instance)
        op = Operation.create_from_module(module=module)
        operation_info: OperationInfo = OperationInfo.create_from_operation(
            kiara=kiara, operation=op
        )
        doc = operation_info.documentation
        authors = operation_info.authors
        context = operation_info.context

        inputs = {}
        for k, v in instance.inputs.items():
            value = kiara.data_registry.get_value(v)
            inputs[k] = ValueInfo.create_from_instance(kiara=kiara, instance=value)

        outputs = {}
        for k, v in instance.outputs.items():
            value = kiara.data_registry.get_value(v)
            outputs[k] = ValueInfo.create_from_instance(kiara=kiara, instance=value)

        return cls(
            type_name=type_name,
            documentation=doc,
            authors=authors,
            context=context,
            job_record=instance,
            operation=operation_info,
            inputs=inputs,
            outputs=outputs,
        )

    def create_renderable(self, **config: Any) -> RenderableType:

        table = Table(show_header=False, box=box.SIMPLE)
        table.add_column("Key", style="i")
        table.add_column("Value")

        table.add_row("Job ID", str(self.job_record.job_id))
        runtime_details = self.job_record.runtime_details
        assert runtime_details is not None
        table.add_row("Job details", runtime_details.create_renderable(**config))
        table.add_row("Operation details", self.operation)

        values_table = Table(show_header=False, box=box.SIMPLE)
        values_table.add_column("field", style="i")
        values_table.add_column("value")
        for k, v in self.inputs.items():
            rendered_value = str(v.value_id)
            values_table.add_row(k, rendered_value)

        table.add_row("Inputs", values_table)

        values_table = Table(show_header=False, box=box.SIMPLE)
        values_table.add_column("field", style="i")
        values_table.add_column("value")
        for k, v in self.outputs.items():
            rendered_value = str(v.value_id)
            values_table.add_row(k, rendered_value)

        table.add_row("Outputs", values_table)

        return table


class JobInfos(InfoItemGroup[JobInfo]):
    @classmethod
    def base_info_class(cls) -> Type[JobInfo]:
        return JobInfo

    def create_renderable(self, **config: Any) -> RenderableType:

        table = Table(
            show_header=True, box=HORIZONTALS_NO_TO_AND_BOTTOM, show_lines=True
        )
        table.add_column("Job ID")
        table.add_column("Module type")
        table.add_column("Runtime (in seconds)")
        table.add_column("Details")

        info: JobInfo
        for info in self.item_infos.values():  # type: ignore

            runtime_details = info.job_record.runtime_details
            assert runtime_details is not None
            row: List[RenderableType] = []
            row.append(str(info.job_record.job_id))
            row.append(info.operation.operation.module_type)
            row.append(str(runtime_details.runtime))
            row.append(runtime_details.create_renderable(**config))

            table.add_row(*row)

        return table


# kiara\kiara\src\kiara\interfaces\python_api\models\job.py
# -*- coding: utf-8 -*-
import os.path
import uuid
from pathlib import Path
from typing import TYPE_CHECKING, Any, ClassVar, Dict, List, Mapping, Union

from dag_cbor import IPLDKind
from pydantic import BaseModel, Field, field_validator, model_validator

from kiara.exceptions import KiaraException
from kiara.models import KiaraModel
from kiara.models.documentation import DocumentationMetadataModel
from kiara.utils.cli import terminal_print
from kiara.utils.files import get_data_from_file
from kiara.utils.string_vars import replace_var_names_in_obj

if TYPE_CHECKING:
    from kiara.api import KiaraAPI
    from kiara.interfaces.python_api.base_api import BaseAPI
    from kiara.models.module.operation import Operation
    from kiara.models.values.value import ValueMap


class JobDesc(KiaraModel):
    """An object describing a compute job with both raw or referenced inputs."""

    _kiara_model_id: ClassVar = "instance.job_desc"

    @classmethod
    def create_from_file(cls, path: Union[str, Path]) -> "JobDesc":
        run_data = cls.parse_from_file(path)
        return cls(**run_data)

    @classmethod
    def parse_from_file(cls, path: Union[str, Path]) -> Mapping[str, Any]:

        if isinstance(path, str):
            path = Path(path)

        if not path.is_file():
            raise KiaraException(
                f"Can't load job description, invalid file path: '{path}'"
            )

        data = get_data_from_file(path)

        repl_dict: Dict[str, Any] = {"this_dir": path.parent.absolute().as_posix()}

        try:
            run_data = cls.parse_data(
                data=data, var_repl_dict=repl_dict, alias=path.stem
            )
            return run_data
        except Exception as e:
            raise KiaraException(f"Invalid run description in file '{path}': {e}")

    @classmethod
    def parse_data(
        cls,
        data: Mapping[str, Any],
        var_repl_dict: Union[Mapping[str, Any], None] = None,
        alias: Union[str, None] = None,
    ) -> Mapping[str, Any]:

        if not isinstance(data, Mapping):
            raise KiaraException("Job description data is not a mapping.")

        if "operation" not in data.keys():
            raise KiaraException("Missing 'operation' key")

        if var_repl_dict:
            run_data: Dict[str, Any] = replace_var_names_in_obj(
                data, repl_dict=var_repl_dict
            )
        else:
            run_data = dict(data)

        if alias:
            run_data["job_alias"] = alias

        return run_data

    @classmethod
    def create_from_data(
        cls,
        data: Mapping[str, Any],
        var_repl_dict: Union[Mapping[str, Any], None] = None,
        alias: Union[str, None] = None,
    ) -> "JobDesc":

        run_data = cls.parse_data(data=data, var_repl_dict=var_repl_dict, alias=alias)
        return cls(**run_data)

    job_alias: str = Field(description="The alias for the job.", default="default")
    operation: str = Field(description="The operation id or module type.")
    module_config: Union[Mapping[str, Any], None] = Field(
        default=None, description="The configuration for the module."
    )
    inputs: Dict[str, Any] = Field(
        description="The inputs for the job.", default_factory=dict
    )
    doc: DocumentationMetadataModel = Field(
        description="A description/doc for this job.",
        default_factory=DocumentationMetadataModel.create,
    )
    save: Dict[str, str] = Field(
        description="Configuration on how/whether to save the job results. Key is the output field name, value is the alias to use for saving.",
        default_factory=dict,
    )

    def _retrieve_data_to_hash(self) -> IPLDKind:
        def get_hash(v: Any):
            if hasattr(v, "instance_cid"):
                return v.instance_cid
            elif hasattr(v, "value_id"):
                return str(v.value_id)
            elif isinstance(v, uuid.UUID):
                return str(v)
            elif isinstance(v, Mapping):
                return {get_hash(k): get_hash(v) for k, v in v.items()}
            return v

        inputs_hash = {k: get_hash(v) for k, v in self.inputs.items()}
        return {
            "operation": self.operation,
            "module_config": self.module_config,  # type: ignore
            "inputs": inputs_hash,
            "save": self.save,  # type: ignore
        }

    @model_validator(mode="before")
    @classmethod
    def validate_inputs(cls, values):

        if len(values) == 1 and "data" in values.keys():
            data = values["data"]
            if isinstance(data, str):
                if os.path.isfile(data):
                    data = Path(data)

            if isinstance(data, Path):
                run_data = cls.parse_from_file(data)
                return run_data
            else:
                values = data
        return values

    @field_validator("doc", mode="before")
    @classmethod
    def validate_doc(cls, value):
        return DocumentationMetadataModel.create(value)

    def get_operation(self, kiara_api: "BaseAPI") -> "Operation":

        if not self.module_config:
            operation: Operation = kiara_api.get_operation(
                self.operation, allow_external=True
            )
        else:
            data = {
                "module_type": self.operation,
                "module_config": self.module_config,
            }
            operation = kiara_api.get_operation(operation=data, allow_external=False)

        return operation


class RunSpec(BaseModel):
    """A list of jobs, ran one after the other, incl saving of results."""

    @classmethod
    def create_from_file(cls, path: Union[str, Path]):

        if isinstance(path, str):
            path = Path(path)

        if not path.is_file():
            raise KiaraException(f"Can't load run spec, invalid file path: '{path}'")

        data = get_data_from_file(path)

        repl_dict: Dict[str, Any] = {"this_dir": path.parent.absolute().as_posix()}

        try:
            run = cls.create_from_data(
                data=data, var_repl_dict=repl_dict, alias=path.stem
            )
            return run
        except Exception as e:
            raise KiaraException(f"Invalid run description in file '{path}': {e}")

    @classmethod
    def create_from_data(
        cls,
        data: Mapping[str, Any],
        var_repl_dict: Union[Mapping[str, Any], None] = None,
        alias: Union[str, None] = None,
    ):

        if not isinstance(data, Mapping):
            raise KiaraException("Run spec data is not a mapping.")

        if "jobs" not in data.keys():
            raise KiaraException("Missing 'jobs' key")

        if var_repl_dict:
            run_data = replace_var_names_in_obj(data, repl_dict=var_repl_dict)
        else:
            run_data = data

        if alias:
            run_data["run_alias"] = alias

        instance = cls(**run_data)
        return instance

    run_alias: str = Field(description="The alias for the run.")
    jobs: List[JobDesc] = Field(description="The jobs to run.", default_factory=list)
    doc: DocumentationMetadataModel = Field(
        description="A description/doc for this run.",
        default_factory=DocumentationMetadataModel.create,
    )

    @model_validator(mode="before")
    @classmethod
    def validate_inputs(cls, values):
        if "jobs" not in values.keys():
            raise ValueError("Missing required 'jobs' key.")

        jobs = values["jobs"]
        if not isinstance(jobs, list):
            raise ValueError("Invalid 'jobs' value, must be a list.")

        new_jobs = []
        for job in jobs:
            if isinstance(job, JobDesc):
                job_spec = job
            elif isinstance(job, Mapping):
                job_spec = JobDesc(**job)
            elif isinstance(job, (str, Path)):
                job_spec = JobDesc.create_from_file(job)
            else:
                raise ValueError(f"Invalid job spec type: {job}")

            # TODO: validate 'save' fields
            new_jobs.append(job_spec)

        values["jobs"] = new_jobs
        return values

    @field_validator("doc", mode="before")
    @classmethod
    def validate_doc(cls, value):
        return DocumentationMetadataModel.create(value)


class JobTest(object):
    def __init__(
        self,
        kiara_api: "BaseAPI",
        job_desc: JobDesc,
        tests: Union[Mapping[str, Mapping[str, Any]], None] = None,
    ):

        self._kiara_api: Union[BaseAPI, KiaraAPI] = kiara_api
        self._job_desc = job_desc
        if tests is None:
            tests = {}
        self._tests: Mapping[str, Mapping[str, Any]] = tests

    def run_tests(self):

        print(f"Running tests for job '{self._job_desc.job_alias}'...")  # noqa

        result = self.run_job()

        if "fail" in self._job_desc.job_alias:
            self.check_failure(result)
        else:
            self.check_result(result)

    def run_job(self) -> Union["ValueMap", Exception]:

        print(f"Running checks for job '{self._job_desc.job_alias}'...")  # noqa

        try:
            result: Union[ValueMap, Exception] = self._kiara_api.run_job(
                operation=self._job_desc,
                comment=f"Test run '{self._job_desc.job_alias}'",
            )
            success = True

        except Exception as e:
            success = False
            result = KiaraException(
                f"Failed to run job '{self._job_desc.job_alias}': {e}"
            )

        if success and "fail" in self._job_desc.job_alias:
            raise KiaraException(
                f"Job '{self._job_desc.job_alias}' should have failed but didn't."
            )
        elif not success and "fail" not in self._job_desc.job_alias:
            raise KiaraException(
                f"Job '{self._job_desc.job_alias}' should have succeeded but didn't."
            )
        elif not success:
            terminal_print(result)
            return result
        else:
            return result

    def check_failure(self, result: Exception):

        try:

            import inspect

            for test_name, test in self._tests.items():

                if not callable(test):

                    if not isinstance(test, str):
                        raise KiaraException(
                            f"Invalid test pattern for error check in test '{test_name}', must be a string: {test}"
                        )

                    tokens = test_name.split("::")
                    if tokens[0] != "error":
                        raise KiaraException(
                            f"Invalid test pattern, must be 'error::msg' or 'error::msg_contains': {test_name}"
                        )

                    if tokens[1] == "msg":
                        if test != str(result):
                            raise AssertionError(
                                f"Error test pattern check for job '{self._job_desc.job_alias}' failed: {result} (result) != {test} (expected)"
                            )
                    elif tokens[1].startswith("msg_contains"):
                        if test not in str(result):
                            raise AssertionError(
                                f"Error test pattern check for job '{self._job_desc.job_alias}' failed: {result} (result) does not contain '{test}' (expected)"
                            )
                    else:
                        raise KiaraException(
                            f"Invalid test pattern, must be 'error::msg' or start with 'error::msg_contains': {test_name}"
                        )

                else:

                    args = inspect.signature(test)
                    arg_values: List[Any] = []

                    for arg_name in args.parameters.keys():
                        if arg_name == "kiara_api":
                            arg_values.append(self._kiara_api)
                        elif arg_name == "error":
                            arg_values.append(result)
                        else:
                            raise KiaraException(
                                f"Invalid test function: '{test_name}', argument '{arg_name}' not available in result. Available arguments: 'kiara', or 'error'."
                            )
                    test(*arg_values)
        except Exception as e:
            exc = KiaraException(
                f"Failed to run test '{test}' for job '{self._job_desc.job_alias}': {e}"
            )
            terminal_print(exc)
            raise e
        return result

    def check_result(self, result: "ValueMap"):

        try:

            import inspect

            from kiara.api import Value
            from kiara.interfaces.python_api.base_api import BaseAPI

            for test_name, test in self._tests.items():

                if not callable(test):
                    tokens = test_name.split("::")
                    value = result.get_value_obj(tokens[0])

                    if len(tokens) > 1:
                        if isinstance(self._kiara_api, BaseAPI):
                            data_to_test = self._kiara_api.query_value(
                                value, "::".join(tokens[1:])
                            )
                        else:
                            data_to_test = self._kiara_api._api.query_value(
                                value, "::".join(tokens[1:])
                            )
                    else:
                        data_to_test = value

                    if isinstance(data_to_test, Value):
                        data_to_test = data_to_test.data

                    if test != data_to_test:
                        raise AssertionError(
                            f"Test pattern '{test_name}' for job '{self._job_desc.job_alias}' failed: {data_to_test} (result) != {test} (expected)"
                        )

                else:

                    args = inspect.signature(test)
                    arg_values: List[Any] = []

                    for arg_name in args.parameters.keys():
                        if arg_name == "kiara_api":
                            arg_values.append(self._kiara_api)
                        elif arg_name == "outputs":
                            arg_values.append(result)
                        elif arg_name in result.field_names:
                            arg_values.append(result.get_value_obj(arg_name))
                        else:
                            raise KiaraException(
                                f"Invalid test function: '{test_name}', argument '{arg_name}' not available in result. Available arguments: {result.field_names} or 'outputs' for all outputs."
                            )
                    test(*arg_values)
        except Exception as e:
            exc = KiaraException(
                f"Failed to run test '{test}' for job '{self._job_desc.job_alias}': {e}"
            )
            terminal_print(exc)
            raise e
        return result


# kiara\kiara\src\kiara\interfaces\python_api\models\workflow.py
# -*- coding: utf-8 -*-
import uuid
from typing import TYPE_CHECKING, Any

from pydantic import Field

from kiara.models import KiaraModel

if TYPE_CHECKING:
    from kiara.context import Kiara


class WorkflowMatcher(KiaraModel):
    """An object describing requirements values should satisfy in order to be included in a query result."""

    @classmethod
    def create_matcher(self, **match_options: Any):

        m = WorkflowMatcher(**match_options)
        return m

    has_alias: bool = Field(
        description="Workflow must have at least one alias.", default=False
    )

    def is_match(self, workflow_id: uuid.UUID, kiara: "Kiara") -> bool:

        if self.has_alias:
            aliases = kiara.workflow_registry.get_aliases(workflow_id=workflow_id)
            if not aliases:
                return False

        return True


# kiara\kiara\src\kiara\interfaces\python_api\models\__init__.py


# kiara\kiara\src\kiara\models\archives.py
# -*- coding: utf-8 -*-
import uuid
from typing import (
    TYPE_CHECKING,
    Any,
    ClassVar,
    List,
    Literal,
    Mapping,
    Type,
    Union,
)

import orjson
from pydantic import Field
from rich import box
from rich.console import RenderableType
from rich.panel import Panel
from rich.syntax import Syntax
from rich.table import Table

from kiara.interfaces.python_api.models.info import (
    InfoItemGroup,
    ItemInfo,
    TypeInfo,
    TypeInfoItemGroup,
)
from kiara.models.documentation import (
    AuthorModel,
    AuthorsMetadataModel,
    ContextMetadataModel,
    DocumentationMetadataModel,
)
from kiara.models.python_class import PythonClass
from kiara.registries import ArchiveDetails, ArchiveMetadata, KiaraArchive
from kiara.utils.json import orjson_dumps

if TYPE_CHECKING:
    from kiara.api import KiArchive
    from kiara.context import Kiara


class ArchiveTypeInfo(TypeInfo):

    _kiara_model_id: ClassVar = "info.archive_type"

    @classmethod
    def create_from_type_class(
        self, type_cls: Type[KiaraArchive], kiara: "Kiara"
    ) -> "ArchiveTypeInfo":

        authors_md = AuthorsMetadataModel.from_class(type_cls)
        doc = DocumentationMetadataModel.from_class_doc(type_cls)
        python_class = PythonClass.from_class(type_cls)
        properties_md = ContextMetadataModel.from_class(type_cls)
        type_name = type_cls._archive_type_name  # type: ignore

        return ArchiveTypeInfo(
            type_name=type_name,
            documentation=doc,
            authors=authors_md,
            context=properties_md,
            python_class=python_class,
            supported_item_types=list(type_cls.supported_item_types()),
        )

    @classmethod
    def base_class(self) -> Type[KiaraArchive]:
        return KiaraArchive

    @classmethod
    def category_name(cls) -> str:
        return "archive_type"

    is_writable: bool = Field(
        description="Whether this archive is writeable.", default=False
    )
    supported_item_types: List[str] = Field(
        description="The item types this archive suports."
    )

    def create_renderable(self, **config: Any) -> RenderableType:

        include_doc = config.get("include_doc", True)

        table = Table(box=box.SIMPLE, show_header=False, padding=(0, 0, 0, 0))
        table.add_column("property", style="i")
        table.add_column("value")

        if include_doc:
            table.add_row(
                "Documentation",
                Panel(self.documentation.create_renderable(), box=box.SIMPLE),
            )
        table.add_row("Author(s)", self.authors.create_renderable())
        table.add_row("Context", self.context.create_renderable())

        table.add_row("Python class", self.python_class.create_renderable())

        # table.add_row("is_writeable", "yes" if self.is_writable else "no")
        table.add_row(
            "supported_item_types", ", ".join(sorted(self.supported_item_types))
        )

        return table


class ArchiveTypeClassesInfo(TypeInfoItemGroup):

    _kiara_model_id: ClassVar = "info.archive_types"

    @classmethod
    def base_info_class(cls) -> Type[ArchiveTypeInfo]:
        return ArchiveTypeInfo

    type_name: Literal["archive_type"] = "archive_type"
    item_infos: Mapping[str, ArchiveTypeInfo] = Field(  # type: ignore
        description="The archive info instances for each type."
    )


class ArchiveInfo(ItemInfo):
    @classmethod
    def base_instance_class(cls) -> Type[KiaraArchive]:
        return KiaraArchive

    @classmethod
    def create_from_instance(cls, kiara: "Kiara", instance: KiaraArchive, **kwargs):

        if kwargs:
            raise ValueError("kwargs not supported.")

        return cls.create_from_archive(kiara=kiara, archive=instance)

    @classmethod
    def create_from_archive(
        cls,
        kiara: "Kiara",
        archive: KiaraArchive,
        # archive_aliases: Union[Iterable[str], None] = None,
    ):

        doc_str = archive.archive_metadata.get("description", None)
        doc = DocumentationMetadataModel.create(doc_str)

        authors_raw = archive.archive_metadata.get("authors", [])
        _authors = []
        for author in authors_raw:
            author = AuthorModel(**author)
            _authors.append(author)
        authors = AuthorsMetadataModel(authors=_authors)

        tags = archive.archive_metadata.get("tags", [])
        labels = archive.archive_metadata.get("labels", {})

        references = archive.archive_metadata.get("references", {})
        # TODO: add references model

        context = ContextMetadataModel(tags=tags, labels=labels, references=references)

        # archive_types = list(archive.supported_item_types())

        archive_type_info = ArchiveTypeInfo.create_from_type_class(
            archive.__class__, kiara=kiara
        )
        # if archive_aliases is None:
        #     archive_aliases = []
        # else:
        #     archive_aliases = list(archive_aliases)
        return ArchiveInfo(
            archive_type_info=archive_type_info,
            archive_alias=archive.archive_name,
            archive_id=archive.archive_id,
            type_name=str(archive.archive_id),
            documentation=doc,
            authors=authors,
            context=context,
            # archive_types=archive_types,
            details=archive.get_archive_details(),
            metadata=archive.archive_metadata,
            config=archive.config.model_dump(),
            # aliases=archive_aliases,
        )

    @classmethod
    def category_name(cls) -> str:
        return "info.archive"

    archive_id: uuid.UUID = Field(description="The (globally unique) archive id.")
    archive_alias: str = Field(description="The archive alias.")

    archive_type_info: ArchiveTypeInfo = Field(
        description="Information about this archives' type."
    )
    # archive_types: List[Literal["data", "alias", "job_record", "workflow"]] = Field(description="The archive type.")

    config: Mapping[str, Any] = Field(description="The configuration of this archive.")
    details: ArchiveDetails = Field(
        description="Type dependent (runtime) details for this archive."
    )
    metadata: ArchiveMetadata = Field(description="Metadata for this archive.")
    # aliases: List[str] = Field(
    #     description="Aliases for this archive.", default_factory=list
    # )

    def create_renderable(self, **config: Any) -> RenderableType:
        from kiara.utils.output import extract_renderable

        table = Table(show_header=False, box=box.SIMPLE)
        table.add_column("property", style="i")
        table.add_column("value")

        details = extract_renderable(self.details, render_config=config)
        metadata = extract_renderable(self.metadata, render_config=config)
        type_info = self.archive_type_info.create_renderable(**config)
        # table.add_row("archive id", str(self.archive_id))
        # table.add_row("archive alias", self.archive_alias)
        # table.add_row("archive type(s)", ", ".join(self.archive_types))
        table.add_row("details", details)
        table.add_row("metadata", metadata)
        table.add_row("archive type info", type_info)
        if self.documentation.is_set:
            table.add_row("doc", self.documentation.create_renderable(**config))
        if self.authors.authors:
            table.add_row("author(s)", self.authors.create_renderable(**config))
        if self.context.labels or self.context.tags or self.context.references:
            table.add_row("context", self.context.create_renderable(**config))

        return table


class ArchiveGroupInfo(InfoItemGroup):

    _kiara_model_id: ClassVar = "info.archives"

    @classmethod
    def base_info_class(cls) -> Type[ItemInfo]:
        return ArchiveInfo

    @classmethod
    def create_from_context(
        cls, kiara: "Kiara", group_title: Union[str, None] = None
    ) -> "ArchiveGroupInfo":

        archives = {}
        for archive, aliases in kiara.get_all_archives().items():
            title = str(archive.archive_id) + ", ".join(aliases)
            archives[title] = ArchiveInfo.create_from_archive(
                kiara=kiara, archive=archive
            )
            # archives[str(archive.archive_id)] = ArchiveInfo.create_from_archive(
            #     kiara=kiara, archive=archive, archive_aliases=aliases
            # )

        info = cls(group_title=group_title, item_infos=archives)
        return info

    item_infos: Mapping[str, ArchiveInfo] = Field(
        description="The info for each archive."
    )

    @property
    def combined_size(self) -> int:

        combined = 0
        archive_ids = set()
        for archive_info in self.item_infos.values():
            if archive_info.archive_id in archive_ids:
                continue
            archive_ids.add(archive_info.archive_id)
            size = archive_info.details.root.get("size", 0)
            if size and size > 0:
                combined = combined + size

        return combined

    def create_renderable(self, **config: Any) -> RenderableType:

        show_archive_id = config.get("show_archive_id", False)
        show_config = config.get("show_config", True)
        show_details = config.get("show_details", False)

        # by_type: Dict[str, Dict[str, ArchiveInfo]] = {}
        # for archive_id, archive in sorted(self.item_infos.items()):
        #     for item_type in archive.archive_type_info.supported_item_types:
        #         by_type.setdefault(item_type, {})[archive.type_name] = archive

        table = Table(show_header=True, box=box.SIMPLE)
        if show_archive_id:
            table.add_column("archive id")
        table.add_column("alias", style="i")
        table.add_column("item type(s)", style="i")
        table.add_column("archive type", style="i")
        if show_config:
            table.add_column("config")
        if show_details:
            table.add_column("details")

        for archive in self.item_infos.values():
            row: List[RenderableType] = []
            if show_archive_id:
                row.append(str(archive.archive_id))
            row.append(archive.archive_alias)
            row.append("\n".join(archive.archive_type_info.supported_item_types))
            row.append(archive.archive_type_info.type_name)

            if show_config:
                config_json = Syntax(
                    orjson_dumps(archive.config, option=orjson.OPT_INDENT_2),
                    "json",
                    background_color="default",
                )
                row.append(config_json)
            if show_details:
                details_json = Syntax(
                    orjson_dumps(archive.details, option=orjson.OPT_INDENT_2),
                    "json",
                    background_color="default",
                )
                row.append(details_json)

            table.add_row(*row)

        return table


class KiArchiveInfo(ItemInfo):
    @classmethod
    def base_instance_class(cls) -> Type["KiArchive"]:
        from kiara.api import KiArchive

        return KiArchive

    @classmethod
    def create_from_instance(
        cls, kiara: "Kiara", instance: "KiArchive", **kwargs
    ) -> "KiArchiveInfo":

        return cls.create_from_kiarchive(kiarchive=instance, **kwargs)

    @classmethod
    def create_from_kiarchive(cls, kiarchive: "KiArchive") -> "KiArchiveInfo":

        data_archive = kiarchive.data_archive
        alias_archive = kiarchive.alias_archive
        job_archive = kiarchive.job_archive
        metadata_archive = kiarchive.metadata_archive

        data_archive_info = None
        alias_archive_info = None
        job_archive_info = None
        metadata_archive_info = None

        documentation: Union[DocumentationMetadataModel, None] = None
        authors: Union[AuthorsMetadataModel, None] = None
        context: Union[ContextMetadataModel, None] = None

        _kiara = kiarchive._kiara
        if _kiara is None:
            raise ValueError("No kiara instance attached to kiarchive instance.")

        if data_archive:
            data_archive_info = ArchiveInfo.create_from_archive(
                kiara=_kiara, archive=data_archive
            )
            documentation = data_archive_info.documentation
            authors = data_archive_info.authors
            context = data_archive_info.context

        if alias_archive:
            alias_archive_info = ArchiveInfo.create_from_archive(
                kiara=_kiara, archive=alias_archive
            )
            # TODO: should we separate those per archive?
            documentation = alias_archive_info.documentation
            authors = alias_archive_info.authors
            context = alias_archive_info.context

        if metadata_archive:
            metadata_archive_info = ArchiveInfo.create_from_archive(
                kiara=_kiara, archive=metadata_archive
            )
            documentation = metadata_archive_info.documentation
            authors = metadata_archive_info.authors
            context = metadata_archive_info.context

        if job_archive:
            job_archive_info = ArchiveInfo.create_from_archive(
                kiara=_kiara, archive=job_archive
            )
            documentation = job_archive_info.documentation
            authors = job_archive_info.authors
            context = job_archive_info.context

        if documentation is None or authors is None or context is None:
            raise ValueError("No documentation, authors or context found.")

        return KiArchiveInfo(
            type_name=kiarchive.archive_file_name,
            data_archive_info=data_archive_info,
            alias_archive_info=alias_archive_info,
            metadata_archive_info=metadata_archive_info,
            job_archive_info=job_archive_info,
            documentation=documentation,
            authors=authors,
            context=context,
        )

    data_archive_info: Union[ArchiveInfo, None] = Field(
        description="The info for the included data archive."
    )
    alias_archive_info: Union[ArchiveInfo, None] = Field(
        description="The info for the included alias archive."
    )
    metadata_archive_info: Union[ArchiveInfo, None] = Field(
        description="The info for the included metadata archive."
    )
    job_archive_info: Union[ArchiveInfo, None] = Field(
        description="The info for the included job archive."
    )

    def create_renderable(self, **config: Any) -> RenderableType:

        table = Table(show_header=False, box=box.SIMPLE)
        table.add_column("property", style="i")
        table.add_column("value")

        if self.data_archive_info:
            content = self.data_archive_info.create_renderable(**config)
        else:
            content = "-- no data archive --"
        table.add_row("data archive", content)

        if self.alias_archive_info:
            content = self.alias_archive_info.create_renderable(**config)
        else:
            content = "-- no alias archive --"
        table.add_row("alias archive", content)

        if self.metadata_archive_info:
            content = self.metadata_archive_info.create_renderable(**config)
        else:
            content = "-- no metadata archive --"
        table.add_row("metadata archive", content)

        if self.job_archive_info:
            content = self.job_archive_info.create_renderable(**config)
        else:
            content = "-- no job archive --"
        table.add_row("job archive", content)

        return table


# kiara\kiara\src\kiara\models\context.py
# -*- coding: utf-8 -*-
import uuid
from typing import TYPE_CHECKING, Any, Dict, List, Mapping, Union

import humanfriendly
from humanfriendly import format_size
from pydantic import Field, PrivateAttr, RootModel
from rich import box
from rich.console import Group, RenderableType
from rich.markdown import Markdown
from rich.table import Table

from kiara.models import KiaraModel
from kiara.models.archives import ArchiveGroupInfo

if TYPE_CHECKING:
    from kiara.context import Kiara, KiaraConfig, KiaraContextConfig, KiaraRuntimeConfig


class ContextInfo(KiaraModel):
    @classmethod
    def create_from_context_config(
        cls,
        config: "KiaraContextConfig",
        context_name: Union[str, None] = None,
        runtime_config: Union["KiaraRuntimeConfig", None] = None,
    ) -> "ContextInfo":

        from kiara.context import Kiara

        kiara = Kiara(config=config, runtime_config=runtime_config)
        return cls.create_from_context(kiara=kiara, context_name=context_name)

    @classmethod
    def create_from_context(
        cls, kiara: "Kiara", context_name: Union[str, None] = None
    ) -> "ContextInfo":

        errors = {}
        try:
            value_ids = list(kiara.data_registry.retrieve_all_available_value_ids())
        except Exception as e:
            errors["values"] = e
            value_ids = []

        try:
            aliases = {
                a.full_alias: a.value_id for a in kiara.alias_registry.aliases.values()
            }
        except Exception as e:
            errors["aliases"] = e
            aliases = {}

        try:
            archives_info = ArchiveGroupInfo.create_from_context(kiara=kiara)
        except Exception as e:
            errors["archives"] = e
            archives_info = ArchiveGroupInfo(item_infos={})

        comment = None
        invalid = False
        if errors:
            invalid = True
            comment = "Errors creating this context, this is most likely a bug:\n\n"
            for k, err in errors.items():
                comment = f"{comment}  - {k}: {err}"

        result = ContextInfo(
            kiara_id=kiara.id,
            value_ids=value_ids,
            aliases=aliases,
            context_name=context_name,
            archives=archives_info,
            invalid=invalid,
            comment=comment,
        )
        result._kiara = kiara
        return result

    kiara_id: uuid.UUID = Field(
        description="The (globally unique) id of the kiara context."
    )
    context_name: Union[str, None] = Field(
        description="The local alias for this context."
    )
    value_ids: List[uuid.UUID] = Field(
        description="The ids of all stored values in this context."
    )
    aliases: Dict[str, uuid.UUID] = Field(
        description="All available aliases within this context (and the value ids they refer to)."
    )
    archives: ArchiveGroupInfo = Field(
        description="The archives registered in this context."
    )
    invalid: bool = Field(description="Whether this context has errors.", default=False)
    comment: Union[str, None] = Field(
        default=None, description="(Optional) comment about this context."
    )

    _kiara: Union["Kiara", None] = PrivateAttr()

    @property
    def kiara_context(self) -> "Kiara":
        if self._kiara is None:
            raise Exception("Kiara context object not set.")
        return self._kiara

    def value_summary(self) -> Dict[str, Any]:

        sum_size = 0
        types: Dict[str, int] = {}
        internal_types: Dict[str, int] = {}
        no_of_values = len(self.value_ids)

        for value_id in self.value_ids:
            value = self.kiara_context.data_registry.get_value(value=value_id)
            sum_size = sum_size + value.value_size
            if self.kiara_context.type_registry.is_internal_type(value.data_type_name):
                if value.data_type_name not in internal_types.keys():
                    internal_types[value.data_type_name] = 1
                else:
                    internal_types[value.data_type_name] += 1
            else:
                if value.data_type_name not in types.keys():
                    types[value.data_type_name] = 1
                else:
                    types[value.data_type_name] += 1

            types.setdefault(value.data_type_name, 0)

        return {
            "size": sum_size,
            "no_values": no_of_values,
            "types": types,
            "internal_types": internal_types,
        }

    def alias_summary(self) -> Dict[str, Any]:

        sum_size = 0
        types: Dict[str, int] = {}
        internal_types: Dict[str, int] = {}
        no_of_values = len(self.value_ids)

        for alias, value_id in self.aliases.items():
            value = self.kiara_context.data_registry.get_value(value=value_id)
            sum_size = sum_size + value.value_size
            if self.kiara_context.type_registry.is_internal_type(value.data_type_name):
                if value.data_type_name not in internal_types.keys():
                    internal_types[value.data_type_name] = 1
                else:
                    internal_types[value.data_type_name] += 1
            else:
                if value.data_type_name not in types.keys():
                    types[value.data_type_name] = 1
                else:
                    types[value.data_type_name] += 1

            types.setdefault(value.data_type_name, 0)

        return {
            "size": sum_size,
            "no_values": no_of_values,
            "types": types,
            "internal_types": internal_types,
        }

    def create_renderable(self, **config: Any) -> RenderableType:

        full_details = config.get("full_details", False)
        show_value_ids = config.get("show_value_ids", False)
        show_archive_info = config.get("show_archive_info", True)

        table = Table(box=box.SIMPLE, show_header=False)

        table.add_column("Property", style="i")
        table.add_column("Value")

        if self.invalid:
            msg = "[red]This context is invalid![/red]\n"
            cmt = self.comment if self.comment else "-- no details --"
            md = Markdown(cmt)
            table.add_row(
                "",
                Group(msg, md, ""),
            )

        if self.context_name:
            table.add_row("context name", self.context_name)
        table.add_row("kiara_id", str(self.kiara_id))

        size_on_disk = humanfriendly.format_size(self.archives.combined_size)
        table.add_row("size on disk", size_on_disk)

        value_sum = self.value_summary()
        v_table = Table(box=box.SIMPLE, show_header=False)
        v_table.add_column("Property")
        v_table.add_column("Value")
        v_table.add_row("no. values", str(value_sum["no_values"]))
        v_table.add_row("combined size", format_size(value_sum["size"]))
        if full_details and show_value_ids:
            if self.value_ids:
                value_ids = sorted((str(v) for v in self.value_ids))
                v_table.add_row("value_ids", value_ids[0])
                for v_id in value_ids[1:]:
                    v_table.add_row("", v_id)
            else:
                v_table.add_row("value_ids", "")
        table.add_row("values", v_table)

        alias_sum = self.alias_summary()
        a_table = Table(box=box.SIMPLE, show_header=False)
        a_table.add_column("Property")
        a_table.add_column("Value")
        a_table.add_row("no. aliases", str(len(self.aliases)))
        a_table.add_row("combined size", format_size(alias_sum["size"]))
        if full_details:
            if self.aliases:
                aliases = sorted(self.aliases.keys())
                a_table.add_row(
                    "aliases", f"{aliases[0]} -> {self.aliases[aliases[0]]}"
                )
                for alias in aliases[1:]:
                    a_table.add_row("", f"{alias} -> {self.aliases[alias]}")
            else:
                a_table.add_row("aliases", "")
        table.add_row("aliases", a_table)

        if show_archive_info:
            table.add_row("archives", self.archives)

        return table


class ContextInfos(RootModel):
    root: Dict[str, ContextInfo]

    @classmethod
    def create_context_infos(
        cls, contexts: Union[Mapping[str, "KiaraContextConfig"], None] = None
    ) -> "ContextInfos":

        if not contexts:
            kc = KiaraConfig()
            contexts = kc.context_configs

        return ContextInfos(
            root={
                a: ContextInfo.create_from_context_config(c, context_name=a)
                for a, c in contexts.items()
            }
        )

    def create_renderable(self, **config: Any) -> RenderableType:

        full_details = config.get("full_details", False)

        if not full_details:
            table = Table(box=box.SIMPLE, show_header=True, show_lines=False)
            table.add_column("context name", style="i")
            table.add_column("context id", style="i")
            table.add_column("size on disk")
            table.add_column("size of all values")
            table.add_column("no. values")
            table.add_column("no. aliaes")
            for context_name, context_summary in self.root.items():
                size_on_disk = context_summary.archives.combined_size
                value_summary = context_summary.value_summary()
                size = humanfriendly.format_size(value_summary["size"])
                no_values = str(value_summary["no_values"])
                no_aliases = str(len(context_summary.aliases))
                table.add_row(
                    context_name,
                    str(context_summary.kiara_id),
                    humanfriendly.format_size(size_on_disk),
                    size,
                    no_values,
                    no_aliases,
                )
        else:

            table = Table(box=box.MINIMAL, show_header=True, show_lines=True)
            table.add_column("context_name", style="i")
            table.add_column("details")

            for context_name, context_summary in self.root.items():

                table.add_row(context_name, context_summary.create_renderable(**config))

        return table


# kiara\kiara\src\kiara\models\data_types.py
# -*- coding: utf-8 -*-

"""
This module contains the metadata (and other) models that are used in the ``kiara_plugin.core_types`` package.

Those models are convenience wrappers that make it easier for *kiara* to find, create, manage and version metadata -- but also
other type of models -- that is attached to data, as well as *kiara* modules.

Metadata models must be a sub-class of [kiara.metadata.MetadataModel][kiara.metadata.MetadataModel]. Other models usually
sub-class a pydantic BaseModel or implement custom base classes.
"""

from typing import Any, Dict, Hashable, Mapping

from pydantic import BaseModel, Field, PrivateAttr

from kiara.models.python_class import PythonClass
from kiara.utils.hashing import compute_cid


class KiaraDict(BaseModel, Mapping):

    dict_data: Dict[Hashable, Any] = Field(description="The data.")
    data_schema: Dict[str, Any] = Field(description="The schema.")
    python_class: PythonClass = Field(
        description="The python class of which model instances are created. This is mostly meant as a hint for client applications."
    )

    _size_cache: int = PrivateAttr(default=None)
    _hash_cache: int = PrivateAttr(default=None)
    _data_hash: int = PrivateAttr(default=None)
    _schema_hash: int = PrivateAttr(default=None)
    _value_hash: int = PrivateAttr(default=None)

    @property
    def size(self):
        if self._size_cache is not None:
            return self._size_cache

        self._size_cache = len(self.dict_data) + len(self.data_schema)
        return self._size_cache

    @property
    def data_hash(self) -> int:
        if self._data_hash is not None:
            return self._data_hash

        self._data_hash = compute_cid(self.dict_data)
        return self._data_hash

    @property
    def schema_hash(self) -> int:
        if self._schema_hash is not None:
            return self._schema_hash

        self._schema_hash = compute_cid(self.data_schema)
        return self._schema_hash

    @property
    def value_hash(self) -> int:
        if self._value_hash is not None:
            return self._value_hash

        obj = {"data": self.data_hash, "data_schema": self.schema_hash}
        self._value_hash = compute_cid(obj)
        return self._value_hash

    def __getitem__(self, item):
        return self.dict_data.__getitem__(item)

    def __iter__(self):
        return self.dict_data.__iter__()

    def __len__(self):
        return self.dict_data.__len__()

    def __repr__(self):
        full = {
            "dict_data": self.dict_data,
            "data_schema": self.data_schema,
        }
        return full.__repr__()


# kiara\kiara\src\kiara\models\documentation.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import inspect
from typing import Any, Callable, ClassVar, Dict, List, Mapping, Tuple, Type, Union

from pydantic import ConfigDict
from pydantic.fields import Field
from pydantic.main import BaseModel
from pydantic.networks import EmailStr
from rich import box
from rich.console import RenderableType
from rich.markdown import Markdown
from rich.table import Table

from kiara.defaults import DEFAULT_NO_DESC_VALUE
from kiara.models import KiaraModel
from kiara.utils import log_exception
from kiara.utils.dicts import merge_dicts
from kiara.utils.global_metadata import get_metadata_for_python_module_or_class


class AuthorModel(BaseModel):
    """Details about an author of a resource."""

    model_config = ConfigDict(title="Author")

    name: str = Field(description="The full name of the author.")
    email: Union[EmailStr, None] = Field(
        description="The email address of the author", default=None
    )


class LinkModel(BaseModel):
    """A description and url for a reference of any kind."""

    model_config = ConfigDict(title="Link")

    # TODO: use AnyUrl instead of str
    url: str = Field(description="The url.")
    desc: Union[str, None] = Field(
        description="A short description of the link content.",
        default=DEFAULT_NO_DESC_VALUE,
    )


class AuthorsMetadataModel(KiaraModel):
    """Information about all authors of a resource."""

    _kiara_model_id: ClassVar[str] = "metadata.authors"
    model_config = ConfigDict(extra="ignore", title="Authors")

    _metadata_key: ClassVar[str] = "origin"

    @classmethod
    def from_class(cls, item_cls: Type) -> "AuthorsMetadataModel":

        data = get_metadata_for_python_module_or_class(item_cls)  # type: ignore
        merged = merge_dicts(*data)
        return cls(**merged)

    authors: List[AuthorModel] = Field(
        description="The authors/creators of this item.", default_factory=list
    )

    def create_renderable(self, **config: Any) -> RenderableType:

        table = Table(show_header=False, box=box.SIMPLE)
        table.add_column("Name")
        table.add_column("Email", style="i")

        for author in reversed(self.authors):
            if author.email:
                authors: Tuple[str, Union[str, EmailStr]] = (author.name, author.email)
            else:
                authors = (author.name, "")
            table.add_row(*authors)

        return table


class ContextMetadataModel(KiaraModel):
    """Information about the context of a resource."""

    _kiara_model_id: ClassVar = "metadata.context"
    model_config = ConfigDict(extra="ignore", title="Context")

    @classmethod
    def from_class(cls, item_cls: Type):

        data = get_metadata_for_python_module_or_class(item_cls)  # type: ignore
        merged = merge_dicts(*data)
        return cls(**merged)

    _metadata_key: ClassVar[str] = "properties"

    references: Dict[str, LinkModel] = Field(
        description="References for the item.", default_factory=dict
    )
    tags: List[str] = Field(
        description="A list of tags for the item.", default_factory=list
    )
    labels: Dict[str, str] = Field(
        description="A list of labels for the item.", default_factory=dict
    )

    def create_renderable(self, **config: Any) -> RenderableType:

        table = Table(show_header=False, box=box.SIMPLE)
        table.add_column("Key", style="i")
        table.add_column("Value")

        if self.tags:
            table.add_row("Tags", ", ".join(self.tags))
        if self.labels:
            labels = []
            for k, v in self.labels.items():
                labels.append(f"[i]{k}[/i]: {v}")
            table.add_row("Labels", "\n".join(labels))

        if self.references:
            references = []
            for _k, _v in self.references.items():
                link = f"[link={_v.url}]{_v.url}[/link]"
                references.append(f"[i]{_k}[/i]: {link}")
            table.add_row("References", "\n".join(references))

        return table

    def add_reference(
        self,
        ref_type: str,
        url: str,
        desc: Union[str, None] = None,
        force: bool = False,
    ):

        if ref_type in self.references.keys() and not force:
            raise Exception(f"Reference of type '{ref_type}' already present.")
        link = LinkModel(url=url, desc=desc)
        self.references[ref_type] = link

    def get_url_for_reference(self, ref: str) -> Union[str, None]:

        link = self.references.get(ref, None)
        if not link:
            return None

        return link.url


class DocumentationMetadataModel(KiaraModel):
    """Documentation about a resource."""

    model_config = ConfigDict(title="Documentation")

    _kiara_model_id: ClassVar = "metadata.documentation"

    _metadata_key: ClassVar[str] = "documentation"

    @classmethod
    def from_class_doc(cls, item_cls: Type) -> "DocumentationMetadataModel":

        doc: Union[str, None] = None

        if hasattr(item_cls, "type_doc"):
            if callable(item_cls.type_doc):
                try:
                    doc = item_cls.type_doc()
                except Exception as e:
                    log_exception(e)
            else:
                doc = item_cls.type_doc

        if not doc:
            doc = item_cls.__doc__

        if not doc:
            doc = DEFAULT_NO_DESC_VALUE

        doc = inspect.cleandoc(doc)

        return cls.from_string(doc)

    @classmethod
    def from_function(cls, func: Callable) -> "DocumentationMetadataModel":

        doc = func.__doc__

        if not doc:
            doc = DEFAULT_NO_DESC_VALUE

        doc = inspect.cleandoc(doc)
        return cls.from_string(doc)

    @classmethod
    def from_string(cls, doc: Union[str, None]) -> "DocumentationMetadataModel":

        if not doc:
            doc = DEFAULT_NO_DESC_VALUE

        if "\n" in doc:
            desc, doc = doc.split("\n", maxsplit=1)
        else:
            desc = doc
            doc = None

        if doc:
            doc = doc.strip()

        return cls(description=desc.strip(), doc=doc)

    @classmethod
    def from_dict(cls, data: Mapping) -> "DocumentationMetadataModel":

        doc = data.get("doc", None)
        desc = data.get("description", None)
        if desc is None:
            desc = data.get("desc", None)

        if not doc and not desc:
            return cls.from_string(DEFAULT_NO_DESC_VALUE)
        elif doc and not desc:
            return cls.from_string(doc)
        elif desc and not doc:
            return cls.from_string(desc)
        else:
            return cls(description=desc, doc=doc)

    @classmethod
    def create(cls, item: Union[Any, None] = None) -> "DocumentationMetadataModel":

        if not item:
            return cls.from_string(DEFAULT_NO_DESC_VALUE)
        elif isinstance(item, DocumentationMetadataModel):
            return item
        elif isinstance(item, Mapping):
            return cls.from_dict(item)
        if isinstance(item, type):
            return cls.from_class_doc(item)
        elif isinstance(item, str):
            return cls.from_string(item)
        else:
            raise TypeError(f"Can't create documentation from type '{type(item)}'.")

    description: str = Field(
        description="Short description of the item.", default=DEFAULT_NO_DESC_VALUE
    )
    doc: Union[str, None] = Field(
        description="Detailed documentation of the item (in markdown).", default=None
    )

    @property
    def is_set(self) -> bool:
        if self.description and self.description != DEFAULT_NO_DESC_VALUE:
            return True
        else:
            return False

    def _retrieve_data_to_hash(self) -> Any:
        return self.full_doc

    @property
    def full_doc(self):

        if self.doc:
            return f"{self.description}\n\n{self.doc}"
        else:
            return self.description

    def create_renderable(self, **config: Any) -> RenderableType:

        return Markdown(self.full_doc)


# kiara\kiara\src\kiara\models\filesystem.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)
import atexit
import os
import shutil
import tempfile
from pathlib import Path
from typing import Any, Callable, ClassVar, Dict, List, Mapping, Union

import structlog
from deepdiff import DeepHash
from multiformats import CID
from pydantic import BaseModel, Field, PrivateAttr
from rich import box
from rich.console import RenderableType
from rich.table import Table

from kiara.defaults import (
    DEFAULT_EXCLUDE_DIRS,
    DEFAULT_EXCLUDE_FILES,
)
from kiara.exceptions import KiaraException
from kiara.models import KiaraModel
from kiara.utils import log_message
from kiara.utils.files import unpack_archive
from kiara.utils.hashing import KIARA_HASH_FUNCTION, compute_cid_from_file

logger = structlog.getLogger()

FILE_BUNDLE_IMPORT_AVAILABLE_COLUMNS = [
    "id",
    "rel_path",
    # "import_time",
    "mime_type",
    "size",
    "content",
    "file_name",
]


class KiaraFile(KiaraModel):
    """Describes properties for the 'file' value type."""

    _kiara_model_id: ClassVar = "instance.data.file"

    @classmethod
    def load_file(
        cls,
        source: str,
        file_name: Union[str, None] = None,
        # import_time: Optional[datetime.datetime] = None,
    ) -> "KiaraFile":
        """Utility method to read metadata of a file from disk."""
        import mimetypes

        import filetype

        if not source:
            raise ValueError("No source path provided.")

        if not os.path.exists(os.path.realpath(source)):
            raise ValueError(f"Path does not exist: {source}")

        if not os.path.isfile(os.path.realpath(source)):
            raise ValueError(f"Path is not a file: {source}")

        if file_name is None:
            file_name = os.path.basename(source)

        path: str = os.path.abspath(source)
        # if import_time:
        #     file_import_time = import_time
        # else:
        #     file_import_time = datetime.datetime.now()  # TODO: timezone

        file_stats = os.stat(path)
        size = file_stats.st_size

        r = mimetypes.guess_type(path)
        if r[0] is not None:
            mime_type = r[0]
        else:
            _mime_type = filetype.guess(path)
            if not _mime_type:
                mime_type = "application/octet-stream"
            else:
                mime_type = _mime_type.MIME

        m = KiaraFile(
            # import_time=file_import_time,
            mime_type=mime_type,
            size=size,
            file_name=file_name,
        )
        m._path = path
        return m

    # import_time: datetime.datetime = Field(
    #     description="The time when the file was imported."
    # )
    mime_type: str = Field(description="The mime type of the file.")
    file_name: str = Field("The name of the file.")
    size: int = Field(description="The size of the file.")
    metadata: Dict[str, Any] = Field(
        description="Additional, ustructured, user-defined metadata.",
        default_factory=dict,
    )
    metadata_schemas: Dict[str, str] = Field(
        description="The metadata schemas for each of the metadata values (if available).",
        default_factory=dict,
    )

    _path: Union[str, None] = PrivateAttr(default=None)
    _path_resolver: Union[Callable, None] = PrivateAttr(default=None)
    _file_hash: Union[str, None] = PrivateAttr(default=None)
    _file_cid: Union[CID, None] = PrivateAttr(default=None)

    # @validator("path")
    # def ensure_abs_path(cls, value):
    #     return os.path.abspath(value)

    @property
    def path(self) -> str:
        if self._path is None:
            if self._path_resolver is not None:
                self._path = self._path_resolver()
            else:
                raise Exception("File path not set for file model.")
        return self._path

    def _retrieve_data_to_hash(self) -> Any:
        data = {
            "file_name": self.file_name,
            "file_cid": self.file_cid,
        }
        return data

    # def get_id(self) -> str:
    #     return self.path

    def get_category_alias(self) -> str:
        return "instance.file_model"

    def copy_file(self, target: str, new_name: Union[str, None] = None) -> "KiaraFile":

        target_path: str = os.path.abspath(target)
        os.makedirs(os.path.dirname(target_path), exist_ok=True)

        shutil.copy2(self.path, target_path)
        fm = KiaraFile.load_file(target, file_name=new_name)

        if self._file_hash is not None:
            fm._file_hash = self._file_hash

        return fm

    @property
    def file_hash(self) -> str:

        if self._file_hash is not None:
            return self._file_hash

        self._file_hash = str(self.file_cid)
        return self._file_hash

    @property
    def file_cid(self) -> CID:

        if self._file_cid is not None:
            return self._file_cid

        # TODO: auto-set codec?
        self._file_cid = compute_cid_from_file(file=self.path, codec="raw")
        return self._file_cid

    @property
    def file_name_without_extension(self) -> str:

        return self.file_name.split(".")[0]

    @property
    def file_extension(self) -> str:
        return self.file_name.split(".")[-1]

    def read_text(self, max_lines: int = -1) -> str:
        """Read the content of a file."""
        with open(self.path, "rt") as f:
            if max_lines <= 0:
                content = f.read()
            else:
                content = "".join((next(f) for x in range(max_lines)))
        return content

    def read_bytes(self, length: int = -1) -> bytes:
        """Read the content of a file."""
        with open(self.path, "rb") as f:
            if length <= 0:
                content = f.read()
            else:
                content = f.read(length)
        return content

    def __repr__(self):
        return f"FileModel(name={self.file_name})"

    def __str__(self):
        return self.__repr__()


class FolderImportConfig(BaseModel):

    sub_path: Union[str, None] = Field(
        description="The sub-path to import from the folder.", default=None
    )

    include_files: Union[List[str], None] = Field(
        description="A list of strings, include all files where the filename ends with that string.",
        default=None,
    )
    exclude_dirs: Union[List[str], None] = Field(
        description="A list of strings, exclude all folders whose name ends with that string.",
        default=DEFAULT_EXCLUDE_DIRS,
    )
    exclude_files: Union[List[str], None] = Field(
        description=f"A list of strings, exclude all files that match those (takes precedence over 'include_files'). Defaults to: {DEFAULT_EXCLUDE_FILES}.",
        default=DEFAULT_EXCLUDE_FILES,
    )


class KiaraFileBundle(KiaraModel):
    """Describes properties for the 'file_bundle' value type."""

    _kiara_model_id: ClassVar = "instance.data.file_bundle"

    @classmethod
    def create_tmp_dir(self) -> Path:
        """Utility method to create a temp folder that gets deleted when kiara exits."""
        temp_f = tempfile.mkdtemp()

        def cleanup():
            shutil.rmtree(temp_f, ignore_errors=True)

        atexit.register(cleanup)

        return Path(temp_f)

    @classmethod
    def from_archive(
        cls,
        archive_path: str,
        import_config: Union[FolderImportConfig, None] = None,
        bundle_name: Union[str, None] = None,
    ) -> "KiaraFileBundle":
        """Extracts the contents of an archive file to a target folder."""

        if not os.path.isfile(archive_path):
            raise KiaraException(
                msg=f"Archive file '{archive_path}' does not exist or is not a file."
            )

        out_dir = tempfile.mkdtemp()

        def del_out_dir():
            shutil.rmtree(out_dir, ignore_errors=True)

        atexit.register(del_out_dir)

        unpack_archive(archive_path, out_dir)

        bundle = KiaraFileBundle.import_folder(
            out_dir, import_config=import_config, bundle_name=bundle_name
        )
        return bundle

    @classmethod
    def from_archive_file(
        cls,
        archive_file: KiaraFile,
        import_config: Union[FolderImportConfig, None] = None,
    ) -> "KiaraFileBundle":
        """Extracts the contents of an archive file to a target folder."""

        bundle = KiaraFileBundle.from_archive(
            archive_path=archive_file.path,
            bundle_name=archive_file.file_name,
            import_config=import_config,
        )

        bundle.metadata = archive_file.metadata
        bundle.metadata_schemas = archive_file.metadata_schemas
        return bundle

    @classmethod
    def import_folder(
        cls,
        source: str,
        bundle_name: Union[str, None] = None,
        import_config: Union[None, Mapping[str, Any], FolderImportConfig] = None,
        # import_time: Optional[datetime.datetime] = None,
    ) -> "KiaraFileBundle":

        if not source:
            raise ValueError("No source path provided.")

        if not os.path.exists(os.path.realpath(source)):
            raise ValueError(f"Path does not exist: {source}")

        if not os.path.isdir(os.path.realpath(source)):
            raise ValueError(f"Path is not a folder: {source}")

        if source.endswith(os.path.sep):
            source = source[0:-1]

        if import_config is None:
            _import_config = FolderImportConfig()
        elif isinstance(import_config, Mapping):
            _import_config = FolderImportConfig(**import_config)
        elif isinstance(import_config, FolderImportConfig):
            _import_config = import_config
        else:
            raise TypeError(
                f"Invalid type for folder import config: {type(import_config)}."
            )

        abs_path = os.path.abspath(source)
        if _import_config.sub_path:
            abs_path = os.path.join(abs_path, _import_config.sub_path)

        included_files: Dict[str, KiaraFile] = {}
        exclude_dirs = _import_config.exclude_dirs
        invalid_extensions = _import_config.exclude_files

        valid_extensions = _import_config.include_files

        sum_size = 0

        def include_file(filename: str) -> bool:

            if invalid_extensions and any(
                filename.endswith(ext) for ext in invalid_extensions
            ):
                return False
            if not valid_extensions:
                return True
            else:
                return any(filename.endswith(ext) for ext in valid_extensions)

        if os.path.isfile(abs_path):
            file_model = KiaraFile.load_file(abs_path)
            sum_size = file_model.size
            included_files[file_model.file_name] = file_model
        else:
            for root, dirnames, filenames in os.walk(abs_path, topdown=True):

                if exclude_dirs:
                    dirnames[:] = [d for d in dirnames if d not in exclude_dirs]

                for filename in [
                    f
                    for f in filenames
                    if os.path.isfile(os.path.join(root, f)) and include_file(f)
                ]:

                    full_path = os.path.join(root, filename)
                    rel_path = os.path.relpath(full_path, abs_path)

                    file_model = KiaraFile.load_file(full_path)
                    sum_size = sum_size + file_model.size
                    included_files[rel_path] = file_model

        if bundle_name is None:
            bundle_name = os.path.basename(source)

        bundle = KiaraFileBundle.create_from_file_models(
            files=included_files,
            path=abs_path,
            bundle_name=bundle_name,
            sum_size=sum_size,
        )
        return bundle

    @classmethod
    def create_from_file_models(
        cls,
        files: Mapping[str, KiaraFile],
        bundle_name: str,
        path: Union[str, None] = None,
        sum_size: Union[int, None] = None,
        # import_time: Optional[datetime.datetime] = None,
    ) -> "KiaraFileBundle":

        # if import_time:
        #     bundle_import_time = import_time
        # else:
        #     bundle_import_time = datetime.datetime.now()  # TODO: timezone

        result: Dict[str, Any] = {}

        result["included_files"] = files

        # result["import_time"] = datetime.datetime.now().isoformat()
        result["number_of_files"] = len(files)
        result["bundle_name"] = bundle_name
        # result["import_time"] = bundle_import_time

        if sum_size is None:
            sum_size = 0
            for f in files.values():
                sum_size = sum_size + f.size
        result["size"] = sum_size

        bundle = KiaraFileBundle(**result)
        bundle._path = path
        return bundle

    _file_bundle_hash: Union[int, None] = PrivateAttr(default=None)

    bundle_name: str = Field(description="The name of this bundle.")
    # import_time: datetime.datetime = Field(
    #     description="The time when the file bundle was imported."
    # )
    number_of_files: int = Field(
        description="How many files are included in this bundle."
    )
    included_files: Dict[str, KiaraFile] = Field(
        description="A map of all the included files, incl. their properties. Uses the relative path of each file as key."
    )
    size: int = Field(description="The size of all files in this folder, combined.")
    metadata: Dict[str, Any] = Field(
        description="Additional, ustructured, user-defined metadata.",
        default_factory=dict,
    )
    metadata_schemas: Dict[str, str] = Field(
        description="The metadata schemas for each metadata value (if available).",
        default_factory=dict,
    )
    _path: Union[str, None] = PrivateAttr(default=None)

    @property
    def path(self) -> str:
        if self._path is None:
            # TODO: better explanation, offer remedy like copying into temp folder
            raise Exception(
                "File bundle path not set, it appears this bundle is comprised of symlinks only."
            )
        return self._path

    def _retrieve_id(self) -> str:
        return str(self.file_bundle_hash)

    # @property
    # def model_data_hash(self) -> int:
    #     return self.file_bundle_hash

    def _retrieve_data_to_hash(self) -> Any:

        return {
            "bundle_name": self.bundle_name,
            "included_files": {
                k: v.instance_cid for k, v in self.included_files.items()
            },
        }

    def get_relative_path(self, file: KiaraFile):
        return os.path.relpath(file.path, self.path)

    def read_text_file_contents(self, ignore_errors: bool = False) -> Mapping[str, str]:

        content_dict: Dict[str, str] = {}

        def read_file(rel_path: str, full_path: str):
            with open(full_path, encoding="utf-8") as f:
                try:
                    content = f.read()
                    content_dict[rel_path] = content  # type: ignore
                except Exception as e:
                    if ignore_errors:
                        log_message(f"Can't read file: {e}")
                        logger.warning("ignore.file", path=full_path, reason=str(e))
                    else:
                        raise Exception(f"Can't read file (as text) '{full_path}: {e}")

        # TODO: common ignore files and folders
        for rel_path, f in self.included_files.items():
            if f._path:
                path = f._path
            else:
                path = self.get_relative_path(f)
            read_file(rel_path=rel_path, full_path=path)

        return content_dict

    @property
    def file_bundle_hash(self) -> int:

        # TODO: use sha256?
        if self._file_bundle_hash is not None:
            return self._file_bundle_hash

        obj = {k: v.file_hash for k, v in self.included_files.items()}
        h = DeepHash(obj, hasher=KIARA_HASH_FUNCTION)

        self._file_bundle_hash = h[obj]
        return self._file_bundle_hash

    def copy_bundle(
        self, target_path: str, bundle_name: Union[str, None] = None
    ) -> "KiaraFileBundle":

        if target_path == self.path:
            raise Exception(f"Target path and current path are the same: {target_path}")

        result = {}
        for rel_path, item in self.included_files.items():
            _target_path = os.path.join(target_path, rel_path)
            new_fm = item.copy_file(_target_path)
            result[rel_path] = new_fm

        if bundle_name is None:
            bundle_name = os.path.basename(target_path)

        fb = KiaraFileBundle.create_from_file_models(
            files=result,
            bundle_name=bundle_name,
            path=target_path,
            sum_size=self.size,
            # import_time=self.import_time,
        )
        if self._file_bundle_hash is not None:
            fb._file_bundle_hash = self._file_bundle_hash

        return fb

    def create_renderable(self, **config: Any) -> RenderableType:

        show_bundle_hash = config.get("show_bundle_hash", False)

        table = Table(show_header=False, box=box.SIMPLE)
        table.add_column("key")
        table.add_column("value", style="i")

        table.add_row("bundle name", self.bundle_name)
        # table.add_row("import_time", str(self.import_time))
        table.add_row("number_of_files", str(self.number_of_files))
        table.add_row("size", str(self.size))
        if show_bundle_hash:
            table.add_row("bundle_hash", str(self.file_bundle_hash))

        content = self._create_content_table(**config)
        table.add_row("included files", content)

        return table

    def _create_content_table(self, **render_config: Any) -> Table:

        # show_content = render_config.get("show_content_preview", False)
        max_no_included_files = render_config.get("max_no_files", 40)

        table = Table(show_header=True, box=box.SIMPLE)
        table.add_column("(relative) path")
        table.add_column("size")
        # if show_content:
        #     table.add_column("content preview")

        if (
            max_no_included_files < 0
            or len(self.included_files) <= max_no_included_files
        ):
            for f, model in self.included_files.items():
                row = [f, str(model.size)]
                table.add_row(*row)
        else:
            files = list(self.included_files.keys())
            half = int((max_no_included_files - 1) / 2)
            head = files[0:half]
            tail = files[-1 * half :]
            for rel_path in head:
                model = self.included_files[rel_path]
                row = [rel_path, str(model.size)]
                table.add_row(*row)
            table.add_row("   ... output skipped ...", "")
            table.add_row("   ... output skipped ...", "")
            for rel_path in tail:
                model = self.included_files[rel_path]
                row = [rel_path, str(model.size)]
                table.add_row(*row)

        return table

    def __repr__(self):
        return f"FileBundle(name={self.bundle_name})"

    def __str__(self):
        return self.__repr__()


# kiara\kiara\src\kiara\models\python_class.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import importlib
import inspect
from types import ModuleType
from typing import TYPE_CHECKING, Any, ClassVar, Dict, Type

from pydantic.fields import Field, PrivateAttr

from kiara.models import KiaraModel
from kiara.models.documentation import ContextMetadataModel
from kiara.models.values.value_schema import ValueSchema
from kiara.modules import KiaraModule

if TYPE_CHECKING:
    pass


class PythonClass(KiaraModel):
    """Python class and module information."""

    _kiara_model_id: ClassVar = "instance.wrapped_python_class"

    @classmethod
    def from_class(cls, item_cls: Type, attach_context_metadata: bool = False):

        cls_name = item_cls.__name__
        module_name = item_cls.__module__

        if module_name == "builtins":
            full_name = cls_name
        else:
            full_name = f"{item_cls.__module__}.{item_cls.__name__}"

        conf: Dict[str, Any] = {
            "python_class_name": cls_name,
            "python_module_name": module_name,
            "full_name": full_name,
        }

        if attach_context_metadata:
            raise NotImplementedError()
            ctx_md = ContextMetadataModel.from_class(item_cls=item_cls)
            conf["items"] = ctx_md

        result = PythonClass(**conf)
        result._cls_cache = item_cls
        return result

    python_class_name: str = Field(description="The name of the Python class.")
    python_module_name: str = Field(
        description="The name of the Python module this class lives in."
    )
    full_name: str = Field(description="The full class namespace.")

    # context_metadata: Optional[ContextMetadataModel] = Field(
    #     description="Context metadata for the class.", default=None
    # )

    _module_cache: ModuleType = PrivateAttr(default=None)
    _cls_cache: Type = PrivateAttr(default=None)
    _src_cache: str = PrivateAttr(default=None)

    def _retrieve_id(self) -> str:
        return self.full_name

    def _retrieve_data_to_hash(self) -> Any:
        return self.full_name

    def get_class(self) -> Type:

        if self._cls_cache is None:
            m = self.get_python_module()
            self._cls_cache = getattr(m, self.python_class_name)
        return self._cls_cache

    def get_source_code(self) -> str:

        if self._src_cache is None:
            self._src_cache = inspect.getsource(self.get_class())
        return self._src_cache

    def get_python_module(self) -> ModuleType:
        if self._module_cache is None:
            self._module_cache = importlib.import_module(self.python_module_name)
        return self._module_cache


class KiaraModuleInstance(PythonClass):

    _kiara_model_id: ClassVar[str] = "metadata.kiara_module_class"

    @classmethod
    def from_module(cls, module: "KiaraModule"):

        item_cls = module.__class__

        cls_name = item_cls.__name__
        module_name = item_cls.__module__
        if module_name == "builtins":
            full_name = cls_name
        else:
            full_name = f"{item_cls.__module__}.{item_cls.__name__}"

        conf: Dict[str, Any] = {
            "python_class_name": cls_name,
            "python_module_name": module_name,
            "full_name": full_name,
        }

        conf["module_config"] = module.config.model_dump()
        conf["inputs_schema"] = module.inputs_schema
        conf["outputs_schema"] = module.outputs_schema

        result = KiaraModuleInstance(**conf)
        result._cls_cache = item_cls
        result._module_instance_cache = module
        return result

    module_config: Dict[str, Any] = Field(description="The module config.")
    inputs_schema: Dict[str, ValueSchema] = Field(
        description="The schema for the module input(s)."
    )
    outputs_schema: Dict[str, ValueSchema] = Field(
        description="The schema for the module output(s)."
    )

    _module_instance_cache: "KiaraModule" = PrivateAttr(default=None)

    def get_kiara_module_instance(self) -> "KiaraModule":

        if self._module_instance_cache is not None:
            return self._module_instance_cache

        m_cls = self.get_class()
        self._module_instance_cache = m_cls(module_config=self.module_config)
        return self._module_instance_cache


# kiara\kiara\src\kiara\models\workflow.py
# -*- coding: utf-8 -*-
import datetime
import uuid
from typing import TYPE_CHECKING, Any, ClassVar, Dict, List, Mapping, Type, Union

from dag_cbor import IPLDKind
from pydantic import Field, PrivateAttr, field_validator
from rich import box
from rich.console import RenderableType
from rich.panel import Panel
from rich.syntax import Syntax
from rich.table import Table

from kiara.interfaces.python_api.models.info import InfoItemGroup, ItemInfo
from kiara.models import KiaraModel
from kiara.models.documentation import (
    AuthorsMetadataModel,
    ContextMetadataModel,
    DocumentationMetadataModel,
)
from kiara.models.module.pipeline import PipelineConfig, PipelineStep
from kiara.models.module.pipeline.pipeline import Pipeline, PipelineInfo
from kiara.models.module.pipeline.structure import PipelineStructure
from kiara.models.values.value import ValueMap
from kiara.utils import is_jupyter
from kiara.utils.json import orjson_dumps

if TYPE_CHECKING:
    from kiara.context import Kiara
    from kiara.interfaces.python_api.workflow import Workflow


class WorkflowState(KiaraModel):
    @classmethod
    def create_from_workflow(self, workflow: "Workflow"):

        steps = list(workflow._steps.values())
        inputs = dict(workflow.current_pipeline_inputs)
        info = PipelineInfo.create_from_pipeline(
            kiara=workflow._kiara, pipeline=workflow.pipeline
        )
        info._kiara = workflow._kiara

        ws = WorkflowState(steps=steps, inputs=inputs, pipeline_info=info)
        ws._kiara = workflow._kiara
        ws.pipeline_info._kiara = workflow._kiara
        return ws

    steps: List[PipelineStep] = Field(
        description="The current steps in the workflow.", default_factory=list
    )
    inputs: Dict[str, uuid.UUID] = Field(
        description="The current (pipeline) input values.", default_factory=dict
    )
    pipeline_info: PipelineInfo = Field(
        description="Details about the pipeline and its state."
    )

    _pipeline: Union[Pipeline, None] = PrivateAttr(default=None)
    _kiara: "Kiara" = PrivateAttr(default=None)

    def _retrieve_data_to_hash(self) -> IPLDKind:
        return {
            "steps": [s.instance_cid for s in self.steps],
            "inputs": {k: str(v) for k, v in self.inputs.items()},
        }

    def set_inputs(self, **inputs: uuid.UUID):

        for k, v in inputs.items():
            if k in self.pipeline_config.structure.pipeline_inputs_schema.keys():
                self.inputs[k] = v

    @property
    def pipeline_config(self) -> PipelineConfig:

        return self.pipeline_info.pipeline_config

    @property
    def pipeline_structure(self) -> PipelineStructure:
        return self.pipeline_info.pipeline_config.structure

    def create_renderable(self, **config: Any) -> RenderableType:

        in_panel = config.get("in_panel", None)
        if in_panel is None:
            if is_jupyter():
                in_panel = True
            else:
                in_panel = False
        table = Table(box=box.SIMPLE, show_header=False, padding=(0, 0, 0, 0))
        table.add_column("property", style="i")
        table.add_column("value")
        table.add_row("state id", self.instance_id)

        self.pipeline_info._fill_table(table=table, config=config)

        if in_panel:
            return Panel(table)
        else:
            return table


class WorkflowMetadata(KiaraModel):
    _kiara_model_id: ClassVar = "instance.workflow"

    workflow_id: uuid.UUID = Field(
        description="The globaly unique uuid for this workflow."
    )
    documentation: DocumentationMetadataModel = Field(
        description="A description for this workflow.",
        default_factory=DocumentationMetadataModel.create,
    )
    authors: AuthorsMetadataModel = Field(
        description="The author(s) of this workflow.",
        default_factory=AuthorsMetadataModel,
    )
    context: ContextMetadataModel = Field(
        description="Workflow context details.", default_factory=ContextMetadataModel
    )
    current_state: Union[str, None] = Field(
        description="A reference to the current state of this workflow.", default=None
    )
    workflow_history: Dict[datetime.datetime, str] = Field(
        description="A history of all the states of this workflow.",
        default_factory=dict,
    )

    input_aliases: Dict[str, str] = Field(
        description="A set of aliases that can be used to forward inputs to their (unaliased) pipeline inputs.",
        default_factory=dict,
    )
    output_aliases: Dict[str, str] = Field(
        description="A set of aliases to make output field names more user friendly.",
        default_factory=dict,
    )

    is_persisted: bool = Field(
        description="Whether this workflow is persisted in it's current state in a kiara store.",
        default=False,
    )

    _kiara: Union["Kiara", None] = PrivateAttr(default=None)
    # _last_update: datetime.datetime = PrivateAttr(default_factory=datetime.datetime.now)

    @field_validator("documentation", mode="before")
    @classmethod
    def validate_doc(cls, value):
        if not isinstance(value, DocumentationMetadataModel):
            return DocumentationMetadataModel.create(value)
        else:
            return value

    @property
    def last_state_id(self) -> Union[None, str]:

        if not self.workflow_history:
            return None
        last_date = max(self.workflow_history.keys())
        workflow_state_id = self.workflow_history[last_date]
        return workflow_state_id


class WorkflowInfo(ItemInfo):

    _kiara_model_id: ClassVar = "info.workflow"

    @classmethod
    def create_from_workflow(cls, workflow: "Workflow") -> "WorkflowInfo":

        wf_info = WorkflowInfo(
            type_name=str(workflow.workflow_id),
            workflow_metadata=workflow.workflow_metadata,
            workflow_state_ids=workflow.all_state_ids,
            pipeline_info=workflow.pipeline_info,
            documentation=workflow.workflow_metadata.documentation,
            authors=workflow.workflow_metadata.authors,
            context=workflow.workflow_metadata.context,
            current_input_values=workflow.current_input_values,
            current_output_values=workflow.current_output_values,
            input_aliases=dict(workflow.input_aliases),
            output_aliases=dict(workflow.output_aliases),
        )
        return wf_info

    @classmethod
    def category_name(cls) -> str:
        return "workflow"

    @classmethod
    def base_instance_class(cls) -> Type["Workflow"]:
        from kiara.interfaces.python_api.workflow import Workflow

        return Workflow

    @classmethod
    def create_from_instance(cls, kiara: "Kiara", instance: "Workflow", **kwargs):

        return cls.create_from_workflow(workflow=instance)

    workflow_metadata: WorkflowMetadata = Field(description="The workflow details.")
    workflow_state_ids: List[str] = Field(description="All states for this workflow.")
    pipeline_info: PipelineInfo = Field(
        description="The current state of the workflows' pipeline."
    )
    current_input_values: ValueMap = Field(
        description="The current workflow inputs (after aliasing)."
    )
    current_output_values: ValueMap = Field(
        description="The current workflow outputs (after aliasing)."
    )
    input_aliases: Dict[str, str] = Field(
        description="The (current) input aliases for this workflow."
    )
    output_aliases: Dict[str, str] = Field(
        description="The (current) output aliases for this workflow."
    )

    def create_renderable(self, **config: Any) -> RenderableType:

        in_panel = config.get("in_panel", None)
        if in_panel is None:
            if is_jupyter():
                in_panel = True
            else:
                in_panel = False

        include_doc = config.get("include_doc", True)
        include_authors = config.get("include_authors", True)
        include_id = config.get("include_id", True)
        include_context = config.get("include_context", True)
        include_history = config.get("include_history", True)
        include_current_inputs = config.get("include_current_inputs", True)
        include_current_outputs = config.get("include_current_outputs", True)
        include_aliases = config.get("include_aliases", True)
        include_current_state = config.get("include_current_state", True)

        table = Table(box=box.SIMPLE, show_header=False, padding=(0, 0, 0, 0))
        table.add_column("property", style="i")
        table.add_column("value")

        if include_doc:
            table.add_row(
                "documentation",
                Panel(self.documentation.create_renderable(), box=box.SIMPLE),
            )
        if include_authors:
            table.add_row("author(s)", self.authors.create_renderable(**config))
        if include_id:
            table.add_row("workflow id", str(self.workflow_metadata.workflow_id))
        if include_context:
            table.add_row("context", self.context.create_renderable(**config))
        if include_aliases:
            aliases = orjson_dumps(
                {"inputs": self.input_aliases, "outputs": self.output_aliases}
            )
            table.add_row(
                "current aliases", Syntax(aliases, "json", background_color="default")
            )
        if include_current_inputs:
            inputs_renderable = self.current_input_values.create_renderable(**config)
            table.add_row("current inputs", inputs_renderable)
        if include_current_outputs:
            outputs_renderable = self.current_output_values.create_renderable(**config)
            table.add_row("current outputs", outputs_renderable)
        if include_history:
            history_table = Table(show_header=False, box=box.SIMPLE)
            history_table.add_column("date", style="i")
            history_table.add_column("id")
            for d, s_id in self.workflow_metadata.workflow_history.items():
                history_table.add_row(str(d), s_id)
            table.add_row("snapshot timeline", history_table)

        if include_current_state:
            current_state_id = (
                "-- n/a --"
                if not self.workflow_metadata.current_state
                else self.workflow_metadata.current_state
            )
            table.add_row("current state id", current_state_id)
            table.add_row(
                "current state details", self.pipeline_info.create_renderable(**config)
            )

        if in_panel:
            return Panel(table)
        else:
            return table


class WorkflowGroupInfo(InfoItemGroup):

    _kiara_model_id: ClassVar = "info.workflows"

    @classmethod
    def base_info_class(cls) -> Type[ItemInfo]:
        return WorkflowInfo

    @classmethod
    def create_from_workflows(
        cls,
        *items: "Workflow",
        group_title: Union[str, None] = None,
        alias_map: Union[None, Mapping[str, uuid.UUID]] = None,
    ) -> "WorkflowGroupInfo":

        workflow_infos = {
            str(w.workflow_id): WorkflowInfo.create_from_workflow(workflow=w)
            for w in items
        }
        if alias_map is None:
            alias_map = {}
        workflow_group_info = cls(
            group_title=group_title, item_infos=workflow_infos, aliases=alias_map
        )
        return workflow_group_info

    item_infos: Mapping[str, WorkflowInfo] = Field(
        description="The workflow infos objects for each workflow."
    )
    aliases: Mapping[str, uuid.UUID] = Field(
        description="The available aliases.", default_factory=dict
    )

    def create_renderable(self, **config: Any) -> RenderableType:

        table = Table(box=box.SIMPLE, show_header=True)
        table.add_column("alias(es)", style="i")
        table.add_column("workflow_id")
        table.add_column("# steps")
        table.add_column("# stages")
        table.add_column("# states")
        table.add_column("description")

        for workflow_id, wf in self.item_infos.items():

            aliases = [k for k, v in self.aliases.items() if str(v) == workflow_id]
            steps = len(wf.pipeline_info.pipeline_config.structure.steps)
            stages = len(wf.pipeline_info.pipeline_config.structure.processing_stages)
            states = len(wf.workflow_state_ids)

            if not aliases:
                alias_str = ""
            else:
                alias_str = ", ".join(aliases)
            table.add_row(
                alias_str,
                workflow_id,
                str(steps),
                str(stages),
                str(states),
                wf.documentation.description,
            )

        return table


# kiara\kiara\src\kiara\models\__init__.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

from abc import ABC
from typing import Any, ClassVar, Dict, Iterable, List, Mapping, Union

import networkx as nx
from dag_cbor import IPLDKind
from multiformats import CID
from pydantic import ConfigDict
from pydantic.fields import PrivateAttr
from pydantic.main import BaseModel
from rich import box
from rich.console import Console, ConsoleOptions, Group, RenderableType, RenderResult
from rich.jupyter import JupyterMixin
from rich.panel import Panel
from rich.table import Table
from rich.tree import Tree

from kiara.defaults import (
    KIARA_MODEL_DATA_KEY,
    KIARA_MODEL_ID_KEY,
    KIARA_MODEL_SCHEMA_KEY,
)
from kiara.registries.templates import TemplateRegistry
from kiara.utils.class_loading import _default_id_func
from kiara.utils.develop import log_dev_message
from kiara.utils.hashing import compute_cid
from kiara.utils.json import orjson_dumps
from kiara.utils.models import (
    assemble_subcomponent_graph,
    create_subcomponent_tree_renderable,
    get_subcomponent_from_model,
    retrieve_data_subcomponent_keys,
)


class KiaraModel(ABC, BaseModel, JupyterMixin):
    """
    Base class that all models in kiara inherit from.

    This class provides utility functions for things like rendering the model on terminal or as html, integration into
    a tree hierarchy of the overall kiara context, hashing, etc.
    """

    __slots__ = ["__weakref__"]

    model_config = ConfigDict(extra="forbid")

    # @classmethod
    # def get_model_title(cls):
    #
    #     return to_camel_case(cls._kiara_model_name)

    @classmethod
    def get_schema_cid(cls) -> CID:
        if cls._schema_hash_cache is not None:
            return cls._schema_hash_cache

        model_schema = cls.model_json_schema()
        try:
            _, cid = compute_cid(data=model_schema)
        except Exception as e:
            from kiara.utils.output import extract_renderable

            msg = "Failed to compute cid for model schema instance."
            item = extract_renderable(model_schema)
            renderable = Group(msg, item, extract_renderable(e))
            log_dev_message(renderable, title="cid computation error")
            raise e

        cls._schema_hash_cache = cid
        return cls._schema_hash_cache

    _graph_cache: Union[nx.DiGraph, None] = PrivateAttr(default=None)
    _subcomponent_names_cache: Union[List[str], None] = PrivateAttr(default=None)
    _dynamic_subcomponents: Dict[str, "KiaraModel"] = PrivateAttr(default_factory=dict)
    _id_cache: Union[str, None] = PrivateAttr(default=None)
    _category_id_cache: Union[str, None] = PrivateAttr(default=None)
    _schema_hash_cache: ClassVar[Union[None, CID]] = None
    _cid_cache: Union[CID, None] = PrivateAttr(default=None)
    _dag_cache: Union[bytes, None] = PrivateAttr(default=None)
    _size_cache: Union[int, None] = PrivateAttr(default=None)

    def _retrieve_data_to_hash(self) -> IPLDKind:
        """
        Return data important for hashing this model instance. Implemented by sub-classes.

        This returns the relevant data that makes this model unique, excluding any secondary metadata that is not
        necessary for this model to be used functionally. Like for example documentation.
        """
        return self.model_dump()

    @property
    def instance_id(self) -> str:
        """The unique id of this model, within its category."""
        if self._id_cache is not None:
            return self._id_cache

        self._id_cache = self._retrieve_id()
        return self._id_cache

    @property
    def instance_cid(self) -> CID:
        if self._cid_cache is None:
            self._compute_cid()
        return self._cid_cache  # type: ignore

    @property
    def instance_dag(self) -> bytes:

        if self._dag_cache is None:
            self._compute_cid()
        return self._dag_cache  # type: ignore

    @property
    def instance_size(self) -> int:

        if self._size_cache is None:
            self._compute_cid()
        return self._size_cache  # type: ignore

    @property
    def model_type_id(self) -> str:
        """The id of the category of this model."""
        if hasattr(self.__class__, "_kiara_model_id"):
            return self._kiara_model_id  # type: ignore
        else:
            return _default_id_func(self.__class__)

    def _retrieve_id(self) -> str:
        return str(self.instance_cid)

    def _compute_cid(self):
        """A hash for this model."""
        if self._cid_cache is not None:
            return

        obj = self._retrieve_data_to_hash()
        try:
            dag, cid = compute_cid(data=obj)
        except Exception as e:
            from kiara.utils.output import extract_renderable

            msg = "Failed to compute cid for model instance."
            item = extract_renderable(obj)
            renderable = Group(msg, item, extract_renderable(e))
            log_dev_message(renderable, title="cid computation error")
            raise e

        self._cid_cache = cid
        self._dag_cache = dag
        self._size_cache = len(dag)

    # ==========================================================================================
    # subcomponent related methods
    @property
    def subcomponent_keys(self) -> Iterable[str]:
        """The keys of available sub-components of this model."""
        if self._subcomponent_names_cache is None:
            self._subcomponent_names_cache = sorted(self._retrieve_subcomponent_keys())
        return self._subcomponent_names_cache

    @property
    def subcomponent_tree(self) -> Union[nx.DiGraph, None]:
        """A tree structure, containing all sub-components (and their subcomponents) of this model."""
        if not self.subcomponent_keys:
            return None

        if self._graph_cache is None:
            self._graph_cache = assemble_subcomponent_graph(self)
        return self._graph_cache

    def get_subcomponent(self, path: str) -> "KiaraModel":
        """Retrieve the subcomponent identified by the specified path."""
        if path not in self._dynamic_subcomponents.keys():
            self._dynamic_subcomponents[path] = self._retrieve_subcomponent(path=path)
        return self._dynamic_subcomponents[path]

    def find_subcomponents(self, category: str) -> Dict[str, "KiaraModel"]:
        """Find and return all subcomponents of this model that are member of the specified category."""
        tree = self.subcomponent_tree
        if tree is None:
            raise Exception(f"No subcomponents found for category: {category}")

        result = {}
        for node_id, node in tree.nodes.items():
            if not hasattr(node["obj"], "get_category_alias"):
                raise NotImplementedError()

            if category != node["obj"].get_category_alias():
                continue

            n_id = node_id[9:]  # remove the __self__. token
            result[n_id] = node["obj"]
        return result

    def _retrieve_subcomponent_keys(self) -> Iterable[str]:
        """
        Retrieve the keys of all subcomponents of this model.

        Can be overwritten in sub-classes, by default it tries to automatically determine the subcomponents.
        """
        return retrieve_data_subcomponent_keys(self)

    def _retrieve_subcomponent(self, path: str) -> "KiaraModel":
        """
        Retrieve the subcomponent under the specified path.

        Can be overwritten in sub-classes, by default it tries to automatically determine the subcomponents.
        """
        m = get_subcomponent_from_model(self, path=path)
        return m

    # ==========================================================================================
    # model rendering related methods
    def create_panel(self, title: Union[str, None] = None, **config: Any) -> Panel:

        rend = self.create_renderable(**config)
        return Panel(rend, box=box.ROUNDED, title=title, title_align="left")

    def create_html(self, **config) -> str:

        template_registry = TemplateRegistry.instance()
        template = template_registry.get_template_for_model_type(
            model_type=self.model_type_id, template_format="html"
        )

        if template:
            try:
                result: str = template.render(instance=self)
                return result
            except Exception as e:
                log_dev_message(
                    title="html-rendering error",
                    msg=f"Failed to render html for model '{self.instance_id}' type '{self.model_type_id}': {e}",
                )

        try:
            from kiara.utils.html import generate_html

            html: str = generate_html(item=self, add_header=False)
            return html
        except Exception as e:
            log_dev_message(
                title="html-generation error",
                msg=f"Failed to generate html for model '{self.instance_id}' type '{self.model_type_id}': {e}",
            )

        r = self.create_renderable(**config)
        mime_bundle: Mapping[str, str] = r._repr_mimebundle_(include=[], exclude=[])  # type: ignore
        return mime_bundle["text/html"]

    def create_renderable(self, **config: Any) -> RenderableType:

        from kiara.utils.output import extract_renderable

        include = config.get("include", None)

        table = Table(show_header=False, box=box.SIMPLE)
        table.add_column("Key", style="i")
        table.add_column("Value")
        for k in self.model_fields.keys():
            if include is not None and k not in include:
                continue
            attr = getattr(self, k)
            v = extract_renderable(attr)
            table.add_row(k, v)
        return table

    def create_renderable_tree(self, **config: Any) -> Tree:

        show_data = config.get("show_data", False)
        tree = create_subcomponent_tree_renderable(data=self, show_data=show_data)
        return tree

    def create_info_data(self, **config) -> Mapping[str, Any]:

        include = config.get("include", None)

        result = {}
        for k in self.model_fields.keys():
            if include is not None and k not in include:
                continue
            attr = getattr(self, k)
            v = attr
            result[k] = v
        return result

    def as_dict_with_schema(self) -> Dict[str, Dict[str, Any]]:
        return {"data": self.model_dump(), "schema": self.model_json_schema()}

    def as_json_with_schema(self, incl_model_id: bool = False) -> str:

        data_json = self.model_dump_json()
        schema_json = self.model_json_schema()
        schema_json_str = orjson_dumps(schema_json)
        if not incl_model_id:
            return (
                '{"'
                + KIARA_MODEL_DATA_KEY
                + '": '
                + data_json
                + ', "'
                + KIARA_MODEL_SCHEMA_KEY
                + '": '
                + schema_json_str
                + "}"
            )
        else:
            return (
                '{"'
                + KIARA_MODEL_DATA_KEY
                + '": '
                + data_json
                + ', "'
                + KIARA_MODEL_SCHEMA_KEY
                + '": '
                + schema_json_str
                + ', "'
                + KIARA_MODEL_ID_KEY
                + '": "'
                + self.model_type_id
                + '"}'
            )

    def __hash__(self):
        return int.from_bytes(self.instance_cid.digest, "big")

    def __eq__(self, other):

        if self.__class__ != other.__class__:
            return False
        else:
            return (self.instance_id, self.instance_cid) == (
                other.instance_id,
                other.instance_cid,
            )

    def __repr__(self):

        try:
            model_id = self.instance_id
        except Exception:
            model_id = "-- n/a --"

        return f"{self.__class__.__name__}(model_id={model_id}, category={self.model_type_id}, fields=[{', '.join(self.model_fields.keys())}])"

    def __str__(self):
        return self.__repr__()

    def _repr_html_(self):
        return str(self.create_html())

    def __rich_console__(
        self, console: Console, options: ConsoleOptions
    ) -> RenderResult:

        yield self.create_renderable()


# kiara\kiara\src\kiara\models\aliases\__init__.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)
import copy
import datetime
import uuid
from typing import TYPE_CHECKING, Any, ClassVar, Dict, Mapping, Union

import structlog
from pydantic import Field, PrivateAttr
from rich.tree import Tree

from kiara.defaults import NONE_VALUE_ID, SpecialValue
from kiara.models.values.value import Value, ValueMap
from kiara.models.values.value_schema import ValueSchema
from kiara.utils.cli import terminal_print

if TYPE_CHECKING:
    from kiara.context import DataRegistry


logger = structlog.getLogger()

VALUE_ALIAS_SEPARATOR = "."


class AliasValueMap(ValueMap):
    """A model class that holds a tree of values and their schemas."""

    _kiara_model_id: ClassVar = "instance.value_map.aliases"

    alias: Union[str, None] = Field(description="This maps own (full) alias.")
    version: int = Field(description="The version of this map (in this maps parent).")
    created: Union[datetime.datetime, None] = Field(
        description="The time this map was created.", default=None
    )
    assoc_schema: Union[ValueSchema, None] = Field(
        description="The schema for this maps associated value."
    )
    assoc_value: Union[uuid.UUID, None] = Field(
        description="The value that is associated with this map."
    )

    value_items: Dict[str, Dict[int, "AliasValueMap"]] = Field(
        description="The values contained in this set.", default_factory=dict
    )

    _data_registry: "DataRegistry" = PrivateAttr(default=None)
    _schema_locked: bool = PrivateAttr(default=False)
    _auto_schema: bool = PrivateAttr(default=True)
    _is_stored: bool = PrivateAttr(default=False)

    def _retrieve_data_to_hash(self) -> Any:
        raise NotImplementedError()

    @property
    def is_stored(self) -> bool:
        return self._is_stored

    def get_child_map(
        self, field_name: str, version: Union[str, None] = None
    ) -> Union["AliasValueMap", None]:
        """
        Get the child map for the specified field / version combination.

        Raises an error if the child field does not exist. Returns 'None' if not value is set yet (but schema is).
        """
        if version is not None:
            raise NotImplementedError()

        if VALUE_ALIAS_SEPARATOR not in field_name:

            if self.values_schema.get(field_name, None) is None:
                if not self.values_schema:
                    msg = "No available fields"
                else:
                    msg = "Available fields: " + ", ".join(self.values_schema.keys())
                raise KeyError(f"No field name '{field_name}'. {msg}")

            field_items = self.value_items[field_name]
            if not field_items:
                return None

            max_version = max(field_items.keys())

            item = field_items[max_version]
            return item

        else:
            child, rest = field_name.split(VALUE_ALIAS_SEPARATOR, maxsplit=1)
            if child not in self.values_schema.keys():
                raise Exception(
                    f"No field name '{child}'. Available fields: {', '.join(self.values_schema.keys())}"
                )
            child_map = self.get_child_map(child)
            assert child_map is not None
            return child_map.get_child_map(rest)

    def get_value_obj(self, field_name: str) -> Value:

        item = self.get_child_map(field_name=field_name)
        if item is None:
            return self._data_registry.NONE_VALUE
        if item.assoc_value is None:
            raise Exception(f"No value associated for field '{field_name}'.")

        return self._data_registry.get_value(value=item.assoc_value)

    def get_value_id(self, field_name: str) -> uuid.UUID:

        item = self.get_child_map(field_name=field_name)
        if item is None:
            result = NONE_VALUE_ID
        else:
            result = item.assoc_value if item.assoc_value is not None else NONE_VALUE_ID

        return result

    def get_all_value_ids(
        self,
    ) -> Dict[str, uuid.UUID]:

        result: Dict[str, uuid.UUID] = {}
        for k in self.values_schema.keys():
            v_id = self.get_value_id(field_name=k)
            if v_id is None:
                v_id = NONE_VALUE_ID
            result[k] = v_id
        return result

    def set_alias_schema(self, alias: str, schema: ValueSchema):

        if self._schema_locked:
            raise Exception(f"Can't add schema for alias '{alias}': schema locked.")

        if VALUE_ALIAS_SEPARATOR not in alias:

            self._set_local_field_schema(field_name=alias, schema=schema)
        else:
            child, rest = alias.split(VALUE_ALIAS_SEPARATOR, maxsplit=1)

            if child in self.values_schema.keys():
                child_map = self.get_child_map(child)
            else:
                self._set_local_field_schema(
                    field_name=child, schema=ValueSchema(type="none")
                )
                child_map = self._set_alias(alias=child, data=None)

            assert child_map is not None

            child_map.set_alias_schema(alias=rest, schema=schema)

    def _set_local_field_schema(self, field_name: str, schema: ValueSchema):

        assert field_name is not None
        if VALUE_ALIAS_SEPARATOR in field_name:
            raise Exception(
                f"Can't add schema, field name has alias separator in name: {field_name}. This is most likely a bug."
            )

        if field_name in self.values_schema.keys():
            raise Exception(
                f"Can't set alias schema for '{field_name}' to map: schema already set."
            )

        try:
            items = self.get_child_map(field_name)
            if items is not None:
                raise Exception(
                    f"Can't set schema for field '{field_name}': already at least one child set for this field."
                )
        except KeyError:
            pass

        self.values_schema[field_name] = schema
        self.value_items[field_name] = {}

    def get_alias(self, alias: str) -> Union["AliasValueMap", None]:

        if VALUE_ALIAS_SEPARATOR not in alias:
            if "@" in alias:
                raise NotImplementedError()

            child_map = self.get_child_map(alias)
            if child_map is None:
                return None

            return child_map

        else:
            child, rest = alias.split(VALUE_ALIAS_SEPARATOR, maxsplit=1)
            if "@" in child:
                raise NotImplementedError()

            child_map = self.get_child_map(field_name=child)

            if child_map is None:
                return None

            return child_map.get_alias(rest)

    def set_value(self, field_name: str, data: Any) -> None:

        assert VALUE_ALIAS_SEPARATOR not in field_name

        self._set_alias(alias=field_name, data=data)

    def _set_aliases(self, **aliases: Any) -> Mapping[str, "AliasValueMap"]:

        result = {}
        for k, v in aliases.items():
            r = self._set_alias(alias=k, data=v)
            result[k] = r

        return result

    def _set_alias(self, alias: str, data: Any) -> "AliasValueMap":

        if VALUE_ALIAS_SEPARATOR not in alias:
            field_name: Union[str, None] = alias

            # means we are setting the alias in this map
            assert field_name is not None

            vs = self.values_schema[alias]
            if vs.type == "none":
                assert data is None
                value_id = None
            else:

                if data in [None, SpecialValue.NO_VALUE, SpecialValue.NOT_SET]:
                    if vs.default:
                        if callable(vs.default):
                            data = vs.default()
                        else:
                            data = copy.deepcopy(vs.default)

                value = self._data_registry.register_data(data=data, schema=vs)
                value_id = value.value_id

            new_map = self._set_local_value_item(
                field_name=field_name, value_id=value_id
            )
            return new_map

        else:
            child, rest = alias.split(VALUE_ALIAS_SEPARATOR, maxsplit=1)
            field_name = None

            # means we are dealing with an intermediate alias map
            assert rest is not None
            assert child is not None
            assert field_name is None
            if child not in self.value_items.keys():
                if not self._auto_schema:
                    raise Exception(
                        f"Can't set alias '{alias}', no schema set for field: '{child}'."
                    )
                else:
                    self.set_alias_schema(alias=child, schema=ValueSchema(type="any"))

            field_item: Union[AliasValueMap, None] = None
            try:
                field_item = self.get_child_map(field_name=child)
            except KeyError:
                pass

            if self.alias:
                new_alias = f"{self.alias}.{child}"
            else:
                new_alias = child

            if field_item is None:
                new_version = 0
                schemas = {}
                self.value_items[child] = {}
            else:
                max_version = len(field_item.keys())
                new_version = max_version + 1
                assert field_item.alias == new_alias
                assert field_item.version == max_version
                schemas = field_item.values_schema

            new_map = AliasValueMap(
                alias=new_alias,
                version=new_version,
                assoc_schema=self.values_schema[child],
                assoc_value=None,
                values_schema=schemas,
            )
            new_map._data_registry = self._data_registry
            self.value_items[child][new_version] = new_map

            new_map._set_alias(alias=rest, data=data)

        return new_map

    def _set_local_value_item(
        self, field_name: str, value_id: Union[uuid.UUID, None] = None
    ) -> "AliasValueMap":

        assert VALUE_ALIAS_SEPARATOR not in field_name

        value: Union[Value, None] = None
        if value_id is not None:
            value = self._data_registry.get_value(value=value_id)
            assert value is not None
            assert value.value_id == value_id

        if field_name not in self.values_schema.keys():
            if not self._auto_schema:
                raise Exception(
                    f"Can't add value for field '{field_name}': field not in schema."
                )
            else:
                if value_id is None:
                    value_schema = ValueSchema(type="none")
                else:
                    value_schema = value.value_schema  # type: ignore
                self.set_alias_schema(alias=field_name, schema=value_schema)

        field_items = self.value_items.get(field_name, None)
        if not field_items:
            assert field_items is not None
            new_version = 0
            values_schema = {}
        else:
            max_version = max(field_items.keys())
            current_map = field_items[max_version]

            if value_id == current_map.assoc_value:
                logger.debug(
                    "set_field.skip",
                    value_id=None,
                    reason=f"Same value id: {value_id}",
                )
                return current_map

            # TODO: check schema
            new_version = max(field_items.keys()) + 1
            values_schema = current_map.values_schema

        if self.alias:
            new_alias = f"{self.alias}.{field_name}"
        else:
            new_alias = field_name
        new_map = AliasValueMap(
            alias=new_alias,
            version=new_version,
            assoc_schema=self.values_schema[field_name],
            assoc_value=value_id,
            values_schema=values_schema,
        )
        new_map._data_registry = self._data_registry
        self.value_items[field_name][new_version] = new_map
        return new_map

    def print_tree(self):

        t = self.get_tree("base")
        terminal_print(t)

    def get_tree(self, base_name: str) -> Tree:

        if self.assoc_schema:
            type_name = self.assoc_schema.type
        else:
            type_name = "none"

        if type_name == "none":
            type_str = ""
        else:
            type_str = f" ({type_name})"

        tree = Tree(f"{base_name}{type_str}")
        if self.assoc_value:
            data = tree.add("__data__")
            value = self._data_registry.get_value(self.assoc_value)
            data.add(str(value.data))

        for field_name, schema in self.values_schema.items():

            alias = self.get_alias(alias=field_name)
            if alias is not None:
                tree.add(alias.get_tree(base_name=field_name))
            else:
                if schema.type == "none":
                    type_str = ""
                else:
                    type_str = f" ({schema.type})"

                tree.add(f"{field_name}{type_str}")

        return tree

    def __repr__(self):

        return f"AliasMap(assoc_value={self.assoc_value}, field_names={self.value_items.keys()})"

    def __str__(self):
        return self.__repr__()


# kiara\kiara\src\kiara\models\events\alias_registry.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import uuid
from typing import Iterable, Literal, Union

from pydantic import Field

from kiara.models.events import RegistryEvent


class AliasArchiveAddedEvent(RegistryEvent):

    event_type: Literal["alias_archive_added"] = "alias_archive_added"
    alias_archive_id: uuid.UUID = Field(
        description="The unique id of this data archive."
    )
    alias_archive_alias: str = Field(
        description="The alias this data archive was added as."
    )
    is_store: bool = Field(
        description="Whether this archive supports write operations (aka implements the 'DataStore' interface)."
    )
    is_default_store: bool = Field(
        description="Whether this store acts as default store."
    )
    mount_point: Union[str, None] = Field(
        description="The mountpoint of this alias archive (if specified)."
    )


class AliasPreStoreEvent(RegistryEvent):

    event_type: Literal["alias_pre_store"] = "alias_pre_store"
    aliases: Iterable[str] = Field(description="The alias.")


class AliasStoredEvent(RegistryEvent):

    event_type: Literal["alias_stored"] = "alias_stored"
    alias: str = Field(description="The alias.")


# kiara\kiara\src\kiara\models\events\data_registry.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import uuid
from typing import Literal

from pydantic import Field

from kiara.models.events import RegistryEvent
from kiara.models.values.value import Value


class DataArchiveAddedEvent(RegistryEvent):

    event_type: Literal["data_archive_added"] = "data_archive_added"
    data_archive_id: uuid.UUID = Field(
        description="The unique id of this data archive."
    )
    data_archive_alias: str = Field(
        description="The alias this data archive was added as."
    )
    is_store: bool = Field(
        description="Whether this archive supports write operations (aka implements the 'DataStore' interface)."
    )
    is_default_store: bool = Field(
        description="Whether this store acts as default store."
    )


class ValueCreatedEvent(RegistryEvent):

    event_type: Literal["value_created"] = "value_created"
    value: Value = Field(description="The value metadata.")


class ValueRegisteredEvent(RegistryEvent):

    event_type: Literal["value_registered"] = "value_registered"
    value: Value = Field(description="The value metadata.")


class ValuePreStoreEvent(RegistryEvent):

    event_type: Literal["value_pre_store"] = "value_pre_store"
    value: Value = Field(description="The value metadata.")


class ValueStoredEvent(RegistryEvent):

    event_type: Literal["value_stored"] = "value_stored"
    value: Value = Field(description="The value metadata.")
    storing_required: bool = Field(
        description="Whether the value was stored or existed already."
    )


# kiara\kiara\src\kiara\models\events\destiny_registry.py
# -*- coding: utf-8 -*-
import uuid
from typing import Literal

from pydantic import Field

from kiara.models.events import RegistryEvent


class DestinyArchiveAddedEvent(RegistryEvent):

    event_type: Literal["destiny_archive_added"] = "destiny_archive_added"
    destiny_archive_id: uuid.UUID = Field(
        description="The unique id of this destiny archive."
    )
    destiny_archive_alias: str = Field(
        description="The alias this destiny archive was added as."
    )
    is_store: bool = Field(
        description="Whether this archive supports write operations (aka implements the 'DestinyStore' interface)."
    )
    is_default_store: bool = Field(
        description="Whether this store acts as default store."
    )


# kiara\kiara\src\kiara\models\events\job_registry.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import uuid
from typing import Literal

from pydantic import Field

from kiara.models.events import RegistryEvent
from kiara.models.module.jobs import JobRecord


class JobArchiveAddedEvent(RegistryEvent):

    event_type: Literal["job_archive_added"] = "job_archive_added"

    job_archive_id: uuid.UUID = Field(description="The unique id of this job archive.")
    job_archive_alias: str = Field(
        description="The alias this job archive was added as."
    )
    is_store: bool = Field(
        description="Whether this archive supports write operations (aka implements the 'JobStore' interface)."
    )
    is_default_store: bool = Field(
        description="Whether this store acts as default store."
    )


class JobRecordPreStoreEvent(RegistryEvent):

    event_type: Literal["job_record_pre_store"] = "job_record_pre_store"
    job_record: JobRecord = Field(description="The job record.")


class JobRecordStoredEvent(RegistryEvent):

    event_type: Literal["job_record_stored"] = "job_record_stored"
    job_record: JobRecord = Field(description="The job record.")


# kiara\kiara\src\kiara\models\events\pipeline.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2022, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import uuid
from typing import (
    TYPE_CHECKING,
    Any,
    ClassVar,
    Dict,
    List,
    Mapping,
    MutableMapping,
    Set,
    Union,
)

from pydantic import BaseModel, ConfigDict, Field, PrivateAttr, field_validator
from rich import box
from rich.console import RenderableType
from rich.panel import Panel
from rich.table import Table
from sortedcontainers import SortedDict

from kiara.defaults import NONE_VALUE_ID, NOT_SET_VALUE_ID
from kiara.models import KiaraModel
from kiara.models.events import KiaraEvent
from kiara.models.module.pipeline import PipelineStep, StepStatus
from kiara.utils.output import create_renderable_from_value_id_map

if TYPE_CHECKING:
    from dag_cbor import IPLDKind

    from kiara.context import Kiara
    from kiara.models.module.pipeline.pipeline import Pipeline


class ChangedValue(BaseModel):

    old: Union[uuid.UUID, None] = None
    new: Union[uuid.UUID, None] = None


class StepDetails(BaseModel):

    kiara_id: uuid.UUID = Field(description="The id of the kiara context.")
    pipeline_id: uuid.UUID = Field(description="The id of the pipeline.")
    step: PipelineStep = Field(description="The pipeline step details.")
    step_id: str = Field(description="The id of the step.")
    processing_stage: int = Field(
        description="The execution stage where this step is executed."
    )
    status: StepStatus = Field(description="The current status of this step.")
    invalid_details: Dict[str, str] = Field(
        description="Details about fields that are invalid (if status < 'INPUTS_READY'.",
        default_factory=dict,
    )
    inputs: Dict[str, uuid.UUID] = Field(description="The current inputs of this step.")
    outputs: Dict[str, uuid.UUID] = Field(
        description="The current outputs of this step."
    )
    _kiara: "Kiara" = PrivateAttr()

    @field_validator("inputs")
    @classmethod
    def replace_none_values_inputs(cls, value):

        result = {}
        for k, v in value.items():
            if v is None:
                v = NONE_VALUE_ID
            result[k] = v
        return result

    @field_validator("outputs")
    @classmethod
    def replace_none_values_outputs(cls, value):

        result = {}
        for k, v in value.items():
            if v is None:
                v = NOT_SET_VALUE_ID
            result[k] = v
        return result

    def _retrieve_data_to_hash(self) -> Any:
        return f"{self.kiara_id}.{self.pipeline_id}.{self.step_id}"

    def _retrieve_id(self) -> str:
        return f"{self.kiara_id}.{self.pipeline_id}.{self.step_id}"

    def create_renderable(self, **config: Any) -> RenderableType:

        display_pipeline_id = config.get("display_pipeline_id", False)
        display_extended_step_details = config.get(
            "display_extended_step_details", False
        )

        table = Table(show_header=False, box=box.SIMPLE)
        table.add_column("key", style="i")
        table.add_column("value")
        table.add_row("step id", self.step_id)
        table.add_row("status", self.status.value)
        if self.invalid_details:
            invalid_table = Table(show_header=False, box=box.SIMPLE)
            invalid_table.add_column("key", style="i")
            invalid_table.add_column("value")
            for k, v in self.invalid_details.items():
                invalid_table.add_row(k, v)
            table.add_row("invalid details", invalid_table)
        if display_pipeline_id:
            table.add_row("pipeline id", str(self.pipeline_id))
        table.add_row("processing stage", str(self.processing_stage))

        if display_extended_step_details:
            step_detail_config = dict(config)
            step_detail_config["display_step_id"] = False
            step_details = self.step.create_renderable(**step_detail_config)
            table.add_row("step details", step_details)

        inputs_rend = create_renderable_from_value_id_map(
            kiara=self._kiara, values=self.inputs, config=config
        )
        table.add_row("inputs", inputs_rend)
        outputs_rend = create_renderable_from_value_id_map(
            kiara=self._kiara, values=self.outputs, config=config
        )
        table.add_row("outputs", outputs_rend)

        return table


class PipelineState(KiaraModel):

    _kiara_model_id: ClassVar = "instance.pipeline_state"

    kiara_id: uuid.UUID = Field(description="The id of the kiara context.")
    pipeline_id: uuid.UUID = Field(description="The id of the pipeline.")

    pipeline_status: StepStatus = Field(
        description="The current status of this pipeline."
    )
    invalid_details: Dict[str, str] = Field(
        description="Details about fields that are invalid (if status < 'INPUTS_READY'.",
        default_factory=dict,
    )

    pipeline_inputs: Dict[str, uuid.UUID] = Field(
        description="The current pipeline inputs."
    )
    # pipeline_inputs_schema: Mapping[str, ValueSchema] = Field(description="The schema of the pipeline inputs.")
    pipeline_outputs: Dict[str, uuid.UUID] = Field(
        description="The current pipeline outputs."
    )
    # pipeline_outputs_schema: Mapping[str, ValueSchema] = Field(description="The schema of the pipeline outputs.")

    step_states: Dict[str, StepDetails] = Field(
        description="The state of each step within this pipeline."
    )
    _kiara: "Kiara" = PrivateAttr()

    def _retrieve_data_to_hash(self) -> "IPLDKind":
        """
        Return data important for hashing this model instance. Implemented by sub-classes.

        This returns the relevant data that makes this model unique, excluding any secondary metadata that is not
        necessary for this model to be used functionally. Like for example documentation.
        """

        # TODO: is this enough?
        return {
            "kiara_id": str(self.kiara_id),
            "pipeline_id": str(self.pipeline_id),
        }

    def get_steps_by_processing_stage(self) -> MutableMapping[int, List[StepDetails]]:

        result: MutableMapping[int, List[StepDetails]] = SortedDict()
        for step_details in self.step_states.values():
            result.setdefault(step_details.processing_stage, []).append(step_details)
        return result

    def get_processing_stage_status(self, stage: int) -> StepStatus:

        step_states = self.get_steps_by_processing_stage()

        status: StepStatus = StepStatus.RESULTS_READY

        for _stage, step_details in step_states.items():
            if _stage > stage:
                break

            for step in step_details:
                if step.status == StepStatus.INPUTS_INVALID:
                    status = StepStatus.INPUTS_INVALID
                    break

                elif step.status == StepStatus.INPUTS_READY:
                    if status != StepStatus.INPUTS_INVALID:
                        status = StepStatus.INPUTS_READY

            # no point in further checking
            if status == StepStatus.INPUTS_INVALID:
                return status

        return status

    def create_renderable(self, **config: Any) -> RenderableType:

        display_step_states = config.get("display_step_details", False)

        table = Table(show_header=False, box=box.SIMPLE)
        table.add_column("key", style="i")
        table.add_column("value")
        table.add_row("pipeline id", str(self.pipeline_id))
        table.add_row("pipeline status", self.pipeline_status.value)
        if self.invalid_details:
            invalid_table = Table(show_header=False, box=box.SIMPLE)
            invalid_table.add_column("key", style="i")
            invalid_table.add_column("value")
            for k, v in self.invalid_details.items():
                invalid_table.add_row(k, v)
            table.add_row("invalid details", invalid_table)

        render_conf = dict(config)
        render_conf["value_title"] = "field"
        render_conf["show_hash"] = False
        render_conf["show_size"] = False
        render_conf["show_data"] = True
        render_conf["max_lines"] = 5
        render_conf["display_extended_step_details"] = False
        render_conf["show_description"] = False

        inputs_rend = create_renderable_from_value_id_map(
            kiara=self._kiara, values=self.pipeline_inputs, config=render_conf
        )
        table.add_row("pipeline inputs", inputs_rend)

        step_details_table = Table(show_header=False, box=box.SIMPLE)
        step_details_table.add_column("step id")
        step_details_table.add_column("details")

        for (
            processing_stage,
            state_step_details,
        ) in self.get_steps_by_processing_stage().items():

            proc_status = self.get_processing_stage_status(processing_stage)
            proc_status_str = StepStatus.to_console_renderable(proc_status)
            step_details_table.add_row(
                f"processing stage: [b]{processing_stage}[/b]",
                f"[b i]{proc_status_str}[/b i]",
            )
            step_details_table.add_row("", "")

            if display_step_states:
                for step_details in state_step_details:
                    step_id = step_details.step_id
                    step_rend = step_details.create_renderable(**render_conf)
                    panel = Panel(step_rend)
                    step_details_table.add_row(f"step: [b i]{step_id}[/b i]", panel)
            else:
                for step_details in state_step_details:
                    step_id = step_details.step_id
                    step_status_str = StepStatus.to_console_renderable(
                        step_details.status
                    )
                    step_details_table.add_row(
                        f"step: [b i]{step_id}[/b i]", step_status_str
                    )

            step_details_table.add_row("", "")

        table.add_row("internal state", step_details_table)

        outputs_rend = create_renderable_from_value_id_map(
            kiara=self._kiara, values=self.pipeline_outputs, config=render_conf
        )
        table.add_row("pipeline outputs", outputs_rend)

        return table


class PipelineEvent(KiaraEvent):
    @classmethod
    def create_event(
        cls,
        pipeline: "Pipeline",
        changed: Mapping[str, Mapping[str, Mapping[str, ChangedValue]]],
    ) -> Union["PipelineEvent", None]:

        pipeline_inputs = changed.get("__pipeline__", {}).get("inputs", {})
        pipeline_outputs = changed.get("__pipeline__", {}).get("outputs", {})

        step_inputs = {}
        step_outputs = {}

        invalidated_steps: Set[str] = set()

        for step_id, change_details in changed.items():
            if step_id == "__pipeline__":
                continue
            inputs = change_details.get("inputs", None)
            if inputs:
                invalidated_steps.add(step_id)
                step_inputs[step_id] = inputs
            outputs = change_details.get("outputs", None)
            if outputs:
                invalidated_steps.add(step_id)
                step_outputs[step_id] = outputs

        if (
            not pipeline_inputs
            and not pipeline_outputs
            and not step_inputs
            and not step_outputs
            and not invalidated_steps
        ):
            return None

        event = PipelineEvent(
            kiara_id=pipeline.kiara_id,
            pipeline_id=pipeline.pipeline_id,
            pipeline_inputs_changed=pipeline_inputs,
            pipeline_outputs_changed=pipeline_outputs,
            step_inputs_changed=step_inputs,
            step_outputs_changed=step_outputs,
            changed_steps=sorted(invalidated_steps),
        )
        return event

    model_config = ConfigDict(frozen=True)

    kiara_id: uuid.UUID = Field(
        description="The id of the kiara context that created the pipeline."
    )
    pipeline_id: uuid.UUID = Field(description="The pipeline id.")

    pipeline_inputs_changed: Dict[str, ChangedValue] = Field(
        description="Details about changed pipeline input values.", default_factory=dict
    )
    pipeline_outputs_changed: Dict[str, ChangedValue] = Field(
        description="Details about changed pipeline output values.",
        default_factory=dict,
    )

    step_inputs_changed: Dict[str, Mapping[str, ChangedValue]] = Field(
        description="Details about changed step input values.", default_factory=dict
    )
    step_outputs_changed: Dict[str, Mapping[str, ChangedValue]] = Field(
        description="Details about changed step output values.", default_factory=dict
    )

    changed_steps: List[str] = Field(
        description="A list of all step ids that have newly invalidated outputs."
    )

    def __repr__(self):
        return f"{self.__class__.__name__}(pipeline_id={self.pipeline_id}, invalidated_steps={', '.join(self.changed_steps)})"

    def __str__(self):
        return self.__repr__()


# class StepInputEvent(PipelineEvent):
#     """Event that gets fired when one or several inputs for steps within a pipeline have changed."""
#
#     event_type: Literal["step_input"] = "step_input"
#     step_id: str = Field(description="The step id.")
#     changed_inputs: Mapping[str, ChangedValue] = Field(
#         description="steps (keys) with updated inputs which need re-processing (value is list of updated input names)"
#     )
#
#
#
# class StepOutputEvent(PipelineEvent):
#     """Event that gets fired when one or several outputs for steps within a pipeline have changed."""
#
#     event_type: Literal["step_output"] = "step_output"
#
#     step_id: str = Field(description="The step id.")
#     changed_outputs: Mapping[str, ChangedValue] = Field(
#         description="steps (keys) with updated inputs which need re-processing (value is list of updated input names)"
#     )
#
#
# class PipelineInputEvent(PipelineEvent):
#     """Event that gets fired when one or several inputs for the pipeline itself have changed."""
#
#     event_type: Literal["pipeline_input"] = "pipeline_input"
#
#     changed_inputs: Mapping[str, ChangedValue] = Field(
#         description="steps (keys) with updated inputs which need re-processing (value is list of updated input names)"
#     )
#
#
# class PipelineOutputEvent(PipelineEvent):
#     """Event that gets fired when one or several outputs for the pipeline itself have changed."""
#
#     event_type: Literal["pipeline_output"] = "pipeline_output"
#     changed_outputs: Mapping[str, ChangedValue] = Field(
#         description="steps (keys) with updated inputs which need re-processing (value is list of updated input names)"
#     )


# kiara\kiara\src\kiara\models\events\workflow_registry.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import uuid
from typing import Literal

from pydantic import Field

from kiara.models.events import RegistryEvent


class WorkflowArchiveAddedEvent(RegistryEvent):

    event_type: Literal["workflow_archive_added"] = "workflow_archive_added"
    workflow_archive_id: uuid.UUID = Field(
        description="The unique id of this data archive."
    )
    workflow_archive_alias: str = Field(
        description="The alias this workflow archive was added as."
    )
    is_store: bool = Field(
        description="Whether this archive supports write operations (aka implements the 'WorkflowStore' interface)."
    )
    is_default_store: bool = Field(
        description="Whether this store acts as default store."
    )


# kiara\kiara\src\kiara\models\events\__init__.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import uuid

from pydantic import BaseModel, Field

from kiara.utils import camel_case_to_snake_case


class KiaraEvent(BaseModel):
    def get_event_type(self) -> str:

        if hasattr(self, "event_type"):
            return self.event_type  # type: ignore

        name = camel_case_to_snake_case(self.__class__.__name__)
        return name


class RegistryEvent(KiaraEvent):

    kiara_id: uuid.UUID = Field(
        description="The id of the kiara context the value was created in."
    )


# kiara\kiara\src\kiara\models\metadata\__init__.py
# -*- coding: utf-8 -*-
from typing import Any, ClassVar

from pydantic import Field

from kiara.models import KiaraModel


class KiaraMetadata(KiaraModel):
    def _retrieve_data_to_hash(self) -> Any:
        return {"metadata": self.model_dump(), "schema": self.schema_json()}


class CommentMetadata(KiaraMetadata):

    _kiara_model_id: ClassVar = "instance.kiara_metadata.comment"

    comment: str = Field(description="A note/comment.")


# kiara\kiara\src\kiara\models\module\destiny.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import copy
import uuid
from typing import TYPE_CHECKING, Any, ClassVar, Dict, Mapping, Union

from pydantic import Field, PrivateAttr

from kiara.defaults import SpecialValue
from kiara.models.module.manifest import Manifest
from kiara.models.python_class import KiaraModuleInstance
from kiara.models.values.value_schema import ValueSchema
from kiara.registries.ids import ID_REGISTRY

if TYPE_CHECKING:
    from kiara.context import Kiara
    from kiara.models.values.value import Value
    from kiara.modules import KiaraModule


class Destiny(Manifest):
    """
    A destiny is basically a link to a potential future transformation result involving one or several values as input.

    It is immutable, once executed, each of the input values can only have one destiny with a specific alias.
    This is similar to what is usually called a 'future' in programming languages, but more deterministic, sorta.
    """

    _kiara_model_id: ClassVar = "instance.destiny"

    @classmethod
    def create_from_values(
        cls,
        kiara: "Kiara",
        destiny_alias: str,
        values: Mapping[str, uuid.UUID],
        manifest: Manifest,
        result_field_name: Union[str, None] = None,
    ) -> "Destiny":

        module = kiara.module_registry.create_module(manifest=manifest)

        if result_field_name is None:
            if len(module.outputs_schema) != 1:
                raise Exception(
                    f"Can't determine result field name for module, not provided, and multiple outputs available for module '{module.module_type_name}': {', '.join(module.outputs_schema.keys())}."
                )

            result_field_name = next(iter(module.outputs_schema.keys()))

        result_schema = module.outputs_schema.get(result_field_name, None)
        if result_schema is None:
            raise Exception(
                f"Can't determine result schema for module '{module.module_type_name}', result field '{result_field_name}' not available. Available field: {', '.join(module.outputs_schema.keys())}"
            )

        fixed_inputs = {}
        deferred_inputs: Dict[str, None] = {}
        for field in module.inputs_schema.keys():
            if field in values.keys():
                fixed_inputs[field] = values[field]
            else:
                deferred_inputs[field] = None

        module_details = KiaraModuleInstance.from_module(module=module)

        # TODO: check whether it'd be better to 'resolve' the module config, as this might change the resulting hash
        destiny_id: uuid.UUID = ID_REGISTRY.generate(obj_type=Destiny)
        destiny = Destiny(
            destiny_id=destiny_id,
            destiny_alias=destiny_alias,
            module_details=module_details,
            module_type=manifest.module_type,
            module_config=manifest.module_config,
            result_field_name=result_field_name,
            result_schema=result_schema,
            fixed_inputs=fixed_inputs,
            inputs_schema=dict(module.inputs_schema),
            deferred_inputs=deferred_inputs,
            result_value_id=None,
        )
        destiny._module = module
        ID_REGISTRY.update_metadata(destiny_id, obj=destiny)
        return destiny

    destiny_id: uuid.UUID = Field(description="The id of this destiny.")

    destiny_alias: str = Field(description="The path to (the) destiny.")
    module_details: KiaraModuleInstance = Field(
        description="The class of the underlying module."
    )
    fixed_inputs: Dict[str, uuid.UUID] = Field(
        description="Inputs that are known in advance."
    )
    inputs_schema: Dict[str, ValueSchema] = Field(
        description="The schemas of all deferred input fields."
    )
    deferred_inputs: Dict[str, Union[uuid.UUID, None]] = Field(
        description="Potentially required external inputs that are needed for this destiny to materialize."
    )
    result_field_name: str = Field(description="The name of the result field.")
    result_schema: ValueSchema = Field(description="The value schema of the result.")
    result_value_id: Union[uuid.UUID, None] = Field(
        description="The value id of the result."
    )

    _is_stored: bool = PrivateAttr(default=False)
    _job_id: Union[uuid.UUID, None] = PrivateAttr(default=None)

    _merged_inputs: Union[Dict[str, uuid.UUID], None] = PrivateAttr(default=None)
    # _job_config_hash: Optional[int] = PrivateAttr(default=None)
    _module: Union["KiaraModule", None] = PrivateAttr(default=None)

    def _retrieve_id(self) -> str:
        return str(self.destiny_id)

    def _retrieve_data_to_hash(self) -> Any:
        return self.destiny_id.bytes

    # @property
    # def job_config_hash(self) -> int:
    #     if self._job_config_hash is None:
    #         self._job_config_hash = self._retrieve_job_config_hash()
    #     return self._job_config_hash

    @property
    def merged_inputs(self) -> Mapping[str, uuid.UUID]:

        if self._merged_inputs is not None:
            return self._merged_inputs

        result = copy.copy(self.fixed_inputs)
        missing = []
        for k in self.inputs_schema.keys():
            if k in self.fixed_inputs.keys():
                if k in self.deferred_inputs.keys():
                    raise Exception(
                        f"Destiny input field '{k}' present in both fixed and deferred inputs, this is invalid."
                    )
                else:
                    continue
            v = self.deferred_inputs.get(k, None)
            if v is None or isinstance(v, SpecialValue):
                missing.append(k)
            else:
                result[k] = v

        if missing:
            raise Exception(
                f"Destiny not valid (yet), missing inputs: {', '.join(missing)}"
            )

        self._merged_inputs = result
        return self._merged_inputs

    @property
    def module(self) -> "KiaraModule":
        if self._module is None:
            m_cls = self.module_details.get_class()
            self._module = m_cls(module_config=self.module_config)
        return self._module

    def execute(self, kiara: "Kiara") -> "Value":

        if self.result_value_id is not None:
            raise Exception("Destiny already resolved.")

        results = kiara.job_registry.execute_and_retrieve(
            manifest=self, inputs=self.merged_inputs
        )
        value = results.get_value_obj(field_name=self.result_field_name)

        self.result_value_id = value.value_id
        return value

    # def _retrieve_job_config_hash(self) -> int:
    #     obj = {"module_config": self.manifest_data, "inputs": self.merged_inputs}
    #     return compute_cid(obj)


# kiara\kiara\src\kiara\models\module\jobs.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import logging
import os
import uuid
from datetime import datetime
from enum import Enum
from typing import TYPE_CHECKING, Any, ClassVar, Dict, List, Mapping, Union

from pydantic import field_validator
from pydantic.fields import Field, PrivateAttr
from pydantic.main import BaseModel
from rich import box
from rich.console import RenderableType
from rich.table import Table

from kiara.exceptions import InvalidValuesException, KiaraException
from kiara.models import KiaraModel
from kiara.models.module.manifest import InputsManifest
from kiara.utils.dates import get_current_time_incl_timezone

if TYPE_CHECKING:
    from kiara.context import DataRegistry, Kiara
    from kiara.modules import KiaraModule


class ExecutionContext(KiaraModel):

    _kiara_model_id: ClassVar = "instance.execution_context"

    working_dir: str = Field(
        description="The path of the working directory.", default_factory=os.getcwd
    )
    pipeline_dir: Union[str, None] = Field(
        description="The path of the pipeline file that is being executed (if applicable).",
        default=None,
    )


class JobStatus(Enum):

    CREATED = "__job_created__"
    STARTED = "__job_started__"
    SUCCESS = "__job_success__"
    FAILED = "__job_failed__"


class LogMessage(BaseModel):

    timestamp: datetime = Field(
        description="The time the message was logged.", default_factory=datetime.now
    )
    log_level: int = Field(description="The log level.")
    msg: str = Field(description="The log message")


class JobLog(BaseModel):

    log: List[LogMessage] = Field(
        description="The logs for this job.", default_factory=list
    )
    percent_finished: int = Field(
        description="Describes how much of the job is finished. A negative number means the module does not support progress tracking.",
        default=-1,
    )

    def add_log(self, msg: str, log_level: int = logging.DEBUG):

        _msg = LogMessage(msg=msg, log_level=log_level)
        self.log.append(_msg)


class PipelineMetadata(BaseModel):

    pipeline_id: uuid.UUID = Field(description="The id of the pipeline.")
    step_id: str = Field(description="The id of the step in the pipeline.")


class JobConfig(InputsManifest):

    _kiara_model_id: ClassVar = "instance.job_config"

    @classmethod
    def create_from_module(
        cls,
        data_registry: "DataRegistry",
        module: "KiaraModule",
        inputs: Mapping[str, Any],
    ) -> "JobConfig":

        augmented = module.augment_module_inputs(inputs=inputs)

        values = data_registry.create_valuemap(
            data=augmented, schema=module.full_inputs_schema
        )

        invalid = values.check_invalid()
        if invalid:
            raise InvalidValuesException(invalid_values=invalid)

        value_ids = values.get_all_value_ids()

        if not module.manifest.is_resolved:
            raise KiaraException(
                msg="Cannot create job config from unresolved manifest."
            )

        return JobConfig(
            module_type=module.manifest.module_type,
            module_config=module.manifest.module_config,
            is_resolved=module.manifest.is_resolved,
            inputs=value_ids,
        )

    def _retrieve_data_to_hash(self) -> Any:
        return {"manifest": self.manifest_cid, "inputs": self.inputs_cid}

    pipeline_metadata: Union[PipelineMetadata, None] = Field(
        description="Metadata for the pipeline this job is part of.", default=None
    )
    # job_metadata: Mapping[str, Any] = Field(
    #     description="Optional metadata for this job.", default_factory=dict
    # )


class ActiveJob(KiaraModel):

    _kiara_model_id: ClassVar = "instance.active_job"

    job_id: uuid.UUID = Field(description="The job id.")

    job_config: JobConfig = Field(description="The job details.")
    status: JobStatus = Field(
        description="The current status of the job.", default=JobStatus.CREATED
    )
    job_log: JobLog = Field(description="The lob jog.")
    submitted: datetime = Field(
        description="When the job was submitted.",
        default_factory=get_current_time_incl_timezone,
    )
    started: Union[datetime, None] = Field(
        description="When the job was started.", default=None
    )
    finished: Union[datetime, None] = Field(
        description="When the job was finished.", default=None
    )
    results: Union[Dict[str, uuid.UUID], None] = Field(
        description="The result(s).", default=None
    )
    error: Union[str, None] = Field(
        description="Potential error message.", default=None
    )
    _exception: Union[Exception, None] = PrivateAttr(default=None)

    def is_finished(self) -> bool:
        return self.finished is not None

    def _retrieve_id(self) -> str:
        return str(self.job_id)

    def _retrieve_data_to_hash(self) -> Any:
        return self.job_id.bytes

    @property
    def exception(self) -> Union[Exception, None]:
        return self._exception

    @property
    def runtime(self) -> Union[float, None]:

        if self.started is None or self.finished is None:
            return None

        runtime = self.finished - self.started
        return runtime.total_seconds()


class JobRuntimeDetails(BaseModel):

    # @classmethod
    # def from_manifest(
    #     cls,
    #     manifest: Manifest,
    #     inputs: Mapping[str, Value],
    #     outputs: Mapping[str, Value],
    # ):
    #
    #     return JobRecord(
    #         module_type=manifest.module_type,
    #         module_config=manifest.module_config,
    #         inputs={k: v.value_id for k, v in inputs.items()},
    #         outputs={k: v.value_id for k, v in outputs.items()},
    #     )

    job_log: JobLog = Field(description="The lob jog.")
    submitted: datetime = Field(description="When the job was submitted.")
    started: datetime = Field(description="When the job was started.")
    finished: datetime = Field(description="When the job was finished.")
    runtime: float = Field(description="The duration of the job (in seconds).")

    def create_renderable(self, **config: Any) -> RenderableType:

        table = Table(show_header=False, box=box.SIMPLE)
        table.add_column("Key", style="i")
        table.add_column("Value")

        table.add_row("submitted", str(self.submitted))
        table.add_row("started", str(self.started))
        table.add_row("finished", str(self.finished))
        table.add_row("runtime", f"{self.runtime} seconds")

        job_log_table = Table(show_header=False, box=box.SIMPLE)
        job_log_table.add_column("timestamp", style="i")
        job_log_table.add_column("message")
        for log in self.job_log.log:
            job_log_table.add_row(str(log.timestamp), log.msg)

        table.add_row("job log", job_log_table)

        return table


class JobRecord(JobConfig):

    _kiara_model_id: ClassVar = "instance.job_record"

    @classmethod
    def from_active_job(self, kiara: "Kiara", active_job: ActiveJob):

        assert active_job.status == JobStatus.SUCCESS
        assert active_job.results is not None

        job_details = JobRuntimeDetails(
            job_log=active_job.job_log,
            submitted=active_job.submitted,
            started=active_job.started,  # type: ignore
            finished=active_job.finished,  # type: ignore
            runtime=active_job.runtime,  # type: ignore
        )

        (
            inputs_data_cid,
            contains_invalid,
        ) = active_job.job_config.calculate_inputs_data_cid(
            data_registry=kiara.data_registry
        )
        inputs_data_hash = str(inputs_data_cid)

        module = kiara.module_registry.create_module(active_job.job_config)
        is_internal = module.characteristics.is_internal

        env_hashes = {
            env.model_type_id: str(env.instance_cid)
            for env in kiara.current_environments.values()
        }

        job_record = JobRecord(
            job_id=active_job.job_id,
            job_submitted=active_job.submitted,
            is_internal=is_internal,
            module_type=active_job.job_config.module_type,
            module_config=active_job.job_config.module_config,
            is_resolved=active_job.job_config.is_resolved,
            inputs=active_job.job_config.inputs,
            outputs=active_job.results,
            runtime_details=job_details,
            environment_hashes=env_hashes,
            # input_ids_hash=active_job.job_config.input_ids_hash,
            inputs_data_hash=inputs_data_hash,
        )
        job_record._manifest_cid = active_job.job_config.manifest_cid
        job_record._manifest_data = active_job.job_config.manifest_data
        job_record._jobs_cid = active_job.job_config.job_cid
        job_record._inputs_cid = active_job.job_config.inputs_cid
        return job_record

    job_id: uuid.UUID = Field(description="The globally unique id for this job.")
    job_submitted: datetime = Field(description="When the job was submitted.")
    environment_hashes: Mapping[str, str] = Field(
        description="Hashes for the environments this value was created in."
    )
    # enviroments: Union[Mapping[str, Mapping[str, Any]], None] = Field(
    #     description="Information about the environments this value was created in.",
    #     default=None,
    # )
    is_internal: bool = Field(description="Whether this job was created by the system.")
    # job_hash: str = Field(description="The hash of the job. Calculated from manifest & input_ids hashes.")
    # manifest_hash: str = Field(description="The hash of the manifest.")
    # input_ids_hash: str = Field(description="The hash of the field names and input ids (the value_ids/uuids).")
    inputs_data_hash: str = Field(
        description="A map of the hashes of this jobs inputs (the hashes of field names and the actual bytes)."
    )

    outputs: Dict[str, uuid.UUID] = Field(description="References to the job outputs.")
    runtime_details: Union[JobRuntimeDetails, None] = Field(
        description="Runtime details for the job."
    )
    # job_metadata: Mapping[str, Any] = Field(
    #     description="Optional metadata for this job.", default_factory=dict
    # )

    _is_stored: bool = PrivateAttr(default=None)
    _outputs_hash: Union[int, None] = PrivateAttr(default=None)

    # @field_validator("job_metadata", mode="before")
    # @classmethod
    # def validate_metadata(cls, value):
    #
    #     if value is None:
    #         value = {}
    #     return value

    def _retrieve_data_to_hash(self) -> Any:
        return {
            "manifest": self.manifest_cid,
            "inputs": self.inputs_cid,
            "outputs": {k: v.bytes for k, v in self.outputs.items()},
        }

    def create_renderable(self, **config: Any) -> RenderableType:

        from kiara.utils.output import extract_renderable

        include = config.get("include", None)

        table = Table(show_header=False, box=box.SIMPLE)
        table.add_column("Key", style="i")
        table.add_column("Value")
        for k in self.model_fields.keys():
            if include is not None and k not in include:
                continue
            attr = getattr(self, k)
            v = extract_renderable(attr)
            table.add_row(k, v)
        table.add_row("job hash", self.job_hash)
        table.add_row("inputs hash", self.input_ids_hash)
        return table

    # @property
    # def outputs_hash(self) -> int:
    #
    #     if self._outputs_hash is not None:
    #         return self._outputs_hash
    #
    #     obj = self.outputs
    #     h = DeepHash(obj, hasher=KIARA_HASH_FUNCTION)
    #     self._outputs_hash = h[obj]
    #     return self._outputs_hash


class JobMatcher(KiaraModel):
    @classmethod
    def create_matcher(self, **match_options: Any):

        m = JobMatcher(**match_options)
        return m

    job_ids: List[uuid.UUID] = Field(
        description="A list of job ids, if specified, only jobs with one of these ids will be included.",
        default_factory=list,
    )
    allow_internal: bool = Field(description="Allow internal jobs.", default=False)
    earliest: Union[None, datetime] = Field(
        description="The earliest time when the job was created.", default=None
    )
    latest: Union[None, datetime] = Field(
        description="The latest time when the job was created.", default=None
    )
    operation_inputs: List[uuid.UUID] = Field(
        description="A list of value ids, if specified, only jobs that use one of them will be included.",
        default_factory=list,
    )
    produced_outputs: List[uuid.UUID] = Field(
        description="A list of value ids, if specified, only jobs that produced one of them will be included.",
        default_factory=list,
    )

    @field_validator("job_ids", mode="before")
    @classmethod
    def validate_job_ids(cls, v):

        if v is None:
            return []
        elif isinstance(v, uuid.UUID):
            return [v]
        elif isinstance(v, str):
            return [uuid.UUID(v)]
        else:
            return [x if isinstance(x, uuid.UUID) else uuid.UUID(x) for x in v]


# kiara\kiara\src\kiara\models\module\manifest.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import uuid
from typing import TYPE_CHECKING, Any, ClassVar, Dict, Mapping, Tuple, Union

import orjson
from dag_cbor import IPLDKind
from multiformats import CID
from pydantic import BaseModel, ConfigDict, Field, PrivateAttr, field_validator
from rich.console import RenderableType
from rich.syntax import Syntax
from rich.table import Table

from kiara.defaults import INVALID_HASH_MARKER, NONE_VALUE_ID
from kiara.exceptions import KiaraException
from kiara.models import KiaraModel
from kiara.utils.develop import log_dev_message
from kiara.utils.hashing import compute_cid
from kiara.utils.json import orjson_dumps
from kiara.utils.pipelines import extract_data_to_hash_from_pipeline_config

if TYPE_CHECKING:
    from kiara.registries.data import DataRegistry


class Manifest(KiaraModel):
    """A class to hold the type and configuration for a module instance."""

    _kiara_model_id: ClassVar = "instance.manifest"
    model_config = ConfigDict(extra="forbid", validate_default=True)

    _manifest_data: Union[Mapping[str, Any], None] = PrivateAttr(default=None)
    _manifest_cid: Union[CID, None] = PrivateAttr(default=None)

    module_type: str = Field(description="The module type.")
    module_config: Mapping[str, Any] = Field(
        default_factory=dict, description="The configuration for the module."
    )
    is_resolved: bool = Field(
        description="Whether the configuration of this module was augmented with the module type defaults etc.",
        default=False,
    )
    # python_class: PythonClass = Field(description="The python class that implements this module.")
    # doc: DocumentationMetadataModel = Field(
    #     description="Documentation for this module instance.", default=None
    # )

    # @validator("module_config")
    # def _validate_module_config(cls, value):
    #
    #     return value

    @field_validator("module_config")
    @classmethod
    def validate_module_config(cls, value):
        if isinstance(value, BaseModel):
            raise ValueError(f"Invalid module config type: {type(value)}")

        return value

    @property
    def manifest_data(self):
        """The configuration data for this module instance."""
        if self._manifest_data is not None:
            return self._manifest_data

        mc = extract_data_to_hash_from_pipeline_config(self.module_config)

        self._manifest_data = {
            "module_type": self.module_type,
            "module_config": mc,
        }
        return self._manifest_data

    @property
    def manifest_cid(self) -> CID:

        if self._manifest_cid is not None:
            return self._manifest_cid

        if not self.is_resolved:

            msg = "Cannot calculate manifest CID for unresolved manifest."
            item = Syntax(
                self.model_dump_json(indent=2),
                "json",
                background_color="default",
            )
            table = Table(show_header=False)
            table.add_column("key")
            table.add_column("value")

            table.add_row("", msg)
            table.add_row()
            table.add_row("type", str(type(self)))
            table.add_row()
            table.add_row("manifest", item)

            log_dev_message(table, title="cid computation error")

            raise KiaraException(msg=msg)

        _, self._manifest_cid = compute_cid(self.manifest_data)
        return self._manifest_cid

    @property
    def manifest_hash(self) -> str:
        return str(self.manifest_cid)

    def manifest_data_as_json(self):

        return self.model_dump_json(include={"module_type", "module_config"})

    def _retrieve_data_to_hash(self) -> Any:

        return self.manifest_data

    def create_renderable(self, **config: Any) -> RenderableType:
        """Create a renderable for this module configuration."""
        data = self.model_dump(exclude_none=True)
        conf = Syntax(
            orjson_dumps(data, option=orjson.OPT_INDENT_2),
            "json",
            background_color="default",
        )
        return conf

    def __repr__(self):

        return f"{self.__class__.__name__}(module_type={self.module_type}, module_config={self.module_config})"

    def __str__(self):

        return self.__repr__()


class InputsManifest(Manifest):

    _kiara_model_id: ClassVar = "instance.manifest_with_inputs"

    inputs: Mapping[str, uuid.UUID] = Field(
        description="A map of all the input fields and value references."
    )
    _inputs_cid: Union[CID, None] = PrivateAttr(default=None)
    # _inputs_hash: Union[str, None] = PrivateAttr(default=None)
    _jobs_cid: Union[CID, None] = PrivateAttr(default=None)
    _inputs_data_cid: Union[CID, None] = PrivateAttr(default=None)
    _input_data_contains_invalid: Union[bool, None] = PrivateAttr(default=None)

    @field_validator("inputs")
    @classmethod
    def replace_none_values(cls, value):
        result = {}
        for k, v in value.items():
            if v is None:
                v = NONE_VALUE_ID
            result[k] = v
        return result

    @property
    def job_hash(self) -> str:

        return str(self.job_cid)

    @property
    def job_cid(self) -> CID:

        if self._jobs_cid is not None:
            return self._jobs_cid

        obj: IPLDKind = {"manifest": self.manifest_cid, "inputs": self.inputs_cid}
        _, self._jobs_cid = compute_cid(data=obj)
        return self._jobs_cid

    @property
    def inputs_cid(self) -> CID:
        if self._inputs_cid is not None:
            return self._inputs_cid

        _, cid = compute_cid(data={k: v.bytes for k, v in self.inputs.items()})
        self._inputs_cid = cid
        return self._inputs_cid

    @property
    def input_ids_hash(self) -> str:

        return str(self.inputs_cid)

    def calculate_inputs_data_cid(
        self, data_registry: "DataRegistry"
    ) -> Tuple[CID, bool]:
        """Calculates the cid of the data hashes contained in this inputs manifest.

        This returns two values in a tuple: the first value is the cid where 'invalid hash markes' (used when a value is  not set) is set to 'None', the second one indicates whether such an
        invalid hash marker was encountered.

        This might be important to know, because if the interface of the module in question changed (which is possible for those types of fields), the computed input might not be valid anymore and would need to be re-computed.
        """

        if self._inputs_data_cid is not None:
            return (self._inputs_data_cid, self._input_data_contains_invalid)  # type: ignore

        data_hashes: Dict[str, Any] = {}
        invalid = False

        for k, v in self.inputs.items():
            value = data_registry.get_value(v)
            if value.value_hash == INVALID_HASH_MARKER:
                invalid = True
                data_hashes[k] = None
            else:
                data_hashes[k] = CID.decode(value.value_hash)

        _, cid = compute_cid(data=data_hashes)
        self._input_data_contains_invalid = invalid
        self._inputs_data_cid = cid
        return (cid, invalid)


# kiara\kiara\src\kiara\models\module\operation.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import abc
from typing import TYPE_CHECKING, Any, ClassVar, Dict, Iterable, Mapping, Union

import structlog
from pydantic import Field, PrivateAttr, field_validator
from rich import box
from rich.console import Group, RenderableType
from rich.markdown import Markdown
from rich.syntax import Syntax
from rich.table import Table

from kiara.defaults import PYDANTIC_USE_CONSTRUCT
from kiara.models import KiaraModel
from kiara.models.documentation import DocumentationMetadataModel
from kiara.models.module.jobs import JobConfig
from kiara.models.module.manifest import Manifest
from kiara.models.module.pipeline import PipelineConfig
from kiara.models.python_class import KiaraModuleInstance
from kiara.models.values.value import ValueMap, ValueMapReadOnly
from kiara.models.values.value_schema import ValueSchema
from kiara.modules import InputOutputObject, KiaraModule, ValueMapSchema
from kiara.utils.output import create_table_from_field_schemas

try:
    from typing import Self  # type: ignore
except ImportError:
    from typing_extensions import Self  # type: ignore

if TYPE_CHECKING:
    from kiara.context import Kiara
    from kiara.models.module.pipeline.structure import PipelineStructure

logger = structlog.getLogger()


class OperationSchema(InputOutputObject):
    def __init__(
        self, alias: str, inputs_schema: ValueMapSchema, outputs_schema: ValueMapSchema
    ):

        allow_empty_inputs = True
        allow_empty_outputs = True

        self._inputs_schema_static: ValueMapSchema = inputs_schema
        self._outputs_schema_static: ValueMapSchema = outputs_schema
        super().__init__(
            alias=alias,
            allow_empty_inputs_schema=allow_empty_inputs,
            allow_empty_outputs_schema=allow_empty_outputs,
        )

    def create_inputs_schema(
        self,
    ) -> Mapping[str, Union[ValueSchema, Mapping[str, Any]]]:
        return self._inputs_schema_static

    def create_outputs_schema(
        self,
    ) -> Mapping[str, Union[ValueSchema, Mapping[str, Any]]]:

        return self._outputs_schema_static


class OperationDetails(KiaraModel):

    _kiara_model_id: ClassVar = "instance.operation_details"

    # inputs_map: Dict[str, str] = Field(description="A map with the operations input fields as keys, and the underlying modules input fields as values, used to translate input value maps.")
    # outputs_map: Dict[str, str] = Field(description="A map with the operations input fields as keys, and the underlying modules input fields as values, used to translate input value maps.")

    @classmethod
    def create_operation_details(cls, **details: Any) -> Self:

        if PYDANTIC_USE_CONSTRUCT:
            result = cls(**details)
        else:
            result = cls(**details)

        return result

    operation_id: str = Field(description="The id of the operation.")
    is_internal_operation: bool = Field(
        description="Whether this operation is mainly used kiara-internally. Helps to hide it in UIs (operation lists etc.).",
        default=False,
    )

    def _retrieve_id(self) -> str:
        return self.operation_id

    @property
    def inputs_schema(self) -> Mapping[str, ValueSchema]:
        """The input schema for this module."""
        return self.get_operation_schema().inputs_schema

    @property
    def outputs_schema(self) -> Mapping[str, ValueSchema]:
        """The input schema for this module."""
        return self.get_operation_schema().outputs_schema

    def get_operation_schema(self) -> OperationSchema:
        raise NotImplementedError()


class BaseOperationDetails(OperationDetails):

    _kiara_model_id: ClassVar = "instance.operation_details.base"

    module_inputs_schema: Mapping[str, ValueSchema] = Field(
        description="The input schemas of the module."
    )
    module_outputs_schema: Mapping[str, ValueSchema] = Field(
        description="The output schemas of the module."
    )
    _op_schema: OperationSchema = PrivateAttr(default=None)

    def get_operation_schema(self) -> OperationSchema:

        if self._op_schema is not None:
            return self._op_schema

        self._op_schema = OperationSchema(
            alias=self.operation_id,
            inputs_schema=self.module_inputs_schema,
            outputs_schema=self.module_outputs_schema,
        )
        return self._op_schema


class OperationConfig(KiaraModel):

    doc: DocumentationMetadataModel = Field(
        description="Documentation for this operation."
    )

    @field_validator("doc", mode="before")
    @classmethod
    def validate_doc(cls, value):
        return DocumentationMetadataModel.create(value)

    @abc.abstractmethod
    def retrieve_module_type(self, kiara: "Kiara") -> str:
        pass

    @abc.abstractmethod
    def retrieve_module_config(self, kiara: "Kiara") -> Mapping[str, Any]:
        pass


class ManifestOperationConfig(OperationConfig):

    _kiara_model_id: ClassVar = "instance.operation_config.manifest"

    module_type: str = Field(description="The module type.")
    module_config: Dict[str, Any] = Field(
        default_factory=dict, description="The configuration for the module."
    )

    _manifest_cache: Union[None, Manifest] = PrivateAttr(default=None)

    @field_validator("doc", mode="before")
    @classmethod
    def validate_doc(cls, value):
        return DocumentationMetadataModel.create(value)

    def retrieve_module_type(self, kiara: "Kiara") -> str:
        return self.module_type

    def retrieve_module_config(self, kiara: "Kiara") -> Mapping[str, Any]:
        return self.module_config

    def get_manifest(self) -> Manifest:

        if self._manifest_cache is None:
            self._manifest_cache = Manifest(
                module_type=self.module_type, module_config=self.module_config
            )
        return self._manifest_cache


class PipelineOperationConfig(OperationConfig):

    _kiara_model_id: ClassVar = "instance.operation_config.pipeline"

    pipeline_name: str = Field(description="The pipeline id.")
    pipeline_config: Mapping[str, Any] = Field(description="The pipeline config data.")
    module_map: Dict[str, Any] = Field(
        description="A lookup map to resolves operation ids to module names/configs.",
        default_factory=dict,
    )
    metadata: Mapping[str, Any] = Field(
        description="Additional metadata for the pipeline.", default_factory=dict
    )

    @field_validator("pipeline_config")
    @classmethod
    def validate_pipeline_config(cls, value):
        # TODO
        assert isinstance(value, Mapping)
        assert "steps" in value.keys()

        return value

    def retrieve_module_type(self, kiara: "Kiara") -> str:
        return "pipeline"

    def retrieve_module_config(self, kiara: "Kiara") -> Mapping[str, Any]:

        # using _from_config here because otherwise we'd enter an infinite loop
        pipeline_config = PipelineConfig._from_config(
            pipeline_name=self.pipeline_name,
            data=self.pipeline_config,
            kiara=kiara,
            module_map=self.module_map,
        )
        # ODO: pydantic refactoring -- maybe test that the dumped config is equivalent to the original one?
        result = pipeline_config.model_dump()

        return result

    @property
    def required_module_types(self) -> Iterable[str]:

        return [step["module_type"] for step in self.pipeline_config["steps"]]

    def __repr__(self):

        return f"{self.__class__.__name__}(pipeline_name={self.pipeline_name} required_modules={list(self.required_module_types)} instance_id={self.instance_id} fields=[{', '.join(self.model_fields.keys())}])"


class Operation(Manifest):

    _kiara_model_id: ClassVar = "instance.operation"

    @classmethod
    def create_from_module(
        cls, module: KiaraModule, doc: Union[Any, None] = None
    ) -> "Operation":

        from kiara.operations.included_core_operations import (
            CustomModuleOperationDetails,
        )

        op_id = f"{module.module_type_name}._{module.module_instance_cid}"
        if module.is_pipeline():
            from kiara.operations.included_core_operations.pipeline import (
                PipelineOperationDetails,
            )

            details = PipelineOperationDetails.create_operation_details(
                operation_id=module.config.pipeline_name,
                pipeline_inputs_schema=module.inputs_schema,
                pipeline_outputs_schema=module.outputs_schema,
                pipeline_config=module.config,
            )
        else:
            details = CustomModuleOperationDetails.create_from_module(module=module)

        if doc is not None:
            doc = DocumentationMetadataModel.create(doc)
        else:
            doc = DocumentationMetadataModel.from_class_doc(module.__class__)

        operation = Operation(
            module_type=module.module_type_name,
            module_config=module.config.model_dump(),
            operation_id=op_id,
            operation_details=details,
            module_details=KiaraModuleInstance.from_module(module),
            doc=doc,
        )
        operation._module = module
        return operation

    operation_id: str = Field(description="The (unique) id of this operation.")
    operation_details: OperationDetails = Field(
        description="The operation specific details of this operation."
    )
    doc: DocumentationMetadataModel = Field(
        description="Documentation for this operation."
    )

    module_details: KiaraModuleInstance = Field(
        description="The class of the underlying module."
    )
    metadata: Mapping[str, Any] = Field(
        description="Additional metadata for this operation.", default_factory=dict
    )

    _module: Union["KiaraModule", None] = PrivateAttr(default=None)
    _pipeline_config: Union[None, PipelineConfig] = PrivateAttr(default=None)

    def _retrieve_data_to_hash(self) -> Any:
        return {"operation_id": self.operation_id, "manifest": self.manifest_cid}

    def _retrieve_id(self) -> str:
        return self.operation_id

    @property
    def module(self) -> "KiaraModule":
        if self._module is None:
            m_cls = self.module_details.get_class()
            self._module = m_cls(module_config=self.module_config)
        return self._module

    @property
    def inputs_schema(self) -> Mapping[str, ValueSchema]:
        return self.operation_details.inputs_schema

    @property
    def outputs_schema(self) -> Mapping[str, ValueSchema]:
        return self.operation_details.outputs_schema

    def prepare_job_config(
        self, kiara: "Kiara", inputs: Mapping[str, Any]
    ) -> JobConfig:

        augmented_inputs = (
            self.operation_details.get_operation_schema().augment_module_inputs(
                inputs=inputs
            )
        )

        # module_inputs = self.operation_details.create_module_inputs(
        #     inputs=augmented_inputs
        # )

        job_config = kiara.job_registry.prepare_job_config(
            manifest=self, inputs=augmented_inputs
        )
        return job_config

    def run(self, kiara: "Kiara", inputs: Mapping[str, Any]) -> ValueMap:

        logger.debug("run.operation", operation_id=self.operation_id)
        job_config = self.prepare_job_config(kiara=kiara, inputs=inputs)

        job_id = kiara.job_registry.execute_job(job_config=job_config)
        outputs: ValueMap = kiara.job_registry.retrieve_result(job_id=job_id)

        result = self.process_job_outputs(outputs=outputs)

        return result

    def process_job_outputs(self, outputs: ValueMap) -> ValueMap:

        # op_outputs = self.operation_details.create_operation_outputs(outputs=outputs)

        value_set = ValueMapReadOnly(value_items=outputs, values_schema=self.outputs_schema)  # type: ignore
        return value_set

    @property
    def pipeline_config(self) -> PipelineConfig:

        if not self.module.is_pipeline():
            raise Exception(
                f"Can't retrieve pipeline details from operation '{self.operation_id}: not a pipeline operation type.'"
            )

        op_details = self.operation_details
        return op_details.pipeline_config  # type: ignore

    @property
    def pipeline_structure(self) -> "PipelineStructure":
        return self.pipeline_config.structure

    def create_renderable(self, **config: Any) -> RenderableType:
        """
        Create a printable overview of this operations details.

        Available render_config options:
          - 'include_full_doc' (default: True): whether to include the full documentation, or just a description
          - 'include_src' (default: False): whether to include the module source code
        """
        include_full_doc = config.get("include_full_doc", True)
        include_src = config.get("include_src", False)
        include_inputs = config.get("include_inputs", True)
        include_outputs = config.get("include_outputs", True)
        include_module_details = config.get("include_module_details", False)

        table = Table(box=box.SIMPLE, show_header=False, show_lines=True)
        table.add_column("Property", style="i")
        table.add_column("Value")

        if self.doc:
            if include_full_doc:
                doc = self.doc.full_doc
                title = "Documentation"
            else:
                doc = self.doc.description
                title = "Description"

            table.add_row(title, Markdown(doc))

        # module_type_md = self.module.get_type_metadata()

        if include_inputs:
            inputs_table = create_table_from_field_schemas(
                _add_required=True,
                _add_default=True,
                _show_header=True,
                _constants=None,
                fields=self.operation_details.inputs_schema,
            )
            table.add_row("Inputs", inputs_table)
        if include_outputs:
            outputs_table = create_table_from_field_schemas(
                _add_required=False,
                _add_default=False,
                _show_header=True,
                _constants=None,
                fields=self.operation_details.outputs_schema,
            )
            table.add_row("Outputs", outputs_table)

        from kiara.interfaces.python_api.models.info import ModuleTypeInfo

        module_type_md: Union[ModuleTypeInfo, None] = None

        if include_module_details:
            table.add_row("Module type", self.module_type)

            module_config = self.module.config.model_dump_json(indent=2)
            conf = Syntax(
                module_config,
                "json",
                background_color="default",
            )
            table.add_row("Module config", conf)

            module_type_md = ModuleTypeInfo.create_from_type_class(
                type_cls=self.module_details.get_class(),  # type: ignore
                kiara=None,  # type: ignore
            )

            desc = module_type_md.documentation.description
            module_md = module_type_md.create_renderable(
                include_doc=False, include_src=False, include_config_schema=False
            )
            m_md = Group(desc, module_md)
            table.add_row("Module metadata", m_md)

        if include_src:
            if module_type_md is None:
                module_type_md = ModuleTypeInfo.create_from_type_class(
                    type_cls=self.module_details.get_class(),  # type: ignore
                    kiara=None,  # type: ignore
                )

            table.add_row("Source code", module_type_md.module_src)

        return table


class Filter(KiaraModel):

    operation: Operation = Field(
        description="The underlying operation providing which does the filtering."
    )
    input_name: str = Field(
        description="The input name to use for the dataset to filter."
    )
    output_name: str = Field(
        description="The output name to use for the dataset to filter."
    )
    data_type: str = Field(description="The type of the dataset that gets filtered.")


# kiara\kiara\src\kiara\models\module\persistence.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)
from enum import Enum
from typing import Any, List, Mapping, Union

from pydantic import BaseModel, Field


class ByteProvisioningStrategy(Enum):

    INLINE = "INLINE"
    BYTES = "bytes"
    FILE_PATH_MAP = "link_map"
    LINK_FOLDER = "folder"
    COPIED_FOLDER = "copied_folder"


class BytesStructure(BaseModel):
    """A data structure that."""

    data_type: str = Field(description="The data type.")
    data_type_config: Mapping[str, Any] = Field(description="The data type config.")
    chunk_map: Mapping[str, List[Union[str, bytes]]] = Field(
        description="References to byte arrays, Keys are field names, values are a list of hash-ids that the data is composed of.",
        default_factory=dict,
    )

    # def provision_as_folder(self, copy_files: bool = False) -> Path:
    #     pass


class BytesAliasStructure(BaseModel):

    data_type: str = Field(description="The data type.")
    data_type_config: Mapping[str, Any] = Field(description="The data type config.")
    chunk_id_map: Mapping[str, List[str]] = Field(
        description="References to byte arrays, Keys are field names, values are a list of hash-ids that the data is composed of.",
        default_factory=dict,
    )


# kiara\kiara\src\kiara\models\module\__init__.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

from typing import TYPE_CHECKING, Any, ClassVar, Dict, Mapping, Union

from pydantic import ConfigDict, PrivateAttr
from pydantic.fields import Field
from pydantic.main import BaseModel
from pydantic_core import PydanticUndefined
from rich import box
from rich.console import RenderableType
from rich.table import Table

from kiara.models import KiaraModel

if TYPE_CHECKING:
    pass


class KiaraModuleConfig(KiaraModel):
    """
    Base class that describes the configuration a [``KiaraModule``][kiara.module.KiaraModule] class accepts.

    This is stored in the ``_config_cls`` class attribute in each ``KiaraModule`` class.

    There are two config options every ``KiaraModule`` supports:

     - ``constants``, and
     - ``defaults``

     Constants are pre-set inputs, and users can't change them and an error is thrown if they try. Defaults are default
     values that override the schema defaults, and those can be overwritten by users. If both a constant and a default
     value is set for an input field, an error is thrown.
    """

    _kiara_model_id: ClassVar = "instance.module_config"

    @classmethod
    def requires_config(cls, config: Union[Mapping[str, Any], None] = None) -> bool:
        """Return whether this class can be used as-is, or requires configuration before an instance can be created."""

        for field_name, field in cls.model_fields.items():

            if not field.is_required():
                continue

            if (
                field.default in [None, PydanticUndefined]
                and field.default_factory is None
            ):
                if config:
                    if config.get(field_name, None) is None:
                        return True
                else:
                    return True
        return False

    _config_hash: str = PrivateAttr(default=None)
    constants: Dict[str, Any] = Field(
        default_factory=dict, description="Value constants for this module."
    )
    defaults: Dict[str, Any] = Field(
        default_factory=dict, description="Value defaults for this module."
    )
    model_config = ConfigDict(extra="forbid", validate_assignment=True)

    def get(self, key: str) -> Any:
        """Get the value for the specified configuation key."""
        if key not in self.model_fields:
            raise Exception(
                f"No config value '{key}' in module config class '{self.__class__.__name__}'."
            )

        return getattr(self, key)

    def create_renderable(self, **config: Any) -> RenderableType:

        my_table = Table(box=box.MINIMAL, show_header=False)
        my_table.add_column("Field name", style="i")
        my_table.add_column("Value")
        for field in self.model_fields:
            attr = getattr(self, field)
            if isinstance(attr, str):
                attr_str = attr
            elif hasattr(attr, "create_renderable"):
                attr_str = attr.create_renderable()
            elif isinstance(attr, BaseModel):
                attr_str = attr.model_dump_json(indent=2)
            else:
                attr_str = str(attr)
            my_table.add_row(field, attr_str)

        return my_table


# def calculate_class_doc_url(base_url: str, module_type_name: str):
#
#     if base_url.endswith("/"):
#         base_url = base_url[0:-1]
#
#     module_type_name = module_type_name.replace(".", "")
#     url = f"{base_url}/latest/modules_list/#{module_type_name}"
#
#     return url


# def calculate_class_source_url(
#     base_url: str, python_class_info: PythonClass, branch: str = "main"
# ):
#
#     if base_url.endswith("/"):
#         base_url = base_url[0:-1]
#
#     m = python_class_info.get_python_module()
#     m_file = m.__file__
#     assert m_file is not None
#
#     base_url = f"{base_url}/blob/{branch}/src/{python_class_info.python_module_name.replace('.', '/')}"
#
#     if m_file.endswith("__init__.py"):
#         url = f"{base_url}/__init__.py"
#     else:
#         url = f"{base_url}.py"
#
#     return url


# kiara\kiara\src\kiara\models\module\pipeline\controller.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import uuid
from typing import Callable, Dict, Mapping, Union

import structlog

from kiara.models.events.pipeline import PipelineEvent, PipelineState
from kiara.models.module.pipeline.pipeline import Pipeline, PipelineListener
from kiara.models.module.pipeline.stages import PipelineStage
from kiara.registries.jobs import JobRegistry
from kiara.utils import log_exception

logger = structlog.getLogger()


class PipelineController(PipelineListener):

    pass


class SinglePipelineController(PipelineController):
    def __init__(
        self, job_registry: JobRegistry, pipeline: Union[Pipeline, None] = None
    ):

        self._pipeline: Union[Pipeline, None] = None
        self._job_registry: JobRegistry = job_registry
        self._pipeline_details: Union[PipelineState, None] = None

        if pipeline is not None:
            self.pipeline = pipeline

    @property
    def pipeline(self) -> Pipeline:

        if self._pipeline is None:
            raise Exception("Pipeline not set (yet).")
        return self._pipeline

    @pipeline.setter
    def pipeline(self, pipeline: Pipeline):

        if self._pipeline is not None:
            # TODO: destroy object?
            self._pipeline._listeners.clear()

        self._pipeline = pipeline
        if self._pipeline is not None:
            self._pipeline.add_listener(self)

    def current_pipeline_state(self) -> PipelineState:

        if self._pipeline_details is None:
            self._pipeline_details = self.pipeline.get_pipeline_details()
        return self._pipeline_details

    def can_be_processed(self, step_id: str) -> bool:
        """Check whether the step with the provided id is ready to be processed."""
        pipeline_state = self.current_pipeline_state()
        step_state = pipeline_state.step_states[step_id]

        return not step_state.invalid_details

    def can_be_skipped(self, step_id: str) -> bool:
        """Check whether the processing of a step can be skipped."""
        required = self.pipeline.structure.step_is_required(step_id=step_id)
        if required:
            required = self.can_be_processed(step_id)
        return required

    def _pipeline_event_occurred(self, event: PipelineEvent):

        if event.pipeline_id != self.pipeline.pipeline_id:
            return

        self._pipeline_details = None

    def set_processing_results(
        self, job_ids: Mapping[str, uuid.UUID]
    ) -> Mapping[uuid.UUID, uuid.UUID]:
        """
        Set the processing results as values of the approrpiate step outputs.

        Returns:
        -------
            a dict with the result value id as key, and the id of the job that produced it as value
        """
        self._job_registry.wait_for(*job_ids.values())

        result: Dict[uuid.UUID, uuid.UUID] = {}
        combined_outputs = {}
        for step_id, job_id in job_ids.items():
            record = self._job_registry.get_job_record(job_id=job_id)
            if record is None:
                continue
            combined_outputs[step_id] = record.outputs
            for output_id in record.outputs.values():
                result[output_id] = job_id

        self.pipeline.set_multiple_step_outputs(
            changed_outputs=combined_outputs, notify_listeners=True
        )

        return result

    def pipeline_is_ready(self) -> bool:
        """
        Return whether the pipeline is ready to be processed.

        A ``True`` result means that all pipeline inputs are set with valid values, and therefore every step within the
        pipeline can be processed.

        Returns:
        -------
            whether the pipeline can be processed as a whole (``True``) or not (``False``)
        """
        pipeline_inputs = self.pipeline._all_values.get_alias("pipeline.inputs")
        assert pipeline_inputs is not None
        return pipeline_inputs.all_items_valid

    def process_step(self, step_id: str, wait: bool = False) -> uuid.UUID:
        """
        Kick off processing for the step with the provided id.

        Arguments:
        ---------
            step_id: the id of the step that should be started
        """

        from kiara.models.module.jobs import PipelineMetadata

        job_config = self.pipeline.create_job_config_for_step(step_id)

        # pipeline_metadata = {
        #     "is_pipeline_step": True,
        #     "step_id": step_id,
        #     "pipeline_id": self.pipeline.pipeline_id,
        # }

        pipeline_metadata = PipelineMetadata(
            pipeline_id=self.pipeline.pipeline_id, step_id=step_id
        )
        job_config.pipeline_metadata = pipeline_metadata

        job_id = self._job_registry.execute_job(job_config=job_config)
        # job_id = self._processor.create_job(job_config=job_config)
        # self._processor.queue_job(job_id=job_id)

        if wait:
            self._job_registry.wait_for(job_id)

        return job_id


class SinglePipelineBatchController(SinglePipelineController):
    """
    A [PipelineController][kiara.models.modules.pipeline.controller.PipelineController] that executes all pipeline steps non-interactively.

    This is the default implementation of a ``PipelineController``, and probably the most simple implementation of one.
    It waits until all inputs are set, after which it executes all pipeline steps in the required order.

    Arguments:
    ---------
        pipeline: the pipeline to control
        auto_process: whether to automatically start processing the pipeline as soon as the input set is valid
    """

    def __init__(
        self,
        pipeline: Pipeline,
        job_registry: JobRegistry,
        auto_process: bool = True,
    ):

        self._auto_process: bool = auto_process
        self._is_running: bool = False
        super().__init__(pipeline=pipeline, job_registry=job_registry)

    @property
    def auto_process(self) -> bool:
        return self._auto_process

    @auto_process.setter
    def auto_process(self, auto_process: bool):
        self._auto_process = auto_process

    def process_pipeline(
        self, event_callback: Union[Callable, None] = None
    ) -> Mapping[str, Union[uuid.UUID, Exception]]:

        log = logger.bind(pipeline_id=self.pipeline.pipeline_id)
        if self._is_running:
            log.debug(
                "ignore.pipeline_process",
                reason="Pipeline already running.",
            )
            raise Exception("Pipeline already running.")

        log.debug("execute.pipeline")
        self._is_running = True
        all_job_ids: Dict[str, Union[Exception, uuid.UUID]] = {}
        try:
            stages = PipelineStage.extract_stages(
                self.pipeline.structure, stages_extraction_type="early"
            )
            for idx, stage in enumerate(stages, start=1):

                if event_callback:
                    event_callback(f"start processing pipeline stage: {idx}")

                log.debug(
                    "execute.pipeline.stage",
                    stage=idx,
                )

                job_ids = {}
                for step_id in stage:
                    if event_callback:
                        event_callback(f"start processing pipeline step: {step_id}")

                    log.debug(
                        "execute.pipeline.step",
                        step_id=step_id,
                    )

                    try:
                        job_id = self.process_step(step_id)
                        job_ids[step_id] = job_id
                        if event_callback:
                            event_callback(f"finished processing step '{step_id}'")
                    except Exception as e:
                        all_job_ids[step_id] = e
                        # TODO: cancel running jobs?
                        log_exception(e)
                        log.error(
                            "error.processing.pipeline",
                            step_id=step_id,
                            error=e,
                        )
                        if event_callback:
                            event_callback(f"Error processing step '{step_id}': {e}")

                self.set_processing_results(job_ids=job_ids)
                log.debug(
                    "execute_finished.pipeline.stage",
                    stage=idx,
                )
                if event_callback:
                    event_callback(f"finished processing pipeline stage: {idx}")
                all_job_ids.update(job_ids)

        finally:
            self._is_running = False

        log.debug("execute_finished.pipeline")
        if event_callback:
            event_callback("finished processing pipeline")
        return all_job_ids


# kiara\kiara\src\kiara\models\module\pipeline\pipeline.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import abc
import uuid
from typing import (
    TYPE_CHECKING,
    Any,
    ClassVar,
    Dict,
    Iterable,
    List,
    Mapping,
    Type,
    Union,
)

import dpath
import networkx as nx
from pydantic import Field, PrivateAttr
from rich import box
from rich.console import RenderableType
from rich.markdown import Markdown
from rich.panel import Panel
from rich.table import Table

from kiara.defaults import NONE_VALUE_ID, NOT_SET_VALUE_ID, SpecialValue
from kiara.exceptions import InvalidValuesException
from kiara.interfaces.python_api.models.info import InfoItemGroup, ItemInfo
from kiara.models.aliases import AliasValueMap
from kiara.models.documentation import (
    AuthorsMetadataModel,
    ContextMetadataModel,
    DocumentationMetadataModel,
)
from kiara.models.events.pipeline import (
    ChangedValue,
    PipelineEvent,
    PipelineState,
    StepDetails,
)
from kiara.models.module.jobs import JobConfig
from kiara.models.module.pipeline import PipelineConfig, StepStatus
from kiara.models.module.pipeline.structure import PipelineStep, PipelineStructure
from kiara.models.module.pipeline.value_refs import (
    PipelineInputRef,
    PipelineOutputRef,
    ValueRef,
)
from kiara.models.values.value import ORPHAN
from kiara.models.values.value_schema import ValueSchema
from kiara.registries.data import DataRegistry
from kiara.utils.operations import create_operation
from kiara.utils.output import (
    create_pipeline_steps_tree,
    create_table_from_model_object,
    create_value_map_status_renderable,
)
from kiara.utils.yaml import StringYAML

if TYPE_CHECKING:
    from kiara.context import Kiara
    from kiara.interfaces.python_api.base_api import BaseAPI

yaml = StringYAML()


class PipelineListener(abc.ABC):
    @abc.abstractmethod
    def _pipeline_event_occurred(self, event: PipelineEvent):
        pass


class Pipeline(object):
    """An instance of a [PipelineStructure][kiara.pipeline.structure.PipelineStructure] that holds state for all of the inputs/outputs of the steps within."""

    @classmethod
    def create_pipeline(
        cls,
        kiara: Union["Kiara", "BaseAPI"],
        pipeline: Union[PipelineConfig, PipelineStructure, Mapping, str],
    ) -> "Pipeline":

        from kiara.interfaces.python_api.base_api import BaseAPI

        if isinstance(kiara, BaseAPI):
            kiara = kiara.context

        if isinstance(pipeline, Mapping):
            pipeline_structure: PipelineStructure = PipelineConfig.from_config(
                pipeline_name="__pipeline__", data=pipeline, kiara=kiara
            ).structure
        elif isinstance(pipeline, PipelineConfig):
            pipeline_structure = pipeline.structure
        elif isinstance(pipeline, PipelineStructure):
            pipeline_structure = pipeline
        elif isinstance(pipeline, str):
            operation = create_operation(module_or_operation=pipeline, kiara=kiara)
            module = operation.module
            if isinstance(module.config, PipelineConfig):
                config: PipelineConfig = module.config
            else:
                step_id = module.module_type_name.replace(".", "_")
                input_aliases = {}
                output_aliases = {}
                for input_name in module.input_names:
                    input_aliases[f"{step_id}.{input_name}"] = input_name
                for output_name in module.output_names:
                    output_aliases[f"{step_id}.{output_name}"] = output_name
                pipeline_config_data = {
                    "doc": module.doc,
                    "steps": [
                        {
                            "module_type": module.module_type_name,
                            "module_config": module.config.model_dump(),
                            "step_id": step_id,
                        },
                    ],
                    "input_aliases": input_aliases,
                    "output_aliases": output_aliases,
                }
                config = PipelineConfig.from_config(
                    pipeline_name=module.module_type_name,
                    data=pipeline_config_data,
                    kiara=kiara,
                )

            pipeline_structure = config.structure
        else:
            raise Exception(f"Invalid type for argument 'pipeline': {type(pipeline)}")

        pipeline_obj = Pipeline(kiara=kiara, structure=pipeline_structure)
        return pipeline_obj

    def __init__(self, structure: PipelineStructure, kiara: "Kiara"):

        self._id: uuid.UUID = uuid.uuid4()

        self._structure: PipelineStructure = structure

        self._value_refs: Mapping[AliasValueMap, Iterable[ValueRef]] = None  # type: ignore
        # self._status: StepStatus = StepStatus.STALE

        self._steps_by_stage: Dict[int, Dict[str, PipelineStep]] = None  # type: ignore
        self._inputs_by_stage: Dict[int, List[str]] = None  # type: ignore
        self._outputs_by_stage: Dict[int, List[str]] = None  # type: ignore

        self._kiara: Kiara = kiara
        self._data_registry: DataRegistry = kiara.data_registry

        self._all_values: AliasValueMap = None  # type: ignore

        self._listeners: List[PipelineListener] = []

        self._init_values()

        # self._update_status()

    @property
    def pipeline_id(self) -> uuid.UUID:
        return self._id

    # @property
    # def pipeline_name(self) -> str:
    #     return self.structure.pipeline_config.pipeline_name

    @property
    def kiara_id(self) -> uuid.UUID:
        return self._kiara.id

    def _init_values(self):
        """
        Initialize this object. This should only be called once.

        Basically, this goes through all the inputs and outputs of all steps, and 'allocates' a PipelineValueInfo object
        for each of them. In case where output/input or pipeline-input/input points are connected, only one
        value item is allocated, since those refer to the same value.
        """
        values = AliasValueMap(
            alias=str(self.id),
            version=0,
            assoc_value=None,
            values_schema={},
            assoc_schema=None,
        )
        values._data_registry = self._data_registry
        inputs_schema = self._structure.pipeline_inputs_schema
        outputs_schema = self._structure.pipeline_outputs_schema
        if inputs_schema:
            for field_name, schema in inputs_schema.items():
                values.set_alias_schema(f"pipeline.inputs.{field_name}", schema=schema)
        else:
            values.set_alias_schema("pipeline.inputs", schema=ValueSchema(type="none"))
        if outputs_schema:
            for field_name, schema in outputs_schema.items():
                values.set_alias_schema(f"pipeline.outputs.{field_name}", schema=schema)
        else:
            values.set_alias_schema("pipeline.outputs", schema=ValueSchema(type="none"))
        for step_id in self.step_ids:
            step = self.get_step(step_id)
            for field_name, value_schema in step.module.inputs_schema.items():
                values.set_alias_schema(
                    f"steps.{step_id}.inputs.{field_name}", schema=value_schema
                )
            for field_name, value_schema in step.module.outputs_schema.items():
                values.set_alias_schema(
                    f"steps.{step_id}.outputs.{field_name}", schema=value_schema
                )

        self._all_values = values

        initial_inputs = {
            k: SpecialValue.NOT_SET
            for k in self._structure.pipeline_inputs_schema.keys()
        }
        self.set_pipeline_inputs(inputs=initial_inputs)

    def __eq__(self, other):

        if not isinstance(other, Pipeline):
            return False

        return self._id == other._id

    def __hash__(self):

        return hash(self._id)

    def add_listener(self, listener: PipelineListener):

        self._listeners.append(listener)

    @property
    def id(self) -> uuid.UUID:
        return self._id

    @property
    def structure(self) -> PipelineStructure:
        return self._structure

    @property
    def pipeline_inputs_schema(self):
        return self._structure.pipeline_inputs_schema

    @property
    def pipeline_outputs_schema(self):
        return self._structure.pipeline_outputs_schema

    @property
    def config(self) -> PipelineConfig:
        return self._structure.pipeline_config

    @property
    def doc(self) -> DocumentationMetadataModel:
        return self.structure.pipeline_config.doc

    def get_current_pipeline_inputs(self) -> Dict[str, uuid.UUID]:
        """All (pipeline) input values of this pipeline."""
        if not self._structure.steps:
            return {}

        alias_map = self._all_values.get_alias("pipeline.inputs")
        return alias_map.get_all_value_ids()  # type: ignore

    def get_current_pipeline_outputs(self) -> Dict[str, uuid.UUID]:
        """All (pipeline) output values of this pipeline."""
        if not self._structure.steps:
            return {}

        alias_map = self._all_values.get_alias("pipeline.outputs")
        return alias_map.get_all_value_ids()  # type: ignore

    def get_current_step_inputs(self, step_id) -> Dict[str, uuid.UUID]:

        alias_map = self._all_values.get_alias(f"steps.{step_id}.inputs")
        return alias_map.get_all_value_ids()  # type: ignore

    def get_current_step_outputs(self, step_id) -> Dict[str, uuid.UUID]:

        alias_map = self._all_values.get_alias(f"steps.{step_id}.outputs")
        return alias_map.get_all_value_ids()  # type: ignore

    def get_inputs_for_steps(self, *step_ids: str) -> Dict[str, Dict[str, uuid.UUID]]:
        """Retrieve value ids for the inputs of the specified steps (or all steps, if no argument provided."""
        result = {}
        for step_id in self._structure.step_ids:
            if step_ids and step_id not in step_ids:
                continue
            ids = self.get_current_step_inputs(step_id=step_id)
            result[step_id] = ids
        return result

    def get_outputs_for_steps(self, *step_ids: str) -> Dict[str, Dict[str, uuid.UUID]]:
        """Retrieve value ids for the outputs of the specified steps (or all steps, if no argument provided."""
        result = {}
        for step_id in self._structure.step_ids:
            if step_ids and step_id not in step_ids:
                continue
            ids = self.get_current_step_outputs(step_id=step_id)
            result[step_id] = ids
        return result

    def _notify_pipeline_listeners(self, event: PipelineEvent):

        for listener in self._listeners:
            listener._pipeline_event_occurred(event=event)

    def get_pipeline_details(self) -> PipelineState:

        pipeline_inputs = self._all_values.get_alias("pipeline.inputs")
        pipeline_outputs = self._all_values.get_alias("pipeline.outputs")

        if pipeline_inputs:
            invalid = pipeline_inputs.check_invalid()
            if not invalid:
                status = StepStatus.INPUTS_READY
                step_outputs = self._all_values.get_alias("pipeline.outputs")
                assert step_outputs is not None
                invalid_outputs = step_outputs.check_invalid()
                # TODO: also check that all the pedigrees match up with current inputs
                if not invalid_outputs:
                    status = StepStatus.RESULTS_READY
            else:
                status = StepStatus.INPUTS_INVALID
            _pipeline_inputs = pipeline_inputs.get_all_value_ids()
        else:
            _pipeline_inputs = {}
            invalid = {}
            status = StepStatus.INPUTS_READY

        if pipeline_outputs:
            _pipeline_outputs = pipeline_outputs.get_all_value_ids()
        else:
            _pipeline_outputs = {}

        step_states = {}
        for step_id in self._structure.step_ids:
            d = self.get_step_details(step_id)
            step_states[step_id] = d

        details = PipelineState(
            kiara_id=self._data_registry.kiara_id,
            pipeline_id=self.pipeline_id,
            pipeline_status=status,
            pipeline_inputs=_pipeline_inputs,
            pipeline_outputs=_pipeline_outputs,
            invalid_details=invalid,
            step_states=step_states,
        )
        details._kiara = self._kiara

        return details

    def get_step_details(self, step_id: str) -> StepDetails:

        step_input_ids = self.get_current_step_inputs(step_id=step_id)
        step_output_ids = self.get_current_step_outputs(step_id=step_id)
        step_inputs = self._all_values.get_alias(f"steps.{step_id}.inputs")

        assert step_inputs is not None
        invalid = step_inputs.check_invalid()

        processing_stage = self._structure.get_processing_stage(step_id)

        if not invalid:
            status = StepStatus.INPUTS_READY
            step_outputs = self._all_values.get_alias(f"steps.{step_id}.outputs")
            assert step_outputs is not None
            invalid_outputs = step_outputs.check_invalid()
            # TODO: also check that all the pedigrees match up with current inputs
            if not invalid_outputs:
                status = StepStatus.RESULTS_READY
        else:
            status = StepStatus.INPUTS_INVALID

        details = StepDetails(
            kiara_id=self._data_registry.kiara_id,
            pipeline_id=self.pipeline_id,
            step=self._structure.get_step(step_id=step_id),
            step_id=step_id,
            status=status,
            inputs=step_input_ids,
            outputs=step_output_ids,
            invalid_details=invalid,
            processing_stage=processing_stage,
        )
        details._kiara = self._kiara
        return details

    def set_pipeline_input(
        self,
        pipeline_input_field: str,
        input_value: Any,
        sync_to_step_inputs: bool = True,
        notify_listeners: bool = True,
    ) -> Mapping[str, Mapping[str, Mapping[str, ChangedValue]]]:
        """Just a utility method, check 'set_pipeline_inputs` for more details."""

        return self.set_pipeline_inputs(
            {pipeline_input_field: input_value},
            sync_to_step_inputs=sync_to_step_inputs,
            notify_listeners=notify_listeners,
        )

    def set_pipeline_inputs(
        self,
        inputs: Mapping[str, Any],
        sync_to_step_inputs: bool = True,
        notify_listeners: bool = True,
    ) -> Mapping[str, Mapping[str, Mapping[str, ChangedValue]]]:
        """Set one or several pipeline inputs.

        The 'sync_to_step_inputs' parameter determines whether the inputs should be synced to the respective step inputs, which is what you usually want. Only in cases where you don't want to reset/clear any intermediate or end-result values you would set this to False.

        Arguments:
            inputs: the inputs to set
            sync_to_step_inputs: whether to sync the inputs to the respective step inputs
            notify_listeners: whether to notify listeners about the change, in most cases, this would be a PipelineController instance.

        """

        values_to_set: Dict[str, uuid.UUID] = {}

        for k, v in inputs.items():
            if v is SpecialValue.NOT_SET:
                values_to_set[k] = NOT_SET_VALUE_ID
            elif v in [None, SpecialValue.NO_VALUE]:
                values_to_set[k] = NONE_VALUE_ID
            else:
                alias_map = self._all_values.get_alias("pipeline.inputs")
                assert alias_map is not None
                # dbg(alias_map.__dict__)
                schema = alias_map.values_schema.get(k, None)
                if schema is None:
                    raise Exception(
                        f"Can't set pipeline input for input '{k}': no such input field. Available fields: {', '.join(alias_map.values_schema.keys())}"
                    )
                value = self._data_registry.register_data(
                    data=v, schema=schema, pedigree=ORPHAN, reuse_existing=True
                )
                values_to_set[k] = value.value_id

        if not values_to_set:
            return {}

        changed_pipeline_inputs = self._set_values("pipeline.inputs", **values_to_set)

        changed_results = {"__pipeline__": {"inputs": changed_pipeline_inputs}}

        if sync_to_step_inputs:
            changed = self.sync_pipeline_inputs(notify_listeners=False)
            dpath.merge(changed_results, changed)  # type: ignore

        if notify_listeners:
            event = PipelineEvent.create_event(pipeline=self, changed=changed_results)
            if event:
                self._notify_pipeline_listeners(event)

        return changed_results

    def sync_pipeline_inputs(
        self, notify_listeners: bool = True
    ) -> Mapping[str, Mapping[str, Mapping[str, ChangedValue]]]:
        """Sync all pipeline input."""

        pipeline_inputs = self.get_current_pipeline_inputs()

        values_to_sync: Dict[str, Dict[str, Union[uuid.UUID, None]]] = {}

        for field_name, ref in self._structure.pipeline_input_refs.items():
            for step_input in ref.connected_inputs:
                step_inputs = self.get_current_step_inputs(step_input.step_id)

                if step_inputs[step_input.value_name] != pipeline_inputs[field_name]:
                    values_to_sync.setdefault(step_input.step_id, {})[
                        step_input.value_name
                    ] = pipeline_inputs[field_name]

        results: Dict[str, Mapping[str, Mapping[str, ChangedValue]]] = {}
        for step_id in values_to_sync.keys():
            values = values_to_sync[step_id]
            step_changed = self._set_step_inputs(step_id=step_id, inputs=values)
            dpath.merge(results, step_changed)  # type: ignore

        if notify_listeners:
            event = PipelineEvent.create_event(pipeline=self, changed=results)
            if event:
                self._notify_pipeline_listeners(event)

        return results

    def _set_step_inputs(
        self, step_id: str, inputs: Mapping[str, Union[uuid.UUID, None]]
    ) -> Mapping[str, Mapping[str, Mapping[str, ChangedValue]]]:

        changed_step_inputs = self._set_values(f"steps.{step_id}.inputs", **inputs)
        if not changed_step_inputs:
            return {}

        result: Dict[str, Dict[str, Dict[str, ChangedValue]]] = {
            step_id: {"inputs": changed_step_inputs}
        }

        step_outputs = self._structure.get_step_output_refs(step_id=step_id)
        null_outputs = {k: NOT_SET_VALUE_ID for k in step_outputs.keys()}

        changed_outputs = self.set_step_outputs(
            step_id=step_id, outputs=null_outputs, notify_listeners=False
        )
        # assert step_id in changed_outputs.keys()

        result.update(changed_outputs)  # type: ignore

        return result

    def set_multiple_step_outputs(
        self,
        changed_outputs: Mapping[str, Mapping[str, Union[uuid.UUID, None]]],
        notify_listeners: bool = True,
    ) -> Mapping[str, Mapping[str, Mapping[str, ChangedValue]]]:

        results: Dict[str, Dict[str, Dict[str, ChangedValue]]] = {}
        for step_id, outputs in changed_outputs.items():
            step_results = self.set_step_outputs(
                step_id=step_id, outputs=outputs, notify_listeners=False
            )
            dpath.merge(results, step_results)  # type: ignore

        if notify_listeners:
            event = PipelineEvent.create_event(pipeline=self, changed=results)
            if event:
                self._notify_pipeline_listeners(event)

        return results

    def set_step_outputs(
        self,
        step_id: str,
        outputs: Mapping[str, Union[uuid.UUID, None]],
        notify_listeners: bool = True,
    ) -> Mapping[str, Mapping[str, Mapping[str, ChangedValue]]]:

        # make sure pedigrees match with respective inputs?

        changed_step_outputs = self._set_values(f"steps.{step_id}.outputs", **outputs)
        if not changed_step_outputs:
            return {}

        result: Dict[str, Dict[str, Dict[str, ChangedValue]]] = {
            step_id: {"outputs": changed_step_outputs}
        }

        output_refs = self._structure.get_step_output_refs(step_id=step_id)

        pipeline_outputs: Dict[str, Union[uuid.UUID, None]] = {}

        inputs_to_set: Dict[str, Dict[str, Union[uuid.UUID, None]]] = {}

        for field_name, ref in output_refs.items():
            if ref.pipeline_output:
                assert ref.pipeline_output not in pipeline_outputs.keys()
                pipeline_outputs[ref.pipeline_output] = outputs[field_name]
            for input_ref in ref.connected_inputs:
                inputs_to_set.setdefault(input_ref.step_id, {})[
                    input_ref.value_name
                ] = outputs[field_name]

        for step_id, step_inputs in inputs_to_set.items():
            changed_step_fields = self._set_step_inputs(
                step_id=step_id, inputs=step_inputs
            )
            dpath.merge(result, changed_step_fields)  # type: ignore

        if pipeline_outputs:
            changed_pipeline_outputs = self._set_pipeline_outputs(**pipeline_outputs)
            dpath.merge(  # type: ignore
                result, {"__pipeline__": {"outputs": changed_pipeline_outputs}}
            )

        if notify_listeners:
            event = PipelineEvent.create_event(pipeline=self, changed=result)
            if event:
                self._notify_pipeline_listeners(event)

        return result

    def _set_pipeline_outputs(
        self, **outputs: Union[uuid.UUID, None]
    ) -> Mapping[str, ChangedValue]:

        changed_pipeline_outputs = self._set_values("pipeline.outputs", **outputs)
        return changed_pipeline_outputs

    def _set_values(
        self, alias: str, **values: Union[uuid.UUID, None]
    ) -> Dict[str, ChangedValue]:
        """Set values (value-ids) for the sub-alias-map with the specified alias path."""
        invalid = {}
        for k in values.keys():
            _alias = self._all_values.get_alias(alias)
            assert _alias is not None
            if k not in _alias.values_schema.keys():
                invalid[k] = (
                    f"Invalid field '{k}'. Available fields: {', '.join(self.get_current_pipeline_inputs().keys())}"
                )

        if invalid:
            raise InvalidValuesException(invalid_values=invalid)

        alias_map: Union[AliasValueMap, None] = self._all_values.get_alias(alias)
        assert alias_map is not None

        values_to_set: Dict[str, Union[uuid.UUID, None]] = {}
        current: Dict[str, Union[uuid.UUID, None]] = {}
        changed: Dict[str, ChangedValue] = {}

        for field_name, new_value in values.items():

            current_value = self._all_values.get_alias(f"{alias}.{field_name}")
            if current_value is not None:
                current_value_id = current_value.assoc_value
            else:
                current_value_id = None
            current[field_name] = current_value_id

            if current_value_id != new_value:
                values_to_set[field_name] = new_value
                changed[field_name] = ChangedValue(old=current_value_id, new=new_value)

        _alias = self._all_values.get_alias(alias)
        assert _alias is not None
        _alias._set_aliases(**values_to_set)

        return changed

    @property
    def step_ids(self) -> Iterable[str]:
        """Return all ids of the steps of this pipeline."""
        return self._structure.step_ids

    @property
    def execution_graph(self) -> nx.DiGraph:
        return self._structure.execution_graph

    @property
    def data_flow_graph(self) -> nx.DiGraph:
        return self._structure.data_flow_graph

    @property
    def data_flow_graph_simple(self) -> nx.DiGraph:
        return self._structure.data_flow_graph_simple

    def get_step(self, step_id: str) -> PipelineStep:
        """Return the object representing a step in this workflow, identified by the step id."""
        return self._structure.get_step(step_id)

    def get_pipeline_inputs_schema_for_step(
        self, step_id: str
    ) -> Mapping[str, ValueSchema]:
        """Return the schema for the inputs of the specified step."""
        return self._structure.get_pipeline_inputs_schema_for_step(step_id=step_id)

    @property
    def pipeline_input_refs(self) -> Dict[str, PipelineInputRef]:
        return self._structure.pipeline_input_refs

    @property
    def pipeline_output_refs(self) -> Dict[str, PipelineOutputRef]:
        return self._structure.pipeline_output_refs

    def get_steps_by_stage(
        self,
    ) -> Mapping[int, Mapping[str, PipelineStep]]:
        """Return a all pipeline steps, ordered by stage they belong to."""
        if self._steps_by_stage is not None:
            return self._steps_by_stage

        result: Dict[int, Dict[str, PipelineStep]] = {}
        for step_id in self.step_ids:
            step = self.get_step(step_id)
            stage = self._structure.get_processing_stage(step.step_id)
            assert stage is not None
            result.setdefault(stage, {})[step_id] = step

        self._steps_by_stage = result
        return self._steps_by_stage

    def create_job_config_for_step(self, step_id: str) -> JobConfig:

        step_inputs: Mapping[str, uuid.UUID] = self.get_current_step_inputs(step_id)
        step_details: StepDetails = self.get_step_details(step_id=step_id)
        step: PipelineStep = self.get_step(step_id=step_id)

        # if the inputs are not valid, ignore this step
        if step_details.status == StepStatus.INPUTS_INVALID:
            invalid_details = step_details.invalid_details
            assert invalid_details is not None
            msg = f"Can't execute step '{step_id}', invalid inputs: {', '.join(invalid_details.keys())}"
            raise InvalidValuesException(msg=msg, invalid_values=invalid_details)

        job_config = JobConfig.create_from_module(
            data_registry=self._data_registry, module=step.module, inputs=step_inputs
        )

        return job_config

    def create_renderable(self, **config: Any) -> RenderableType:

        return PipelineInfo.create_from_pipeline(
            kiara=self._kiara, pipeline=self
        ).create_renderable(**config)


class PipelineInfo(ItemInfo):

    _kiara_model_id: ClassVar = "info.pipeline"

    @classmethod
    def base_instance_class(cls) -> Type[Pipeline]:
        return Pipeline

    @classmethod
    def create_from_instance(
        cls, kiara: "Kiara", instance: Any, **kwargs
    ) -> "PipelineInfo":

        return cls.create_from_pipeline(kiara=kiara, pipeline=instance)

    @classmethod
    def category_name(cls) -> str:
        return "pipeline"

    @classmethod
    def create_from_pipeline(cls, kiara: "Kiara", pipeline: Pipeline) -> "PipelineInfo":

        # doc = DocumentationMetadataModel.create(None)

        doc = pipeline.doc
        authors = AuthorsMetadataModel()
        context = ContextMetadataModel()

        # stages = PipelineStage.from_pipeline_structure(structure=pipeline.structure)

        pipeline_info = PipelineInfo(
            type_name=str(pipeline.pipeline_id),
            documentation=doc,
            authors=authors,
            context=context,
            # pipeline_structure=pipeline.structure,
            pipeline_config=pipeline.structure.pipeline_config,
            pipeline_state=pipeline.get_pipeline_details(),
            # stages=stages
        )
        pipeline_info._kiara = kiara
        return pipeline_info

    # pipeline_structure: PipelineStructure = Field(description="The pipeline structure.")
    pipeline_config: PipelineConfig = Field(
        description="The configuration of the pipeline."
    )
    pipeline_state: PipelineState = Field(description="The current input details.")
    # stages: Mapping[int, PipelineStage] = Field(description="Details about this pipelines stages/execution order.")
    _kiara: "Kiara" = PrivateAttr(default=None)
    _structure: "PipelineStructure" = PrivateAttr(default=None)

    @property
    def pipeline_structure(self):
        return self.pipeline_config.structure

    def create_pipeline_table(self, **config: Any) -> Table:

        table = Table(box=box.SIMPLE, show_header=False, padding=(0, 0, 0, 0))
        table.add_column("property", style="i")
        table.add_column("value")

        self._fill_table(table=table, config=config)
        return table

    def _fill_table(self, table: Table, config: Mapping[str, Any]):

        include_pipeline_inputs = config.get("include_pipeline_inputs", True)
        include_pipeline_outputs = config.get("include_pipeline_outputs", True)
        include_steps = config.get("include_steps", True)

        if include_pipeline_inputs:
            input_values = self._kiara.data_registry.create_valuemap(
                data=self.pipeline_state.pipeline_inputs,
                schema=self.pipeline_structure.pipeline_inputs_schema,
            )

            ordered_fields: Dict[str, List[str]] = {}
            for field_name, ref in self.pipeline_structure.pipeline_input_refs.items():
                for con_input in ref.connected_inputs:
                    details = self.pipeline_structure.get_step_details(
                        step_id=con_input.step_id
                    )
                    stage = details.processing_stage
                    ordered_fields.setdefault(stage, []).append(field_name)

            fields = []
            for stage in sorted(ordered_fields.keys()):

                for f in sorted(ordered_fields[stage]):
                    if f not in fields:
                        fields.append(f)

            inputs = create_value_map_status_renderable(
                input_values,
                render_config={
                    "show_description": False,
                    "show_type": False,
                    "show_default": True,
                    "show_value_ids": True,
                },
                fields=fields,
            )

            table.add_row("pipeline inputs", inputs)
        if include_steps:
            steps = create_pipeline_steps_tree(
                pipeline_structure=self.pipeline_structure,
                pipeline_details=self.pipeline_state,
            )
            table.add_row("steps", steps)

        if include_pipeline_outputs:
            output_values = self._kiara.data_registry.load_values(
                values=self.pipeline_state.pipeline_outputs
            )
            ordered_fields = {}
            for (
                field_name,
                o_ref,
            ) in self.pipeline_structure.pipeline_output_refs.items():
                con_step_id = o_ref.connected_output.step_id
                details = self.pipeline_structure.get_step_details(step_id=con_step_id)
                stage = details.processing_stage
                ordered_fields.setdefault(stage, []).append(field_name)

            fields = []
            for stage in sorted(ordered_fields.keys()):
                for f in sorted(ordered_fields[stage]):
                    fields.append(f)

            t_outputs = create_value_map_status_renderable(
                output_values,
                render_config={
                    "show_description": False,
                    "show_type": True,
                    "show_default": False,
                    "show_required": False,
                    "show_value_ids": True,
                },
                fields=fields,
            )

            table.add_row("pipeline outputs", t_outputs)

    def create_renderable(self, **config: Any) -> RenderableType:

        include_details = config.get("include_details", False)
        include_doc = config.get("include_doc", False)
        include_authors = config.get("include_authors", False)
        include_context = config.get("include_context", False)
        include_structure = config.get("include_structure", False)

        table = self.create_pipeline_table(**config)

        if include_details:
            t_details = create_table_from_model_object(
                self.pipeline_state, render_config=config
            )
            table.add_row("details", t_details)

        if include_doc:
            table.add_row(
                "Documentation",
                Panel(self.documentation.create_renderable(), box=box.SIMPLE),
            )
        if include_authors:
            table.add_row("Author(s)", self.authors.create_renderable(**config))
        if include_context:
            table.add_row("Context", self.context.create_renderable(**config))

        if include_structure:
            table.add_row(
                "Pipeline structure",
                self.pipeline_structure.create_renderable(**config),
            )

        return table


class PipelineGroupInfo(InfoItemGroup):

    _kiara_model_id: ClassVar = "info.pipelines"

    @classmethod
    def base_info_class(cls) -> Type[ItemInfo]:
        return PipelineInfo

    @classmethod
    def create_from_pipelines(
        cls, kiara: "Kiara", group_title: Union[str, None] = None, **items: Pipeline
    ) -> "PipelineGroupInfo":

        p_infos = {
            k: PipelineInfo.create_from_pipeline(kiara=kiara, pipeline=v)
            for k, v in items.items()
        }

        op_group_info = cls(group_title=group_title, item_infos=p_infos)
        return op_group_info

    # type_name: Literal["operation_type"] = "operation_type"
    item_infos: Mapping[str, PipelineInfo] = Field(
        description="The pipeline info instances for each type."
    )

    def create_renderable(self, **config: Any) -> RenderableType:

        return self._create_renderable_list(**config)

    def _create_renderable_list(self, **config) -> RenderableType:

        full_doc = config.get("full_doc", False)
        filter = config.get("filter", [])

        table = Table(box=box.SIMPLE, show_header=True)
        table.add_column("Id", no_wrap=True, style="i")
        table.add_column("Description")

        p_info: PipelineInfo
        for op_id, p_info in self.item_infos.items():

            desc_str = p_info.documentation.description
            if full_doc:
                desc = Markdown(p_info.documentation.full_doc)
            else:
                desc = Markdown(p_info.documentation.description)

            if filter:
                match = True
                for f in filter:
                    if (
                        f.lower() not in op_id.lower()
                        and f.lower() not in desc_str.lower()
                    ):
                        match = False
                        break
                if match:
                    table.add_row(op_id, desc)

            else:
                table.add_row(op_id, desc)

        return table


# kiara\kiara\src\kiara\models\module\pipeline\stages.py
# -*- coding: utf-8 -*-
from typing import TYPE_CHECKING, Any, ClassVar, Dict, List, Set, Union

import networkx as nx
from pydantic import Field, PrivateAttr
from rich import box
from rich.console import RenderableType
from rich.table import Table

from kiara.defaults import KIARA_DEFAULT_STAGES_EXTRACTION_TYPE
from kiara.exceptions import KiaraException
from kiara.models import KiaraModel

if TYPE_CHECKING:
    from kiara.models.module.pipeline import PipelineStructure


class PipelineStage(KiaraModel):

    _kiara_model_id: ClassVar = "info.pipeline_stage"

    @classmethod
    def extract_stages(
        cls,
        structure: "PipelineStructure",
        stages_extraction_type: str = KIARA_DEFAULT_STAGES_EXTRACTION_TYPE,
    ) -> List[List[str]]:

        func_name = f"extract_stages__{stages_extraction_type}"
        if not hasattr(cls, func_name):
            msg = f"Invalid stages extraction type: {stages_extraction_type}."
            available = [x for x in dir(cls) if x.startswith("extract_stages__")]
            details = "Available stages extraction types:\n"
            for avail in available:
                details += f" - {avail.replace('extract_stages__', '')}\n"
            raise KiaraException(msg, details=details)

        result: List[List[str]] = getattr(cls, func_name)(structure=structure)
        return result

    @classmethod
    def extract_stages__single_stage(
        cls, structure: "PipelineStructure"
    ) -> List[List[str]]:
        """Extract a single stage from the pipeline structure.."""
        return [[step.step_id for step in structure.steps]]

    @classmethod
    def extract_stages__stage_per_step(
        cls, structure: "PipelineStructure"
    ) -> List[List[str]]:
        """Extract a stage for each step in the pipeline structure."""
        flat_list = [
            [item] for sublist in structure.processing_stages for item in sublist
        ]
        return flat_list

    @classmethod
    def extract_stages__late(cls, structure: "PipelineStructure") -> List[List[str]]:
        """Extract stages in a way so that steps are processed as late as possible."""

        execution_graph = structure.execution_graph
        leaf_nodes = [
            node
            for node in execution_graph.nodes()
            if execution_graph.in_degree(node) != 0
            and execution_graph.out_degree(node) == 0
        ]

        layers = {}
        for leaf_node in leaf_nodes:
            node_layers = nx.bfs_layers(execution_graph.reverse(), leaf_node)
            layers[leaf_node] = list(node_layers)

        stages: Dict[int, List[str]] = {}
        for step in structure.steps:
            step_id = step.step_id
            max_idx = 0
            for node_layers in layers.values():
                for idx, node_layer in enumerate(node_layers):
                    if step_id in node_layer:
                        if idx > max_idx:
                            max_idx = idx
                        break
            stages.setdefault(max_idx, []).append(step_id)

        processing_stages = []
        for stage_idx in sorted(stages.keys(), reverse=True):
            stage = stages[stage_idx]
            processing_stages.append(stage)

        return processing_stages

    @classmethod
    def extract_stages__early(cls, structure: "PipelineStructure") -> List[List[str]]:
        """Extract stages in a way so that steps are processed as early as possible."""
        execution_graph = structure.execution_graph
        processing_stages = []
        path_lengths: Dict[str, int] = {}
        for step in structure.steps:

            step_id = step.step_id

            paths = list(nx.all_simple_paths(execution_graph, "__root__", step_id))
            max_steps = max(paths, key=lambda x: len(x))
            path_lengths[step_id] = len(max_steps) - 1

        if path_lengths.values():
            max_length = max(path_lengths.values())

            for i in range(1, max_length + 1):
                stage: List[str] = [
                    m for m, length in path_lengths.items() if length == i
                ]
                processing_stages.append(stage)
                # for _step_id in stage:
                #     steps_details[_step_id]["processing_stage"] = i

        return processing_stages

    @classmethod
    def stages_info_from_pipeline_structure(
        cls,
        structure: "PipelineStructure",
        stages: Union[List[List[str]], str] = KIARA_DEFAULT_STAGES_EXTRACTION_TYPE,
    ) -> List["PipelineStage"]:

        if isinstance(stages, str):
            if stages == "late":
                stages = cls.extract_stages__late(structure=structure)
            elif stages == "early":
                stages = cls.extract_stages__early(structure=structure)
            else:
                raise Exception(
                    "Invalid value for 'stages': {stages!r} (must be 'late' or 'early'."
                )

        used_pipeline_inputs: Set[str] = set()
        used_pipeline_outputs: Set[str] = set()
        result = []
        for idx, stage in enumerate(stages, start=1):
            stage_steps = []
            pipeline_inputs = []
            pipeline_outputs = []
            connected_outputs = []
            stage_outputs = []

            for step_id in stage:
                step = structure.get_step(step_id=step_id)
                stage_steps.append(step.step_id)

                for pipeline_output, out_ref in structure.pipeline_output_refs.items():
                    if pipeline_output in used_pipeline_outputs:
                        continue
                    if out_ref.connected_output.step_id == step_id:
                        pipeline_outputs.append(pipeline_output)

                for field_name, input_ref in structure.get_step_input_refs(
                    step_id=step_id
                ).items():
                    if input_ref.connected_pipeline_input:
                        pipeline_inputs.append(input_ref.connected_pipeline_input)
                    elif input_ref.connected_outputs:
                        for con_out in input_ref.connected_outputs:
                            connected_outputs.append(con_out.alias)

                for field_name, output_ref in structure.get_step_output_refs(
                    step_id=step_id
                ).items():
                    if output_ref.pipeline_output:
                        pipeline_outputs.append(output_ref.pipeline_output)
                    if output_ref.connected_inputs:
                        stage_outputs.append(output_ref.alias)

            stage_used_pipeline_inputs = list(used_pipeline_inputs)
            stage_used_pipeline_outputs = list(used_pipeline_outputs)

            result.append(
                PipelineStage(
                    stage_index=idx,
                    steps=stage_steps,
                    connected_outputs=connected_outputs,
                    stage_outputs=stage_outputs,
                    pipeline_inputs=pipeline_inputs,
                    pipeline_outputs=pipeline_outputs,
                    previous_pipeline_inputs=stage_used_pipeline_inputs,
                    previous_pipeline_outputs=stage_used_pipeline_outputs,
                )
            )

            used_pipeline_inputs.update(pipeline_inputs)
            used_pipeline_outputs.update(pipeline_outputs)

        return result

    stage_index: int = Field(description="The index of this stage.")
    steps: List[str] = Field(
        description="The pipeline steps that are executed in this stage."
    )
    connected_outputs: List[str] = Field(
        description="Previous step outputs that are connected to this stage."
    )
    stage_outputs: List[str] = Field(description="The outputs of this stage.")
    pipeline_inputs: List[str] = Field(
        description="The pipeline inputs required for this stage."
    )
    pipeline_outputs: List[str] = Field(
        description="The pipeline outputs that are ready once this stage is processed."
    )
    previous_pipeline_inputs: List[str] = Field(
        description="Pipeline inputs that are already set by this stage."
    )
    previous_pipeline_outputs: List[str] = Field(
        description="Pipeline outputs that are already computed by this stage."
    )

    _graph: Union[None, nx.DiGraph] = PrivateAttr(default=None)

    def get_graph_fragment(self) -> nx.DiGraph:
        if self._graph is not None:
            return self._graph

        fragment = nx.DiGraph()
        stage_id = f"Stage: {self.stage_index}"
        fragment.add_node(stage_id, type="stage", stage_index=self.stage_index)

        for pi in self.pipeline_inputs:
            node_id = f"Input: {pi}"
            fragment.add_node(node_id, type="pipeline_input")
            fragment.add_edge(node_id, stage_id, type="pipeline_input")
        for co in self.connected_outputs:
            fragment.add_node(co, type="connected_output")
            fragment.add_edge(co, stage_id, type="connected_output")
        for so in self.stage_outputs:
            fragment.add_node(so, type="stage_output")
            fragment.add_edge(stage_id, so, type="stage_output")
        for po in self.pipeline_outputs:
            node_id = f"Output: {po}"
            fragment.add_node(node_id, type="pipeline_output")
            fragment.add_edge(stage_id, node_id, type="pipeline_output")

        self._graph = fragment
        return self._graph


class PipelineStages(KiaraModel):
    @classmethod
    def create(
        cls,
        structure: "PipelineStructure",
        stages_extraction_type: str = KIARA_DEFAULT_STAGES_EXTRACTION_TYPE,
    ) -> "PipelineStages":

        stages_info = structure.extract_processing_stages_info(
            stages_extraction_type=stages_extraction_type
        )

        result = cls(stages=stages_info)
        result._structure = structure
        return result

    stages: List[PipelineStage] = Field(description="The pipeline stages.")
    _structure: Union[None, "PipelineStructure"] = PrivateAttr(default=None)

    def create_renderable(self, **config: Any) -> RenderableType:

        table = Table(show_header=True, box=box.SIMPLE)
        table.add_column("Stage")
        table.add_column("Details")

        for stage in self.stages:
            row = [f"Stage {stage.stage_index}", stage.create_renderable(**config)]
            table.add_row(*row)

        return table


# kiara\kiara\src\kiara\models\module\pipeline\structure.py
# -*- coding: utf-8 -*-
#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

from functools import lru_cache
from typing import Any, ClassVar, Dict, Iterable, List, Mapping, Set, Union

import networkx as nx
from pydantic import Field, PrivateAttr, model_validator
from rich.console import RenderableType
from rich.tree import Tree

from kiara.defaults import KIARA_DEFAULT_STAGES_EXTRACTION_TYPE
from kiara.exceptions import InvalidPipelineConfig
from kiara.models import KiaraModel
from kiara.models.documentation import DocumentationMetadataModel
from kiara.models.module.pipeline import PipelineConfig, PipelineStep
from kiara.models.module.pipeline.stages import PipelineStage
from kiara.models.module.pipeline.value_refs import (
    PipelineInputRef,
    PipelineOutputRef,
    StepInputRef,
    StepOutputRef,
    StepValueAddress,
    generate_step_alias,
)
from kiara.models.values.value_schema import ValueSchema


def generate_pipeline_endpoint_name(step_id: str, value_name: str):

    return f"{step_id}__{value_name}"


class StepInfo(KiaraModel):

    _kiara_model_id: ClassVar = "info.pipeline_step"

    step: PipelineStep = Field(description="The pipeline step object.", exclude=True)
    inputs: Dict[str, StepInputRef] = Field(
        description="Reference(s) to the fields that feed this steps inputs."
    )
    outputs: Dict[str, StepOutputRef] = Field(
        description="Reference(s) to the fields that are fed by this steps outputs."
    )
    required: bool = Field(
        description="Whether this step is always required or whether all his outputs feed into optional input fields."
    )
    doc: DocumentationMetadataModel = Field(
        description="The step documentation.",
        default_factory=DocumentationMetadataModel.create,
    )
    # processing_stage: int = Field(
    #     description="The index of the processing stage of this step."
    # )
    _processing_stage: Union[None, int] = PrivateAttr(default=None)
    _structure: Union[None, "PipelineStructure"] = PrivateAttr(default=None)

    @property
    def step_id(self) -> str:
        return self.step.step_id

    @property
    def processing_stage(self) -> int:

        if self._processing_stage is not None:
            return self._processing_stage

        if not self._structure:
            raise Exception(
                f"Can't look up processing stage for step '{self.step_id}': no structure assigned for those step details, set for this step info."
            )

        for idx, stage in enumerate(self._structure.processing_stages, start=1):
            if self.step_id in stage:
                self._processing_stage = idx
                return idx

        raise Exception(
            f"Can't look up processing stage for step '{self.step_id}': pipeline structure does not contain step."
        )

    def create_renderable(self, **config: Any) -> RenderableType:
        return self.step.create_renderable(**config)


class PipelineStructure(KiaraModel):
    """An object that holds one or several steps, and describes the connections between them."""

    _kiara_model_id: ClassVar = "instance.pipeline_structure"

    pipeline_config: PipelineConfig = Field(
        description="The underlying pipeline config."
    )
    steps: List[PipelineStep] = Field(description="The pipeline steps ")
    # stages: Mapping[int, PipelineStage] = Field(description="Details about each of the pipeline stages.")
    input_aliases: Dict[str, str] = Field(description="The input aliases.")
    output_aliases: Dict[str, str] = Field(description="The output aliases.")

    @model_validator(mode="before")
    @classmethod
    def validate_pipeline_config(cls, values: Dict[str, Any]) -> Dict[str, Any]:

        pipeline_config = values.get("pipeline_config", None)
        if not pipeline_config:
            raise ValueError("No 'pipeline_config' provided.")

        if len(values) != 1:
            raise ValueError(
                "Only 'pipeline_config' key allowed when creating a pipeline structure object."
            )

        if isinstance(pipeline_config, Mapping):
            pipeline_config = PipelineConfig(**pipeline_config)
        _config: PipelineConfig = pipeline_config
        _steps: List[PipelineStep] = list(_config.steps)

        _input_aliases: Dict[str, str] = dict(_config.input_aliases)
        _output_aliases: Dict[str, str] = dict(_config.output_aliases)

        invalid_input_aliases = [a for a in _input_aliases.values() if "." in a]
        if invalid_input_aliases:
            raise InvalidPipelineConfig(
                f"Invalid input aliases, aliases can't contain special characters: {', '.join(invalid_input_aliases)}.",
                config=values.get("pipeline_config", None),
                details=f"Invalid characters: {', '.join(invalid_input_aliases)}.",
            )
        invalid_output_aliases = [a for a in _input_aliases.values() if "." in a]
        if invalid_input_aliases:
            pc = values.get("pipeline_config", None)
            if pc is None:
                raise ValueError("No pipeline config provided.")
            else:
                raise InvalidPipelineConfig(
                    f"Invalid input aliases, aliases can't contain special characters: {', '.join(invalid_output_aliases)}.",
                    config=pc,
                    details=f"Invalid characters: {', '.join(invalid_output_aliases)}.",
                )

        valid_input_names = set()
        for step in _steps:
            for input_name in step.module.input_names:
                valid_input_names.add(f"{step.step_id}.{input_name}")
        invalid_input_aliases = [
            a for a in _input_aliases.keys() if a not in valid_input_names
        ]
        if invalid_input_aliases:
            msg = "Invalid input reference(s)."
            details = "Invalid reference(s):\n"
            for iia in sorted(invalid_input_aliases):
                details += f" - {iia}\n"
            details += "\nMust be one of: \n"
            for name in sorted(valid_input_names):
                details += f"  - {name}\n"

            raise InvalidPipelineConfig(
                msg, config=values.get("pipeline_config", {}), details=details
            )

        valid_output_names = set()
        for step in _steps:
            for output_name in step.module.output_names:
                valid_output_names.add(f"{step.step_id}.{output_name}")
        invalid_output_names = [
            a for a in _output_aliases.keys() if a not in valid_output_names
        ]
        if invalid_output_names:

            msg = "Invalid output reference(s)."
            details = "Invalid reference(s):\n"
            for iia in sorted(invalid_output_names):
                details += f" - {iia}\n"
            details += "\nMust be one of: \n"
            for name in sorted(valid_output_names):
                details += f"  - {name}\n"

            raise InvalidPipelineConfig(msg, values.get("pipeline_config", {}), details)

        # stages = PipelineStage.from_pipeline_structure(stages=)

        # values["steps"] = {step.step_id: step for step in _steps}
        values["steps"] = _steps
        values["input_aliases"] = _input_aliases
        values["output_aliases"] = _output_aliases
        return values

    # this is hardcoded for now
    _add_all_workflow_outputs: bool = PrivateAttr(default=False)
    _constants: Dict[str, Any] = PrivateAttr(default=None)  # type: ignore
    _defaults: Dict[str, Any] = PrivateAttr(None)  # type: ignore

    _execution_graph: nx.DiGraph = PrivateAttr(None)  # type: ignore
    _data_flow_graph: nx.DiGraph = PrivateAttr(None)  # type: ignore
    _data_flow_graph_simple: nx.DiGraph = PrivateAttr(None)  # type: ignore

    _processing_stages: List[List[str]] = PrivateAttr(None)  # type: ignore
    # _stages_info: Mapping[int, PipelineStage] = PrivateAttr(None)  # type: ignore

    # holds details about the (current) processing steps contained in this workflow
    _steps_details: Dict[str, StepInfo] = PrivateAttr(None)  # type: ignore
    # _info: "PipelineStructureInfo" = PrivateAttr(None)  # type: ignore

    def _retrieve_data_to_hash(self) -> Any:

        return {
            "steps": [step.instance_cid for step in self.steps],
            "input_aliases": self.input_aliases,
            "output_aliases": self.output_aliases,
        }

    def _retrieve_id(self) -> str:
        return self.pipeline_config.instance_id

    @property
    def steps_details(self) -> Mapping[str, StepInfo]:

        if self._steps_details is None:
            self._process_steps()
        return self._steps_details  # type: ignore

    @property
    def step_ids(self) -> Iterable[str]:
        if self._steps_details is None:
            self._process_steps()
        return self._steps_details.keys()  # type: ignore

    @property
    def constants(self) -> Mapping[str, Any]:

        if self._constants is None:
            self._process_steps()
        return self._constants  # type: ignore

    @property
    def defaults(self) -> Mapping[str, Any]:

        if self._defaults is None:
            self._process_steps()
        return self._defaults  # type: ignore

    def get_step(self, step_id: str) -> PipelineStep:

        d = self.steps_details.get(step_id, None)
        if d is None:
            raise Exception(f"No step with id: {step_id}")

        return d.step

    def get_step_input_refs(self, step_id: str) -> Mapping[str, StepInputRef]:

        d = self.steps_details.get(step_id, None)
        if d is None:
            raise Exception(f"No step with id: {step_id}")

        return d.inputs

    def get_step_output_refs(self, step_id: str) -> Mapping[str, StepOutputRef]:

        d = self.steps_details.get(step_id, None)
        if d is None:
            raise Exception(f"No step with id: {step_id}")

        return d.outputs

    def get_step_details(self, step_id: str) -> StepInfo:

        d = self.steps_details.get(step_id, None)
        if d is None:
            raise Exception(f"No step with id: {step_id}")

        return d

    @property
    def execution_graph(self) -> nx.DiGraph:
        if self._execution_graph is None:
            self._process_steps()
        return self._execution_graph

    @property
    def data_flow_graph(self) -> nx.DiGraph:
        if self._data_flow_graph is None:
            self._process_steps()
        return self._data_flow_graph

    @property
    def data_flow_graph_simple(self) -> nx.DiGraph:
        if self._data_flow_graph_simple is None:
            self._process_steps()
        return self._data_flow_graph_simple

    @property
    def processing_stages(self) -> List[List[str]]:
        if self._processing_stages is not None:
            return self._processing_stages

        # calculate execution order
        # process_late = self.pipeline_config.pipeline_name == "topic_modeling"
        processing_stages = []

        processing_stages = PipelineStage.extract_stages(
            self, stages_extraction_type=KIARA_DEFAULT_STAGES_EXTRACTION_TYPE
        )

        self._processing_stages = processing_stages
        return self._processing_stages

    def extract_processing_stages(
        self, stages_extraction_type: str = KIARA_DEFAULT_STAGES_EXTRACTION_TYPE
    ) -> List[List[str]]:
        """
        Extract a list of lists of steps, representing the order of groups in which they will be executed.

        It is possible to extract the stages in different ways, depending on the use-case you have in mind. For most cases,
        'late' will be appropriate. Currently available:
        - 'late': process steps as late in the process as possible
        - 'early': process steps as early in the process as possible
        """
        return PipelineStage.extract_stages(
            self, stages_extraction_type=stages_extraction_type
        )

    def extract_processing_stages_info(
        self, stages_extraction_type: str = KIARA_DEFAULT_STAGES_EXTRACTION_TYPE
    ) -> List[PipelineStage]:

        stages = self.extract_processing_stages(
            stages_extraction_type=stages_extraction_type
        )
        return PipelineStage.stages_info_from_pipeline_structure(self, stages)

    def get_stages_graph(
        self,
        stages_extraction_type: str = KIARA_DEFAULT_STAGES_EXTRACTION_TYPE,
        flatten: bool = True,
    ) -> nx.DiGraph:
        """
        Creates a networx graph that represents the processing stages of the pipeline and how they are connecte.

        Arguments:
        ---------
            stages_extraction_type: how to extract the stages
            flatten: if True, the nodes representing connections between stages will be removed, leaving only the edge

        Returns:
        -------
            a networkx graph object
        """
        stages = self.extract_processing_stages_info(
            stages_extraction_type=stages_extraction_type
        )

        graph = nx.DiGraph()
        for stage in stages:
            fragment = stage.get_graph_fragment()
            graph = nx.compose(graph, fragment)

        if flatten:
            to_flatten = []
            for node_id in graph.nodes:
                if graph.nodes[node_id]["type"] in ["stage_output", "connected_output"]:
                    to_flatten.append(node_id)

            for f in to_flatten:

                in_edges = tuple(graph.in_edges(f))[0]  # noqa
                out_edges = tuple(graph.out_edges(f))[0]  # noqa
                assert in_edges[1] == out_edges[0]
                graph.remove_edge(in_edges[0], in_edges[1])
                graph.remove_edge(out_edges[0], out_edges[1])
                graph.remove_node(f)
                graph.add_edge(
                    in_edges[0],
                    out_edges[1],
                    type="stage_connection",
                    output_name=in_edges[1],
                )

        return graph

    @lru_cache()
    def _get_node_of_type(self, node_type: str):
        if self._steps_details is None:
            self._process_steps()

        return [
            node
            for node, attr in self._data_flow_graph.nodes(data=True)
            if attr["type"] == node_type
        ]

    @property
    def steps_input_refs(self) -> Dict[str, StepInputRef]:
        return {
            node.alias: node
            for node in self._get_node_of_type(node_type=StepInputRef.__name__)
        }

    @property
    def steps_output_refs(self) -> Dict[str, StepOutputRef]:
        return {
            node.alias: node
            for node in self._get_node_of_type(node_type=StepOutputRef.__name__)
        }

    @property
    def pipeline_input_refs(self) -> Dict[str, PipelineInputRef]:
        return {
            node.value_name: node
            for node in self._get_node_of_type(node_type=PipelineInputRef.__name__)
        }

    @property
    def pipeline_output_refs(self) -> Dict[str, PipelineOutputRef]:
        return {
            node.value_name: node
            for node in self._get_node_of_type(node_type=PipelineOutputRef.__name__)
        }

    @property
    def pipeline_inputs_schema(self) -> Mapping[str, ValueSchema]:

        schemas = {
            input_name: w_in.value_schema
            for input_name, w_in in self.pipeline_input_refs.items()
        }
        return schemas

    @property
    def pipeline_outputs_schema(self) -> Mapping[str, ValueSchema]:
        return {
            output_name: w_out.value_schema
            for output_name, w_out in self.pipeline_output_refs.items()
        }

    def get_pipeline_inputs_schema_for_step(
        self, step_id: str
    ) -> Mapping[str, ValueSchema]:

        result = {}
        for field, ref in self.pipeline_input_refs.items():
            for con in ref.connected_inputs:
                if con.step_id == step_id:
                    result[field] = ref.value_schema
                    break
        return result

    def get_pipeline_outputs_schema_for_step(
        self, step_id: str
    ) -> Mapping[str, ValueSchema]:

        result = {}
        for field, ref in self.pipeline_output_refs.items():
            if ref.connected_output.step_id == step_id:
                result[field] = ref.value_schema

        return result

    def get_processing_stage(self, step_id: str) -> int:
        """
        Return the processing stage for the specified step_id.

        Returns the stage nr (starting with '1').
        """
        for index, stage in enumerate(self.processing_stages, start=1):
            if step_id in stage:
                return index

        raise Exception(f"Invalid step id '{step_id}'.")

    def step_is_required(self, step_id: str) -> bool:
        """Check if the specified step is required, or can be omitted."""
        return self.get_step_details(step_id=step_id).required

    def _process_steps(self) -> None:
        """The core method of this class, it connects all the processing modules, their inputs and outputs."""
        steps_details: Dict[str, Any] = {}
        execution_graph = nx.DiGraph()
        execution_graph.add_node("__root__")
        data_flow_graph = nx.DiGraph()
        data_flow_graph_simple = nx.DiGraph()
        constants = {}
        structure_defaults = {}

        # temp variable, to hold all outputs
        outputs: Dict[str, StepOutputRef] = {}

        # process all pipeline and step outputs first
        _temp_steps_map: Dict[str, PipelineStep] = {}
        pipeline_outputs: Dict[str, PipelineOutputRef] = {}
        for step in self.steps:

            _temp_steps_map[step.step_id] = step

            if step.step_id in steps_details.keys():
                raise Exception(
                    f"Can't process steps: duplicate step_id '{step.step_id}'"
                )

            steps_details[step.step_id] = {
                "step": step,
                "outputs": {},
                "inputs": {},
                "required": True,
            }

            data_flow_graph.add_node(step, type="step")
            data_flow_graph_simple.add_node(step, type="step")

            # go through all the module outputs, create points for them and connect them to pipeline outputs
            for output_name, schema in step.module.outputs_schema.items():

                step_output = StepOutputRef(
                    value_name=output_name,
                    value_schema=schema,
                    step_id=step.step_id,
                    pipeline_output=None,
                )

                steps_details[step.step_id]["outputs"][output_name] = step_output
                step_alias = generate_step_alias(step.step_id, output_name)
                outputs[step_alias] = step_output

                # step_output_name = generate_pipeline_endpoint_name(
                #     step_id=step.step_id, value_name=output_name
                # )
                step_output_name: Union[None, str] = f"{step.step_id}.{output_name}"
                if not self.output_aliases:
                    raise NotImplementedError()
                if step_output_name in self.output_aliases.keys():
                    step_output_name = self.output_aliases[step_output_name]  # type: ignore
                else:
                    if not self._add_all_workflow_outputs:
                        # this output is not interesting for the workflow
                        step_output_name = None

                if step_output_name:
                    step_output_address = StepValueAddress(
                        step_id=step.step_id, value_name=output_name
                    )
                    pipeline_output = PipelineOutputRef(
                        value_name=step_output_name,
                        connected_output=step_output_address,
                        value_schema=schema,
                    )
                    pipeline_outputs[step_output_name] = pipeline_output
                    step_output.pipeline_output = pipeline_output.value_name

                    data_flow_graph.add_node(
                        pipeline_output, type=PipelineOutputRef.__name__
                    )
                    data_flow_graph.add_edge(step_output, pipeline_output)

                    data_flow_graph_simple.add_node(
                        pipeline_output, type=PipelineOutputRef.__name__
                    )
                    data_flow_graph_simple.add_edge(step, pipeline_output)

                data_flow_graph.add_node(step_output, type=StepOutputRef.__name__)
                data_flow_graph.add_edge(step, step_output)

        # now process inputs, and connect them to the appropriate output/pipeline-input points
        existing_pipeline_input_points: Dict[str, PipelineInputRef] = {}
        for step in self.steps:

            other_step_dependency: Set = set()
            # go through all the inputs of a module, create input points and connect them to either
            # other module outputs, or pipeline inputs (which need to be created)

            module_constants: Mapping[str, Any] = step.module.get_config_value(
                "constants"
            )

            for input_name, schema in step.module.inputs_schema.items():

                matching_input_links: List[StepValueAddress] = []
                is_constant = input_name in module_constants.keys()

                for value_name, input_links in step.input_links.items():
                    if value_name == input_name:
                        for input_link in input_links:
                            if input_link in matching_input_links:
                                raise Exception(f"Duplicate input link: {input_link}")
                            matching_input_links.append(input_link)

                if matching_input_links:
                    # this means we connect to other steps output

                    connected_output_points: List[StepOutputRef] = []
                    connected_outputs: List[StepValueAddress] = []

                    for input_link in matching_input_links:
                        output_id = generate_step_alias(
                            input_link.step_id, input_link.value_name
                        )

                        if output_id not in outputs.keys():
                            raise Exception(
                                f"Can't connect input '{input_name}' for step '{step.step_id}': no output '{output_id}' available. Available output names: {', '.join(outputs.keys())}"
                            )
                        connected_output_points.append(outputs[output_id])
                        connected_outputs.append(input_link)

                        other_step_dependency.add(input_link.step_id)

                    step_input_point = StepInputRef(
                        step_id=step.step_id,
                        value_name=input_name,
                        value_schema=schema,
                        is_constant=is_constant,
                        connected_pipeline_input=None,
                        connected_outputs=connected_outputs,
                    )

                    for op in connected_output_points:
                        op.connected_inputs.append(step_input_point.address)
                        data_flow_graph.add_edge(op, step_input_point)
                        data_flow_graph_simple.add_edge(
                            _temp_steps_map[op.step_id], step_input_point
                        )  # TODO: name edge
                        data_flow_graph_simple.add_edge(
                            step_input_point, step
                        )  # TODO: name edge

                else:
                    # this means we connect to pipeline input
                    # pipeline_input_name = generate_pipeline_endpoint_name(
                    #     step_id=step.step_id, value_name=input_name
                    # )
                    pipeline_input_ref = f"{step.step_id}.{input_name}"

                    # check whether this input has an alias associated with it
                    if not self.input_aliases:
                        raise NotImplementedError()

                    if pipeline_input_ref in self.input_aliases.keys():
                        # this means we use the pipeline alias
                        pipeline_input_name = self.input_aliases[pipeline_input_ref]
                    else:
                        pipeline_input_name = generate_pipeline_endpoint_name(
                            step_id=step.step_id, value_name=input_name
                        )

                    if pipeline_input_name in existing_pipeline_input_points.keys():
                        # we already created a pipeline input with this name
                        # TODO: check whether schema fits
                        connected_pipeline_input = existing_pipeline_input_points[
                            pipeline_input_name
                        ]
                        assert connected_pipeline_input.is_constant == is_constant
                    else:
                        # we need to create the pipeline input
                        connected_pipeline_input = PipelineInputRef(
                            value_name=pipeline_input_name,
                            value_schema=schema,
                            is_constant=is_constant,
                        )

                        existing_pipeline_input_points[pipeline_input_name] = (
                            connected_pipeline_input
                        )

                        data_flow_graph.add_node(
                            connected_pipeline_input, type=PipelineInputRef.__name__
                        )
                        data_flow_graph_simple.add_node(
                            connected_pipeline_input, type=PipelineInputRef.__name__
                        )
                        if is_constant:
                            constants[pipeline_input_name] = (
                                step.module.get_config_value("constants")[input_name]
                            )

                        default_val = step.module.get_config_value("defaults").get(
                            input_name, None
                        )
                        if is_constant and default_val is not None:
                            raise Exception(
                                f"Module config invalid for step '{step.step_id}': both default value and constant provided for input '{input_name}'."
                            )
                        elif default_val is not None:
                            structure_defaults[pipeline_input_name] = default_val

                    step_input_point = StepInputRef(
                        step_id=step.step_id,
                        value_name=input_name,
                        value_schema=schema,
                        connected_pipeline_input=connected_pipeline_input.value_name,
                        connected_outputs=None,
                        is_constant=is_constant,
                    )
                    connected_pipeline_input.connected_inputs.append(
                        step_input_point.address
                    )
                    data_flow_graph.add_edge(connected_pipeline_input, step_input_point)
                    data_flow_graph_simple.add_edge(connected_pipeline_input, step)

                data_flow_graph.add_node(step_input_point, type=StepInputRef.__name__)

                steps_details[step.step_id]["inputs"][input_name] = step_input_point

                if step.doc.is_set:
                    steps_details[step.step_id]["doc"] = step.doc
                else:
                    steps_details[step.step_id]["doc"] = step.module.doc
                data_flow_graph.add_edge(step_input_point, step)

            if other_step_dependency:
                for module_id in other_step_dependency:
                    execution_graph.add_edge(module_id, step.step_id)
            else:
                execution_graph.add_edge("__root__", step.step_id)

        self._constants = constants
        self._defaults = structure_defaults
        self._steps_details = {}
        for step_id, data in steps_details.items():
            _step = StepInfo(**data)
            _step._structure = self
            self._steps_details[step_id] = _step

        self._execution_graph = execution_graph
        self._data_flow_graph = data_flow_graph
        self._data_flow_graph_simple = data_flow_graph_simple
        self._processing_stages = None  # type: ignore

        self._get_node_of_type.cache_clear()

    def export_stages(self):

        # TODO: implement different processing stages possibilities
        processing_stages = self.processing_stages

        for stage in processing_stages:

            input_links = []

            for step_id in stage:
                step = self.get_step(step_id=step_id)
                for input_link in step.input_links:
                    input_links.append(input_link)

    def create_renderable(self, **config: Any) -> RenderableType:

        show_pipeline_inputs_for_steps = config.get(
            "show_pipeline_inputs_for_steps", False
        )
        stages_extraction_type = config.get(
            "stages_extraction_type", KIARA_DEFAULT_STAGES_EXTRACTION_TYPE
        )

        tree = Tree("pipeline")
        inputs = tree.add("inputs")
        for field_name, schema in self.pipeline_inputs_schema.items():
            inputs.add(f"[i]{field_name}[i] (type: {schema.type})")

        steps = tree.add("steps")
        processing_stages = PipelineStage.extract_stages(
            structure=self, stages_extraction_type=stages_extraction_type
        )
        for idx, stage in enumerate(processing_stages, start=1):
            stage_node = steps.add(f"stage {idx}")
            for step_id in stage:
                step_node = stage_node.add(f"step: {step_id}")
                step = self.get_step(step_id=step_id)
                if show_pipeline_inputs_for_steps:
                    pipeline_inputs = self.get_pipeline_inputs_schema_for_step(
                        step_id=step_id
                    )
                    if pipeline_inputs:
                        inps = step_node.add("pipeline inputs")
                        for pi in pipeline_inputs:
                            inps.add(f"[i]{pi}[i]")
                if step.doc.is_set:
                    step_node.add(f"desc: {step.doc.description}")
                step_node.add(f"operation: {step.manifest_src.module_type}")

        outputs = tree.add("outputs")
        for field_name, schema in self.pipeline_outputs_schema.items():
            outputs.add(f"[i]{field_name}[i] (type: {schema.type})")

        return tree


# kiara\kiara\src\kiara\models\module\pipeline\value_refs.py
# -*- coding: utf-8 -*-
import typing

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)
import uuid
from typing import Any, Dict, List, Union

from pydantic import BaseModel, ConfigDict, Field, PrivateAttr, model_validator

from kiara.defaults import PIPELINE_PARENT_MARKER
from kiara.models.values.value_schema import ValueSchema
from kiara.utils import camel_case_to_snake_case

if typing.TYPE_CHECKING:
    pass


def generate_step_alias(step_id: str, value_name) -> str:
    return f"{step_id}.{value_name}"


class StepValueAddress(BaseModel):
    """Small model to describe the address of a value of a step, within a Pipeline/PipelineStructure."""

    model_config = ConfigDict(extra="forbid")

    step_id: str = Field(description="The id of a step within a pipeline.")
    value_name: str = Field(
        description="The name of the value (output name or pipeline input name)."
    )
    sub_value: Union[Dict[str, Any], None] = Field(
        default=None,
        description="A reference to a subitem of a value (e.g. column, list item)",
    )

    @property
    def alias(self):
        """An alias string for this address (in the form ``[step_id].[value_name]``)."""
        return generate_step_alias(self.step_id, self.value_name)

    def __eq__(self, other):

        if not isinstance(other, StepValueAddress):
            return False

        return (self.step_id, self.value_name, self.sub_value) == (
            other.step_id,
            other.value_name,
            other.sub_value,
        )

    def __hash__(self):

        return hash((self.step_id, self.value_name, self.sub_value))

    def __repr__(self):

        if self.sub_value:
            sub_value = f" sub_value={self.sub_value}"
        else:
            sub_value = ""
        return f"{self.__class__.__name__}(step_id={self.step_id}, value_name={self.value_name}{sub_value})"

    def __str__(self):
        return self.__repr__()


class ValueRef(BaseModel):
    """
    An object that holds information about the location of a value within a pipeline (or other structure).

    Basically, a `ValueRef` helps the containing object where in its structure the value belongs (for example so
    it can update dependent other values). A `ValueRef` object (obviously) does not contain the value itself.

    There are four different ValueRef type that are relevant for pipelines:

    - [kiara.pipeline.values.StepInputRef][]: an input to a step
    - [kiara.pipeline.values.StepOutputRef][]: an output of a step
    - [kiara.pipeline.values.PipelineInputRef][]: an input to a pipeline
    - [kiara.pipeline.values.PipelineOutputRef][]: an output for a pipeline

    Several `ValueRef` objects can target the same value, for example a step output and a connected step input would
    reference the same `Value` (in most cases)..
    """

    model_config = ConfigDict(frozen=False, extra="forbid")

    _id: uuid.UUID = PrivateAttr(default_factory=uuid.uuid4)
    value_name: str
    value_schema: ValueSchema

    def __eq__(self, other):

        if not isinstance(other, self.__class__):
            return False

        return self._id == other._id

    def __hash__(self):
        return hash(self._id)

    def __repr__(self):
        step_id = ""
        if hasattr(self, "step_id"):
            step_id = f" step_id='{self.step_id}'"
        return f"{self.__class__.__name__}(value_name='{self.value_name}' {step_id})"

    def __str__(self):
        name = camel_case_to_snake_case(self.__class__.__name__[0:-5], repl=" ")
        return f"{name}: {self.value_name} ({self.value_schema.type})"


class StepInputRef(ValueRef):
    """
    An input to a step.

    This object can either have a 'connected_outputs' set, or a 'connected_pipeline_input', not both.
    """

    step_id: str = Field(description="The step id.")
    connected_outputs: Union[List[StepValueAddress], None] = Field(
        default=None,
        description="A potential connected list of one or several module outputs.",
    )
    connected_pipeline_input: Union[str, None] = Field(
        default=None, description="A potential pipeline input."
    )
    is_constant: bool = Field(
        description="Whether this input is a constant and can't be changed by the user."
    )

    @model_validator(mode="before")
    @classmethod
    def ensure_single_connected_item(cls, values):

        if values.get("connected_outputs", None) and values.get(
            "connected_pipeline_input"
        ):
            raise ValueError("Multiple connected items, only one allowed.")

        return values

    @property
    def alias(self) -> str:
        return generate_step_alias(self.step_id, self.value_name)

    @property
    def address(self) -> StepValueAddress:
        return StepValueAddress(step_id=self.step_id, value_name=self.value_name)

    def __str__(self):
        name = camel_case_to_snake_case(self.__class__.__name__[0:-5], repl=" ")
        return f"{name}: {self.step_id}.{self.value_name} ({self.value_schema.type})"


class StepOutputRef(ValueRef):
    """An output to a step."""

    model_config = ConfigDict(frozen=False)

    step_id: str = Field(description="The step id.")
    pipeline_output: Union[str, None] = Field(
        description="The connected pipeline output."
    )
    connected_inputs: List[StepValueAddress] = Field(
        description="The step inputs that are connected to this step output",
        default_factory=list,
    )

    @property
    def alias(self) -> str:
        return generate_step_alias(self.step_id, self.value_name)

    @property
    def address(self) -> StepValueAddress:
        return StepValueAddress(step_id=self.step_id, value_name=self.value_name)

    def __str__(self):
        name = camel_case_to_snake_case(self.__class__.__name__[0:-5], repl=" ")
        return f"{name}: {self.step_id}.{self.value_name} ({self.value_schema.type})"


class PipelineInputRef(ValueRef):
    """An input to a pipeline."""

    connected_inputs: List[StepValueAddress] = Field(
        description="The step inputs that are connected to this pipeline input",
        default_factory=list,
    )
    is_constant: bool = Field(
        description="Whether this input is a constant and can't be changed by the user."
    )

    @property
    def alias(self) -> str:
        return generate_step_alias(PIPELINE_PARENT_MARKER, self.value_name)


class PipelineOutputRef(ValueRef):
    """An output to a pipeline."""

    connected_output: StepValueAddress = Field(description="Connected step outputs.")

    @property
    def alias(self) -> str:
        return generate_step_alias(PIPELINE_PARENT_MARKER, self.value_name)


StepInputRef.model_rebuild()
StepOutputRef.model_rebuild()


# kiara\kiara\src\kiara\models\module\pipeline\__init__.py
# -*- coding: utf-8 -*-
import os
import uuid
from enum import Enum
from typing import TYPE_CHECKING, Any, ClassVar, Dict, Iterable, List, Mapping, Union

import orjson
from boltons.strutils import slugify
from pydantic import ConfigDict, Field, PrivateAttr, field_validator, model_validator
from rich import box
from rich.console import RenderableType
from rich.markdown import Markdown
from rich.panel import Panel
from rich.syntax import Syntax
from rich.table import Table

from kiara.exceptions import InvalidPipelineStepConfig
from kiara.models.documentation import DocumentationMetadataModel
from kiara.models.module import KiaraModuleConfig
from kiara.models.module.jobs import ExecutionContext
from kiara.models.module.manifest import Manifest
from kiara.models.module.pipeline.value_refs import (
    PipelineInputRef,
    PipelineOutputRef,
    StepValueAddress,
)
from kiara.models.python_class import KiaraModuleInstance
from kiara.utils import find_free_id, is_jupyter
from kiara.utils.data import get_data_from_string
from kiara.utils.files import get_data_from_file
from kiara.utils.json import orjson_dumps
from kiara.utils.modules import module_config_is_empty
from kiara.utils.output import create_table_from_field_schemas
from kiara.utils.pipelines import (
    ensure_step_value_addresses,
    extract_data_to_hash_from_pipeline_config,
)
from kiara.utils.string_vars import replace_var_names_in_obj

#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)


if TYPE_CHECKING:
    from kiara.context import Kiara
    from kiara.models.module.pipeline.pipeline import Pipeline
    from kiara.models.module.pipeline.structure import PipelineStructure
    from kiara.modules import KiaraModule


class StepStatus(Enum):
    """Enum to describe the state of a workflow."""

    INPUTS_INVALID = "inputs_invalid"
    INPUTS_READY = "inputs_ready"
    RESULTS_READY = "results_ready"

    def to_console_renderable(self) -> RenderableType:
        if self == StepStatus.INPUTS_INVALID:
            return "[red]inputs invalid[/red]"
        elif self == StepStatus.INPUTS_READY:
            return "[yellow]inputs ready[/yellow]"
        elif self == StepStatus.RESULTS_READY:
            return "[green]results ready[/green]"


class PipelineStep(Manifest):
    """A step within a pipeline-structure, includes information about it's connection(s) and other metadata."""

    _kiara_model_id: ClassVar = "instance.pipeline_step"
    model_config = ConfigDict(validate_assignment=True, extra="forbid")

    @classmethod
    def create_step(
        cls,
        step: Union["PipelineStep", Mapping[str, Any]],
        kiara: "Kiara",
        module_map: Union[Mapping[str, Any], None] = None,
        auto_step_id: bool = False,
        taken_step_ids: Union[List[str], None] = None,
    ):
        """
        Create a step object from step data.

        Beware: the provided 'module_map' dictionary will be modified, as will the 'taken_step_ids'.
        """
        if module_map is None:
            module_map = {}

        if taken_step_ids is None:
            taken_step_ids = []

        if not isinstance(step, PipelineStep):

            module_type = step.get("module_type", None)

            if not module_type:
                raise InvalidPipelineStepConfig(
                    "Can't create step, no 'module_type' specified.", step_config=step
                )

            module_config = step.get("module_config", {})

            src_manifest = Manifest(
                module_type=module_type, module_config=module_config
            )

            if module_type not in kiara.module_type_names:

                if module_type in module_map.keys():

                    resolved_module_type = module_map[module_type]["module_type"]
                    resolved_module_config = module_map[module_type]["module_config"]

                    if module_config:
                        merged_module_config = dict(resolved_module_config)
                        merged_module_config.setdefault("defaults", {})
                        merged_module_config.setdefault("constants", {})
                        defaults = module_config.get("defaults", {})
                        constants = module_config.get("constants", {})
                        merged_module_config["defaults"].update(defaults)
                        merged_module_config["constants"].update(constants)
                    else:
                        merged_module_config = resolved_module_config

                    manifest = kiara.create_manifest(
                        module_or_operation=resolved_module_type,
                        config=merged_module_config,
                    )

                elif (
                    kiara.operation_registry.is_initialized
                    and module_type in kiara.operation_registry.operation_ids
                ):

                    op = kiara.operation_registry.operations[module_type]
                    resolved_module_type = op.module_type
                    resolved_module_config = op.module_config

                    if module_config:
                        merged_module_config = dict(resolved_module_config)
                        merged_module_config.setdefault("defaults", {})
                        merged_module_config.setdefault("constants", {})
                        defaults = module_config.get("defaults", {})
                        constants = module_config.get("constants", {})
                        merged_module_config["defaults"].update(defaults)
                        merged_module_config["constants"].update(constants)
                    else:
                        merged_module_config = resolved_module_config

                    manifest = kiara.create_manifest(
                        module_or_operation=resolved_module_type,
                        config=merged_module_config,
                    )
                else:
                    raise InvalidPipelineStepConfig(
                        f"Can't resolve module type: {module_type}", step_config=step
                    )
            else:
                manifest = kiara.create_manifest(
                    module_or_operation=module_type, config=module_config
                )
                resolved_module_type = module_type
                resolved_module_config = module_config

            module = kiara.module_registry.create_module(manifest=manifest)

            step_id = step.get("step_id", None)
            if not step_id:
                if not auto_step_id:
                    raise InvalidPipelineStepConfig(
                        "Can't create step, no 'step_id' specified in config.",
                        step_config=step,
                    )

                else:
                    step_id = find_free_id(
                        slugify(manifest.module_type, delim="_"),
                        current_ids=taken_step_ids,
                    )

            if step_id in taken_step_ids:
                raise ValueError(f"Can't create step: duplicate step id '{step_id}'.")

            taken_step_ids.append(step_id)

            input_links = {}
            for input_field, sources in step.get("input_links", {}).items():
                if isinstance(sources, str):
                    sources = [sources]
                input_links[input_field] = sources

            doc = step.get("doc", None)
            if doc is None:
                if module.module_type_name == "pipeline":
                    pc: PipelineConfig = module.config
                    doc = pc.doc
                else:
                    doc = module.doc

            # TODO: do we really need the deepcopy here?
            _s = PipelineStep(
                step_id=step_id,
                module_type=resolved_module_type,
                module_config=dict(resolved_module_config),
                input_links=input_links,  # type: ignore
                doc=doc,
                module_details=KiaraModuleInstance.from_module(module=module),
                manifest_src=src_manifest,
            )
            _s._module = module
        else:
            _s = step

        return _s

    @classmethod
    def create_steps(
        cls,
        *steps: Union["PipelineStep", Mapping[str, Any]],
        kiara: "Kiara",
        module_map: Union[Mapping[str, Any], None] = None,
        auto_step_ids: bool = False,
    ) -> List["PipelineStep"]:

        if module_map is None:
            module_map = {}
        else:
            module_map = dict(module_map)

        result: List[PipelineStep] = []

        step_ids: List[str] = []
        for step in steps:

            _s = cls.create_step(
                step=step,
                kiara=kiara,
                module_map=module_map,
                auto_step_id=auto_step_ids,
                taken_step_ids=step_ids,
            )
            result.append(_s)

        return result

    @field_validator("step_id")
    @classmethod
    def _validate_step_id(cls, v):

        assert isinstance(v, str)
        if "." in v:
            raise ValueError("Step ids can't contain '.' characters.")

        return v

    step_id: str = Field(
        description="Locally unique id (within a pipeline) of this step."
    )

    module_type: str = Field(description="The module type.")
    module_config: Dict[str, Any] = Field(
        description="The module config.", default_factory=dict
    )
    manifest_src: Manifest = Field(
        description="The original manfifest provided by the user."
    )

    # required: bool = Field(
    #     description="Whether this step is required within the workflow.\n\nIn some cases, when none of the pipeline outputs have a required input that connects to a step, then it is not necessary for this step to have been executed, even if it is placed before a step in the execution hierarchy. This also means that the pipeline inputs that are connected to this step might not be required.",
    #     default=True,
    # )
    # processing_stage: Optional[int] = Field(
    #     default=None,
    #     description="The stage number this step is executed within the pipeline.",
    # )
    input_links: Mapping[str, List[StepValueAddress]] = Field(
        description="The links that connect to inputs of the module. Keys are field names, value(s) are connected outputs.",
        default_factory=dict,
    )
    module_details: KiaraModuleInstance = Field(
        description="The class of the underlying module."
    )
    doc: DocumentationMetadataModel = Field(
        description="A description what this step does."
    )
    _module: Union["KiaraModule", None] = PrivateAttr(default=None)

    @model_validator(mode="before")
    @classmethod
    def create_step_id(cls, values):

        if "module_type" not in values:
            raise ValueError("No 'module_type' specified.")
        if "step_id" not in values or not values["step_id"]:
            values["step_id"] = slugify(values["module_type"], delim="_")

        return values

    def _retrieve_data_to_hash(self) -> Any:

        data = extract_data_to_hash_from_pipeline_config(self.module_config)
        return {
            "module_type": self.module_type,
            "module_config": data,
        }

    @field_validator("doc", mode="before")
    @classmethod
    def validate_doc(cls, value):
        doc = DocumentationMetadataModel.create(value)
        return doc

    @field_validator("step_id")
    @classmethod
    def ensure_valid_id(cls, v):

        # TODO: check with regex
        if "." in v or " " in v:
            raise ValueError(
                f"Step id can't contain special characters or whitespaces: {v}"
            )

        return v

    @field_validator("module_config", mode="before")
    @classmethod
    def ensure_dict(cls, v):

        if v is None:
            v = {}
        return v

    @field_validator("input_links", mode="before")
    @classmethod
    def ensure_input_links_valid(cls, v):

        if v is None:
            v = {}

        result = {}
        for input_name, output in v.items():

            input_links = ensure_step_value_addresses(
                default_field_name=input_name, link=output
            )
            result[input_name] = input_links

        return result

    @property
    def module(self) -> "KiaraModule":
        if self._module is None:
            m_cls = self.module_details.get_class()
            self._module = m_cls(module_config=self.module_config)
        return self._module

    def find_pipeline_inputs(
        self, pipeline: Union["Pipeline", "PipelineStructure"]
    ) -> Dict[str, PipelineInputRef]:
        """Return all the pipeline inputs that are connected to this step.

        Returns a dictionary with the name of the step input as key, and a reference to the pipeline input as value.
        """

        result = {}
        for field, field_ref in pipeline.pipeline_input_refs.items():
            for con_inp in field_ref.connected_inputs:
                if con_inp.step_id == self.step_id:
                    result[con_inp.value_name] = field_ref

        return result

    def find_pipeline_outputs(
        self, pipeline: Union["Pipeline", "PipelineStructure"]
    ) -> Dict[str, PipelineOutputRef]:

        result = {}
        for field, field_ref in pipeline.pipeline_output_refs.items():
            if field_ref.connected_output.step_id == self.step_id:
                result[field] = field_ref
        return result

    def __repr__(self):

        return f"{self.__class__.__name__}(step_id={self.step_id} module_type={self.module_type})"

    def __str__(self):
        return f"step: {self.step_id} (module: {self.module_type})"

    def create_renderable(self, **config: Any) -> RenderableType:

        in_panel = config.get("in_panel", None)
        display_step_id = config.get("display_step_id", True)
        if in_panel is None:
            if is_jupyter():
                in_panel = True
            else:
                in_panel = False

        table = Table(show_header=False, box=box.SIMPLE)
        table.add_column("key", style="i")
        table.add_column("value")

        if self.doc.is_set:
            table.add_row("", Markdown(self.doc.full_doc))
        if display_step_id:
            table.add_row("step_id", self.step_id)
        table.add_row("module type", self.module_type)
        if not module_config_is_empty(self.module_config):
            mc = dict(self.module_config)
            if not mc.get("defaults", None):
                mc.pop("defaults", None)
            if not mc.get("constants", None):
                mc.pop("constants", None)

            if "steps" in mc.keys():
                _steps = []
                for step in mc["steps"]:
                    _s = {
                        "step_id": step["step_id"],
                        "module_type": step["manifest_src"]["module_type"],
                    }
                    sc = step.get("module_config", {})
                    if sc:
                        _s["module_config"] = sc
                    _steps.append(_s)
                mc["steps"] = _steps
            config_str = orjson_dumps(mc, option=orjson.OPT_INDENT_2)
            table.add_row(
                "module_config",
                Syntax(config_str, "json", background_color="default", theme="default"),
            )
        module_doc = DocumentationMetadataModel.from_class_doc(self.module.__class__)
        table.add_row("module doc", Markdown(module_doc.full_doc))
        inputs = create_table_from_field_schemas(
            _add_default=True,
            _add_required=True,
            _show_header=True,
            fields={
                f"{self.step_id}.{k}": v for k, v in self.module.inputs_schema.items()
            },
        )
        table.add_row("inputs", inputs)
        outputs = create_table_from_field_schemas(
            _add_default=False,
            _add_required=False,
            _show_header=True,
            fields={
                f"{self.step_id}.{k}": v for k, v in self.module.outputs_schema.items()
            },
        )
        table.add_row("outputs", outputs)

        if in_panel:
            return Panel(table, title=f"Step: {self.step_id}", title_align="left")
        else:
            return table


def create_input_alias_map(steps: Iterable[PipelineStep]) -> Dict[str, str]:

    aliases: Dict[str, str] = {}
    for step in steps:
        field_names = step.module.input_names
        for field_name in field_names:
            alias = generate_pipeline_endpoint_name(
                step_id=step.step_id, value_name=field_name
            )
            assert alias not in aliases.keys()
            aliases[f"{step.step_id}.{field_name}"] = alias

    return aliases


def create_output_alias_map(steps: Iterable[PipelineStep]) -> Dict[str, str]:

    aliases: Dict[str, str] = {}
    for step in steps:
        field_names = step.module.output_names
        for field_name in field_names:
            alias = generate_pipeline_endpoint_name(
                step_id=step.step_id, value_name=field_name
            )
            assert alias not in aliases.keys()
            aliases[f"{step.step_id}.{field_name}"] = alias

    return aliases


class PipelineConfig(KiaraModuleConfig):
    """
    A class to hold the configuration for a [PipelineModule][kiara.pipeline.module.PipelineModule].

    If you want to control the pipeline input and output names, you need to have to provide a map that uses the
    autogenerated field name ([step_id]__[alias] -- 2 underscores!!) as key, and the desired field name
    as value. The reason that schema for the autogenerated field names exist is that it's hard to ensure
    the uniqueness of each field; some steps can have the same input field names, but will need different input
    values. In some cases, some inputs of different steps need the same input. Those sorts of things.
    So, to make sure that we always use the right values, I chose to implement a conservative default approach,
    accepting that in some cases the user will be prompted for duplicate inputs for the same value.

    To remedy that, the pipeline creator has the option to manually specify a mapping to rename some or all of
    the input/output fields.

    Further, because in a lot of cases there won't be any overlapping fields, the creator can specify ``auto``,
    in which case *Kiara* will automatically create a mapping that tries to map autogenerated field names
    to the shortest possible names for each case.

    Examples:
    --------
        Configuration for a pipeline module that functions as a ``nand`` logic gate (in Python):

        ``` python
        and_step = PipelineStepConfig(module_type="and", step_id="and")
        not_step = PipelineStepConfig(module_type="not", step_id="not", input_links={"a": ["and.y"]}
        nand_p_conf = PipelineConfig(doc="Returns 'False' if both inputs are 'True'.",
                            steps=[and_step, not_step],
                            input_aliases={
                                "and.a": "a",
                                "and.b": "b"
                            },
                            output_aliases={
                                "not.y": "y"
                            }}
        ```

        Or, the same thing in json:

        ``` json
        {
          "module_type_name": "nand",
          "doc": "Returns 'False' if both inputs are 'True'.",
          "steps": [
            {
              "module_type": "and",
              "step_id": "and"
            },
            {
              "module_type": "not",
              "step_id": "not",
              "input_links": {
                "a": "and.y"
              }
            }
          ],
          "input_aliases": {
            "and.a": "a",
            "and.b": "b"
          },
          "output_aliases": {
            "not.y": "y"
          }
        }
        ```
    """

    _kiara_model_id: ClassVar = "instance.module_config.pipeline"

    @classmethod
    def from_file(
        cls,
        path: str,
        kiara: Union["Kiara", None] = None,
        pipeline_name: Union[None, str] = None,
        # module_map: Optional[Mapping[str, Any]] = None,
    ) -> "PipelineConfig":

        data = get_data_from_file(path)
        _pipeline_name = data.pop("pipeline_name", None)

        if pipeline_name:
            _pipeline_name = pipeline_name

        if _pipeline_name is None:
            _pipeline_name = os.path.basename(path)

        pipeline_dir = os.path.abspath(os.path.dirname(path))

        execution_context = ExecutionContext(pipeline_dir=pipeline_dir)
        return cls.from_config(
            pipeline_name=_pipeline_name,
            data=data,
            kiara=kiara,
            execution_context=execution_context,
        )

    @classmethod
    def from_string(
        cls,
        string_data: str,
        kiara: Union["Kiara", None] = None,
        pipeline_name: Union[None, str] = None,
        # module_map: Optional[Mapping[str, Any]] = None,
    ) -> "PipelineConfig":

        data = get_data_from_string(string_data)
        _pipeline_name = data.pop("pipeline_name", None)

        if pipeline_name:
            _pipeline_name = pipeline_name

        return cls.from_config(
            pipeline_name=_pipeline_name,
            data=data,
            kiara=kiara,
        )

    @classmethod
    def from_config(
        cls,
        data: Mapping[str, Any],
        pipeline_name: Union[str, None] = None,
        kiara: Union["Kiara", None] = None,
        module_map: Union[Mapping[str, Any], None] = None,
        execution_context: Union[ExecutionContext, None] = None,
        auto_step_ids: bool = False,
    ) -> "PipelineConfig":

        if kiara is None:
            from kiara.context import Kiara

            kiara = Kiara.instance()

        if not kiara.operation_registry.is_initialized:
            kiara.operation_registry.operations

        if execution_context is None:
            execution_context = ExecutionContext()

        config = cls._from_config(
            pipeline_name=pipeline_name,
            data=data,
            kiara=kiara,
            module_map=module_map,
            execution_context=execution_context,
            auto_step_ids=auto_step_ids,
        )
        return config

    @classmethod
    def _from_config(
        cls,
        data: Mapping[str, Any],
        kiara: "Kiara",
        pipeline_name: Union[str, None] = None,
        module_map: Union[Mapping[str, Any], None] = None,
        execution_context: Union[ExecutionContext, None] = None,
        auto_step_ids: bool = False,
    ) -> "PipelineConfig":

        if execution_context is None:
            execution_context = ExecutionContext()

        repl_dict = execution_context.model_dump()

        data = dict(data)

        _pipeline_name = data.pop("pipeline_name", None)
        if pipeline_name:
            _pipeline_name = pipeline_name

        if not _pipeline_name:
            _pipeline_name = str(uuid.uuid4())

        _steps = data.pop("steps")
        steps = PipelineStep.create_steps(
            *_steps, kiara=kiara, module_map=module_map, auto_step_ids=auto_step_ids
        )
        data["steps"] = steps
        if not data.get("input_aliases"):
            data["input_aliases"] = create_input_alias_map(steps)
        if not data.get("output_aliases"):
            data["output_aliases"] = create_output_alias_map(steps)

        if "defaults" in data.keys():
            defaults = data.pop("defaults")
            replaced = replace_var_names_in_obj(defaults, repl_dict=repl_dict)
            data["defaults"] = replaced

        if "constants" in data.keys():
            constants = data.pop("constants")
            replaced = replace_var_names_in_obj(constants, repl_dict=repl_dict)
            data["constants"] = replaced

        if "inputs" in data.keys():
            inputs = data.pop("inputs")
            replaced = replace_var_names_in_obj(inputs, repl_dict=repl_dict)
            data["inputs"] = replaced

        if "doc" not in data.keys():
            data["doc"] = None

        result = cls(pipeline_name=_pipeline_name, **data)
        return result

    model_config = ConfigDict(extra="ignore", validate_assignment=True)

    pipeline_name: str = Field(description="The name of this pipeline.")
    steps: List[PipelineStep] = Field(
        description="A list of steps/modules of this pipeline, and their connections.",
    )
    input_aliases: Dict[str, str] = Field(
        description="A map of input aliases, with the location of the input (in the format '[step_id].[input_field]') as key, and the pipeline input field name as value.",
    )
    output_aliases: Dict[str, str] = Field(
        description="A map of output aliases, with the location of the output (in the format '[step_id].[output_field]') as key, and the pipeline output field name as value.",
    )
    doc: DocumentationMetadataModel = Field(
        default="-- n/a --", description="Documentation about what the pipeline does."  # type: ignore
    )
    context: Dict[str, Any] = Field(
        default_factory=dict, description="Metadata for this workflow."
    )
    _structure: Union["PipelineStructure", None] = PrivateAttr(default=None)

    @field_validator("doc", mode="before")
    @classmethod
    def validate_doc(cls, value):
        return DocumentationMetadataModel.create(value)

    @field_validator("steps", mode="before")
    @classmethod
    def _validate_steps(cls, v):

        steps = []
        for step in v:
            if not step:
                raise ValueError("No step data provided.")
            if isinstance(step, PipelineStep):
                steps.append(step)
            elif isinstance(step, Mapping):
                steps.append(PipelineStep(**step))  # type: ignore
            else:
                raise TypeError(step)
        return steps

    @property
    def structure(self) -> "PipelineStructure":

        if self._structure is not None:
            return self._structure

        from kiara.models.module.pipeline.structure import PipelineStructure

        self._structure = PipelineStructure(pipeline_config=self)  # type: ignore
        return self._structure

    def get_raw_config(self) -> Dict[str, Any]:

        steps = []
        for step in self.steps:
            src: Dict[str, Any] = {
                "module_type": step.manifest_src.module_type,
            }
            if step.manifest_src.module_config:
                src["module_config"] = step.manifest_src.module_config
            src["step_id"] = step.step_id
            for field, links in step.input_links.items():
                for link in links:
                    src.setdefault("input_links", {})[
                        field
                    ] = f"{link.step_id}.{link.value_name}"
            steps.append(src)

        return {
            "pipeline_name": self.pipeline_name,
            "doc": self.doc.full_doc,
            "steps": steps,
            "input_aliases": self.input_aliases,
            "output_aliases": self.output_aliases,
        }

    def _retrieve_data_to_hash(self) -> Any:

        data = {
            "defaults": self.defaults,
            "constants": self.constants,
            "steps": [step.model_dump() for step in self.steps],
            "input_aliases": self.input_aliases,
            "output_aliases": self.output_aliases,
        }
        hash_data = extract_data_to_hash_from_pipeline_config(data)
        return hash_data

    def create_renderable(self, **config: Any) -> RenderableType:

        table = Table(show_header=False, box=box.SIMPLE)
        table.add_column("key", style="i")
        table.add_column("value")

        table.add_row("doc", self.doc.full_doc)
        table.add_row("structure", self.structure.create_renderable(**config))
        return table

        # return create_table_from_model_object(self, exclude_fields={"steps"})

        # return create_table_from_model_object(self)

    # def create_input_alias_map(self) -> Dict[str, str]:
    #
    #     aliases: Dict[str, List[str]] = {}
    #     for step in self.steps:
    #         field_names = step.module.input_names
    #         for field_name in field_names:
    #             aliases.setdefault(field_name, []).append(step.step_id)
    #
    #     result: Dict[str, str] = {}
    #     for field_name, step_ids in aliases.items():
    #         for step_id in step_ids:
    #             generated = generate_pipeline_endpoint_name(step_id, field_name)
    #             result[generated] = generated
    #
    #     return result
    #
    # def create_output_alias_map(self) -> Dict[str, str]:
    #
    #     aliases: Dict[str, List[str]] = {}
    #     for step in self.steps:
    #         field_names = step.module.input_names
    #         for field_name in field_names:
    #             aliases.setdefault(field_name, []).append(step.step_id)
    #
    #     result: Dict[str, str] = {}
    #     for field_name, step_ids in aliases.items():
    #         for step_id in step_ids:
    #             generated = generate_pipeline_endpoint_name(step_id, field_name)
    #             result[generated] = generated
    #
    #     return result


def generate_pipeline_endpoint_name(step_id: str, value_name: str):

    return f"{step_id}__{value_name}"


# kiara\kiara\src\kiara\models\rendering\values.py
# -*- coding: utf-8 -*-
# import abc
# from typing import Union, Iterable, Mapping, Dict, Any, TYPE_CHECKING
#
# from pydantic import Extra, Field, BaseModel
#
# from kiara.models import KiaraModel
#
# if TYPE_CHECKING:
#     from kiara.models.values.value import Value
#
#
# class RenderScene(KiaraModel):
#     class Config:
#         extra = Extra.ignore
#
#     parent_scene: Union[None, "RenderScene"] = Field(
#         description="The parent of this scene (if applicable).", default=None
#     )
#
#     @classmethod
#     @abc.abstractmethod
#     def retrieve_source_type(cls) -> str:
#         pass
#
#     @classmethod
#     def retrieve_supported_target_types(cls) -> Iterable[str]:
#
#         result = []
#         for attr in dir(cls):
#             if len(attr) <= 11 or not attr.startswith("render_as__"):
#                 continue
#
#             attr = attr[11:]
#             target_type = attr[0:]
#             result.append(target_type)
#
#         return result
#
#     scene_name: Union[None, str] = Field(
#         description="The name of the current scene to be rendered.", default=None
#     )
#     scenes: Union[None, Mapping[str, "RenderScene"]] = Field(
#         description="Child scenes, in case this itself is not going to be rendered.",
#         default=None,
#     )
#
#     def get_render_parameters(self) -> Dict[str, Any]:
#         if self.scene_name:
#             if not self.scenes or self.scene_name not in self.scenes.keys():
#                 if self.scenes:
#                     _msg = "Available names: " + ", ".join(self.scenes.keys())
#                 else:
#                     _msg = "No child scenes available"
#
#                 raise Exception(
#                     f"Can't render scene, no scene named '{self.scene_name}' available. {_msg}."
#                 )
#             else:
#                 return self.scenes[self.scene_name].get_render_parameters()
#         else:
#             return self.dict(exclude={"scene_name", "scenes"})
#
#
# class RenderMetadata(KiaraModel):
#
#     this_scene: RenderScene = Field(
#         description="The render instruction for the current scene."
#     )
#     related_scenes: Dict[str, Union[RenderScene, None]] = Field(
#         description="Related instructions, to be used by implementing frontends as hints.",
#         default_factory=dict,
#     )
#
#
# class RenderValueResult(BaseModel):
#
#     rendered: Any
#     metadata: RenderMetadata
#
#
# class RenderAnyValueScene(RenderScene):
#     @classmethod
#     def retrieve_source_type(cls) -> str:
#         return "any"
#
#     def render_as__terminal_renderable(self, value: "Value"):
#         render_config = {
#             "show_pedigree": False,
#             "show_serialized": False,
#             "show_data_preview": False,
#             "show_properties": True,
#             "show_destinies": True,
#             "show_destiny_backlinks": True,
#             "show_lineage": True,
#             "show_environment_hashes": False,
#             "show_environment_data": False,
#         }
#         value_info = value.create_info()
#         rend = value_info.create_renderable(**render_config)
#         return RenderValueResult(
#             rendered=rend, metadata=RenderMetadata(this_scene=self)
#         )
#
#     # def render_as__string(self, value: "Value"):
#     #     return RenderValueResult(rendered="xxx", metadata=RenderMetadata(this_scene=self))


# kiara\kiara\src\kiara\models\rendering\__init__.py
# -*- coding: utf-8 -*-
import uuid
from typing import Any, ClassVar, Dict, Mapping, TypeVar, Union

import orjson
from dag_cbor import IPLDKind
from pydantic import Field, field_validator
from rich.console import RenderableType
from rich.syntax import Syntax
from rich.table import Table

from kiara.defaults import DEFAULT_NO_DESC_VALUE
from kiara.models import KiaraModel
from kiara.models.module.manifest import Manifest
from kiara.utils.json import orjson_dumps
from kiara.utils.output import extract_renderable

DataT = TypeVar("DataT")


class RenderScene(KiaraModel):

    _kiara_model_id: ClassVar = "instance.render_scene"

    title: str = Field(description="The title of this scene.")
    disabled: bool = Field(
        description="Whether this scene should be displayed as 'disabled' in a UI.",
        default=False,
    )
    description: str = Field(
        description="Description of what this scene renders.",
        default=DEFAULT_NO_DESC_VALUE,
    )
    manifest_hash: str = Field(
        description="The hash of the manifest of the referenced render scene."
    )
    render_config: Mapping[str, Any] = Field(
        description="The inputs used with the referenced manifest.",
        default_factory=dict,
    )
    related_scenes: Mapping[str, Union[None, "RenderScene"]] = Field(
        description="Other render scenes, related to this one.", default_factory=dict
    )

    @field_validator("manifest_hash", mode="before")
    @classmethod
    def validate_manifest_hash(cls, value):

        if hasattr(value, "manifest_hash"):
            return value.manifest_hash  # type: ignore
        else:
            return value

    # @validator("description", pre=True)
    # def validate_desc(cls, value):
    #     return DocumentationMetadataModel.create(value)


class RenderValueResult(KiaraModel):
    """Object containing all the result properties of a 'render_value' operation."""

    value_id: uuid.UUID = Field(description="The value that was rendered.")
    render_config: Mapping[str, Any] = Field(
        description="The config that was used to render this.", default_factory=dict
    )
    render_manifest: str = Field(
        description="The id of the manifest that was used to render this."
    )
    related_scenes: Mapping[str, Union[None, RenderScene]] = Field(
        description="Other render scenes, related to this one.", default_factory=dict
    )
    manifest_lookup: Dict[str, Manifest] = Field(
        description="The manifests referenced in this model, indexed by the hashes.",
        default_factory=dict,
    )
    rendered: Any = Field(description="The rendered object.")

    def _retrieve_data_to_hash(self) -> IPLDKind:
        return {
            "value_id": str(self.value_id),
            "render_config": self.render_config,  # type: ignore
            "render_manifest": self.render_manifest,
        }

    def create_renderable(self, **config: Any) -> RenderableType:

        show_render_result = config.get("show_render_result", True)
        show_render_metadata = config.get("show_render_metadata", False)
        if show_render_metadata:

            table: Table = Table(show_header=False)
            table.add_column("key")
            table.add_column("value")

            table.add_row("value_id", str(self.value_id))

            rc_data = orjson_dumps(self.render_config, option=orjson.OPT_INDENT_2)
            render_config = Syntax(rc_data, "json", background_color="default")
            table.add_row("applied render config", render_config)

            applied_module = self.manifest_lookup[self.render_manifest]
            table.add_row("applied module", applied_module.create_renderable(**config))  # type: ignore

            related_scenes: Dict[str, Union[str, Dict[str, Any]]] = {}
            for k, v in self.related_scenes.items():
                if v is None:
                    related_scenes[k] = "-- disabled --"
                else:
                    related_scenes[k] = v.model_dump()
            rel_scenes_json = orjson_dumps(related_scenes, option=orjson.OPT_INDENT_2)
            table.add_row(
                "related scenes",
                Syntax(rel_scenes_json, "json", background_color="default"),
            )
            if show_render_result:
                table.add_row(
                    "rendered", extract_renderable(self.rendered, render_config=config)
                )

            result: RenderableType = table
        else:
            result = extract_renderable(self.rendered, render_config=config)

        return result


# class ValueRenderSceneString(RenderScene[str]):
#
#     pass
#
# class ValueRenderSceneTerminal(RenderScene[RenderableType]):
#
#     class Config:
#         arbitrary_types_allowed = True
#
# class ValueRenderSceneHtml(RenderScene[str]):
#
#     pass


# kiara\kiara\src\kiara\models\runtime_environment\kiara.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2022, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

from typing import TYPE_CHECKING

if TYPE_CHECKING:
    pass


# class KiaraDataTypesRuntimeEnvironment(RuntimeEnvironment):
#
#     _kiara_model_id: ClassVar = "info.runtime.kiara_data_types"
#
#     environment_type: Literal["kiara_data_types"]
#     data_types: DataTypeClassesInfo = Field(
#         description="The available data types and their metadata."
#     )
#
#     @classmethod
#     def retrieve_environment_data(cls) -> Dict[str, Any]:
#
#         from kiara.api import KiaraAPI
#
#         kiara_api = KiaraAPI.instance()
#
#         data_types_infos: DataTypeClassesInfo = kiara_api.retrieve_data_types_info()
#         data_types = data_types_infos.model_dump()
#
#         return {"data_types": data_types}


# class KiaraTypesRuntimeEnvironment(RuntimeEnvironment):
#
#     _kiara_model_id: ClassVar = "info.runtime.kiara_types"
#
#     environment_type: Literal["kiara_types"]
#     archive_types: ArchiveTypeClassesInfo = Field(
#         description="The available implemented store types."
#     )
#     metadata_types: MetadataTypeClassesInfo = Field(
#         description="The available metadata types."
#     )
#
#     @classmethod
#     def retrieve_environment_data(cls) -> Dict[str, Any]:
#
#         from kiara.utils.archives import find_archive_types
#
#         result: Dict[str, Any] = {}
#         result["metadata_types"] = find_metadata_models()
#         result["archive_types"] = find_archive_types()
#
#         return result


# kiara\kiara\src\kiara\models\runtime_environment\operating_system.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2022, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import os
import platform
from typing import Any, ClassVar, Dict, Literal

from pydantic import Field

from kiara.models.runtime_environment import RuntimeEnvironment


class OSRuntimeEnvironment(RuntimeEnvironment):
    """
    Manages information about the OS this kiara instance is running in.

    # TODO: details for other OS's (mainly BSDs)
    """

    _kiara_model_id: ClassVar = "info.runtime.os"

    environment_type: Literal["operating_system"]
    operation_system: str = Field(description="The operation system name.")
    platform: str = Field(description="The platform name.")
    release: str = Field(description="The platform release name.")
    version: str = Field(description="The platform version name.")
    machine: str = Field(description="The architecture.")
    os_specific: Dict[str, Any] = Field(
        description="OS specific platform metadata.", default_factory=dict
    )

    @classmethod
    def retrieve_environment_data(self) -> Dict[str, Any]:

        os_specific: Dict[str, Any] = {}
        platform_system = platform.system()
        if platform_system == "Linux":
            import distro

            os_specific["distribution"] = {
                "name": distro.name(),
                "version": distro.version(),
                "codename": distro.codename(),
            }
        elif platform_system == "Darwin":
            mac_version = platform.mac_ver()
            os_specific["mac_ver_release"] = mac_version[0]
            os_specific["mac_ver_machine"] = mac_version[2]

        result = {
            "operation_system": os.name,
            "platform": platform_system,
            "release": platform.release(),
            "version": platform.version(),
            "machine": platform.machine(),
            "os_specific": os_specific,
        }

        # if config.include_all_info:
        #     result["uname"] = platform.uname()._asdict()

        return result


# kiara\kiara\src\kiara\models\runtime_environment\python.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2022, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import sys
from functools import lru_cache
from typing import Any, ClassVar, Dict, List, Literal, Mapping, Union

from pydantic import BaseModel, Field
from rich import box
from rich.console import RenderableType
from rich.table import Table

from kiara.models.runtime_environment import RuntimeEnvironment
from kiara.utils.output import extract_renderable

try:
    from importlib.metadata import distribution, packages_distributions  # type: ignore
except Exception:
    from importlib_metadata import distribution, packages_distributions  # type:ignore


class PythonPackage(BaseModel):

    name: str = Field(description="The name of the Python package.")
    version: str = Field(description="The version of the package.")


@lru_cache()
def find_all_distributions():
    all_packages = packages_distributions()
    return all_packages


class PythonRuntimeEnvironment(RuntimeEnvironment):

    _kiara_model_id: ClassVar = "info.runtime.python"

    environment_type: Literal["python"]
    python_version: str = Field(description="The version of Python.")
    packages: List[PythonPackage] = Field(
        description="The packages installed in the Python (virtual) environment."
    )
    # python_config: typing.Dict[str, str] = Field(
    #     description="Configuration details about the Python installation."
    # )

    def _create_renderable_for_field(
        self, field_name: str, for_summary: bool = False
    ) -> Union[RenderableType, None]:

        if field_name != "packages":
            return extract_renderable(getattr(self, field_name))

        if for_summary:
            return ", ".join(p.name for p in self.packages)

        table = Table(show_header=True, box=box.SIMPLE)
        table.add_column("package name")
        table.add_column("version")

        for package in self.packages:
            table.add_row(package.name, package.version)

        return table

    def _retrieve_sub_profile_env_data(self) -> Mapping[str, Any]:

        only_packages = [p.name for p in self.packages]
        full = {k.name: k.version for k in self.packages}

        return {
            "package_names": only_packages,
            "packages": full,
            "package_names_incl_python_version": {
                "python_version": self.python_version,
                "packages": only_packages,
            },
            "packages_incl_python_version": {
                "python_version": self.python_version,
                "packages": full,
            },
        }

    @classmethod
    def retrieve_environment_data(cls) -> Dict[str, Any]:

        packages: Dict[str, str] = {}
        all_packages = find_all_distributions()

        for name, pkgs in all_packages.items():
            for pkg in pkgs:
                dist = distribution(pkg)
                if pkg in packages.keys() and packages[pkg] != dist.version:
                    raise Exception(
                        f"Multiple versions of package '{pkg}' available: {packages[pkg]} and {dist.version}."
                    )
                packages[pkg] = dist.version

        result: Dict[str, Any] = {
            "python_version": sys.version,
            "packages": [
                {"name": p, "version": packages[p]}
                for p in sorted(packages.keys(), key=lambda x: x.lower())
            ],
        }

        # if config.include_all_info:
        #     import sysconfig
        #     result["python_config"] = sysconfig.get_config_vars()

        return result


class KiaraPluginsRuntimeEnvironment(RuntimeEnvironment):

    _kiara_model_id: ClassVar = "info.runtime.kiara_plugins"

    environment_type: Literal["kiara_plugins"]
    kiara_plugins: List[PythonPackage] = Field(
        description="The kiara plugin packages installed in the Python (virtual) environment."
    )

    def _create_renderable_for_field(
        self, field_name: str, for_summary: bool = False
    ) -> Union[RenderableType, None]:

        if field_name != "packages":
            return extract_renderable(getattr(self, field_name))

        if for_summary:
            return ", ".join(p.name for p in self.kiara_plugins)

        table = Table(show_header=True, box=box.SIMPLE)
        table.add_column("package name")
        table.add_column("version")

        for package in self.kiara_plugins:
            table.add_row(package.name, package.version)

        return table

    def _retrieve_sub_profile_env_data(self) -> Mapping[str, Any]:

        only_packages = [p.name for p in self.kiara_plugins]
        full = {k.name: k.version for k in self.kiara_plugins}

        return {"package_names": only_packages, "packages": full}

    @classmethod
    def retrieve_environment_data(cls) -> Dict[str, Any]:

        packages = []
        all_packages = find_all_distributions()
        all_pkg_names = set()
        for name, pkgs in all_packages.items():

            for pkg in pkgs:
                if not pkg.startswith("kiara_plugin.") and not pkg.startswith(
                    "kiara-plugin."
                ):
                    continue
                else:
                    all_pkg_names.add(pkg)

        for pkg in sorted(all_pkg_names):
            dist = distribution(pkg)
            packages.append({"name": pkg, "version": dist.version})

        result: Dict[str, Any] = {
            "kiara_plugins": packages,
        }

        # if config.include_all_info:
        #     import sysconfig
        #     result["python_config"] = sysconfig.get_config_vars()

        return result


# kiara\kiara\src\kiara\models\runtime_environment\__init__.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)
from abc import abstractmethod
from typing import Any, Dict, Mapping, Sequence, Union, get_args

import orjson
import structlog
from pydantic import ConfigDict, PrivateAttr
from rich import box
from rich.console import RenderableType
from rich.syntax import Syntax
from rich.table import Table

from kiara.defaults import DEFAULT_ENV_HASH_KEY, ENVIRONMENT_TYPE_CATEGORY_ID
from kiara.models.metadata import KiaraMetadata
from kiara.utils.hashing import compute_cid
from kiara.utils.json import orjson_dumps
from kiara.utils.output import extract_renderable

logger = structlog.get_logger()


class RuntimeEnvironment(KiaraMetadata):
    model_config = ConfigDict(frozen=True)

    @classmethod
    def get_environment_type_name(cls) -> str:

        env_type = cls.model_fields["environment_type"]
        args: Sequence[str] = get_args(env_type.annotation)
        assert len(args) == 1

        return args[0]

    @classmethod
    def create_environment_model(cls):

        try:
            type_name = cls.get_environment_type_name()
            data = cls.retrieve_environment_data()
            assert (
                "environment_type" not in data.keys()
                or data["environment_keys"] == type_name
            )
            data["environment_type"] = type_name

        except Exception as e:
            raise Exception(f"Can't create environment model for '{cls.__name__}': {e}")
        return cls(**data)

    def get_category_alias(self) -> str:
        return f"{ENVIRONMENT_TYPE_CATEGORY_ID}.{self.environment_type}"  # type: ignore

    @classmethod
    @abstractmethod
    def retrieve_environment_data(cls) -> Dict[str, Any]:
        pass

    _env_hashes: Union[Mapping[str, str], None] = PrivateAttr(default=None)

    def _create_renderable_for_field(
        self, field_name: str, for_summary: bool = False
    ) -> Union[RenderableType, None]:

        return extract_renderable(getattr(self, field_name))

    def _retrieve_id(self) -> str:
        return self.__class__.get_environment_type_name()

    @property
    def env_hashes(self) -> Mapping[str, str]:

        if self._env_hashes is not None:
            return self._env_hashes

        result = {}
        for k, v in self._retrieve_sub_profile_env_data().items():
            _, cid = compute_cid(data=v)
            result[k] = str(cid)

        self._env_hashes = result
        return self._env_hashes

    def _retrieve_sub_profile_env_data(self) -> Mapping[str, Any]:
        """
        Return a dictionary with data that identifies one hash profile per key.

        In most cases, this will only return a single-key dictionary containing all the data in that environment. But
        in some cases one might want to have several sub-hash profiles, for example a list of Python packages with and
        without version information, to have more fine-grained control about when to consider two environments functionally
        equal.
        """
        return {DEFAULT_ENV_HASH_KEY: self._retrieve_data_to_hash()}

    def create_renderable(self, **config: Any) -> RenderableType:

        summary = config.get("summary", False)

        table = Table(show_header=False, box=box.SIMPLE)
        table.add_column("field")
        table.add_column("summary")

        hashes_str = orjson_dumps(self.env_hashes, option=orjson.OPT_INDENT_2)
        table.add_row(
            "environment hashes", Syntax(hashes_str, "json", background_color="default")
        )

        for field_name, field in self.model_fields.items():
            summary_item = self._create_renderable_for_field(
                field_name, for_summary=summary
            )
            if summary_item is not None:
                table.add_row(field_name, summary_item)

        return table


# kiara\kiara\src\kiara\models\values\data_type.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)
# import orjson
# from pydantic.fields import Field, PrivateAttr
# from rich import box
# from rich.console import RenderableType
# from rich.markdown import Markdown
# from rich.panel import Panel
# from rich.syntax import Syntax
# from rich.table import Table
# from typing import TYPE_CHECKING, Any, List, Literal, Mapping, Type, Union
#
# from kiara.data_types import DataType
# from kiara.interfaces.python_api.models.info import TypeInfo
# from kiara.models.documentation import (
#     AuthorsMetadataModel,
#     ContextMetadataModel,
#     DocumentationMetadataModel,
# )
# # from kiara.models.info import TypeInfo, TypeInfoModelGroup
# from kiara.models.python_class import PythonClass
# from kiara.utils.json import orjson_dumps
#
# if TYPE_CHECKING:
#     from kiara.context import Kiara
#

# class DataTypeClassInfo(TypeInfo[DataType]):
#
#     _kiara_model_id: ClassVar = "info.data_type"
#
#     @classmethod
#     def create_from_type_class(
#         self, type_cls: Type[DataType], kiara: Union["Kiara", None] = None
#     ) -> "DataTypeClassInfo":
#
#         authors = AuthorsMetadataModel.from_class(type_cls)
#         doc = DocumentationMetadataModel.from_class_doc(type_cls)
#         properties_md = ContextMetadataModel.from_class(type_cls)
#
#         if kiara is not None:
#             qual_profiles = kiara.type_registry.get_associated_profiles(type_cls._data_type_name)  # type: ignore
#             lineage = kiara.type_registry.get_type_lineage(type_cls._data_type_name)  # type: ignore
#         else:
#             qual_profiles = None
#             lineage = None
#
#         try:
#             result = DataTypeClassInfo(
#                 type_name=type_cls._data_type_name,  # type: ignore
#                 python_class=PythonClass.from_class(type_cls),
#                 value_cls=PythonClass.from_class(type_cls.python_class()),
#                 data_type_config_cls=PythonClass.from_class(
#                     type_cls.data_type_config_class()
#                 ),
#                 lineage=lineage,  # type: ignore
#                 qualifier_profiles=qual_profiles,
#                 documentation=doc,
#                 authors=authors,
#                 context=properties_md,
#             )
#         except Exception as e:
#             if isinstance(
#                 e, TypeError
#             ) and "missing 1 required positional argument: 'cls'" in str(e):
#                 raise Exception(
#                     f"Invalid implementation of TypeValue subclass '{type_cls.__name__}': 'python_class' method must be marked as a '@classmethod'. This is a bug."
#                 )
#             raise e
#
#         result._kiara = kiara
#         return result
#
#     @classmethod
#     def base_class(self) -> Type[DataType]:
#         return DataType
#
#     @classmethod
#     def category_name(cls) -> str:
#         return "data_type"
#
#     value_cls: PythonClass = Field(description="The python class of the value itself.")
#     data_type_config_cls: PythonClass = Field(
#         description="The python class holding the schema for configuring this type."
#     )
#     lineage: Union[List[str], None] = Field(description="This types lineage.")
#     qualifier_profiles: Union[Mapping[str, Mapping[str, Any]], None] = Field(
#         description="A map of qualifier profiles for this data types."
#     )
#     _kiara: Union["Kiara", None] = PrivateAttr(default=None)
#
#     def _retrieve_id(self) -> str:
#         return self.type_name
#
#     def _retrieve_data_to_hash(self) -> Any:
#         return self.type_name
#
#     def create_renderable(self, **config: Any) -> RenderableType:
#
#         include_doc = config.get("include_doc", True)
#
#         table = Table(box=box.SIMPLE, show_header=False, padding=(0, 0, 0, 0))
#         table.add_column("property", style="i")
#         table.add_column("value")
#
#         if self.lineage:
#             table.add_row("lineage", "\n".join(self.lineage[0:]))
#         else:
#             table.add_row("lineage", "-- n/a --")
#
#         if self.qualifier_profiles:
#             qual_table = Table(show_header=False, box=box.SIMPLE)
#             qual_table.add_column("name")
#             qual_table.add_column("config")
#             for name, details in self.qualifier_profiles.items():
#                 json_details = orjson_dumps(details, option=orjson.OPT_INDENT_2)
#                 qual_table.add_row(
#                     name, Syntax(json_details, "json", background_color="default")
#                 )
#             table.add_row("qualifier profile(s)", qual_table)
#         else:
#             table.add_row("qualifier profile(s)", "-- n/a --")
#
#         if include_doc:
#             table.add_row(
#                 "Documentation",
#                 Panel(self.documentation.create_renderable(), box=box.SIMPLE),
#             )
#
#         table.add_row("Author(s)", self.authors.create_renderable())
#         table.add_row("Context", self.context.create_renderable())
#
#         table.add_row("Python class", self.python_class.create_renderable())
#         table.add_row("Config class", self.data_type_config_cls.create_renderable())
#         table.add_row("Value class", self.value_cls.create_renderable())
#
#         return table


# class DataTypeClassesInfo(TypeInfoModelGroup):
#
#     _kiara_model_id: ClassVar = "info.data_types"
#
#     @classmethod
#     def create_from_type_items(
#         cls,
#         group_title: Union[str, None] = None,
#         **items: Type,
#     ) -> "TypeInfoModelGroup":
#
#         type_infos = {
#             k: cls.base_info_class().create_from_type_class(v) for k, v in items.items()  # type: ignore
#         }
#         data_types_info = cls(group_alias=group_title, item_infos=type_infos)  # type: ignore
#         return data_types_info
#
#     @classmethod
#     def create_augmented_from_type_items(
#         cls,
#         kiara: Union["Kiara", None] = None,
#         group_alias: Union[str, None] = None,
#         **items: Type,
#     ) -> "TypeInfoModelGroup":
#
#         type_infos = {
#             k: cls.base_info_class().create_from_type_class(v, kiara=kiara) for k, v in items.items()  # type: ignore
#         }
#         data_types_info = cls(group_alias=group_alias, item_infos=type_infos)  # type: ignore
#         data_types_info._kiara = kiara
#         return data_types_info
#
#     @classmethod
#     def base_info_class(cls) -> Type[DataTypeClassInfo]:
#         return DataTypeClassInfo
#
#     type_name: Literal["data_type"] = "data_type"
#     item_infos: Mapping[str, DataTypeClassInfo] = Field(
#         description="The data_type info instances for each type."
#     )
#     _kiara: Union["Kiara", None] = PrivateAttr(default=None)
#
#     def create_renderable(self, **config: Any) -> RenderableType:
#
#         full_doc = config.get("full_doc", False)
#         show_subtypes_inline = config.get("show_qualifier_profiles_inline", True)
#         show_lineage = config.get("show_type_lineage", True)
#
#         show_lines = full_doc or show_subtypes_inline or show_lineage
#
#         table = Table(show_header=True, box=box.SIMPLE, show_lines=show_lines)
#         table.add_column("type name", style="i")
#
#         if show_lineage:
#             table.add_column("type lineage")
#
#         if show_subtypes_inline:
#             table.add_column("(qualifier) profiles")
#
#         if full_doc:
#             table.add_column("documentation")
#         else:
#             table.add_column("description")
#
#         all_types = self.item_infos.keys()
#
#         for type_name in sorted(all_types):  # type: ignore
#
#             t_md = self.item_infos[type_name]  # type: ignore
#             row: List[Any] = [type_name]
#
#             if show_lineage:
#                 if self._kiara is None:
#                     lineage_str = "-- n/a --"
#                 else:
#                     lineage = list(
#                         self._kiara.type_registry.get_type_lineage(type_name)
#                     )
#                     lineage_str = ", ".join(reversed(lineage[1:]))
#                 row.append(lineage_str)
#             if show_subtypes_inline:
#                 if self._kiara is None:
#                     qual_profiles = "-- n/a --"
#                 else:
#                     qual_p = self._kiara.type_registry.get_associated_profiles(
#                         data_type_name=type_name
#                     ).keys()
#                     if qual_p:
#                         qual_profiles = "\n".join(qual_p)
#                     else:
#                         qual_profiles = "-- n/a --"
#                 row.append(qual_profiles)
#
#             if full_doc:
#                 md = Markdown(t_md.documentation.full_doc)
#             else:
#                 md = Markdown(t_md.documentation.description)
#             row.append(md)
#             table.add_row(*row)
#
#         return table


# kiara\kiara\src\kiara\models\values\lineage.py
# -*- coding: utf-8 -*-
from typing import TYPE_CHECKING, Any, Dict, Union

import orjson
from networkx import DiGraph
from rich.console import Console, ConsoleOptions, RenderableType, RenderResult
from rich.jupyter import JupyterMixin
from rich.tree import Tree

from kiara.models.values.value import ORPHAN, Value, ValuePedigree

if TYPE_CHECKING:
    from kiara.context import Kiara

COLOR_LIST = [
    "green",
    "blue",
    "bright_magenta",
    "dark_red",
    "gold3",
    "cyan",
    "orange1",
    "light_yellow3",
    "light_slate_grey",
    "deep_pink4",
]


def fill_renderable_lineage_tree(
    kiara: "Kiara",
    pedigree: ValuePedigree,
    node: Union[Tree, None] = None,
    include_ids: bool = False,
    level: int = 0,
) -> Tree:

    color = COLOR_LIST[level % len(COLOR_LIST)]
    title = f"[b {color}]{pedigree.module_type}[/b {color}]"
    if node is None:
        main = Tree(title)
    else:
        main = node.add(title)

    for input_name in sorted(pedigree.inputs.keys()):

        child_value_id = pedigree.inputs[input_name]

        child_value = kiara.data_registry.get_value(child_value_id)

        value_type = child_value.data_type_name
        if include_ids:
            v_id_str = f" = {child_value.value_id}"
        else:
            v_id_str = ""
        input_node = main.add(
            f"input: [i {color}]{input_name} ({value_type})[/i {color}]{v_id_str}"
        )
        if child_value.pedigree != ORPHAN:
            fill_renderable_lineage_tree(
                kiara=kiara,
                pedigree=child_value.pedigree,
                node=input_node,
                level=level + 1,
                include_ids=include_ids,
            )

    return main


def fill_dict_with_lineage(
    kiara: "Kiara",
    value: Value,
    node: Union[Dict[str, Any], None] = None,
    include_preview: bool = False,
    include_module_info: bool = False,
    level: int = 0,
) -> Dict[str, Any]:

    pedigree = value.pedigree
    title = pedigree.module_type
    if node is None:
        root: Dict[str, Any] = {
            "pedigree": {
                "module": {"name": title, "module_config": pedigree.module_config},
                "output_name": value.pedigree_output_name,
                "inputs": {},
            },
            "type": value.data_type_name,
            "id": str(value.value_id),
        }
        if include_preview:
            preview = kiara.render_registry.render(
                source_type="value",
                item=value,
                target_type="string",
                render_config={},
            )
            root["preview"] = preview
        main: Dict[str, Any] = root["pedigree"]
    else:
        node["inputs"] = {}
        node["module"] = {"name": title, "module_config": pedigree.module_config}
        main = node

    if include_module_info:
        info = kiara.module_registry.get_module_type_metadata(title)
        main["module"]["info"] = info.model_dump()

    for input_name in sorted(pedigree.inputs.keys()):

        child_value_id = pedigree.inputs[input_name]
        child_value = kiara.data_registry.get_value(child_value_id)

        value_type = child_value.data_type_name
        main["inputs"][input_name] = {
            "type": value_type,
            "id": str(child_value.value_id),
        }
        if include_preview:
            preview = kiara.render_registry.render(
                source_type="value",
                item=child_value,
                target_type="string",
                render_config={},
            )
            main["inputs"][input_name]["preview"] = preview

        if child_value.pedigree != ORPHAN:
            main["inputs"][input_name]["pedigree"] = {}
            fill_dict_with_lineage(
                kiara=kiara,
                value=child_value,
                node=main["inputs"][input_name]["pedigree"],
                level=level + 1,
                include_preview=include_preview,
                include_module_info=include_module_info,
            )

    if node is None:
        return root  # type: ignore
    else:
        return node


def create_lineage_graph(
    kiara: "Kiara",
    value: Value,
    graph: Union[DiGraph, None] = None,
    parent: Union[None, str] = None,
    level: int = 1,
) -> DiGraph:

    if graph is None:
        graph = DiGraph()
        graph.add_node(
            f"value:{value.value_id}",
            data_type=value.data_type_name,
            label="root_value",
            node_type="value",
            data_type_config=value.data_type_config,
            level=1,
        )
        parent = f"value:{value.value_id}"

    module_id = f"module:{value.pedigree.job_hash}"
    module_label = f"module:{value.pedigree.module_type}"
    graph.add_node(
        module_id,
        module_type=value.pedigree.module_type,
        module_config=value.pedigree.module_config,
        label=module_label,
        node_type="operation",
        level=(level * 2) + 1,
    )
    graph.add_edge(
        parent,
        module_id,
        id=f"{parent}:{module_id}",
        field_name=value.pedigree_output_name,
        label=value.pedigree_output_name,
    )

    for input_name in sorted(value.pedigree.inputs.keys()):

        child_value_id = value.pedigree.inputs[input_name]
        child_value = kiara.data_registry.get_value(child_value_id)

        input_id = f"value:{child_value.value_id}"
        input_label = f"{input_name}:{input_name}"

        graph.add_node(
            input_id,
            label=input_label,
            node_type="value",
            data_type=child_value.data_type_name,
            data_type_config=child_value.data_type_config,
            level=(level * 2) + 2,
        )
        graph.add_edge(
            module_id,
            input_id,
            id=f"{module_id}:{input_id}",
            field_name=input_name,
            label=input_name,
        )

        if child_value.pedigree != ORPHAN:
            create_lineage_graph(
                kiara=kiara,
                value=child_value,
                graph=graph,
                parent=input_id,
                level=level + 1,
            )
    return graph


def create_lineage_graph_modules(
    kiara: "Kiara",
    value: Value,
    graph: Union[DiGraph, None] = None,
    parent: Union[None, str] = None,
    input_field: Union[None, str] = None,
    level: int = 1,
) -> DiGraph:

    if graph is None:
        graph = DiGraph()
        graph.add_node(
            f"value:{value.value_id}",
            data_type=value.data_type_name,
            label="[this value]",
            node_type="value",
            data_type_config=value.data_type_config,
            level=1,
        )

    module_id = f"module:{value.pedigree.job_hash}"
    module_label = value.pedigree.module_type
    graph.add_node(
        module_id,
        module_type=value.pedigree.module_type,
        module_config=value.pedigree.module_config,
        label=module_label,
        node_type="operation",
        level=(level * 2) + 1,
    )

    if parent is None:
        parent = f"value:{value.value_id}"
        graph.add_edge(
            parent,
            module_id,
            id=f"{parent}:{module_id}",
            field_name=value.pedigree_output_name,
            label=f"{value.pedigree_output_name} ({value.data_type_name})",
        )
    else:
        assert input_field is not None
        graph.add_edge(
            parent,
            module_id,
            id=f"{parent}:{input_field}",
            field_name=input_field,
            label=f"{input_field} ({value.data_type_name})",
        )

    for input_name in sorted(value.pedigree.inputs.keys()):

        child_value_id = value.pedigree.inputs[input_name]
        child_value = kiara.data_registry.get_value(child_value_id)

        if child_value.pedigree != ORPHAN:
            create_lineage_graph_modules(
                kiara=kiara,
                value=child_value,
                graph=graph,
                parent=module_id,
                input_field=input_name,
                level=level + 1,
            )
        else:
            input_id = f"value:{child_value.value_id}"
            input_label = f"{input_name} ({child_value.data_type_name})"

            graph.add_node(
                input_id,
                label=input_label,
                node_type="value",
                data_type=child_value.data_type_name,
                data_type_config=child_value.data_type_config,
                level=(level * 2) + 2,
            )
            graph.add_edge(
                module_id,
                input_id,
                id=f"{module_id}:{input_id}",
                field_name=input_name,
                label=f"{input_name} ({child_value.data_type_name})",
            )

    return graph


class ValueLineage(JupyterMixin):
    def __init__(self, kiara: "Kiara", value: Value) -> None:

        self._value: Value = value
        self._kiara: Kiara = kiara
        self._full_graph: Union[None, DiGraph] = None
        self._module_graph: Union[None, DiGraph] = None

    @property
    def value(self) -> Value:
        return self._value

    @property
    def full_graph(self) -> DiGraph:

        if self._full_graph is not None:
            return self._full_graph

        self._full_graph = create_lineage_graph(kiara=self._kiara, value=self._value)
        return self._full_graph

    @property
    def module_graph(self) -> DiGraph:

        if self._module_graph is not None:
            return self._module_graph

        self._module_graph = create_lineage_graph_modules(
            kiara=self._kiara, value=self._value
        ).reverse()
        return self._module_graph

    def as_dict(
        self,
        include_preview: bool = False,
        include_module_info: bool = False,
        ensure_json_serializable: bool = False,
    ) -> Dict[str, Any]:

        data = fill_dict_with_lineage(
            kiara=self._kiara,
            value=self._value,
            include_preview=include_preview,
            include_module_info=include_module_info,
        )

        if ensure_json_serializable:
            data = orjson.loads(orjson.dumps(data))

        return data

    def create_renderable(self, **config: Any) -> RenderableType:

        include_ids: bool = config.get("include_ids", True)
        tree = fill_renderable_lineage_tree(
            kiara=self._kiara, pedigree=self._value.pedigree, include_ids=include_ids
        )
        return tree

    def __rich_console__(
        self, console: Console, options: ConsoleOptions
    ) -> RenderResult:

        yield self.create_renderable()


# kiara\kiara\src\kiara\models\values\matchers.py
# -*- coding: utf-8 -*-
from typing import TYPE_CHECKING, Any, List, Union

from pydantic import Field, field_validator

from kiara.defaults import DEFAULT_DATA_STORE_MARKER, DEFAULT_STORE_MARKER
from kiara.models import KiaraModel
from kiara.models.values.value import Value

if TYPE_CHECKING:
    from kiara.context import Kiara


class ValueMatcher(KiaraModel):
    """An object describing requirements values should satisfy in order to be included in a query result."""

    @classmethod
    def create_matcher(self, **match_options: Any):

        m = ValueMatcher(**match_options)
        return m

    data_types: List[str] = Field(description="The data type.", default_factory=list)
    allow_sub_types: bool = Field(description="Allow subtypes.", default=True)
    min_size: int = Field(description="The minimum size for the dataset.", default=0)
    max_size: Union[None, int] = Field(
        description="The maximum size for the dataset.", default=None
    )
    allow_internal: bool = Field(
        description="Allow internal data types.", default=False
    )
    has_alias: bool = Field(
        description="Value must have at least one alias.", default=True
    )
    alias_matchers: Union[None, List[str]] = Field(
        description="Values must have an alias that matches one of the provided matchers. Assumes 'has_alias' is set to 'True'.",
        default=None,
    )
    in_data_archives: Union[None, List[str]] = Field(
        description="A list of registered names of archives the value must be in. If 'None', all archives will be used.",
        default=None,
    )

    @field_validator("in_data_archives", mode="before")
    @classmethod
    def validate_in_archives(cls, v):

        if v is None:
            return v
        elif isinstance(v, str):
            v = set(v)
        else:
            v = set(v)

        if DEFAULT_STORE_MARKER in v or DEFAULT_DATA_STORE_MARKER in v:
            v.add(DEFAULT_DATA_STORE_MARKER)
            v.add(DEFAULT_STORE_MARKER)

        return list(v)

    @field_validator("alias_matchers")
    @classmethod
    def validate_matchers(cls, v):
        if v is None:
            return v
        elif isinstance(v, str):
            return [v]
        else:
            return list(v)

    def is_match(self, value: Value, kiara: "Kiara") -> bool:

        has_alias = self.has_alias or self.alias_matchers

        match = False
        if self.data_types:
            if not self.allow_sub_types:
                for data_type in self.data_types:
                    if data_type == value.data_type_name:
                        match = True
                        break
            else:
                if value.data_type_name not in kiara.type_registry.get_data_type_names(
                    include_profiles=True
                ):
                    return False
                lineage = kiara.type_registry.get_type_lineage(value.data_type_name)
                for data_type in self.data_types:
                    if data_type in lineage:
                        match = True
                        break
            if not match:
                return False

        if self.min_size:
            if value.value_size < self.min_size:
                return False
        if self.max_size:
            if value.value_size > self.max_size:
                return False

        if not self.allow_internal:
            if kiara.type_registry.is_internal_type(
                data_type_name=value.data_type_name
            ):
                return False

        if has_alias:

            aliases = kiara.alias_registry.find_aliases_for_value_id(
                value_id=value.value_id
            )
            if not aliases:
                return False

            if self.alias_matchers:
                for token in self.alias_matchers:
                    for alias in aliases:
                        if token in alias:
                            return True

                return False

        return True


# kiara\kiara\src\kiara\models\values\value.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import abc
import atexit
import hashlib
import logging
import os
import tempfile
import uuid
from datetime import datetime
from typing import (
    TYPE_CHECKING,
    Any,
    ClassVar,
    Dict,
    Generator,
    Iterable,
    List,
    Literal,
    Mapping,
    MutableMapping,
    Sequence,
    Set,
    Union,
)

import orjson
from humanfriendly import format_size
from multiformats import CID, multihash
from multiformats.multihash import Multihash
from multiformats.varint import BytesLike
from pydantic import BaseModel, ConfigDict, PrivateAttr, model_validator
from pydantic.fields import Field
from rich import box
from rich.console import Group, RenderableType
from rich.panel import Panel
from rich.rule import Rule
from rich.syntax import Syntax
from rich.table import Table

from kiara.defaults import (
    NO_MODULE_TYPE,
    NO_SERIALIZATION_MARKER,
    VOID_KIARA_ID,
    SpecialValue,
)
from kiara.exceptions import DataTypeUnknownException, InvalidValuesException
from kiara.models import KiaraModel
from kiara.models.module.manifest import InputsManifest, Manifest
from kiara.models.python_class import PythonClass
from kiara.models.values import DataTypeCharacteristics, ValueStatus
from kiara.models.values.value_schema import ValueSchema
from kiara.utils import is_jupyter, log_exception
from kiara.utils.dates import get_current_time_incl_timezone
from kiara.utils.hashing import create_cid_digest
from kiara.utils.json import orjson_dumps
from kiara.utils.yaml import StringYAML

log = logging.getLogger("kiara")
yaml = StringYAML()

if TYPE_CHECKING:
    from kiara.context import Kiara
    from kiara.data_types import DataType
    from kiara.interfaces.python_api.models.info import ValueInfo
    from kiara.models.values.lineage import ValueLineage
    from kiara.registries.data import DataRegistry


class SerializedChunks(BaseModel, abc.ABC):

    model_config = ConfigDict(extra="forbid")

    _size_cache: Union[int, None] = PrivateAttr(default=None)
    _hashes_cache: Dict[str, Sequence[CID]] = PrivateAttr(default_factory=dict)

    @abc.abstractmethod
    def get_chunks(
        self, as_files: bool = True, symlink_ok: bool = True
    ) -> Generator[Union[str, "BytesLike"], None, None]:
        """
        Retrieve the chunks belonging to this data instance.

        If 'as_file' is False, return the data as bytes. If set to 'True' store it to an arbitrary location (or use
        an existing one), and return the path to that file. If 'as_file' is a string, write the data (bytes) into
        a new file using the string as path. If 'symlink_ok' is set to True, symlinking an existing file to the value of
        'as_file' is also ok, otherwise copy the content.
        """

    @abc.abstractmethod
    def get_number_of_chunks(self) -> int:
        pass

    @abc.abstractmethod
    def _get_size(self) -> int:
        pass

    @abc.abstractmethod
    def _create_cids(self, hash_codec: str) -> Sequence[CID]:
        pass

    def get_size(self) -> int:

        if self._size_cache is None:
            self._size_cache = self._get_size()
        return self._size_cache

    def get_cids(self, hash_codec: str) -> Sequence[CID]:

        if self._hashes_cache.get(hash_codec, None) is None:
            self._hashes_cache[hash_codec] = self._create_cids(hash_codec=hash_codec)
        return self._hashes_cache[hash_codec]

    def _store_bytes_to_file(
        self, chunks: Iterable[bytes], file: Union[str, None] = None
    ) -> str:
        "Utility method to store bytes to a file."
        if file is None:
            file_desc, file = tempfile.mkstemp()

            def del_temp_file():
                os.remove(file)

            atexit.register(del_temp_file)

        else:
            if os.path.exists(file):
                raise Exception(f"Can't write to file, file exists: {file}")
            file_desc = os.open(file, 0o600)

        with os.fdopen(file_desc, "wb") as tmp:
            for chunk in chunks:
                tmp.write(chunk)

        return file

    def _read_bytes_from_file(self, file: str) -> bytes:

        with open(file, "rb") as f:
            content = f.read()

        return content


class SerializedPreStoreChunks(SerializedChunks):

    codec: str = Field(
        description="The codec used to encode the chunks in this model. Using the [multicodecs](https://github.com/multiformats/multicodec) codec table."
    )

    def _create_cid_from_chunk(self, chunk: bytes, hash_codec: str) -> CID:

        multihash = Multihash(codec=hash_codec)
        hash = multihash.digest(chunk)
        return create_cid_digest(digest=hash, codec=self.codec)

    def _create_cid_from_file(self, file: str, hash_codec: str) -> CID:

        assert hash_codec == "sha2-256"

        hash_func = hashlib.sha256
        file_hash = hash_func()

        CHUNK_SIZE = 65536
        with open(file, "rb") as f:
            fb = f.read(CHUNK_SIZE)
            while len(fb) > 0:
                file_hash.update(fb)
                fb = f.read(CHUNK_SIZE)

        wrapped = multihash.wrap(file_hash.digest(), "sha2-256")
        return create_cid_digest(digest=wrapped, codec=self.codec)


class SerializedBytes(SerializedPreStoreChunks):

    type: Literal["chunk"] = "chunk"
    chunk: bytes = Field(description="A byte-array")

    def get_chunks(
        self, as_files: Union[bool, str, Sequence[str]] = True, symlink_ok: bool = True
    ) -> Generator[Union[str, BytesLike], None, None]:

        if as_files is False:
            yield self.chunk
        else:
            if as_files is True:
                file = None
            elif isinstance(as_files, str):
                file = as_files
            else:
                assert len(as_files) == 1
                file = as_files[0]
            path = self._store_bytes_to_file([self.chunk], file=file)
            yield path

    def get_number_of_chunks(self) -> int:
        return 1

    def _get_size(self) -> int:
        return len(self.chunk)

    def _create_cids(self, hash_codec: str) -> Sequence[CID]:
        return [self._create_cid_from_chunk(self.chunk, hash_codec=hash_codec)]


class SerializedListOfBytes(SerializedPreStoreChunks):

    type: Literal["chunks"] = "chunks"
    chunks: List[bytes] = Field(description="A list of byte arrays.")

    def get_chunks(
        self, as_files: Union[bool, str, Sequence[str]] = True, symlink_ok: bool = True
    ) -> Generator[Union[str, BytesLike], None, None]:
        if as_files is False:
            for chunk in self.chunks:
                yield chunk
        else:
            if as_files is None or as_files is True or isinstance(as_files, str):
                # means we write all the chunks into one file
                file = None if as_files is True else as_files
                path = self._store_bytes_to_file(self.chunks, file=file)
                yield path
            else:
                assert len(as_files) == self.get_number_of_chunks()
                for idx, chunk in enumerate(self.chunks):
                    _file = as_files[idx]
                    path = self._store_bytes_to_file([chunk], file=_file)
                    yield path

    def get_number_of_chunks(self) -> int:
        return len(self.chunks)

    def _get_size(self) -> int:
        size = 0
        for chunk in self.chunks:
            size = size + len(chunk)
        return size

    def _create_cids(self, hash_codec: str) -> Sequence[CID]:
        return [
            self._create_cid_from_chunk(chunk, hash_codec=hash_codec)
            for chunk in self.chunks
        ]


class SerializedFile(SerializedPreStoreChunks):

    type: Literal["file"] = "file"
    file: str = Field(description="A path to a file containing the serialized data.")

    def get_chunks(
        self, as_files: Union[bool, str, Sequence[str]] = True, symlink_ok: bool = True
    ) -> Generator[Union[str, BytesLike], None, None]:

        if as_files is False:
            chunk = self._read_bytes_from_file(self.file)
            yield chunk
        else:
            if as_files is True:
                yield self.file
            else:
                if isinstance(as_files, str):
                    file = as_files
                else:
                    assert len(as_files) == 1
                    file = as_files[0]
                if os.path.exists(file):
                    raise Exception(f"Can't write to file '{file}': file exists.")
                if symlink_ok:
                    os.symlink(self.file, file)
                    yield file
                else:
                    raise NotImplementedError()

    def get_number_of_chunks(self) -> int:
        return 1

    def _get_size(self) -> int:
        return os.path.getsize(os.path.realpath(self.file))

    def _create_cids(self, hash_codec: str) -> Sequence[CID]:
        return [self._create_cid_from_file(self.file, hash_codec=hash_codec)]


class SerializedFiles(SerializedPreStoreChunks):

    type: Literal["files"] = "files"
    files: List[str] = Field(
        description="A list of strings, pointing to files containing parts of the serialized data."
    )

    def get_chunks(
        self, as_files: Union[bool, str, Sequence[str]] = True, symlink_ok: bool = True
    ) -> Generator[Union[str, BytesLike], None, None]:
        raise NotImplementedError()

    def get_number_of_chunks(self) -> int:
        return len(self.files)

    def _get_size(self) -> int:

        size = 0
        for file in self.files:
            size = size + os.path.getsize(os.path.realpath(file))
        return size

    def _create_cids(self, hash_codec: str) -> Sequence[CID]:
        return [
            self._create_cid_from_file(file, hash_codec=hash_codec)
            for file in self.files
        ]


class SerializedInlineJson(SerializedPreStoreChunks):

    type: Literal["inline-json"] = "inline-json"
    inline_data: Any = Field(
        None,
        description="Data that will not be stored externally, but inline in the containing model. This should only contain data types that can be serialized reliably using json (scalars, etc.).",
    )
    _json_cache: Union[bytes, None] = PrivateAttr(default=None)

    def as_json(self) -> bytes:
        assert self.inline_data is not None
        if self._json_cache is None:
            self._json_cache = orjson.dumps(
                self.inline_data,
                option=orjson.OPT_NON_STR_KEYS,
            )
        return self._json_cache

    def get_chunks(
        self, as_files: Union[bool, str, Sequence[str]] = True, symlink_ok: bool = True
    ) -> Generator[Union[str, BytesLike], None, None]:

        if as_files is False:
            yield self.as_json()
        else:
            raise NotImplementedError()

    def get_number_of_chunks(self) -> int:
        return 1

    def _get_size(self) -> int:
        return len(self.as_json())

    def _create_cids(self, hash_codec: str) -> Sequence[CID]:
        return [self._create_cid_from_chunk(self.as_json(), hash_codec=hash_codec)]


class SerializedChunkIDs(SerializedChunks):

    type: Literal["chunk-ids"] = "chunk-ids"
    chunk_id_list: List[str] = Field(
        description="A list of chunk ids, which will be resolved via the attached data registry."
    )
    archive_id: Union[uuid.UUID, None] = Field(
        None, description="The preferred data archive to get the chunks from."
    )
    size: int = Field(description="The size of all chunks combined.")
    _data_registry: "DataRegistry" = PrivateAttr(default=None)

    def get_chunks(
        self, as_files: bool = True, symlink_ok: bool = True
    ) -> Generator[Union[str, BytesLike], None, None]:
        """Retrieve the chunks of this value data.

        If 'as_files' is 'True', it will return strings representing paths to files containing the chunk data. If symlink_ok is also set to 'True', the returning Path could potentially be a symlink, which means the underlying function might not need to copy the file. In this case, you are responsible to not change the contents of the path, ever.

        If 'as_files' is 'False', BytesLike objects will be returned, containing the chunk data bytes directly.

        """

        chunk_ids = self.chunk_id_list
        return self._data_registry.retrieve_chunks(
            chunk_ids=chunk_ids,
            as_files=as_files,
            symlink_ok=symlink_ok,
            archive_id=self.archive_id,
        )

        # return (
        #     self._data_registry.retrieve_chunk(
        #         chunk_id=chunk,
        #         archive_id=self.archive_id,
        #         as_file=as_files,
        #         symlink_ok=symlink_ok,
        #     )
        #     for chunk in self.chunk_id_list
        # )

    def get_number_of_chunks(self) -> int:
        return len(self.chunk_id_list)

    def _get_size(self) -> int:
        return self.size

    def _create_cids(self, hash_codec: str) -> Sequence[CID]:

        result = []
        for chunk_id in self.chunk_id_list:
            cid = CID.decode(chunk_id)
            result.append(cid)

        return result


SERIALIZE_TYPES = {
    "chunk": SerializedBytes,
    "chunks": SerializedListOfBytes,
    "file": SerializedFile,
    "files": SerializedFiles,
    "inline-json": SerializedInlineJson,
    "chunk-ids": SerializedChunkIDs,
}


class SerializationMetadata(KiaraModel):

    _kiara_model_id: ClassVar = "metadata.serialized_data"

    environment: Mapping[str, int] = Field(
        description="Hash(es) for the environments the value was created/serialized.",
        default_factory=dict,
    )
    deserialize: Mapping[str, Manifest] = Field(
        description="Suggested manifest configs to use to de-serialize the data.",
        default_factory=dict,
    )


class SerializedData(KiaraModel):

    data_type: str = Field(
        description="The name of the data type for this serialized value."
    )
    data_type_config: Mapping[str, Any] = Field(
        description="The (optional) config for the data type for this serialized value.",
        default_factory=dict,
    )
    serialization_profile: str = Field(
        description="An identifying name for the serialization method used."
    )
    metadata: SerializationMetadata = Field(
        description="Optional metadata describing aspects of the serialization used.",
        default_factory=SerializationMetadata,
    )

    hash_codec: str = Field(
        description="The codec used to hash the value.", default="sha2-256"
    )
    _cids_cache: Dict[str, Sequence[CID]] = PrivateAttr(default_factory=dict)

    _cached_data_size: Union[int, None] = PrivateAttr(default=None)
    _cached_dag: Union[Dict[str, Sequence[CID]], None] = PrivateAttr(default=None)
    # _cached_cid: Optional[CID] = PrivateAttr(default=None)

    def _retrieve_data_to_hash(self) -> Any:

        return self.dag

    @property
    def data_size(self) -> int:
        if self._cached_data_size is not None:
            return self._cached_data_size

        size = 0
        for k in self.get_keys():
            model = self.get_serialized_data(k)
            size = size + model.get_size()
        self._cached_data_size = size
        return self._cached_data_size

    @abc.abstractmethod
    def get_keys(self) -> Iterable[str]:
        pass

    @abc.abstractmethod
    def get_serialized_data(self, key: str) -> SerializedChunks:
        pass

    def get_cids_for_key(self, key) -> Sequence[CID]:

        if key in self._cids_cache.keys():
            return self._cids_cache[key]

        model = self.get_serialized_data(key)
        self._cids_cache[key] = model.get_cids(hash_codec=self.hash_codec)
        return self._cids_cache[key]

    @property
    def dag(self) -> Mapping[str, Sequence[CID]]:

        if self._cached_dag is not None:
            return self._cached_dag

        dag: Dict[str, Sequence[CID]] = {}
        for key in self.get_keys():
            dag[key] = self.get_cids_for_key(key)

        self._cached_dag = dag
        return self._cached_dag


class SerializationResult(SerializedData):

    _kiara_model_id: ClassVar[str] = "instance.serialization_result"

    data: Dict[
        str,
        Union[
            SerializedBytes,
            SerializedListOfBytes,
            SerializedFile,
            SerializedFiles,
            SerializedInlineJson,
        ],
    ] = Field(
        description="One or several byte arrays representing the serialized state of the value."
    )

    def get_keys(self) -> Iterable[str]:
        return self.data.keys()

    def get_serialized_data(self, key: str) -> SerializedChunks:
        return self.data[key]

    @model_validator(mode="before")
    @classmethod
    def validate_data(cls, values):

        codec = values.get("codec", None)
        if codec is None:
            codec = "sha2-256"
            values["hash_codec"] = codec

        v = values.get("data")
        assert isinstance(v, Mapping)

        result = {}
        for field_name, data in v.items():
            if isinstance(data, SerializedChunks):
                result[field_name] = data
            elif isinstance(data, Mapping):
                s_type = data.get("type", None)
                if not s_type:
                    raise ValueError(
                        f"Invalid serialized data config, missing 'type' key: {data}"
                    )

                if s_type not in SERIALIZE_TYPES.keys():
                    raise ValueError(
                        f"Invalid serialized data type '{s_type}'. Allowed types: {', '.join(SERIALIZE_TYPES.keys())}"
                    )

                assert s_type != "chunk-ids"
                cls = SERIALIZE_TYPES[s_type]
                result[field_name] = cls(**data)

        values["data"] = result
        return values

    def create_renderable(self, **config: Any) -> RenderableType:

        table = Table(show_header=False, box=box.SIMPLE)
        table.add_column("key")
        table.add_column("value")
        table.add_row("data_type", self.data_type)
        _config = Syntax(
            orjson_dumps(self.data_type_config), "json", background_color="default"
        )
        table.add_row("data_type_config", _config)

        data_fields = {}
        for field, model in self.data.items():
            data_fields[field] = {"type": model.type}
        data_json = Syntax(
            orjson_dumps(data_fields), "json", background_color="default"
        )
        table.add_row("data", data_json)
        table.add_row("size", str(self.data_size))
        table.add_row("hash", self.instance_id)

        return table

    def __repr__(self):

        return f"{self.__class__.__name__}(type={self.data_type} size={self.data_size})"

    def __str__(self):
        return self.__repr__()


class PersistedData(SerializedData):

    _kiara_model_id: ClassVar = "instance.persisted_data"

    archive_id: uuid.UUID = Field(
        description="The id of the store that persisted the data."
    )
    chunk_id_map: Mapping[str, SerializedChunkIDs] = Field(
        description="Reference-ids that resolve to the values' serialized chunks."
    )

    def get_keys(self) -> Iterable[str]:
        return self.chunk_id_map.keys()

    def get_serialized_data(self, key: str) -> SerializedChunks:
        return self.chunk_id_map[key]


class ValuePedigree(InputsManifest):

    _kiara_model_id: ClassVar = "instance.value_pedigree"

    kiara_id: uuid.UUID = Field(
        description="The id of the kiara context a value was created in."
    )
    environments: Dict[str, str] = Field(
        description="References to the runtime environment details a value was created in."
    )

    def _retrieve_data_to_hash(self) -> Any:
        return {
            "manifest": self.manifest_cid,
            "inputs": self.inputs_cid,
            "environments": self.environments,
        }

    def __repr__(self):
        return f"ValuePedigree(module_type={self.module_type}, inputs=[{', '.join(self.inputs.keys())}], instance_id={self.instance_id})"

    def __str__(self):
        return self.__repr__()


class DataTypeInfo(KiaraModel):

    _kiara_model_id: ClassVar = "info.data_type_instance"

    data_type_name: str = Field(description="The registered name of this data type.")
    data_type_config: Mapping[str, Any] = Field(
        description="The (optional) configuration for this data type.",
        default_factory=dict,
    )
    characteristics: DataTypeCharacteristics = Field(
        description="Characteristics of this data type."
    )
    data_type_class: PythonClass = Field(
        description="The python class that is associated with this model."
    )
    _data_type_instance: "DataType" = PrivateAttr(default=None)

    @property
    def data_type_instance(self) -> "DataType":

        if self._data_type_instance is not None:
            return self._data_type_instance

        self._data_type_instance = self.data_type_class.get_class()(
            **self.data_type_config
        )
        return self._data_type_instance


class ValueDetails(KiaraModel):
    """A wrapper class that manages and retieves value data and its details."""

    _kiara_model_id: ClassVar = "instance.value_details"

    value_id: uuid.UUID = Field(description="The id of the value.")

    kiara_id: uuid.UUID = Field(
        description="The id of the kiara context this value belongs to."
    )

    value_schema: ValueSchema = Field(
        description="The schema that was used for this Value."
    )

    value_created: datetime = Field(
        description="The time when this value was created.",
        default_factory=get_current_time_incl_timezone,
    )
    value_status: ValueStatus = Field(description="The set/unset status of this value.")
    value_size: int = Field(description="The size of this value, in bytes.")
    value_hash: str = Field(description="The hash of this value.")
    pedigree: ValuePedigree = Field(
        description="Information about the module and inputs that went into creating this value."
    )
    pedigree_output_name: str = Field(
        description="The output name that produced this value (using the manifest inside the pedigree)."
    )
    data_type_info: DataTypeInfo = Field(
        description="Information about the data type this value is made of."
    )

    def _retrieve_id(self) -> str:
        return str(self.value_id)

    def _retrieve_data_to_hash(self) -> Any:
        return {
            "value_type": self.value_schema.type,
            "value_hash": self.value_hash,
            "value_size": self.value_size,
        }

    @property
    def data_type_name(self) -> str:
        return self.data_type_info.data_type_name

    @property
    def data_type_config(self) -> Mapping[str, Any]:
        return self.data_type_info.data_type_config

    @property
    def is_optional(self) -> bool:
        return self.value_schema.optional

    @property
    def is_valid(self) -> bool:
        """Check whether the current value is valid."""
        if self.is_optional:
            return True
        else:
            return self.value_status == ValueStatus.SET

    @property
    def is_set(self) -> bool:
        return self.value_status in [ValueStatus.SET, ValueStatus.DEFAULT]

    @property
    def value_status_string(self) -> str:
        """Print a human readable short description of this values status."""
        if self.value_status == ValueStatus.DEFAULT:
            return "set (default)"
        elif self.value_status == ValueStatus.SET:
            return "set"
        elif self.value_status == ValueStatus.NONE:
            result = "no value"
        elif self.value_status == ValueStatus.NOT_SET:
            result = "not set"
        else:
            raise Exception(
                f"Invalid internal status of value '{self.value_id}'. This is most likely a bug."
            )

        if self.is_optional:
            result = f"{result} (not required)"
        return result

    def __repr__(self):

        return f"{self.__class__.__name__}(id={self.value_id}, type={self.data_type_name}, status={self.value_status.value})"

    def __str__(self):

        return self.__repr__()


class Value(ValueDetails):

    _kiara_model_id: ClassVar = "instance.value"

    _value_data: Any = PrivateAttr(default=SpecialValue.NOT_SET)
    _serialized_data: Union[None, str, SerializedData] = PrivateAttr(default=None)
    _data_retrieved: bool = PrivateAttr(default=False)
    _data_registry: "DataRegistry" = PrivateAttr(default=None)
    # _data_type: "DataType" = PrivateAttr(default=None)
    _is_stored: bool = PrivateAttr(default=False)
    _cached_properties: Union["ValueMap", None] = PrivateAttr(default=None)
    _lineage: Union["ValueLineage", None] = PrivateAttr(default=None)

    environment_hashes: Mapping[str, Mapping[str, str]] = Field(
        description="Hashes for the environments this value was created in."
    )
    # enviroments: Union[Mapping[str, Mapping[str, Any]], None] = Field(
    #     description="Information about the environments this value was created in.",
    #     default=None,
    # )
    property_links: Mapping[str, uuid.UUID] = Field(
        description="Links to values that are properties of this value.",
        default_factory=dict,
    )
    destiny_backlinks: Mapping[uuid.UUID, str] = Field(
        description="Backlinks to values that this value acts as destiny/or property for.",
        default_factory=dict,
    )
    job_id: Union[uuid.UUID, None] = Field(
        description="The id of the job that created this value (if applicable).",
        default=None,
    )

    def add_property(
        self,
        value_id: Union[uuid.UUID, "Value"],
        property_path: str,
        add_origin_to_property_value: bool = True,
    ):

        value = None
        try:
            value_temp = value
            value_id = value_id.value_id  # type: ignore
            value = value_temp
        except Exception:
            # in case a Value object was provided
            pass
        finally:
            del value_temp

        if add_origin_to_property_value:
            if value is None:
                value = self._data_registry.get_value(value=value_id)  # type: ignore

            if value._is_stored:
                raise Exception(
                    f"Can't add property to value '{self.value_id}': referenced value '{value.value_id}' already locked, so it's not possible to add the property backlink (as requested)."
                )

        assert value is not None

        if self._is_stored:
            raise Exception(
                f"Can't add property to value '{self.value_id}': value already locked."
            )

        if property_path in self.property_links.keys():
            raise Exception(
                f"Can't add property to value '{self.value_id}': property '{property_path}' already set."
            )

        self.property_links[property_path] = value_id  # type: ignore

        if add_origin_to_property_value:
            value.add_destiny_details(
                value_id=self.value_id, destiny_alias=property_path
            )

        self._cached_properties = None

    def add_destiny_details(self, value_id: uuid.UUID, destiny_alias: str):

        if self._is_stored:
            raise Exception(
                f"Can't set destiny_refs to value '{self.value_id}': value already locked."
            )

        self.destiny_backlinks[value_id] = destiny_alias  # type: ignore

    @property
    def is_serializable(self) -> bool:

        try:
            if self._serialized_data == NO_SERIALIZATION_MARKER:
                return False
            self.serialized_data
            return True
        except Exception:
            pass

        return False

    # @property
    # def data_type_class(self) -> "PythonClass":
    #     """Return the (Python) type of the underlying 'DataType' subclass."""
    #     return self.data_type_info.data_type_class

    @property
    def serialized_data(self) -> SerializedData:

        # if not self.is_set:
        #     raise Exception(f"Can't retrieve serialized data: value not set.")

        if self._serialized_data is not None:
            if isinstance(self._serialized_data, str):
                raise Exception(
                    f"Data type '{self.data_type_name}' does not support serializing: {self._serialized_data}"
                )

            return self._serialized_data

        self._serialized_data = self._data_registry.retrieve_persisted_value_details(
            self.value_id
        )
        return self._serialized_data

    @property
    def data(self) -> Any:
        if not self.is_initialized:
            raise Exception(
                f"Can't retrieve data for value '{self.value_id}': value not initialized yet. This is most likely a bug."
            )
        try:
            return self._retrieve_data()
        except DataTypeUnknownException as dtue:
            dtue._value = self
            raise dtue

    def _retrieve_data(self) -> Any:

        if self._value_data is not SpecialValue.NOT_SET:
            return self._value_data

        if self.value_status in [ValueStatus.NOT_SET, ValueStatus.NONE]:
            self._value_data = None
            return self._value_data
        elif self.value_status not in [ValueStatus.SET, ValueStatus.DEFAULT]:
            raise Exception(f"Invalid internal state of value '{self.value_id}'.")

        retrieved = self._data_registry.retrieve_value_data(value=self)

        if retrieved is None or isinstance(retrieved, SpecialValue):
            raise Exception(
                f"Can't set value data, invalid data type: {type(retrieved)}"
            )

        self._value_data = retrieved
        self._data_retrieved = True
        return self._value_data

    # def retrieve_load_config(self) -> Optional[LoadConfig]:
    #     return self._data_registry.retrieve_persisted_value_details(
    #         value_id=self.value_id
    #     )

    def __repr__(self):

        return f"{self.__class__.__name__}(id={self.value_id}, type={self.data_type_name}, status={self.value_status.value}, initialized={self.is_initialized} optional={self.value_schema.optional})"

    def _set_registry(self, data_registry: "DataRegistry") -> None:
        self._data_registry = data_registry

    @property
    def is_initialized(self) -> bool:
        result = not self.is_set or self._data_registry is not None
        return result

    @property
    def is_stored(self) -> bool:
        return self._is_stored

    @property
    def data_type(self) -> "DataType":

        return self.data_type_info.data_type_instance

    @property
    def lineage(self) -> "ValueLineage":
        if self._lineage is not None:
            return self._lineage

        from kiara.models.values.lineage import ValueLineage

        self._lineage = ValueLineage(kiara=self._data_registry._kiara, value=self)
        return self._lineage

    @property
    def property_values(self) -> "ValueMap":
        """Return a dictionary of all of this values properties."""

        if self._cached_properties is not None:
            return self._cached_properties

        self._cached_properties = self._data_registry.load_values(self.property_links)
        return self._cached_properties

    @property
    def property_names(self) -> Iterable[str]:
        return self.property_links.keys()

    def get_property_value(self, property_key) -> "Value":

        if property_key not in self.property_links.keys():
            raise Exception(
                f"Value '{self.value_id}' has no property with key '{property_key}."
            )

        return self._data_registry.get_value(self.property_links[property_key])

    def get_property_data(self, property_key: str) -> Any:

        try:
            return self.get_property_value(property_key=property_key).data
        except Exception as e:
            log_exception(e)
            return None

    def get_all_property_data(self, flatten_models: bool = False) -> Mapping[str, Any]:

        result = {k: self.get_property_data(k) for k in self.property_names}
        if not flatten_models:
            return result

        flat = {}
        for k, v in result.items():
            if hasattr(v, "model_dump"):
                flat[k] = v.model_dump()
            elif hasattr(v, "dict"):
                flat[k] = v.dict()
            else:
                flat[k] = v
        return flat

    def lookup_self_aliases(self) -> Set[str]:

        if not self._data_registry:
            raise Exception(
                f"Can't lookup aliases for value '{self.value_id}': data registry not set (yet)."
            )

        return self._data_registry.lookup_aliases(self)

    def create_info(self) -> "ValueInfo":

        if not self._data_registry:
            raise Exception(
                f"Can't create info object for value '{self.value_id}': data registry not set (yet)."
            )

        return self._data_registry.create_value_info(value=self.value_id)

    def create_info_data(self, **config: Any) -> Mapping[str, Any]:

        show_pedigree = config.get("show_pedigree", False)
        show_lineage = config.get("show_lineage", False)
        show_properties = config.get("show_properties", False)
        # show_destinies = config.get("show_destinies", False)
        # show_destiny_backlinks = config.get("show_destiny_backlinks", False)
        # show_data = config.get("show_data_preview", False)
        show_serialized = config.get("show_serialized", False)
        show_env_data_hashes = config.get("show_environment_hashes", False)
        show_env_data = config.get("show_environment_data", False)

        ignore_fields = config.get("ignore_fields", [])

        table: Dict[str, Any] = {}

        if "value_id" not in ignore_fields:
            table["value_id"] = self.value_id
        if "aliases" not in ignore_fields:
            if hasattr(self, "aliases"):
                table["aliases"] = self.aliases  # type: ignore

        if "kiara_id" not in ignore_fields:
            table["kiara_id"] = self.kiara_id

        for k in sorted(self.model_fields.keys()):

            if (
                k
                in [
                    "serialized",
                    "value_id",
                    "aliases",
                    "kiara_id",
                    "environments",
                    "lineage",
                    "environment_hashes",
                ]
                or k in ignore_fields
            ):
                continue

            attr = getattr(self, k)
            if k in ["pedigree_output_name", "pedigree"]:
                continue
            else:
                v = attr

            table[k] = v

        if show_pedigree:
            pedigree = getattr(self, "pedigree")

            table["pedigree"] = pedigree
            if pedigree == ORPHAN:
                pedigree_output_name: Union[Any, None] = None
            else:
                pedigree_output_name = getattr(self, "pedigree_output_name")

            table["pedigree_output_name"] = pedigree_output_name

        if show_lineage:
            table["lineage"] = self.lineage

        if show_serialized:
            serialized = self._data_registry.retrieve_persisted_value_details(
                self.value_id
            )
            table["serialized"] = serialized

        if show_env_data_hashes:
            env_hashes = Syntax(
                orjson_dumps(self.environment_hashes, option=orjson.OPT_INDENT_2),
                "json",
                background_color="default",
            )
            table["environment_hashes"] = env_hashes

        if show_env_data:
            raise NotImplementedError()

        if show_properties:
            if not self.property_links:
                table["properties"] = {}
            else:
                properties = self._data_registry.load_values(self.property_links)
                table["properties"] = properties

        # if hasattr(self, "destiny_links") and show_destinies:
        #     if not self.destiny_links:  # type: ignore
        #         table["destinies"] = {}
        #     else:
        #         destinies = self._data_registry.load_values(self.destiny_links)  # type: ignore
        #         table["destinies"] = destinies
        #
        # if show_destiny_backlinks:
        #     if not self.destiny_backlinks:
        #         table["destiny backlinks"] = {}
        #     else:
        #         destiny_items: List[Any] = []
        #         for v_id, alias in self.destiny_backlinks.items():
        #             destiny_items.append(
        #                 f"[b]Value: [i]{v_id}[/i] (destiny alias: {alias})[/b]"
        #             )
        #             rendered = self._data_registry.pretty_print_data(
        #                 value_id=v_id, **config
        #             )
        #             destiny_items.append(rendered)
        #         table["destiny backlinks"] = destiny_items
        #
        # if show_data:
        #     rendered = self._data_registry.pretty_print_data(
        #         self.value_id, target_type="terminal_renderable"
        #     )
        #     table["data preview"] = rendered

        return table

    def create_renderable(self, **render_config: Any) -> RenderableType:

        from kiara.utils.output import extract_renderable

        show_pedigree = render_config.get("show_pedigree", False)
        show_lineage = render_config.get("show_lineage", False)
        show_properties = render_config.get("show_properties", False)
        show_destinies = render_config.get("show_destinies", False)
        show_destiny_backlinks = render_config.get("show_destiny_backlinks", False)
        show_data = render_config.get("show_data_preview", False)
        show_serialized = render_config.get("show_serialized", False)
        show_env_data_hashes = render_config.get("show_environment_hashes", False)
        show_env_data = render_config.get("show_environment_data", False)

        ignore_fields = render_config.get("ignore_fields", [])

        table = Table(show_header=False, box=box.SIMPLE)
        table.add_column("Key", style="i")
        table.add_column("Value")

        info_data = self.create_info_data(**render_config)

        if "value_id" not in ignore_fields:
            table.add_row("value_id", str(info_data["value_id"]))
        if "aliases" not in ignore_fields:
            if info_data.get("aliases", None):
                aliases_str = ", ".join(info_data["aliases"])  # type: ignore
                table.add_row("aliases", aliases_str)
            # else:
            #     aliases_str = "-- n/a --"
            #     table.add_row("aliases", aliases_str)

        if "kiara_id" not in ignore_fields:
            table.add_row("kiara_id", str(info_data["kiara_id"]))

        table.add_row("", "")
        table.add_row("", Rule())
        for k in sorted(info_data.keys()):

            if (
                k
                in [
                    "serialized",
                    "value_id",
                    "aliases",
                    "kiara_id",
                    "lineage",
                    "properties",
                    "environments",
                    "environment_hashes",
                ]
                or k in ignore_fields
            ):
                continue

            attr = info_data[k]
            if k in ["pedigree_output_name", "pedigree"]:
                continue

            elif k == "value_status":
                v: RenderableType = f"[i]-- {attr.value} --[/i]"
            elif k == "value_size":
                v = format_size(attr)
            else:
                v = extract_renderable(attr)

            table.add_row(k, v)

        if (
            show_pedigree
            or show_lineage
            or show_serialized
            or show_properties
            or show_destinies
            or show_destiny_backlinks
            or show_env_data_hashes
            or show_env_data
        ):
            table.add_row("", "")
            table.add_row("", Rule())
            table.add_row("", "")

        if show_pedigree:
            pedigree = info_data["pedigree"]

            if pedigree == ORPHAN:
                v = "[i]-- external data --[/i]"
                pedigree_output_name: Union[Any, None] = None
            else:
                v = extract_renderable(pedigree)
                pedigree_output_name = info_data["pedigree_output_name"]

            row = ["pedigree", v]
            table.add_row(*row)
            if pedigree_output_name:
                row = ["pedigree_output_name", pedigree_output_name]
                table.add_row(*row)

        if show_lineage:
            table.add_row(
                "lineage", info_data["lineage"].create_renderable(include_ids=True)
            )

        if show_serialized:
            serialized = info_data["serialized"]
            table.add_row("serialized", serialized.create_renderable())

        if show_env_data_hashes:
            env_hashes = Syntax(
                orjson_dumps(
                    info_data["environment_hashes"], option=orjson.OPT_INDENT_2
                ),
                "json",
                background_color="default",
            )
            table.add_row("environment_hashes", env_hashes)

        if show_env_data:
            raise NotImplementedError()

        if show_properties:
            if not info_data["properties"]:
                table.add_row("properties", "{}")
            else:
                properties = info_data["properties"]
                pr = properties.create_renderable(show_header=False)
                table.add_row("properties", pr)

        if hasattr(self, "destiny_links") and show_destinies:
            if not self.destiny_links:  # type: ignore
                table.add_row("destinies", "{}")
            else:
                destinies = self._data_registry.load_values(self.destiny_links)  # type: ignore
                dr = destinies.create_renderable(show_header=False)
                table.add_row("destinies", dr)

        if show_destiny_backlinks:
            if not self.destiny_backlinks:
                table.add_row("destiny backlinks", "{}")
            else:
                destiny_items: List[Any] = []
                for v_id, alias in self.destiny_backlinks.items():
                    destiny_items.append(Rule())
                    destiny_items.append(
                        f"[b]Value: [i]{v_id}[/i] (destiny alias: {alias})[/b]"
                    )
                    rendered = self._data_registry.pretty_print_data(
                        value_id=v_id, **render_config
                    )
                    destiny_items.append(rendered)
                table.add_row("destiny backlinks", Group(*destiny_items))

        if show_data:
            rendered = self._data_registry.pretty_print_data(
                self.value_id, target_type="terminal_renderable"
            )
            table.add_row("", "")
            table.add_row("", Rule())
            table.add_row("data preview", rendered)

        return table


class UnloadableData(KiaraModel):
    """
    A special 'marker' model, indicating that the data of value can't be loaded.

    In most cases, the reason this happens is because the current kiara context is missing some value types and/or modules.
    """

    _kiara_model_id: ClassVar = "instance.unloadable_data"

    value: Value = Field(description="A reference to the value.")

    def _retrieve_id(self) -> str:
        return self.value.instance_id

    def _retrieve_data_to_hash(self) -> Any:
        return self.value.value_id.bytes


class ValueMap(KiaraModel, MutableMapping[str, Value]):  # type: ignore

    values_schema: Dict[str, ValueSchema] = Field(
        description="The schemas for all the values in this set."
    )

    @property
    def field_names(self) -> Iterable[str]:
        return sorted(self.values_schema.keys())

    @abc.abstractmethod
    def get_value_obj(self, field_name: str) -> Value:
        pass

    @property
    def all_items_valid(self) -> bool:
        for field_name in self.values_schema.keys():
            item = self.get_value_obj(field_name)
            if not item.is_valid:
                return False
        return True

    def _retrieve_data_to_hash(self) -> Any:
        return {
            k: self.get_value_obj(k).instance_cid for k in self.values_schema.keys()
        }

    def check_invalid(self) -> Dict[str, str]:
        """Check whether the value set is invalid, if it is, return a description of what's wrong."""
        invalid: Dict[str, str] = {}
        for field_name in self.values_schema.keys():

            item = self.get_value_obj(field_name)
            field_schema = self.values_schema[field_name]
            if not field_schema.optional:
                msg: Union[str, None] = None
                if not item.value_status == ValueStatus.SET:

                    item_schema = self.values_schema[field_name]
                    if item_schema.is_required():

                        if not item.is_set:
                            msg = "not set"
                        elif item.value_status == ValueStatus.NONE:
                            msg = "no value"
                if msg:
                    invalid[field_name] = msg

        return invalid

    def get_value_data_for_fields(
        self, *field_names: str, raise_exception_when_unset: bool = False
    ) -> Dict[str, Any]:
        """
        Return the data for a one or several fields of this ValueMap.

        If a value is unset, by default 'None' is returned for it. Unless 'raise_exception_when_unset' is set to 'True',
        in which case an Exception will be raised (obviously).
        """
        if raise_exception_when_unset:
            unset: List[str] = []
            for k in field_names:
                v = self.get_value_obj(k)
                if not v.is_set:
                    if raise_exception_when_unset:
                        unset.append(k)
            if unset:
                raise Exception(
                    f"Can't get data for fields, one or several of the requested fields are not set yet: {', '.join(unset)}."
                )

        result: Dict[str, Any] = {}
        for k in field_names:
            v = self.get_value_obj(k)
            if not v.is_set:
                result[k] = None
            else:
                result[k] = v.data
        return result

    def get_value_data(
        self, field_name: str, raise_exception_when_unset: bool = False
    ) -> Any:
        return self.get_value_data_for_fields(
            field_name, raise_exception_when_unset=raise_exception_when_unset
        )[field_name]

    def get_all_value_ids(self) -> Dict[str, uuid.UUID]:
        return {k: self.get_value_obj(k).value_id for k in self.field_names}

    def get_all_value_data(
        self, raise_exception_when_unset: bool = False
    ) -> Dict[str, Any]:
        return self.get_value_data_for_fields(
            *self.field_names,
            raise_exception_when_unset=raise_exception_when_unset,
        )

    def set_values(self, **values) -> None:

        for k, v in values.items():
            self.set_value(k, v)

    def set_value(self, field_name: str, data: Any) -> None:
        raise Exception(
            f"The value set implementation '{self.__class__.__name__}' is read-only, and does not support the setting or changing of values."
        )

    def __getitem__(self, item: str) -> Value:

        return self.get_value_obj(item)

    def __setitem__(self, key: str, value):

        raise NotImplementedError()
        # self.set_value(key, value)

    def __delitem__(self, key: str):

        raise Exception(f"Removing items not supported: {key}")

    def __iter__(self):
        return iter(self.field_names)

    def __len__(self):
        return len(list(self.values_schema))

    def __repr__(self):
        return f"{self.__class__.__name__}(field_names={self.field_names})"

    def __str__(self):
        return self.__repr__()

    def create_invalid_renderable(self, **config) -> Union[RenderableType, None]:

        inv = self.check_invalid()
        if not inv:
            return None

        table = Table(show_header=False, box=box.SIMPLE)
        table.add_column("field name", style="i")
        table.add_column("details", style="b red")

        for field, err in inv.items():
            table.add_row(field, err)

        return table

    def create_renderable(self, **config: Any) -> RenderableType:

        in_panel = config.get("in_panel", None)
        if in_panel is None:
            if is_jupyter():
                in_panel = True
            else:
                in_panel = False

        render_value_data = config.get("render_value_data", True)
        field_title = config.get("field_title", "field")
        value_title = config.get("value_title", "value")
        show_header = config.get("show_header", True)
        show_type = config.get("show_data_type", False)

        table = Table(show_lines=False, show_header=show_header, box=box.SIMPLE)
        table.add_column(field_title, style="b")
        if show_type:
            table.add_column("data_type")
        table.add_column(value_title, style="i")

        for field_name in self.field_names:

            value = self.get_value_obj(field_name=field_name)
            if render_value_data:
                rendered = value._data_registry.pretty_print_data(
                    value_id=value.value_id, target_type="terminal_renderable", **config
                )
            else:
                rendered = value.create_renderable(**config)

            if show_type:
                table.add_row(field_name, value.value_schema.type, rendered)
            else:
                table.add_row(field_name, rendered)

        if in_panel:
            return Panel(table)
        else:
            return table


class ValueMapReadOnly(ValueMap):  # type: ignore

    _kiara_model_id: ClassVar = "instance.value_map.readonly"

    @classmethod
    def create_from_ids(cls, data_registry: "DataRegistry", **value_ids: uuid.UUID):

        values = {k: data_registry.get_value(v) for k, v in value_ids.items()}
        values_schema = {k: v.value_schema for k, v in values.items()}
        return ValueMapReadOnly(value_items=values, values_schema=values_schema)

    @classmethod
    def create_from_values(cls, **values: Value) -> "ValueMapReadOnly":

        values_schema = {k: v.value_schema for k, v in values.items()}
        return ValueMapReadOnly(value_items=values, values_schema=values_schema)

    value_items: Dict[str, Value] = Field(
        description="The values contained in this set."
    )

    def get_value_obj(self, field_name: str) -> Value:

        if field_name not in self.value_items.keys():
            raise KeyError(
                f"Field '{field_name}' not available in value set. Available fields: {', '.join(self.field_names)}"
            )
        return self.value_items[field_name]


class ValueMapWritable(ValueMap):  # type: ignore

    _kiara_model_id: ClassVar = "instance.value_map.writeable"

    @classmethod
    def create_from_schema(
        cls,
        kiara: "Kiara",
        schema: Mapping[str, ValueSchema],
        pedigree: ValuePedigree,
        unique_value_ids: bool = False,
    ) -> "ValueMapWritable":

        v = ValueMapWritable(
            values_schema=dict(schema),
            pedigree=pedigree,
            unique_value_ids=unique_value_ids,
        )
        v._kiara = kiara
        v._data_registry = kiara.data_registry
        return v

    value_items: Dict[str, Value] = Field(
        description="The values contained in this set.", default_factory=dict
    )
    pedigree: ValuePedigree = Field(
        description="The pedigree to add to all of the result values."
    )
    unique_value_ids: bool = Field(
        description="Whether this value map always creates new value(id)s, even when a dataset with matching hash is found.",
        default=True,
    )

    _values_uncommitted: Dict[str, Any] = PrivateAttr(default_factory=dict)
    _kiara: "Kiara" = PrivateAttr(default=None)
    _data_registry: "DataRegistry" = PrivateAttr(default=None)
    _auto_commit: bool = PrivateAttr(default=True)

    def get_value_obj(self, field_name: str) -> Value:
        """
        Retrieve the value object for the specified field.

        This class only creates the actual value object the first time it is requested, because there is a potential
        cost to assembling it, and it might not be needed ever.
        """
        if field_name not in self.values_schema.keys():
            raise Exception(
                f"Can't set data for field '{field_name}': field not valid, valid field names: {', '.join(self.field_names)}."
            )

        if field_name in self.value_items.keys():
            return self.value_items[field_name]
        elif field_name not in self._values_uncommitted.keys():
            raise Exception(
                f"Can't retrieve value for field '{field_name}': value not set (yet)."
            )

        schema = self.values_schema[field_name]
        value_data = self._values_uncommitted[field_name]
        if isinstance(value_data, Value):
            value = value_data
        elif isinstance(value_data, uuid.UUID):
            value = self._data_registry.get_value(value_data)
        else:
            value = self._data_registry.register_data(
                data=value_data,
                schema=schema,
                pedigree=self.pedigree,
                pedigree_output_name=field_name,
                reuse_existing=not self.unique_value_ids,
            )

        self._values_uncommitted.pop(field_name)
        self.value_items[field_name] = value
        return self.value_items[field_name]

    def sync_values(self):

        for field_name in self.field_names:
            self.get_value_obj(field_name)

        invalid = self.check_invalid()
        if invalid:
            e = InvalidValuesException(invalid_values=invalid)
            try:
                raise e
            except Exception:
                # this is silly, I know
                log_exception(e)
                raise e

    def set_value(self, field_name: str, data: Any) -> None:
        """Set the value for the specified field."""
        if field_name not in self.field_names:
            raise Exception(
                f"Can't set data for field '{field_name}': field not valid, valid field names: {', '.join(self.field_names)}."
            )
        if self.value_items.get(field_name, False):
            raise Exception(
                f"Can't set data for field '{field_name}': field already committed."
            )
        if self._values_uncommitted.get(field_name, None) is not None:
            raise Exception(
                f"Can't set data for field '{field_name}': field already set."
            )

        self._values_uncommitted[field_name] = data
        if self._auto_commit:
            self.get_value_obj(field_name=field_name)


ValuePedigree.model_rebuild()
ORPHAN = ValuePedigree(
    kiara_id=VOID_KIARA_ID,
    environments={},
    module_type=NO_MODULE_TYPE,
    inputs={},
    is_resolved=True,
)
# GENESIS_PEDIGREE = None


# kiara\kiara\src\kiara\models\values\value_schema.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

from typing import Any, ClassVar, Dict

from pydantic import ConfigDict, Field, field_serializer, field_validator

from kiara.defaults import SpecialValue
from kiara.models import KiaraModel
from kiara.models.documentation import DocumentationMetadataModel


class ValueSchema(KiaraModel):
    """
    The schema of a value.

    The schema contains the [ValueTypeOrm][kiara.data.values.ValueTypeOrm] of a value, as well as an optional default that
    will be used if no user input was given (yet) for a value.

    For more complex container data_types like array, tables, unions etc, data_types can also be configured with values from the ``type_config`` field.
    """

    _kiara_model_id: ClassVar = "instance.value_schema"
    model_config = ConfigDict(use_enum_values=True)

    type: str = Field(description="The type of the value.")
    type_config: Dict[str, Any] = Field(
        description="Configuration for the type, in case it's complex.",
        default_factory=dict,
    )
    default: Any = Field(description="A default value.", default=SpecialValue.NOT_SET)

    optional: bool = Field(
        description="Whether this value is required (True), or whether 'None' value is allowed (False).",
        default=False,
    )
    is_constant: bool = Field(
        description="Whether the value is a constant.", default=False
    )

    doc: DocumentationMetadataModel = Field(
        default_factory=DocumentationMetadataModel,
        description="A description for the value of this input field.",
    )

    @field_serializer("default")
    def serialize_default(self, value):
        if value in [SpecialValue.NOT_SET, SpecialValue.NO_VALUE]:
            return None
        elif callable(value):
            return value()
        else:
            return value

    @field_validator("doc", mode="before")
    @classmethod
    def validate_doc(cls, value):
        doc = DocumentationMetadataModel.create(value)
        return doc

    def _retrieve_data_to_hash(self) -> Any:

        return {"type": self.type, "type_config": self.type_config}

    def is_required(self):

        if self.optional:
            return False
        else:
            if self.default in [None, SpecialValue.NOT_SET, SpecialValue.NO_VALUE]:
                return True
            else:
                return False

    # def validate_types(self, kiara: "Kiara"):
    #
    #     if self.type not in kiara.value_type_names:
    #         raise ValueError(
    #             f"Invalid value type '{self.type}', available data_types: {kiara.value_type_names}"
    #         )

    def __eq__(self, other):

        if not isinstance(other, ValueSchema):
            return False

        return (self.type, self.default) == (other.type, other.default)

    def __hash__(self):

        return hash((self.type, self.default))

    def __repr__(self):

        return f"ValueSchema(type={self.type}, default={self.default}, optional={self.optional})"

    def __str__(self):

        return self.__repr__()


# kiara\kiara\src\kiara\models\values\__init__.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

from enum import Enum

from pydantic import BaseModel, Field


class ValueStatus(Enum):

    UNKNONW = "unknown"
    NOT_SET = "not set"
    NONE = "none"
    DEFAULT = "default"
    SET = "set"


class DataTypeCharacteristics(BaseModel):

    is_scalar: bool = Field(
        description="Whether the data desribed by this data type behaves like a skalar.",
        default=False,
    )
    is_json_serializable: bool = Field(
        description="Whether the data can be serialized to json without information loss.",
        default=False,
    )


DEFAULT_SCALAR_DATATYPE_CHARACTERISTICS = DataTypeCharacteristics(
    is_scalar=True, is_json_serializable=True
)


# kiara\kiara\src\kiara\models\values\value_metadata\__init__.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import abc
from typing import (
    TYPE_CHECKING,
    Any,
    Dict,
    Iterable,
    Union,
)

from kiara.models import KiaraModel

# from kiara.models.info import TypeInfo

if TYPE_CHECKING:
    from kiara.models.values.value import Value


class ValueMetadata(KiaraModel):
    @classmethod
    @abc.abstractmethod
    def retrieve_supported_data_types(cls) -> Iterable[str]:
        pass

    @classmethod
    @abc.abstractmethod
    def create_value_metadata(
        cls, value: "Value"
    ) -> Union["ValueMetadata", Dict[str, Any]]:
        pass

    # @property
    # def metadata_key(self) -> str:
    #     return self._metadata_key  # type: ignore  # this is added by the kiara class loading functionality

    def _retrieve_id(self) -> str:
        return self._metadata_key  # type: ignore

    def _retrieve_data_to_hash(self) -> Any:
        return {"metadata": self.model_dump(), "schema": self.schema_json()}


# kiara\kiara\src\kiara\models\values\value_metadata\included_metadata_types\__init__.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

from typing import TYPE_CHECKING, ClassVar, Iterable

from pydantic import Field

from kiara.models.filesystem import KiaraFile, KiaraFileBundle
from kiara.models.python_class import PythonClass
from kiara.models.values.value_metadata import ValueMetadata

if TYPE_CHECKING:
    from kiara.models.values.value import Value


class PythonClassMetadata(ValueMetadata):
    """Python class and module information."""

    _metadata_key: ClassVar[str] = "python_class"
    _kiara_model_id: ClassVar = "metadata.python_class"

    @classmethod
    def retrieve_supported_data_types(cls) -> Iterable[str]:
        return ["any"]

    @classmethod
    def create_value_metadata(cls, value: "Value") -> "PythonClassMetadata":

        return PythonClassMetadata(
            python_class=PythonClass.from_class(value.data.__class__)
        )

    # metadata_key: Literal["python_class"]
    python_class: PythonClass = Field(
        description="Details about the Python class that backs this value."
    )


class FileMetadata(ValueMetadata):
    """File stats."""

    _metadata_key: ClassVar[str] = "file"
    _kiara_model_id: ClassVar = "metadata.file"

    @classmethod
    def retrieve_supported_data_types(cls) -> Iterable[str]:
        return ["file"]

    @classmethod
    def create_value_metadata(cls, value: "Value") -> "FileMetadata":

        return FileMetadata(file=value.data)

    file: KiaraFile = Field(description="The file-specific metadata.")


class FileBundleMetadata(ValueMetadata):
    """File bundle stats."""

    _metadata_key: ClassVar[str] = "file_bundle"
    _kiara_model_id: ClassVar = "metadata.file_bundle"

    @classmethod
    def retrieve_supported_data_types(cls) -> Iterable[str]:
        return ["file_bundle"]

    @classmethod
    def create_value_metadata(cls, value: "Value") -> "FileBundleMetadata":

        return FileBundleMetadata(file_bundle=value.data)

    file_bundle: KiaraFileBundle = Field(description="The file-specific metadata.")


# kiara\kiara\src\kiara\modules\__init__.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import abc
import inspect
import uuid
from abc import abstractmethod
from typing import (
    TYPE_CHECKING,
    Any,
    Dict,
    Generic,
    Iterable,
    Mapping,
    Type,
    TypeVar,
    Union,
)

import structlog
from multiformats import CID
from pydantic import BaseModel, ConfigDict, Field, ValidationError
from rich.console import RenderableType

from kiara.exceptions import (
    InvalidValuesException,
    KiaraException,
    KiaraModuleConfigException,
    KiaraProcessingException,
)
from kiara.models.documentation import DocumentationMetadataModel
from kiara.models.module import KiaraModuleConfig
from kiara.models.module.jobs import JobLog
from kiara.models.values.value_schema import ValueSchema
from kiara.utils import is_debug, is_develop, log_exception
from kiara.utils.hashing import compute_cid
from kiara.utils.values import (
    augment_values,
    create_schema_dict,
    overlay_constants_and_defaults,
)
from kiara.utils.yaml import StringYAML

if TYPE_CHECKING:
    from kiara.context import Kiara
    from kiara.models.module.manifest import Manifest
    from kiara.models.values.value import ValueMap
    from kiara.operations import Operation


yaml = StringYAML()

KIARA_CONFIG = TypeVar("KIARA_CONFIG", bound=KiaraModuleConfig)

ValueMapSchema = Mapping[str, Union[ValueSchema, Mapping[str, Any]]]
ValueSetSchema = Mapping[
    str, Union[ValueSchema, Mapping[str, Any]]
]  # TODO: remove once all references to this are gone (legacy)

log = structlog.getLogger()


class InputOutputObject(abc.ABC):
    """
    Abstract base class for classes that define inputs and outputs schemas.

    Both the 'create_inputs_schema` and `creawte_outputs_schema` methods implemented by child classes return a description of the input schema of this module.

    If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional):

    ```
        {
          "[input_field_name]: {
              "type": "[type]",
              "doc*": "[a description of this input]",
              "optional*': [boolean whether this input is optional or required (defaults to 'False')]
          "[other_input_field_name]: {
              "type: ...
              ...
          }
              ```
    """

    def __init__(
        self,
        alias: str,
        config: Union[KiaraModuleConfig, None] = None,
        allow_empty_inputs_schema: bool = False,
        allow_empty_outputs_schema: bool = False,
    ):

        self._alias: str = alias
        self._inputs_schema: Mapping[str, ValueSchema] = None  # type: ignore
        self._full_inputs_schema: Mapping[str, ValueSchema] = None  # type: ignore
        self._outputs_schema: Mapping[str, ValueSchema] = None  # type: ignore
        self._constants: Mapping[str, ValueSchema] = None  # type: ignore

        if config is None:
            config = KiaraModuleConfig()
        self._config: KiaraModuleConfig = config

        self._allow_empty_inputs: bool = allow_empty_inputs_schema
        self._allow_empty_outputs: bool = allow_empty_outputs_schema

    @property
    def alias(self) -> str:
        return self._alias

    def input_required(self, input_name: str):

        if input_name not in self._inputs_schema.keys():
            raise Exception(
                f"No input '{input_name}', available inputs: {', '.join(self._inputs_schema)}"
            )

        if not self._inputs_schema[input_name].is_required():
            return False

        if input_name in self.constants.keys():
            return False
        else:
            return True

    @abstractmethod
    def create_inputs_schema(
        self,
    ) -> ValueMapSchema:
        """Return the schema for this types' inputs."""

    @abstractmethod
    def create_outputs_schema(
        self,
    ) -> ValueMapSchema:
        """Return the schema for this types' outputs."""

    @property
    def inputs_schema(self) -> Mapping[str, ValueSchema]:
        """The input schema for this module."""
        if self._inputs_schema is None:
            self._create_inputs_schema()

        return self._inputs_schema  # type: ignore

    @property
    def full_inputs_schema(self) -> Mapping[str, ValueSchema]:

        if self._full_inputs_schema is not None:
            return self._full_inputs_schema

        self._full_inputs_schema = dict(self.inputs_schema)
        self._full_inputs_schema.update(self._constants)
        return self._full_inputs_schema

    @property
    def constants(self) -> Mapping[str, ValueSchema]:

        if self._constants is None:
            self._create_inputs_schema()
        return self._constants  # type: ignore

    def _create_inputs_schema(self) -> None:
        """
        Assemble the inputs schema and assign it to the approriate instance attributes.

        DEV NOTE: if anything in this method is changed, also change the method of the AutoInputsKiaraModule
        in the kiara_pluginc.core_types package, since it's a carbon copy if this, except for a small change.
        """
        try:
            _input_schemas_data = self.create_inputs_schema()

            if _input_schemas_data is None:
                raise Exception(
                    f"Invalid inputs implementation for '{self.alias}': no inputs schema"
                )

            if not _input_schemas_data and not self._allow_empty_inputs:

                raise Exception(
                    f"Invalid inputs implementation for '{self.alias}': empty inputs schema"
                )
            try:
                _input_schemas = create_schema_dict(schema_config=_input_schemas_data)

            except Exception as e:
                raise Exception(f"Can't create input schemas for '{self.alias}': {e}")

            defaults = self._config.defaults
            constants = self._config.constants

            for k, v in defaults.items():
                if k not in _input_schemas.keys():
                    raise Exception(
                        f"Can't create inputs for '{self.alias}', invalid default field name '{k}'. Available field names: '{', '.join(_input_schemas.keys())}'"  # type: ignore
                    )

            for k, v in constants.items():
                if k not in _input_schemas.keys():
                    raise Exception(
                        f"Can't create inputs for '{self.alias}', invalid constant field name '{k}'. Available field names: '{', '.join(_input_schemas.keys())}'"  # type: ignore
                    )

            self._inputs_schema, self._constants = overlay_constants_and_defaults(
                _input_schemas, defaults=defaults, constants=constants
            )

        except Exception as e:
            raise KiaraException(f"Can't create input schemas for instance of '{self.alias}'.", parent=e)  # type: ignore

    @property
    def outputs_schema(self) -> Mapping[str, ValueSchema]:
        """The output schema for this module."""
        if self._outputs_schema is not None:
            return self._outputs_schema

        try:
            _output_schema = self.create_outputs_schema()

            if _output_schema is None:
                raise Exception(
                    f"Invalid outputs implementation for '{self.alias}': no outputs schema"
                )

            if not _output_schema and not self._allow_empty_outputs:
                raise Exception(
                    f"Invalid outputs implementation for '{self.alias}': empty outputs schema"
                )

            try:
                self._outputs_schema = create_schema_dict(schema_config=_output_schema)
            except Exception as e:
                raise Exception(
                    f"Can't create output schemas for module {self.alias}: {e}"
                )

            return self._outputs_schema
        except Exception as e:
            if is_debug():
                import traceback

                traceback.print_exc()
            raise KiaraException(
                f"Can't create output schemas for instance of module '{self.alias}': {e}",
                parent=e,
            )

    @property
    def input_names(self) -> Iterable[str]:
        """A list of input field names for this module."""
        return self.inputs_schema.keys()

    @property
    def output_names(self) -> Iterable[str]:
        """A list of output field names for this module."""
        return self.outputs_schema.keys()

    def augment_module_inputs(self, inputs: Mapping[str, Any]) -> Dict[str, Any]:

        augmented = augment_values(
            values=inputs, schemas=self.inputs_schema, constants=self.constants
        )

        return augmented

    # def augment_outputs(self, outputs: Mapping[str, Any]) -> Dict[str, Any]:
    #     return augment_values(values=outputs, schemas=self.outputs_schema)


class ModuleCharacteristics(BaseModel):
    model_config = ConfigDict(frozen=True)

    is_idempotent: bool = Field(
        description="Whether this module is idempotent (aka always produces the same output with the same inputs.",
        default=True,
    )
    is_internal: bool = Field(
        description="Hint for frontends whether this module is used predominantly internally, and users won't need to know of its existence.",
        default=False,
    )
    unique_result_values: bool = Field(
        description="Don't re-use existing values for outputs that have matching hashes in the data store.",
        default=True,
    )


DEFAULT_IDEMPOTENT_MODULE_CHARACTERISTICS = ModuleCharacteristics()
DEFAULT_NO_IDEMPOTENT_MODULE_CHARACTERISTICS = ModuleCharacteristics(
    is_idempotent=False
)
DEFAULT_IDEMPOTENT_INTERNAL_MODULE_CHARACTERISTICS = ModuleCharacteristics(
    is_internal=True
)


class KiaraModule(InputOutputObject, Generic[KIARA_CONFIG]):
    """
    The base class that every custom module in *Kiara* needs to inherit from.

    The core of every ``KiaraModule`` is a ``process`` method, which should be a 'pure',
     idempotent function that creates one or several output values from the given input(s), and its purpose is to transfor
     a set of inputs into a set of outputs.

     Every module can be configured. The module configuration schema can differ, but every one such configuration needs to
     subclass the [KiaraModuleConfig][kiara.module_config.KiaraModuleConfig] class and set as the value to the
     ``_config_cls`` attribute of the module class. This is useful, because it allows for some modules to serve a much
     larger variety of use-cases than non-configurable modules would be, which would mean more code duplication because
     of very simlilar, but slightly different module data_types.

     Each module class (type) has a unique -- within a *kiara* context -- module type id which can be accessed via the
     ``_module_type_name`` class attribute.

    Examples:
    --------
        A simple example would be an 'addition' module, with ``a`` and ``b`` configured as inputs, and ``z`` as the output field name.

        An implementing class would look something like this:

    Todo:
    ----

    Arguments:
    ---------
        module_config: the configuation for this module
    """

    # TODO: not quite sure about this generic type here, mypy doesn't seem to like it
    _config_cls: Type[KIARA_CONFIG] = KiaraModuleConfig  # type: ignore

    @classmethod
    def is_pipeline(cls) -> bool:
        """Check whether this module type is a pipeline, or not."""
        return False

    @classmethod
    def _calculate_module_cid(
        cls, module_type_config: Union[Mapping[str, Any], KIARA_CONFIG]
    ) -> CID:

        if isinstance(module_type_config, Mapping):
            module_type_config = cls._resolve_module_config(**module_type_config)

        obj = {
            "module_type": cls._module_type_name,  # type: ignore
            "module_type_config": module_type_config.instance_cid,
        }

        _, cid = compute_cid(data=obj)
        return cid

    @classmethod
    def _resolve_module_config(cls, **config: Any) -> KIARA_CONFIG:

        _config: KIARA_CONFIG = cls._config_cls(**config)

        return _config

    def __init__(
        self,
        module_config: Union[None, KIARA_CONFIG, Mapping[str, Any]] = None,
    ):
        self._id: uuid.UUID = uuid.uuid4()

        if isinstance(module_config, KiaraModuleConfig):
            self._config: KIARA_CONFIG = module_config  # type: ignore
        elif module_config is None:
            self._config = self.__class__._config_cls()
        elif isinstance(module_config, Mapping):
            try:
                self._config = self.__class__._config_cls(**module_config)
            except ValidationError as ve:
                raise KiaraModuleConfigException(
                    f"Error creating module '{id}'. {ve}",
                    self.__class__,
                    module_config,
                    ve,
                )
        else:
            raise TypeError(f"Invalid type for module config: {type(module_config)}")

        self._module_cid: Union[CID, None] = None
        self._characteristics: Union[ModuleCharacteristics, None] = None

        self._cached_doc: Union[None, DocumentationMetadataModel] = None
        super().__init__(alias=self.__class__._module_type_name, config=self._config)  # type: ignore

        self._operation: Union[Operation, None] = None
        # self._merged_input_schemas: typing.Mapping[str, ValueSchema] = None  # type: ignore
        self._manifest_cache: Union[None, Manifest] = None

    @property
    def manifest(self) -> "Manifest":
        if self._manifest_cache is None:
            from kiara.models.module.manifest import Manifest

            self._manifest_cache = Manifest(
                module_type=self.module_type_name,
                module_config=self.config.model_dump(),
                is_resolved=True,
            )
        return self._manifest_cache

    @property
    def doc(self) -> "DocumentationMetadataModel":

        if self._cached_doc is None:
            if hasattr(self, "get_operation_doc"):
                doc = self.get_operation_doc()
                self._cached_doc = DocumentationMetadataModel.create(doc)
            else:
                self._cached_doc = DocumentationMetadataModel.from_class_doc(
                    self.__class__
                )
        return self._cached_doc

    @property
    def module_id(self) -> uuid.UUID:
        """The id of this module."""
        return self._id

    @property
    def module_type_name(self) -> str:
        if not self._module_type_name:  # type: ignore
            raise Exception(
                f"Module class '{self.__class__.__name__}' does not have a '_module_type_name' attribute. This is a bug."
            )
        return self._module_type_name  # type: ignore

    @property
    def config(self) -> KIARA_CONFIG:
        """
        Retrieve the configuration object for this module.

        Returns:
        -------
            the module-class-specific config object
        """
        return self._config

    @property
    def module_instance_cid(self) -> CID:

        if self._module_cid is None:
            self._module_cid = self.__class__._calculate_module_cid(self._config)
        return self._module_cid

    @property
    def characteristics(self) -> ModuleCharacteristics:
        if self._characteristics is not None:
            return self._characteristics

        self._characteristics = self._retrieve_module_characteristics()
        return self._characteristics

    def _retrieve_module_characteristics(self) -> ModuleCharacteristics:

        return DEFAULT_IDEMPOTENT_MODULE_CHARACTERISTICS

    def get_config_value(self, key: str) -> Any:
        """
        Retrieve the value for a specific configuration option.

        Arguments:
        ---------
            key: the config key

        Returns:
        -------
            the value for the provided key
        """
        try:
            return self.config.get(key)
        except Exception:
            raise Exception(
                f"Error accessing config value '{key}' in module {self.__class__._module_type_name}."  # type: ignore
            )

    def run(self, kiara: "Kiara", **inputs: Any) -> "ValueMap":
        """
        Run the module ad-hoc.

        This is mostly used in unit tests, you typically want to run a module via the KiaraAPI instance.

        Arguments:
        ---------
            kiara: the kiara context
            inputs: the inputs for this module

        Returns:
        -------
            the outputs of this module run as a ValueMap instance
        """
        from kiara.models.values.value import ValueMap, ValueMapWritable, ValuePedigree

        _inputs: ValueMap = kiara.data_registry.create_valuemap(
            data=inputs, schema=self.inputs_schema
        )

        if _inputs.check_invalid():
            raise InvalidValuesException(
                msg=f"Invalid inputs for module '{self.module_type_name}'.",
                invalid_values=_inputs.check_invalid(),
            )
        environments = {
            env_name: env.instance_id
            for env_name, env in kiara.current_environments.items()
        }

        result_pedigree = ValuePedigree(
            kiara_id=kiara.id,
            module_type=self.module_type_name,
            module_config=self.config.model_dump(),
            inputs={k: v.value_id for k, v in _inputs.items()},
            environments=environments,
        )

        unique_result_values = self.characteristics.unique_result_values

        outputs = ValueMapWritable.create_from_schema(
            kiara=kiara,
            schema=self.outputs_schema,
            pedigree=result_pedigree,
            unique_value_ids=unique_result_values,
        )
        job_log = JobLog()
        self.process_step(inputs=_inputs, outputs=outputs, job_log=job_log)

        return outputs

    def process_step(
        self, inputs: "ValueMap", outputs: "ValueMap", job_log: JobLog
    ) -> None:
        """
        Kick off processing for a specific set of input/outputs.

        This method calls the implemented [process][kiara.module.KiaraModule.process] method of the inheriting class,
        as well as wrapping input/output-data related functionality.

        Arguments:
        ---------
            inputs: the input value set
            outputs: the output value set
        """
        signature = inspect.signature(self.process)  # type: ignore

        process_inputs: Dict[str, Any] = {
            "inputs": inputs,
            "outputs": outputs,
        }

        if "job_log" in signature.parameters.keys():
            process_inputs["job_log"] = job_log

        try:
            self.process(**process_inputs)  # type: ignore
        except KiaraProcessingException as kpe:
            log_exception(kpe)
            raise kpe
        except Exception as e:
            if is_debug() or is_develop():
                import traceback

                traceback.print_exc()
            exc = KiaraProcessingException(e, module=self, inputs=inputs)
            log_exception(exc)
            raise exc

    def __eq__(self, other):
        if self.__class__ != other.__class__:
            return False
        return self.module_instance_cid == other.module_instance_cid

    def __hash__(self):
        return int.from_bytes(self.module_instance_cid.digest, "big")

    def __repr__(self):
        return f"{self.__class__.__name__}(id={self.module_id} module_type={self.module_type_name} input_names={list(self.input_names)} output_names={list(self.output_names)})"

    @property
    def operation(self) -> "Operation":

        if self._operation is not None:
            return self._operation

        from kiara.models.module.operation import Operation

        self._operation = Operation.create_from_module(self)
        return self._operation

    def create_renderable(self, **config) -> RenderableType:

        return self.operation.create_renderable(**config)


# kiara\kiara\src\kiara\modules\included_core_modules\create_from.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)
import inspect
from typing import Any, Dict, Iterable, Mapping, Union

from pydantic import Field

from kiara.models.module import KiaraModuleConfig
from kiara.models.module.jobs import JobLog
from kiara.models.values.value import Value, ValueMap, ValueMapReadOnly
from kiara.models.values.value_schema import ValueSchema
from kiara.modules import KiaraModule


class CreateFromModuleConfig(KiaraModuleConfig):

    source_type: str = Field(description="The value type of the source value.")
    target_type: str = Field(description="The value type of the target.")


class CreateFromModule(KiaraModule):

    _module_type_name: str = None  # type: ignore
    _config_cls = CreateFromModuleConfig

    @classmethod
    def retrieve_supported_create_combinations(cls) -> Iterable[Mapping[str, str]]:

        result = []
        for attr in dir(cls):
            if (
                len(attr) <= 16
                or not attr.startswith("create__")
                or "__from__" not in attr
            ):
                continue

            tokens = attr.split("__")
            if len(tokens) != 4:
                continue

            source_type = tokens[3]
            target_type = tokens[1]

            data = {
                "source_type": source_type,
                "target_type": target_type,
                "func": attr,
            }
            result.append(data)
        return result

    def get_operation_doc(self) -> str:
        source_type = self.get_config_value("source_type")
        target_type = self.get_config_value("target_type")

        doc = f"Creates a '{target_type}' instance from a source value of type '{source_type}'."
        return doc

    def create_optional_inputs(
        self, source_type: str, target_type
    ) -> Union[Mapping[str, Mapping[str, Any]], None]:
        return None

    def create_inputs_schema(
        self,
    ) -> Mapping[str, Union[ValueSchema, Mapping[str, Any]]]:

        source_type = self.get_config_value("source_type")
        assert source_type not in ["target", "base_name"]

        target_type = self.get_config_value("target_type")
        optional = self.create_optional_inputs(
            source_type=source_type, target_type=target_type
        )

        schema = {
            source_type: {
                "type": source_type,
                "doc": f"The source value (of type '{source_type}').",
            },
        }
        if optional:
            for field, field_schema in optional.items():
                field_schema = dict(field_schema)
                if field in schema.keys():
                    raise Exception(
                        f"Can't create inputs schema for '{self.module_type_name}': duplicate field '{field}'."
                    )
                if field == source_type:
                    raise Exception(
                        f"Can't create inputs schema for '{self.module_type_name}': invalid field name '{field}'."
                    )

                optional = field_schema.get("optional", True)
                if not optional:
                    raise Exception(
                        f"Can't create inputs schema for '{self.module_type_name}': non-optional field '{field}' specified."
                    )
                field_schema["optional"] = True
                schema[field] = field_schema
        return schema

    def create_outputs_schema(
        self,
    ) -> Mapping[str, Union[ValueSchema, Mapping[str, Any]]]:

        return {
            self.get_config_value("target_type"): {
                "type": self.get_config_value("target_type"),
                "doc": f"The result value (of type '{self.get_config_value('target_type')}').",
            }
        }

    def process(self, inputs: ValueMap, outputs: ValueMap, job_log: JobLog) -> None:

        source_type = self.get_config_value("source_type")
        target_type = self.get_config_value("target_type")

        func_name = f"create__{target_type}__from__{source_type}"
        func = getattr(self, func_name)

        source_value = inputs.get_value_obj(source_type)

        signature = inspect.signature(func)
        args: Dict[str, Any] = {"source_value": source_value}

        if "optional" in signature.parameters:
            optional: Dict[str, Value] = {}
            op_schemas = {}
            for field, schema in self.inputs_schema.items():
                if field == source_type:
                    continue
                optional[field] = inputs.get_value_obj(field)
                op_schemas[field] = schema
            args["optional"] = ValueMapReadOnly(
                value_items=optional, values_schema=op_schemas
            )

        if "job_log" in signature.parameters:
            args["job_log"] = job_log

        result = func(**args)
        outputs.set_value(target_type, result)


# kiara\kiara\src\kiara\modules\included_core_modules\export_as.py
# -*- coding: utf-8 -*-
import os
from pathlib import Path
from typing import Any, Dict, Iterable, List, Mapping, Union

from pydantic import BaseModel, Field, field_validator

from kiara.exceptions import KiaraProcessingException
from kiara.models.module import KiaraModuleConfig
from kiara.models.values.value import ValueMap
from kiara.modules import KiaraModule, ValueMapSchema


class DataExportResult(BaseModel):

    files: List[str] = Field(description="A list of exported files.")

    @field_validator("files", mode="before")
    @classmethod
    def validate_files(cls, value):

        if isinstance(value, str):
            value = [value]

        # TODO: make sure file exists
        value = [x.as_posix() if isinstance(x, Path) else x for x in value]

        return value


class DataExportModuleConfig(KiaraModuleConfig):

    target_profile: str = Field(
        description="The name of the target profile. Used to distinguish different target formats for the same data type."
    )
    source_type: str = Field(
        description="The type of the source data that is going to be exported."
    )


class DataExportModule(KiaraModule):

    _config_cls = DataExportModuleConfig
    _module_type_name: Union[str, None] = None

    @classmethod
    def retrieve_supported_export_combinations(cls) -> Iterable[Mapping[str, str]]:

        result = []
        for attr in dir(cls):
            if (
                len(attr) <= 16
                or not attr.startswith("export__")
                or "__as__" not in attr
            ):
                continue

            tokens = attr.split("__", maxsplit=4)
            if len(tokens) != 4:
                continue

            source_type = tokens[1]
            target_profile = tokens[3]

            data = {
                "source_type": source_type,
                "target_profile": target_profile,
                "func": attr,
            }
            result.append(data)
        return result

    def create_optional_inputs(
        self, source_type: str, target_profile: str
    ) -> Union[None, Mapping[str, Mapping[str, Any]]]:
        return None

    def create_inputs_schema(
        self,
    ) -> ValueMapSchema:

        source_type = self.get_config_value("source_type")
        target_profile = self.get_config_value("target_profile")

        inputs: Dict[str, Any] = {
            source_type: {
                "type": source_type,
                "doc": f"A value of type '{source_type}'.",
            },
            "base_path": {
                "type": "string",
                "doc": "The directory to export the file(s) to.",
                "optional": True,
            },
            "name": {
                "type": "string",
                "doc": "The (base) name of the exported file(s).",
                "optional": True,
            },
            "export_metadata": {
                "type": "boolean",
                "doc": "Whether to also export the value metadata.",
                "default": False,
            },
        }

        optional = self.create_optional_inputs(
            source_type=source_type, target_profile=target_profile
        )
        if optional:
            for field, field_schema in optional.items():
                field_schema = dict(field_schema)
                if field in inputs.keys():
                    raise Exception(
                        f"Can't create inputs schema for '{self.module_type_name}': duplicate field '{field}'."
                    )
                if field == source_type:
                    raise Exception(
                        f"Can't create inputs schema for '{self.module_type_name}': invalid field name '{field}'."
                    )

                optional = field_schema.get("optional", True)
                if not optional:
                    raise Exception(
                        f"Can't create inputs schema for '{self.module_type_name}': non-optional field '{field}' specified."
                    )
                field_schema["optional"] = True
                inputs[field] = field_schema

        return inputs

    def create_outputs_schema(
        self,
    ) -> ValueMapSchema:

        outputs = {
            "export_details": {
                "type": "dict",
                "doc": "Details about the exported files/folders.",
            }
        }
        return outputs

    def process(self, inputs: ValueMap, outputs: ValueMap) -> None:

        target_profile: str = self.get_config_value("target_profile")
        source_type: str = self.get_config_value("source_type")

        export_metadata = inputs.get_value_data("export_metadata")

        source_obj = inputs.get_value_obj(source_type)
        source = source_obj.data

        func_name = f"export__{source_type}__as__{target_profile}"
        if not hasattr(self, func_name):
            raise Exception(
                f"Can't export '{source_type}' value: missing function '{func_name}' in class '{self.__class__.__name__}'. Please check this modules documentation or source code to determine which source types and profiles are supported."
            )

        base_path = inputs.get_value_data("base_path")
        if base_path is None:
            base_path = os.getcwd()
        name = inputs.get_value_data("name")
        if not name:
            name = str(source_obj.value_id)

        func = getattr(self, func_name)
        # TODO: check signature?

        base_path = os.path.abspath(base_path)
        os.makedirs(base_path, exist_ok=True)
        result = func(value=source, base_path=base_path, name=name)

        if isinstance(result, Mapping):
            result = DataExportResult(**result)
        elif isinstance(result, str):
            result = DataExportResult(files=[result])

        if not isinstance(result, DataExportResult):
            raise KiaraProcessingException(
                f"Can't export value: invalid result type '{type(result)}' from internal method. This is most likely a bug in the '{self.module_type_name}' module code."
            )

        if export_metadata:
            metadata_file = Path(os.path.join(base_path, f"{name}.metadata"))
            value_info = source_obj.create_info()
            value_json = value_info.model_dump_json()
            metadata_file.write_text(value_json)

            result.files.append(metadata_file.as_posix())

        # schema = ValueSchema(type=self.get_target_value_type(), doc="Imported dataset.")

        # value_lineage = ValueLineage.from_module_and_inputs(
        #     module=self, output_name=output_key, inputs=inputs
        # )
        # value: Value = self._kiara.data_registry.register_data(
        #     value_data=result, value_schema=schema, lineage=None
        # )

        outputs.set_value("export_details", result)


# kiara\kiara\src\kiara\modules\included_core_modules\filesystem.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)
import os
import shutil
from typing import Any, List, Mapping, Type, Union

import orjson
from pydantic import Field

from kiara.api import KiaraModuleConfig
from kiara.exceptions import KiaraProcessingException
from kiara.models.filesystem import FolderImportConfig, KiaraFile, KiaraFileBundle
from kiara.models.values.value import SerializedData, ValueMap
from kiara.modules import (
    DEFAULT_NO_IDEMPOTENT_MODULE_CHARACTERISTICS,
    KiaraModule,
    ModuleCharacteristics,
    ValueMapSchema,
)
from kiara.modules.included_core_modules.export_as import DataExportModule
from kiara.modules.included_core_modules.serialization import DeserializeValueModule


class ImportLocalFileModule(KiaraModule):
    """Import a file from the local filesystem."""

    _module_type_name = "import.local.file"

    def create_inputs_schema(
        self,
    ) -> ValueMapSchema:

        return {
            "path": {
                "type": "string",
                "doc": "The local path to the file (absolute, or relative to current directory.",
            }
        }

    def create_outputs_schema(
        self,
    ) -> ValueMapSchema:

        return {"file": {"type": "file", "doc": "The loaded files."}}

    def _retrieve_module_characteristics(self) -> ModuleCharacteristics:
        return ModuleCharacteristics(is_idempotent=False)

    def process(self, inputs: ValueMap, outputs: ValueMap):

        path = inputs.get_value_data("path")

        file = KiaraFile.load_file(source=path)
        outputs.set_value("file", file)


class DeserializeFileModule(DeserializeValueModule):
    """Deserialize data to a 'file' value instance."""

    _module_type_name = "deserialize.file"

    @classmethod
    def retrieve_supported_target_profiles(cls) -> Mapping[str, Type]:
        return {"python_object": KiaraFile}

    @classmethod
    def retrieve_serialized_value_type(cls) -> str:
        return "file"

    @classmethod
    def retrieve_supported_serialization_profile(cls) -> str:
        return "copy"

    def to__python_object(self, data: SerializedData, **config: Any):

        keys = list(data.get_keys())
        keys.remove("__file_metadata__")
        assert len(keys) == 1

        file_metadata_chunks = data.get_serialized_data("__file_metadata__")
        assert file_metadata_chunks.get_number_of_chunks() == 1
        file_metadata_json = list(file_metadata_chunks.get_chunks(as_files=False))
        assert len(file_metadata_json) == 1
        file_metadata = orjson.loads(file_metadata_json[0])

        chunks = data.get_serialized_data(keys[0])
        assert chunks.get_number_of_chunks() == 1

        files = list(chunks.get_chunks(as_files=True, symlink_ok=True))
        assert len(files) == 1

        file: str = files[0]  # type: ignore

        _file_name = file_metadata.pop("file_name")
        _file_metadata = file_metadata.pop("metadata")
        _file_metadata_schemas = file_metadata.pop("metadata_schemas")

        fm = KiaraFile.load_file(
            source=file,
            file_name=_file_name,
        )
        fm.metadata = _file_metadata
        fm.metadata_schemas = _file_metadata_schemas
        return fm


class ImportFileBundleConfig(KiaraModuleConfig):

    include_file_types: Union[None, List[str]] = Field(
        description="File types to include. Type is list of strings, which will be matched using 'endswith' test.",
        default=None,
    )
    exclude_file_types: Union[None, List[str]] = Field(
        description="File types to exclude. Type is list of strings, which will be matched with the 'endswith' test.",
        default=None,
    )


class ImportLocalFileBundleModule(KiaraModule):
    """Import a folder (file_bundle) from the local filesystem."""

    _module_type_name = "import.local.file_bundle"
    _config_cls = ImportFileBundleConfig

    def create_inputs_schema(
        self,
    ) -> ValueMapSchema:

        return {
            "path": {"type": "string", "doc": "The local path of the folder to import."}
        }

    def create_outputs_schema(
        self,
    ) -> ValueMapSchema:

        return {
            "file_bundle": {"type": "file_bundle", "doc": "The imported file bundle."}
        }

    def _retrieve_module_characteristics(self) -> ModuleCharacteristics:
        return DEFAULT_NO_IDEMPOTENT_MODULE_CHARACTERISTICS

    def process(self, inputs: ValueMap, outputs: ValueMap):

        path = inputs.get_value_data("path")

        include = self.get_config_value("include_file_types")
        exclude = self.get_config_value("exclude_file_types")

        config = FolderImportConfig(include_files=include, exclude_files=exclude)

        file_bundle = KiaraFileBundle.import_folder(source=path, import_config=config)
        outputs.set_value("file_bundle", file_bundle)


class DeserializeFileBundleModule(DeserializeValueModule):
    """Deserialize data to a 'file' value instance."""

    _module_type_name = "deserialize.file_bundle"

    @classmethod
    def retrieve_supported_target_profiles(cls) -> Mapping[str, Type]:
        return {"python_object": KiaraFileBundle}

    @classmethod
    def retrieve_serialized_value_type(cls) -> str:
        return "file_bundle"

    @classmethod
    def retrieve_supported_serialization_profile(cls) -> str:
        return "copy"

    def to__python_object(self, data: SerializedData, **config: Any):

        keys = list(data.get_keys())
        keys.remove("__file_metadata__")

        file_metadata_chunks = data.get_serialized_data("__file_metadata__")
        assert file_metadata_chunks.get_number_of_chunks() == 1
        file_metadata_json = list(file_metadata_chunks.get_chunks(as_files=False))
        assert len(file_metadata_json) == 1
        metadata = orjson.loads(file_metadata_json[0])
        file_metadata = metadata["included_files"]
        bundle_name = metadata["bundle_name"]
        # bundle_import_time = metadata["import_time"]
        sum_size = metadata["size"]
        number_of_files = metadata["number_of_files"]

        included_files = {}
        for rel_path in keys:

            if "size" not in file_metadata[rel_path].keys():
                # old style, can be removed at some point
                # file metadata was added feb 2024

                chunks = data.get_serialized_data(rel_path)
                assert chunks.get_number_of_chunks() == 1
                files = list(chunks.get_chunks(as_files=True, symlink_ok=True))
                assert len(files) == 1

                file: str = files[0]  # type: ignore
                file_name = file_metadata[rel_path]["file_name"]
                # import_time = file_metadata[rel_path]["import_time"]
                fm = KiaraFile.load_file(source=file, file_name=file_name)
            else:
                fm = KiaraFile(**file_metadata[rel_path])

                def _load_file():
                    chunks = data.get_serialized_data(rel_path)
                    assert chunks.get_number_of_chunks() == 1
                    files = list(chunks.get_chunks(as_files=True, symlink_ok=True))
                    assert len(files) == 1
                    return files[0]

                fm._path_resolver = _load_file

            included_files[rel_path] = fm

        fb_metadata = metadata.pop("metadata")
        fb_metadata_schemas = metadata.pop("metadata_schemas")

        fb = KiaraFileBundle(
            included_files=included_files,
            bundle_name=bundle_name,
            # import_time=bundle_import_time,
            number_of_files=number_of_files,
            size=sum_size,
            metadata=fb_metadata,
            metadata_schemas=fb_metadata_schemas,
        )
        return fb


class ExportFileModule(DataExportModule):
    """Export files."""

    _module_type_name = "export.file"

    def export__file__as__file(self, value: KiaraFile, base_path: str, name: str):

        target_path = os.path.join(base_path, value.file_name)

        shutil.copy2(value.path, target_path)

        return {"files": target_path}


class PickFileFromFileBundleModule(KiaraModule):
    """Pick a single file from a file_bundle value."""

    _module_type_name = "file_bundle.pick.file"

    def create_inputs_schema(
        self,
    ) -> ValueMapSchema:

        return {
            "file_bundle": {"type": "file_bundle", "doc": "The file bundle."},
            "path": {"type": "string", "doc": "The relative path of the file to pick."},
        }

    def create_outputs_schema(
        self,
    ) -> ValueMapSchema:

        return {"file": {"type": "file", "doc": "The file."}}

    def process(self, inputs: ValueMap, outputs: ValueMap):

        file_bundle: KiaraFileBundle = inputs.get_value_data("file_bundle")
        path: str = inputs.get_value_data("path")

        if path not in file_bundle.included_files.keys():
            raise KiaraProcessingException(
                f"Can't pick file '{path}' from file bundle: file not available."
            )

        file: KiaraFile = file_bundle.included_files[path]

        outputs.set_value("file", file)


class PickSubBundle(KiaraModule):
    """Pick a sub-folder from a file_bundle, resulting in a new file_bundle."""

    _module_type_name = "file_bundle.pick.sub_folder"

    def create_inputs_schema(
        self,
    ) -> ValueMapSchema:

        return {
            "file_bundle": {"type": "file_bundle", "doc": "The file bundle."},
            "sub_path": {
                "type": "string",
                "doc": "The relative path of the sub-folder to pick.",
            },
        }

    def create_outputs_schema(self) -> ValueMapSchema:
        return {
            "file_bundle": {
                "type": "file_bundle",
                "doc": "The picked (sub-)file_bundle.",
            }
        }

    def process(self, inputs: ValueMap, outputs: ValueMap):

        file_bundle: KiaraFileBundle = inputs.get_value_data("file_bundle")
        sub_path: str = inputs.get_value_data("sub_path")

        result = {}
        for path, file in file_bundle.included_files.items():
            if path.startswith(sub_path):
                result[path] = file

        if not result:
            raise KiaraProcessingException(
                f"Can't pick sub-folder '{sub_path}' from file bundle: no matches."
            )

        new_file_bundle: KiaraFileBundle = KiaraFileBundle.create_from_file_models(
            result, bundle_name=f"{file_bundle.bundle_name}_{sub_path}"
        )

        outputs.set_value("file_bundle", new_file_bundle)


# kiara\kiara\src\kiara\modules\included_core_modules\filter.py
# -*- coding: utf-8 -*-
from abc import abstractmethod
from typing import Any, Dict, List, Union

from pydantic import Field

from kiara.models.module import KiaraModuleConfig
from kiara.models.values.value import ValueMap
from kiara.modules import KiaraModule, ValueMapSchema
from kiara.utils import is_develop
from kiara.utils.develop import log_dev_message


class FilterModuleConfig(KiaraModuleConfig):

    filter_name: str = Field(description="The name of the filter.")


class FilterModule(KiaraModule):

    _module_type_name: Union[str, None] = None

    @classmethod
    def get_supported_filters(cls) -> List[str]:

        result = []
        for attr in dir(cls):
            if len(attr) <= 8 or not attr.startswith("filter__"):
                continue

            filter_name = attr[8:]
            result.append(filter_name)

        if not result and is_develop():
            from rich.table import Table

            from kiara.models.python_class import PythonClass

            pcls = PythonClass.from_class(cls)
            tbl = Table.grid()
            tbl.add_column("key", style="i")
            tbl.add_column("value")
            tbl.add_row(
                "details",
                "Module class inherits from the 'FilterModule' class, but doesn't implement any methods that start with 'filter__.",
            )
            tbl.add_row("reference", "TODO")
            tbl.add_row("python class", pcls)
            log_dev_message(tbl)
        return result

    @classmethod
    @abstractmethod
    def retrieve_supported_type(cls) -> Union[Dict[str, Any], str]:
        pass

    @classmethod
    def get_supported_type(cls) -> Dict[str, Any]:

        data = cls.retrieve_supported_type()
        if isinstance(data, str):
            data = {"type": data, "type_config": {}}
        else:
            # TODO: more validation?
            assert "type" in data.keys()
            if "type_config" not in data.keys():
                data["type_config"] = {}

        return data

    _config_cls = FilterModuleConfig

    def create_filter_inputs(self, filter_name: str) -> Union[None, ValueMapSchema]:
        return None

    def create_inputs_schema(
        self,
    ) -> ValueMapSchema:

        filter_name = self.get_config_value("filter_name")

        data_type_data = self.get_supported_type()
        data_type = data_type_data["type"]
        data_type_config = data_type_data["type_config"]

        inputs: Dict[str, Any] = {
            "value": {
                "type": data_type,
                "type_config": data_type_config,
                "doc": f"A value of type '{data_type}'.",
            },
        }

        filter_inputs = self.create_filter_inputs(filter_name=filter_name)

        if filter_inputs:
            for field, field_schema in filter_inputs.items():
                field_schema = dict(field_schema)
                if field in inputs.keys():
                    raise Exception(
                        f"Can't create inputs schema for '{self.module_type_name}': duplicate field '{field}'."
                    )

                filter_inputs_optional = field_schema.get("optional", False)
                filter_inputs_default = field_schema.get("default", None)
                if not filter_inputs_optional and filter_inputs_default is None:
                    raise Exception(
                        f"Can't create inputs schema for '{self.module_type_name}': non-optional field '{field}' specified."
                    )
                field_schema["optional"] = True
                inputs[field] = field_schema

        return inputs

    def create_outputs_schema(
        self,
    ) -> ValueMapSchema:

        data_type_data = self.get_supported_type()
        data_type = data_type_data["type"]
        data_type_config = data_type_data["type_config"]

        outputs = {
            "value": {
                "type": data_type,
                "type_config": data_type_config,
                "doc": "The filtered value.",
            }
        }
        return outputs

    def process(self, inputs: ValueMap, outputs: ValueMap) -> None:

        filter_name: str = self.get_config_value("filter_name")
        data_type_data = self.__class__.get_supported_type()
        data_type = data_type_data["type"]
        # data_type_config = data_type_data["type_config"]
        # TODO: ensure value is of the right type?

        source_obj = inputs.get_value_obj("value")

        func_name = f"filter__{filter_name}"
        if not hasattr(self, func_name):
            raise Exception(
                f"Can't apply filter '{filter_name}': missing function '{func_name}' in class '{self.__class__.__name__}'. Please check this modules documentation or source code to determine which filters are supported."
            )

        func = getattr(self, func_name)
        # TODO: check signature?

        filter_inputs = {}
        for k, v in inputs.items():
            if k == data_type:
                continue
            filter_inputs[k] = v.data

        result = func(value=source_obj, filter_inputs=filter_inputs)

        if result is None:
            outputs.set_value("value", source_obj)
        else:
            outputs.set_value("value", result)


# kiara\kiara\src\kiara\modules\included_core_modules\metadata.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

from typing import Any, Mapping, Type, Union

from pydantic import Field

from kiara.exceptions import KiaraProcessingException
from kiara.models.module import KiaraModuleConfig
from kiara.models.values.value import ValueMap
from kiara.models.values.value_metadata import ValueMetadata
from kiara.models.values.value_schema import ValueSchema
from kiara.modules import KiaraModule, ModuleCharacteristics
from kiara.registries.models import ModelRegistry


class MetadataModuleConfig(KiaraModuleConfig):

    data_type: str = Field(description="The data type this module will be used for.")
    kiara_model_id: str = Field(description="The id of the kiara (metadata) model.")


class ExtractMetadataModule(KiaraModule):
    """
    Base class to use when writing a module to extract metadata from a file.

    It's possible to use any arbitrary *kiara* module for this purpose, but sub-classing this makes it easier.
    """

    _config_cls = MetadataModuleConfig
    _module_type_name: str = "value.extract_metadata"

    def _retrieve_module_characteristics(self) -> ModuleCharacteristics:

        return ModuleCharacteristics(
            is_idempotent=True, is_internal=True, unique_result_values=True
        )

    def create_inputs_schema(
        self,
    ) -> Mapping[str, Union[ValueSchema, Mapping[str, Any]]]:

        data_type_name = self.get_config_value("data_type")
        inputs = {
            "value": {
                "type": data_type_name,
                "doc": f"A value of type '{data_type_name}'",
                "optional": False,
            }
        }
        return inputs

    def create_outputs_schema(
        self,
    ) -> Mapping[str, Union[ValueSchema, Mapping[str, Any]]]:

        kiara_model_id: str = self.get_config_value("kiara_model_id")

        # TODO: check it's subclassing the right class

        outputs = {
            "value_metadata": {
                "type": "internal_model",
                "type_config": {"kiara_model_id": kiara_model_id},
                "doc": "The metadata for the provided value.",
            }
        }

        return outputs

    def process(self, inputs: ValueMap, outputs: ValueMap) -> None:

        value = inputs.get_value_obj("value")

        kiara_model_id: str = self.get_config_value("kiara_model_id")

        model_registry = ModelRegistry.instance()
        metadata_model_cls: Type[ValueMetadata] = model_registry.get_model_cls(kiara_model_id=kiara_model_id, required_subclass=ValueMetadata)  # type: ignore

        metadata = metadata_model_cls.create_value_metadata(value=value)

        if not isinstance(metadata, metadata_model_cls):
            raise KiaraProcessingException(
                f"Invalid metadata model result, should be class '{metadata_model_cls.__name__}', but is: {metadata.__class__.__name__}. This is most likely a bug."
            )

        outputs.set_value("value_metadata", metadata)


# kiara\kiara\src\kiara\modules\included_core_modules\mock.py
# -*- coding: utf-8 -*-

from typing import Any, ClassVar, Dict, Mapping

from boltons.strutils import slugify
from pydantic import BaseModel, Field

from kiara.api import KiaraModule, KiaraModuleConfig, ValueMap, ValueMapSchema
from kiara.defaults import DEFAULT_NO_DESC_VALUE
from kiara.models.module.pipeline import PipelineConfig
from kiara.modules import ModuleCharacteristics


class MockOutput(BaseModel):
    field_schema: Dict[str, Any] = Field(description="The schema of the output.")
    data: Any = Field(description="The data of the output.", default="mock result data")


def default_mock_output() -> Dict[str, MockOutput]:

    schema = {
        "type": "any",
        "doc": "A result",
        "optional": False,
    }
    return {"result": MockOutput(field_schema=schema, data="mock result data")}


class MockModuleConfig(KiaraModuleConfig):

    _kiara_model_id: ClassVar = "instance.module_config.mock"

    @classmethod
    def create_pipeline_config(
        cls, title: str, description: str, author: str, *steps: "MockModuleConfig"
    ) -> PipelineConfig:

        data: Dict[str, Any] = {
            "pipeline_name": slugify(title),
            "doc": description,
            "context": {"authors": [author]},
            "steps": [],
        }
        for step in steps:
            step_data = {
                "step_id": slugify(step.title),
                "module_type": "dummy",
                "module_config": {
                    "title": step.title,
                    "inputs_schema": step.inputs_schema,
                    "outputs": step.outputs,
                    "desc": step.desc,
                },
            }
            data["steps"].append(step_data)

        pipeline_config = PipelineConfig.from_config(data)
        return pipeline_config

    inputs_schema: Dict[str, Dict[str, Any]] = Field(
        description="The input fields and their types.",
    )

    outputs: Dict[str, MockOutput] = Field(
        description="The outputs fields of the operation, along with their types and mock data.",
        default_factory=default_mock_output,
    )

    title: str = Field(
        description="The title of this operation.", default="mock_operation"
    )
    desc: str = Field(
        description="A description of what this step does.",
        default=DEFAULT_NO_DESC_VALUE,
    )


class MockKiaraModule(KiaraModule):

    _module_type_name = "mock"
    _config_cls = MockModuleConfig

    def create_inputs_schema(
        self,
    ) -> ValueMapSchema:

        result = {}
        v: Mapping[str, Any]
        for k, v in self.get_config_value("inputs_schema").items():
            data = {
                "type": v["type"],
                "doc": v.get("doc", "-- n/a --"),
                "optional": v.get("optional", True),
            }
            result[k] = data

        return result

    def create_outputs_schema(
        self,
    ) -> ValueMapSchema:

        result = {}
        field_name: str
        field_output: MockOutput
        for field_name, field_output in self.get_config_value("outputs").items():
            field_schema = field_output.field_schema
            if field_schema:
                data = {
                    "type": field_schema["type"],
                    "doc": field_schema.get("doc", DEFAULT_NO_DESC_VALUE),
                    "optional": field_schema.get("optional", False),
                }
            else:
                data = {
                    "type": "any",
                    "doc": DEFAULT_NO_DESC_VALUE,
                    "optional": False,
                }
            result[field_name] = data

        return result

    def _retrieve_module_characteristics(self) -> ModuleCharacteristics:

        return ModuleCharacteristics(
            is_idempotent=True, is_internal=True, unique_result_values=True
        )

    def process(self, inputs: ValueMap, outputs: ValueMap) -> None:

        # config = self.get_config_value("desc")

        mock_outputs = self.get_config_value("outputs")
        field_name: str
        field_output: MockOutput
        for field_name, field_output in mock_outputs.items():

            outputs.set_value(field_name, field_output.data)


# kiara\kiara\src\kiara\modules\included_core_modules\pipeline.py
# -*- coding: utf-8 -*-
import uuid
from typing import TYPE_CHECKING, Any, Dict, Mapping, Union

from kiara.exceptions import KiaraProcessingException
from kiara.models.module.jobs import JobLog
from kiara.models.module.pipeline import PipelineConfig
from kiara.models.module.pipeline.controller import SinglePipelineBatchController
from kiara.models.module.pipeline.pipeline import Pipeline
from kiara.models.values.value import ValueMap, ValueMapWritable
from kiara.modules import KIARA_CONFIG, KiaraModule, ValueMapSchema

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)


if TYPE_CHECKING:
    from kiara.models.module.operation import Operation
    from kiara.models.module.pipeline.structure import PipelineStructure
    from kiara.registries.jobs import JobRegistry


class PipelineModule(KiaraModule):
    """A utility module to run multiple connected inner-modules and present it as its own entity."""

    _config_cls = PipelineConfig
    _module_type_name = "pipeline"

    def __init__(
        self,
        module_config: Union[None, KIARA_CONFIG, Mapping[str, Any]] = None,
    ):
        self._job_registry: Union[JobRegistry, None] = None
        super().__init__(module_config=module_config)

    @classmethod
    def is_pipeline(cls) -> bool:
        return True

    def _set_job_registry(self, job_registry: "JobRegistry"):
        self._job_registry = job_registry

    @property
    def operation(self) -> "Operation":

        if self._operation is not None:
            return self._operation

        from kiara.models.module.operation import Operation

        self._operation = Operation.create_from_module(self, doc=self.config.doc)
        return self._operation

    def create_inputs_schema(
        self,
    ) -> ValueMapSchema:

        pipeline_structure: PipelineStructure = self.config.structure
        inputs_schema = pipeline_structure.pipeline_inputs_schema
        return inputs_schema

    def create_outputs_schema(
        self,
    ) -> ValueMapSchema:
        pipeline_structure: PipelineStructure = self.config.structure
        return pipeline_structure.pipeline_outputs_schema

    def process(self, inputs: ValueMap, outputs: ValueMapWritable, job_log: JobLog):

        pipeline_structure: PipelineStructure = self.config.structure

        pipeline = Pipeline(structure=pipeline_structure, kiara=outputs._kiara)

        assert self._job_registry is not None
        controller = SinglePipelineBatchController(
            pipeline=pipeline, job_registry=self._job_registry
        )

        job_log.add_log("setting pipeline inputs")
        pipeline.set_pipeline_inputs(inputs=inputs)
        job_log.add_log("starting pipeline processing")

        def event_callback(msg: str):
            job_log.add_log(msg)

        step_details = controller.process_pipeline(event_callback=event_callback)

        errors: Dict[str, Union[Exception, uuid.UUID]] = {}
        for step_id, details in step_details.items():
            if isinstance(details, Exception):
                errors[step_id] = details
            else:
                job = self._job_registry.get_job(details)
                if job.error:
                    if job._exception:
                        errors[step_id] = job._exception
                    else:
                        errors[step_id] = Exception(job.error)

        if errors:
            msg = "Error processing pipeline:"
            for f, e in errors.items():
                msg = f"{msg}\n  - {f}: {e}"

            raise KiaraProcessingException(f"Errors while processing pipeline: {msg}")

        # TODO: resolve values first?
        outputs.set_values(**pipeline.get_current_pipeline_outputs())


# kiara\kiara\src\kiara\modules\included_core_modules\pretty_print.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

from typing import Any, Iterable, Mapping, Tuple, Union

from pydantic import Field, field_validator

from kiara.models.module import KiaraModuleConfig
from kiara.models.values.value import ValueMap
from kiara.models.values.value_schema import ValueSchema
from kiara.modules import (
    DEFAULT_IDEMPOTENT_INTERNAL_MODULE_CHARACTERISTICS,
    KiaraModule,
    ModuleCharacteristics,
)
from kiara.utils import log_message


class PrettyPrintConfig(KiaraModuleConfig):

    source_type: str = Field(description="The value type of the source value.")
    target_type: str = Field(description="The value type of the rendered value.")

    @field_validator("source_type")
    @classmethod
    def validate_source_type(cls, value):
        if value == "render_config":
            raise ValueError(f"Invalid source type: {value}.")
        return value


class PrettyPrintModule(KiaraModule):

    _module_type_name: str = None  # type: ignore
    _config_cls = PrettyPrintConfig

    @classmethod
    def retrieve_supported_render_combinations(cls) -> Iterable[Tuple[str, str]]:

        result = []
        for attr in dir(cls):
            if (
                len(attr) <= 19
                or not attr.startswith("pretty_print__")
                or "__as__" not in attr
            ):
                continue

            attr = attr[14:]
            end_start_type = attr.find("__as__")
            source_type = attr[0:end_start_type]
            target_type = attr[end_start_type + 6 :]
            result.append((source_type, target_type))
        return result

    # def create_persistence_config_schema(self) -> Optional[Mapping[str, Mapping[str, Any]]]:
    #     return None

    def _retrieve_module_characteristics(self) -> ModuleCharacteristics:
        return DEFAULT_IDEMPOTENT_INTERNAL_MODULE_CHARACTERISTICS

    def create_inputs_schema(
        self,
    ) -> Mapping[str, Union[ValueSchema, Mapping[str, Any]]]:

        source_type = self.get_config_value("source_type")
        assert source_type not in ["target", "base_name"]

        schema = {
            "value": {"type": source_type, "doc": "The value to render."},
            "render_config": {
                "type": "any",
                "doc": "Value type dependent render configuration.",
                "optional": True,
            },
        }

        return schema

    def create_outputs_schema(
        self,
    ) -> Mapping[str, Union[ValueSchema, Mapping[str, Any]]]:

        return {
            "rendered_value": {
                "type": self.get_config_value("target_type"),
                "doc": "The rendered value.",
            }
        }

    def process(self, inputs: ValueMap, outputs: ValueMap):

        source_type = self.get_config_value("source_type")
        target_type = self.get_config_value("target_type")

        value = inputs.get_value_obj("value")
        render_config = inputs.get_value_data("render_config")

        func_name = f"pretty_print__{source_type}__as__{target_type}"

        func = getattr(self, func_name)
        # TODO: check function signature is valid

        if render_config is None:
            render_config = {}

        result = func(value=value, render_config=render_config)

        outputs.set_value("rendered_value", result)


class ValueTypePrettyPrintModule(KiaraModule):

    _module_type_name = "pretty_print.value"
    _config_cls = PrettyPrintConfig

    def _retrieve_module_characteristics(self) -> ModuleCharacteristics:
        return DEFAULT_IDEMPOTENT_INTERNAL_MODULE_CHARACTERISTICS

    def create_inputs_schema(
        self,
    ) -> Mapping[str, Union[ValueSchema, Mapping[str, Any]]]:

        source_type = self.get_config_value("source_type")
        assert source_type not in ["target", "base_name"]

        schema = {
            "value": {
                "type": source_type,
                "doc": "The value to render.",
                "optional": True,
            },
            "render_config": {
                "type": "any",
                "doc": "Value type dependent render configuration.",
                "optional": True,
            },
        }

        return schema

    def create_outputs_schema(
        self,
    ) -> Mapping[str, Union[ValueSchema, Mapping[str, Any]]]:

        return {
            "rendered_value": {
                "type": self.get_config_value("target_type"),
                "doc": "The rendered value.",
            }
        }

    def process(self, inputs: ValueMap, outputs: ValueMap):

        # source_type = self.get_config_value("source_type")
        target_type = self.get_config_value("target_type")

        source_value = inputs.get_value_obj("value")
        render_config = inputs.get_value_obj("render_config")

        if not source_value.is_set:
            outputs.set_value("rendered_value", "-- none/not set --")
            return

        try:
            data_type_cls = source_value.data_type_info.data_type_class.get_class()
            data_type = data_type_cls(**source_value.value_schema.type_config)
        except Exception as e:
            source_data_type = source_value.data_type_name
            log_message("data_type.unknown", data_type=source_data_type, error=e)

            from kiara.data_types.included_core_types import AnyType

            data_type = AnyType()

        func_name = f"pretty_print_as__{target_type}"
        func = getattr(data_type, func_name)

        render_config_dict = render_config.data
        if render_config_dict is None:
            render_config_dict = {}

        result = func(value=source_value, render_config=render_config_dict)
        # TODO: check we have the correct type?
        outputs.set_value("rendered_value", result)


class PrettyPrintAnyValueModule(PrettyPrintModule):

    _module_type_name = "pretty_print.any.value"

    # def pretty_print__any__as__string(self, value: Value, render_config: Dict[str, Any]):
    #
    #     data = value.data
    #     if isinstance(data, KiaraModel):
    #         return data.model_dump_json(option=orjson.OPT_INDENT_2)
    #     else:
    #         return str(data)


# kiara\kiara\src\kiara\modules\included_core_modules\render_value.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

from typing import Any, Iterable, Mapping, Tuple, Union

from pydantic import Field

from kiara.exceptions import KiaraProcessingException
from kiara.models.data_types import KiaraDict
from kiara.models.module import KiaraModuleConfig
from kiara.models.rendering import RenderValueResult
from kiara.models.values.value import Value, ValueMap
from kiara.models.values.value_schema import ValueSchema
from kiara.modules import (
    DEFAULT_IDEMPOTENT_INTERNAL_MODULE_CHARACTERISTICS,
    KiaraModule,
    ModuleCharacteristics,
)
from kiara.utils import log_message


class RenderValueModuleConfig(KiaraModuleConfig):

    # render_scene_type: str = Field(
    #     description="The id of the model that describes (and handles) the actual rendering."
    # )
    source_type: str = Field(description="The (kiara) data type to be rendered.")
    target_type: str = Field(
        description="The (kiara) data type of210 the rendered result."
    )

    # @validator("render_scene_type")
    # def validate_render_scene(cls, value: Any):
    #
    #     registry = ModelRegistry.instance()
    #
    #     if value not in registry.all_models.item_infos.keys():
    #         raise ValueError(
    #             f"Invalid model type '{value}'. Value model ids: {', '.join(registry.all_models.item_infos.keys())}."
    #         )
    #
    #     return value


class RenderValueModule(KiaraModule):
    @classmethod
    def retrieve_supported_render_combinations(cls) -> Iterable[Tuple[str, str]]:

        result = []
        for attr in dir(cls):
            if (
                len(attr) <= 16
                or not attr.startswith("render__")
                or "__as__" not in attr
            ):
                continue

            attr = attr[8:]
            end_start_type = attr.find("__as__")
            source_type = attr[0:end_start_type]
            target_type = attr[end_start_type + 6 :]
            result.append((source_type, target_type))
        return result

    _config_cls = RenderValueModuleConfig
    _module_type_name: str = None  # type: ignore

    def _retrieve_module_characteristics(self) -> ModuleCharacteristics:
        return DEFAULT_IDEMPOTENT_INTERNAL_MODULE_CHARACTERISTICS

    def create_inputs_schema(
        self,
    ) -> Mapping[str, Union[ValueSchema, Mapping[str, Any]]]:

        # instruction = self.get_config_value("render_scene_type")
        # model_registry = ModelRegistry.instance()
        # instr_model_cls: Type[RenderScene] = model_registry.get_model_cls(instruction, required_subclass=RenderScene)  # type: ignore

        # data_type_name = instr_model_cls.retrieve_source_type()
        # assert data_type_name

        source_type = self.get_config_value("source_type")
        optional = source_type == "none"
        inputs = {
            "value": {
                "type": source_type,
                "doc": f"A value of type '{source_type}'",
                "optional": optional,
            },
            "render_config": {
                "type": "dict",
                "doc": "Instructions/config on how (or what) to render the provided value.",
                "default": {},
            },
        }
        return inputs

    def create_outputs_schema(
        self,
    ) -> Mapping[str, Union[ValueSchema, Mapping[str, Any]]]:

        outputs = {
            "render_value_result": {
                "type": "render_value_result",
                "doc": "The rendered value, incl. some metadata.",
            },
        }

        return outputs

    def process(self, inputs: ValueMap, outputs: ValueMap) -> None:

        source_type = self.get_config_value("source_type")
        target_type = self.get_config_value("target_type")

        value: Value = inputs.get_value_obj("value")

        render_scene: KiaraDict = inputs.get_value_data("render_config")
        if render_scene:
            rc = render_scene.dict_data
        else:
            rc = {}

        func_name = f"render__{source_type}__as__{target_type}"

        func = getattr(self, func_name)
        result = func(value=value, render_config=rc)
        if isinstance(result, RenderValueResult):
            render_scene_result: RenderValueResult = result
        else:
            render_scene_result = RenderValueResult(
                value_id=value.value_id,
                render_config=rc,
                render_manifest=self.manifest.manifest_hash,
                rendered=result,
                related_scenes={},
            )
        render_scene_result.manifest_lookup[self.manifest.manifest_hash] = self.manifest

        outputs.set_value("render_value_result", render_scene_result)


class ValueTypeRenderModule(KiaraModule):
    """A module that uses render methods attached to DataType classes."""

    _module_type_name = "render.value"
    _config_cls = RenderValueModuleConfig

    def _retrieve_module_characteristics(self) -> ModuleCharacteristics:
        return DEFAULT_IDEMPOTENT_INTERNAL_MODULE_CHARACTERISTICS

    def create_inputs_schema(
        self,
    ) -> Mapping[str, Union[ValueSchema, Mapping[str, Any]]]:

        source_type = self.get_config_value("source_type")
        assert source_type not in ["target", "base_name"]

        schema = {
            "value": {
                "type": source_type,
                "doc": "The value to render.",
                "optional": False,
            },
            "render_config": {
                "type": "dict",
                "doc": "Instructions/config on how (or what) to render the provided value.",
                "default": {},
            },
        }

        return schema

    def create_outputs_schema(
        self,
    ) -> Mapping[str, Union[ValueSchema, Mapping[str, Any]]]:

        outputs = {
            "render_value_result": {
                "type": "render_value_result",
                "doc": "The rendered value, incl. some metadata.",
            },
        }

        return outputs

    def process(self, inputs: ValueMap, outputs: ValueMap):

        source_value = inputs.get_value_obj("value")
        if not source_value.is_set:
            raise KiaraProcessingException(
                f"Can't render value '{source_value.value_id}': value not set."
            )

        # source_type = self.get_config_value("source_type")
        target_type = self.get_config_value("target_type")

        render_scene: KiaraDict = inputs.get_value_data("render_config")

        try:
            data_type_cls = source_value.data_type_info.data_type_class.get_class()
            data_type = data_type_cls(**source_value.value_schema.type_config)

        except Exception as e:
            source_data_type = source_value.data_type_name
            log_message("data_type.unknown", data_type=source_data_type, error=e)

            from kiara.data_types.included_core_types import AnyType

            data_type = AnyType()

        func_name = f"render_as__{target_type}"
        func = getattr(data_type, func_name)

        if render_scene:
            rc = render_scene.dict_data
        else:
            rc = {}

        result = func(
            value=source_value,
            render_config=rc,
            manifest=self.manifest,
        )

        if isinstance(result, RenderValueResult):
            render_scene_result = result
        else:
            render_scene_result = RenderValueResult(
                value_id=source_value.value_id,
                render_config=rc,
                render_manifest=self.manifest.manifest_hash,
                rendered=result,
                related_scenes={},
                manifest_lookup={self.manifest.manifest_hash: self.manifest},
            )

        outputs.set_value("render_value_result", render_scene_result)


# kiara\kiara\src\kiara\modules\included_core_modules\serialization.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import abc
from typing import Any, Mapping, Type, Union

import orjson
from pydantic import Field, field_validator

from kiara.models import KiaraModel
from kiara.models.module import KiaraModuleConfig
from kiara.models.values.value import SerializedData, Value, ValueMap
from kiara.models.values.value_schema import ValueSchema
from kiara.modules import (
    DEFAULT_IDEMPOTENT_INTERNAL_MODULE_CHARACTERISTICS,
    KiaraModule,
    ModuleCharacteristics,
    ValueMapSchema,
)
from kiara.registries.models import ModelRegistry


class SerializeConfig(KiaraModuleConfig):

    value_type: str = Field(
        description="The value type of the actual (unserialized) value."
    )
    target_profile: str = Field(
        description="The profile name of the de-serialization result data."
    )
    serialization_profile: str = Field(
        description="The name of the serialization profile used to serialize the source value."
    )

    @field_validator("value_type")
    @classmethod
    def validate_source_type(cls, value):
        if value == "serialization_config":
            raise ValueError(f"Invalid source type: {value}.")
        return value


class DeserializeValueModule(KiaraModule):

    _config_cls = SerializeConfig

    @classmethod
    @abc.abstractmethod
    def retrieve_serialized_value_type(cls) -> str:
        raise NotImplementedError()

    @classmethod
    @abc.abstractmethod
    def retrieve_supported_target_profiles(cls) -> Mapping[str, Type]:
        raise NotImplementedError()

    @classmethod
    @abc.abstractmethod
    def retrieve_supported_serialization_profile(cls) -> str:
        raise NotImplementedError()

    def create_inputs_schema(
        self,
    ) -> Mapping[str, Union[ValueSchema, Mapping[str, Any]]]:

        value_type = self.get_config_value("value_type")
        return {
            value_type: {
                "type": value_type,
                "doc": "The value object.",
            },
            "deserialization_config": {
                "type": "any",
                "doc": "Serialization-format specific configuration.",
                "optional": True,
            },
        }

    def create_outputs_schema(
        self,
    ) -> Mapping[str, Union[ValueSchema, Mapping[str, Any]]]:

        return {
            "python_object": {
                "type": "python_object",
                "doc": "The deserialized python object instance.",
            },
        }

    def _retrieve_module_characteristics(self) -> ModuleCharacteristics:
        return DEFAULT_IDEMPOTENT_INTERNAL_MODULE_CHARACTERISTICS

    def process(self, inputs: ValueMap, outputs: ValueMap) -> None:

        value_type = self.get_config_value("value_type")
        serialized_value = inputs.get_value_obj(value_type)
        config = inputs.get_value_obj("deserialization_config")

        target_profile = self.get_config_value("target_profile")
        func_name = f"to__{target_profile}"
        func = getattr(self, func_name)

        if config.is_set:
            _config = config.data
        else:
            _config = {}

        result: Any = func(data=serialized_value.serialized_data, **_config)
        outputs.set_value("python_object", result)


class UnpickleModule(DeserializeValueModule):

    _module_type_name = "unpickle.value"

    @classmethod
    def retrieve_supported_target_profiles(cls) -> Mapping[str, Type]:

        return {"python_object": object}

    @classmethod
    def retrieve_supported_serialization_profile(cls) -> str:
        return "pickle"

    @classmethod
    def retrieve_serialized_value_type(cls) -> str:

        return "any"

    def to__python_object(self, data: SerializedData, **config: Any):

        try:
            import pickle5 as pickle
        except Exception:
            import pickle  # type: ignore

        assert "python_object" in data.get_keys()
        python_object_data = data.get_serialized_data("python_object")
        assert python_object_data.get_number_of_chunks() == 1

        _bytes = next(python_object_data.get_chunks(as_files=False))
        data = pickle.loads(_bytes)  # noqa

        return data


class LoadBytesModule(DeserializeValueModule):

    _module_type_name = "load.bytes"

    @classmethod
    def retrieve_supported_target_profiles(cls) -> Mapping[str, Type]:
        return {"python_object": bytes}

    @classmethod
    def retrieve_supported_serialization_profile(cls) -> str:
        return "raw"

    @classmethod
    def retrieve_serialized_value_type(cls) -> str:
        return "bytes"

    def to__python_object(self, data: SerializedData, **config: Any) -> bytes:

        chunks = data.get_serialized_data("bytes")
        assert chunks.get_number_of_chunks() == 1
        _chunks = list(chunks.get_chunks(as_files=False))
        assert len(_chunks) == 1
        _chunk: bytes = _chunks[0]  # type: ignore
        return _chunk


class LoadStringModule(DeserializeValueModule):

    _module_type_name = "load.string"

    @classmethod
    def retrieve_supported_target_profiles(cls) -> Mapping[str, Type]:
        return {"python_object": str}

    @classmethod
    def retrieve_supported_serialization_profile(cls) -> str:
        return "raw"

    @classmethod
    def retrieve_serialized_value_type(cls) -> str:
        return "string"

    def to__python_object(self, data: SerializedData, **config: Any) -> str:

        chunks = data.get_serialized_data("string")
        assert chunks.get_number_of_chunks() == 1
        _chunks = list(chunks.get_chunks(as_files=False))
        assert len(_chunks) == 1

        bytes_string: bytes = _chunks[0]  # type: ignore
        return bytes_string.decode("utf-8")


class LoadInternalModel(DeserializeValueModule):

    _module_type_name = "load.internal_model"

    @classmethod
    def retrieve_supported_target_profiles(cls) -> Mapping[str, Type]:
        return {"python_object": KiaraModel}

    @classmethod
    def retrieve_supported_serialization_profile(cls) -> str:
        return "json"

    @classmethod
    def retrieve_serialized_value_type(cls) -> str:
        return "internal_model"

    def to__python_object(self, data: SerializedData, **config: Any) -> KiaraModel:

        chunks = data.get_serialized_data("data")
        assert chunks.get_number_of_chunks() == 1
        _chunks = list(chunks.get_chunks(as_files=False))
        assert len(_chunks) == 1

        bytes_string: bytes = _chunks[0]  # type: ignore
        model_data = orjson.loads(bytes_string)

        model_id: str = data.data_type_config["kiara_model_id"]
        model_registry = ModelRegistry.instance()
        m_cls = model_registry.get_model_cls(kiara_model_id=model_id)

        obj = m_cls(**model_data)
        return obj


class DeserializeJsonConfig(KiaraModuleConfig):

    result_path: Union[str, None] = Field(
        description="The path of the result dictionary to return.", default="data"
    )


class DeserializeFromJsonModule(KiaraModule):

    _module_type_name: str = "deserialize.from_json"
    _config_cls = DeserializeJsonConfig

    def _retrieve_module_characteristics(self) -> ModuleCharacteristics:
        return DEFAULT_IDEMPOTENT_INTERNAL_MODULE_CHARACTERISTICS

    def create_inputs_schema(
        self,
    ) -> ValueMapSchema:

        return {
            "value": {
                "type": "any",
                "doc": "The value object to deserialize the data for.",
            }
        }

    def create_outputs_schema(
        self,
    ) -> ValueMapSchema:

        return {
            "python_object": {
                "type": "python_object",
                "doc": "The deserialized python object.",
            }
        }

    def process(self, inputs: ValueMap, outputs: ValueMap):

        value: Value = inputs.get_value_obj("value")
        serialized: SerializedData = value.serialized_data

        chunks = serialized.get_serialized_data(self.get_config_value("result_path"))
        assert chunks.get_number_of_chunks() == 1
        _data = list(chunks.get_chunks(as_files=False))
        assert len(_data) == 1
        _chunk: bytes = _data[0]  # type: ignore

        deserialized = orjson.loads(_chunk)

        outputs.set_value("python_object", deserialized)


# kiara\kiara\src\kiara\modules\included_core_modules\__init__.py


# kiara\kiara\src\kiara\operations\__init__.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import abc
from typing import TYPE_CHECKING, Generic, Iterable, Mapping, TypeVar, Union

from kiara.interfaces.python_api.models.info import OperationTypeInfo
from kiara.models.module.operation import Operation, OperationConfig, OperationDetails

if TYPE_CHECKING:
    from kiara.context import Kiara
    from kiara.modules import KiaraModule


OPERATION_TYPE_DETAILS = TypeVar("OPERATION_TYPE_DETAILS", bound=OperationDetails)


class OperationType(abc.ABC, Generic[OPERATION_TYPE_DETAILS]):
    def __init__(self, kiara: "Kiara", op_type_name: str):
        self._kiara: Kiara = kiara
        self._op_type_name: str = op_type_name

    @property
    def operations(self) -> Mapping[str, Operation]:
        return {
            op_id: self._kiara.operation_registry.get_operation(op_id)
            for op_id in self._kiara.operation_registry.operations_by_type[
                self._op_type_name
            ]
        }

    def retrieve_included_operation_configs(
        self,
    ) -> Iterable[Union[Mapping, OperationConfig]]:
        return []

    @abc.abstractmethod
    def check_matching_operation(
        self, module: "KiaraModule"
    ) -> Union[OPERATION_TYPE_DETAILS, None]:
        """Check whether the provided module is a valid operation for this type."""

    def retrieve_operation_details(
        self, operation: Union[Operation, str]
    ) -> OPERATION_TYPE_DETAILS:
        """
        Retrieve operation details for provided operation.

        This is really just a utility method, to make the type checker happy.
        """
        if isinstance(operation, str):
            operation = self.operations[operation]

        return operation.operation_details  # type: ignore

    def create_renderable(self, **config):

        info = OperationTypeInfo.create_from_type_class(
            kiara=None, type_cls=self.__class__
        )
        return info.create_renderable(**config)


# kiara\kiara\src\kiara\operations\included_core_operations\create_from.py
# -*- coding: utf-8 -*-
from typing import TYPE_CHECKING, ClassVar, Iterable, Mapping, Union

import structlog
from pydantic import Field

from kiara.models.documentation import DocumentationMetadataModel
from kiara.models.module.operation import (
    BaseOperationDetails,
    ManifestOperationConfig,
    OperationConfig,
)
from kiara.models.values.value_schema import ValueSchema
from kiara.modules.included_core_modules.create_from import CreateFromModule
from kiara.operations import OperationType
from kiara.utils import log_exception

if TYPE_CHECKING:
    from kiara.modules import KiaraModule

logger = structlog.getLogger()


class CreateValueFromDetails(BaseOperationDetails):

    source_type: str = Field(description="The type of the value to be created.")
    target_type: str = Field(description="The result type.")
    optional_args: Mapping[str, ValueSchema] = Field(description="Optional arguments.")

    # def retrieve_inputs_schema(self) -> ValueSetSchema:
    #
    #     result: Dict[str, Union[ValueSchema, Dict[str, Any]]] = {
    #         self.source_type: {"type": self.source_type, "doc": "The source value."},
    #     }
    #     for field, schema in self.optional_args.items():
    #         if field in result.keys():
    #             raise Exception(
    #                 f"Can't create 'create_from' operation '{self.source_type}' -> '{self.target_type}': duplicate input field '{field}'."
    #             )
    #         result[field] = schema
    #     return result
    #
    # def retrieve_outputs_schema(self) -> ValueSetSchema:
    #
    #     return {
    #         self.target_type: {"type": self.target_type, "doc": "The result value."}
    #     }


class CreateFromOperationType(OperationType[CreateValueFromDetails]):
    """Create a dataset from a dataset of another value.

    This operation always has at least one input named after the source type, and one output named after the target type. It can,
    in addition, also have other, optional inputs, to control how exactly the target value is created.
    """

    _operation_type_name: ClassVar[str] = "create_from"

    def _calculate_op_id(self, source_type: str, target_type: str):

        if source_type == "any":
            operation_id = f"create.{target_type}"
        else:
            operation_id = f"create.{target_type}.from.{source_type}"

        return operation_id

    def retrieve_included_operation_configs(
        self,
    ) -> Iterable[Union[Mapping, OperationConfig]]:

        result = {}
        for name, module_cls in self._kiara.module_type_classes.items():
            if not hasattr(module_cls, "retrieve_supported_create_combinations"):
                continue

            try:
                supported_combinations = module_cls.retrieve_supported_create_combinations()  # type: ignore
                for sup_comb in supported_combinations:
                    source_type = sup_comb["source_type"]
                    target_type = sup_comb["target_type"]
                    func = sup_comb["func"]

                    if source_type not in self._kiara.data_type_names:
                        logger.debug(
                            "ignore.operation_config",
                            module_type=name,
                            reason=f"Source type '{source_type}' not registered.",
                        )
                        continue
                    if target_type not in self._kiara.data_type_names:
                        logger.debug(
                            "ignore.operation_config",
                            module_type=name,
                            reason=f"Target type '{target_type}' not registered.",
                        )
                        continue
                    if not hasattr(module_cls, func):
                        logger.debug(
                            "ignore.operation_config",
                            module_type=name,
                            reason=f"Specified create function '{func}' not available.",
                        )
                        continue

                    mc = {"source_type": source_type, "target_type": target_type}
                    # TODO: check whether module config actually supports those, for now, only 'CreateFromModule' subtypes are supported
                    _func = getattr(module_cls, func)
                    doc = DocumentationMetadataModel.from_function(_func)

                    oc = ManifestOperationConfig(
                        module_type=name, module_config=mc, doc=doc
                    )
                    op_id = self._calculate_op_id(
                        source_type=source_type, target_type=target_type
                    )
                    result[op_id] = oc
            except Exception as e:
                log_exception(e)
                logger.debug(
                    "ignore.create_operation_instance", module_type=name, reason=e
                )
                continue

        return result.values()

    def check_matching_operation(
        self, module: "KiaraModule"
    ) -> Union[CreateValueFromDetails, None]:

        if not isinstance(module, CreateFromModule):
            return None

        source_type = None
        for field_name, schema in module.inputs_schema.items():
            if field_name == schema.type:
                if source_type is not None:
                    logger.debug(
                        "ignore.operation",
                        operation_type="create_from",
                        reason=f"more than one possible target type field: {field_name}",
                    )
                    return None
                source_type = field_name

        if source_type is None:
            return None

        target_type = None
        for field_name, schema in module.outputs_schema.items():
            if field_name == schema.type:
                if target_type is not None:
                    logger.debug(
                        "ignore.operation",
                        operation_type="create_from",
                        reason=f"more than one possible target type field: {field_name}",
                    )
                    return None
                target_type = field_name

        if target_type is None:
            return None

        op_id = self._calculate_op_id(source_type=source_type, target_type=target_type)

        if (
            "any" in self._kiara.type_registry.get_type_lineage(target_type)
            and target_type != "any"
        ):
            is_internal = False
        else:
            is_internal = True

        optional = {}
        for field, schema in module.inputs_schema.items():
            if field == source_type:
                continue
            optional[field] = schema

        details = {
            "module_inputs_schema": module.inputs_schema,
            "module_outputs_schema": module.outputs_schema,
            "operation_id": op_id,
            "source_type": source_type,
            "target_type": target_type,
            "optional_args": optional,
            "is_internal_operation": is_internal,
        }

        result: CreateValueFromDetails = (
            CreateValueFromDetails.create_operation_details(**details)
        )
        return result


# kiara\kiara\src\kiara\operations\included_core_operations\export_as.py
# -*- coding: utf-8 -*-
from typing import TYPE_CHECKING, ClassVar, Iterable, Mapping, Union

import structlog
from pydantic import Field

from kiara.models.documentation import DocumentationMetadataModel
from kiara.models.module.operation import (
    BaseOperationDetails,
    ManifestOperationConfig,
    OperationConfig,
)
from kiara.models.values.value_schema import ValueSchema
from kiara.modules.included_core_modules.export_as import DataExportModule
from kiara.operations import OperationType
from kiara.utils import log_exception

if TYPE_CHECKING:
    from kiara.modules import KiaraModule

logger = structlog.getLogger()


class ExportAsOperationDetails(BaseOperationDetails):

    source_type: str = Field(description="The type of the value to be created.")
    target_profile: str = Field(description="The result profile type.")
    optional_args: Mapping[str, ValueSchema] = Field(description="Optional arguments.")

    # def retrieve_inputs_schema(self) -> ValueSetSchema:
    #
    #     result: Dict[str, Union[ValueSchema, Dict[str, Any]]] = {
    #         self.source_type: {"type": self.source_type, "doc": "The source value."},
    #     }
    #     for field, schema in self.optional_args.items():
    #         if field in result.keys():
    #             raise Exception(
    #                 f"Can't create 'create_from' operation '{self.source_type}' -> '{self.target_profile}': duplicate input field '{field}'."
    #             )
    #         result[field] = schema
    #     return result
    #
    # def retrieve_outputs_schema(self) -> ValueSetSchema:
    #
    #     return {
    #         "export_details": {
    #             "type": "dict",
    #             "doc": "Details about the exported data/files.",
    #         }
    #     }


class ExportAsOperationType(OperationType[ExportAsOperationDetails]):

    _operation_type_name: ClassVar[str] = "export_as"

    def _calculate_op_id(self, source_type: str, target_profile: str):

        if source_type == "any":
            operation_id = f"export.as.{target_profile}"
        else:
            operation_id = f"export.{source_type}.as.{target_profile}"

        return operation_id

    def retrieve_included_operation_configs(
        self,
    ) -> Iterable[Union[Mapping, OperationConfig]]:

        result = []
        for name, module_cls in self._kiara.module_type_classes.items():
            if not hasattr(module_cls, "retrieve_supported_export_combinations"):
                continue

            try:
                supported_combinations = module_cls.retrieve_supported_export_combinations()  # type: ignore
                for sup_comb in supported_combinations:
                    source_type = sup_comb["source_type"]
                    target_profile = sup_comb["target_profile"]
                    func = sup_comb["func"]

                    if source_type not in self._kiara.data_type_names:
                        logger.debug(
                            "ignore.operation_config",
                            module_type=name,
                            reason=f"Source type '{source_type}' not registered.",
                        )
                        continue

                    if not hasattr(module_cls, func):
                        logger.debug(
                            "ignore.operation_config",
                            module_type=name,
                            reason=f"Specified create function '{func}' not available.",
                        )
                        continue

                    mc = {"source_type": source_type, "target_profile": target_profile}
                    # TODO: check whether module config actually supports those, for now, only 'DataExportModule' subtypes are supported
                    _func = getattr(module_cls, func)
                    doc = DocumentationMetadataModel.from_function(_func)

                    oc = ManifestOperationConfig(
                        module_type=name, module_config=mc, doc=doc
                    )
                    result.append(oc)
            except Exception as e:
                log_exception(e)
                logger.debug(
                    "ignore.create_operation_instance", module_type=name, reason=e
                )
                continue

        return result

    def check_matching_operation(
        self, module: "KiaraModule"
    ) -> Union[ExportAsOperationDetails, None]:

        if not isinstance(module, DataExportModule):
            return None

        source_type = None
        for field_name, schema in module.inputs_schema.items():
            if field_name == schema.type:
                if source_type is not None:
                    logger.debug(
                        "ignore.operation",
                        operation_type="create_from",
                        reason=f"more than one possible target type field: {field_name}",
                    )
                    return None
                source_type = field_name

        if source_type is None:
            return None

        target_profile = module.config.target_profile

        op_id = self._calculate_op_id(
            source_type=source_type, target_profile=target_profile
        )

        optional = {}
        for field, schema in module.inputs_schema.items():
            if field in [source_type]:
                continue
            optional[field] = schema

        details = {
            "module_inputs_schema": module.inputs_schema,
            "module_outputs_schema": module.outputs_schema,
            "operation_id": op_id,
            "source_type": source_type,
            "target_profile": target_profile,
            "optional_args": optional,
            "is_internal_operation": False,
        }

        result: ExportAsOperationDetails = (
            ExportAsOperationDetails.create_operation_details(**details)
        )
        return result


# kiara\kiara\src\kiara\operations\included_core_operations\filter.py
# -*- coding: utf-8 -*-

from typing import TYPE_CHECKING, Any, ClassVar, Dict, Iterable, List, Mapping, Union

import structlog
from pydantic import Field

from kiara.models.documentation import DocumentationMetadataModel
from kiara.models.module.manifest import Manifest
from kiara.models.module.operation import (
    BaseOperationDetails,
    Filter,
    ManifestOperationConfig,
    Operation,
    OperationConfig,
)
from kiara.models.module.pipeline import PipelineConfig
from kiara.models.python_class import KiaraModuleInstance
from kiara.models.values.value_schema import ValueSchema
from kiara.modules.included_core_modules.filter import FilterModule
from kiara.operations import OperationType
from kiara.operations.included_core_operations.pipeline import PipelineOperationDetails
from kiara.utils import find_free_id, log_exception

if TYPE_CHECKING:
    from kiara.modules import KiaraModule

logger = structlog.getLogger()


class FilterOperationDetails(BaseOperationDetails):

    data_type: str = Field(description="The data type of the value to be filtered.")
    data_type_config: Mapping[str, Any] = Field(
        description="The configuration of the data type to be filtered.",
        default_factory=dict,
    )
    filter_name: str = Field(description="The filter operation name.")
    optional_args: Mapping[str, ValueSchema] = Field(description="Optional arguments.")

    # def retrieve_inputs_schema(self) -> ValueSetSchema:
    #
    #     result: Dict[str, Union[ValueSchema, Dict[str, Any]]] = {
    #         self.data_type: {
    #             "type": self.data_type,
    #             "type_config": self.data_type_config,
    #             "doc": "The value.",
    #         },
    #     }
    #     for field, schema in self.optional_args.items():
    #         if field in result.keys():
    #             raise Exception(
    #                 f"Can't create 'filter' operation '{self.filter_name}': duplicate input field '{field}'."
    #             )
    #         result[field] = schema
    #     return result
    #
    # def retrieve_outputs_schema(self) -> ValueSetSchema:
    #
    #     return {
    #         self.data_type: {
    #             "type": self.data_type,
    #             "type_config": self.data_type_config,
    #             "doc": "Details about the exported data/files.",
    #         },
    #     }

    # def create_module_inputs(self, inputs: Mapping[str, Any]) -> Mapping[str, Any]:
    #
    #     _inputs = dict(inputs)
    #     v = _inputs.pop("value")
    #     assert self.data_type not in _inputs.keys()
    #     _inputs[self.data_type] = v
    #     return _inputs
    #
    # def create_operation_outputs(self, outputs: ValueMap) -> Mapping[str, Value]:
    #
    #     _outputs = dict(outputs)
    #     v = _outputs.pop(self.data_type)
    #     assert "value" not in _outputs.keys()
    #     _outputs["value"] = v
    #     return _outputs


class FilterOperationType(OperationType[FilterOperationDetails]):

    _operation_type_name: ClassVar[str] = "filter"

    def retrieve_included_operation_configs(
        self,
    ) -> Iterable[Union[Mapping, OperationConfig]]:

        result = []

        for name, module_cls in self._kiara.module_type_classes.items():

            if not issubclass(module_cls, FilterModule):
                continue

            try:
                data_type_data = module_cls.get_supported_type()
                data_type: str = data_type_data["type"]  # type: ignore
                # data_type_config: Mapping[str, Any] = data_type["type_config"]  # type: ignore

                # TODO; try to create data type obj?
                if data_type not in self._kiara.data_type_names:
                    logger.debug(
                        "ignore.operation_config",
                        module_type=name,
                        reason=f"Data type '{data_type}' not registered.",
                    )
                    continue

                supported_filters = module_cls.get_supported_filters()
                for filter in supported_filters:

                    func_name = f"filter__{filter}"

                    if not hasattr(module_cls, func_name):
                        logger.debug(
                            "ignore.operation_config",
                            module_type=name,
                            reason=f"Specified filter function '{func_name}' not available.",
                        )
                        continue

                    mc = {"filter_name": filter}
                    # TODO: check whether module config actually supports those, for now, only 'DataExportModule' subtypes are supported
                    _func = getattr(module_cls, func_name)
                    doc = DocumentationMetadataModel.from_function(_func)
                    oc = ManifestOperationConfig(
                        module_type=name, module_config=mc, doc=doc
                    )
                    result.append(oc)
            except Exception as e:
                log_exception(e)
                logger.debug(
                    "ignore.create_operation_instance", module_type=name, reason=e
                )
                continue

        return result

    def check_matching_operation(
        self, module: "KiaraModule"
    ) -> Union[FilterOperationDetails, None]:

        if not isinstance(module, FilterModule):
            return None

        data_type_data = module.__class__.get_supported_type()
        data_type: str = data_type_data["type"]  # type: ignore
        data_type_config: Mapping[str, Any] = data_type_data["type_config"]  # type: ignore

        filter_name = module.get_config_value("filter_name")

        op_id = f"{data_type}_filter.{filter_name}"

        optional = {}
        for field, schema in module.inputs_schema.items():
            if field in [data_type]:
                continue
            optional[field] = schema

        details = {
            "module_inputs_schema": module.inputs_schema,
            "module_outputs_schema": module.outputs_schema,
            "operation_id": op_id,
            "data_type": data_type,
            "data_type_config": data_type_config,
            "filter_name": filter_name,
            "optional_args": optional,
            "is_internal_operation": False,
        }

        result: FilterOperationDetails = (
            FilterOperationDetails.create_operation_details(**details)
        )
        return result

    def find_filter_operations_for_data_type(
        self, data_type: str
    ) -> Dict[str, Operation]:

        result = {}
        for op in self.operations.values():
            details: FilterOperationDetails = op.operation_details  # type: ignore
            if details.data_type == data_type:
                result[details.filter_name] = op

        return result

    def get_filter(self, data_type: str, filter_name: str) -> Filter:

        try:
            op = self._kiara.operation_registry.get_operation(operation_id=filter_name)
        except Exception:
            op_id = f"{data_type}_filter.{filter_name}"
            op = self.operations.get(op_id, None)  # type: ignore
            if op is None:
                raise Exception(
                    f"No filter operation '{filter_name}' available for type '{data_type}'."
                )

        inp_match = []
        for input_name, schema in op.inputs_schema.items():
            # TODO: check lineage/profiles
            if schema.type == data_type:
                inp_match.append(input_name)

        if not inp_match:
            raise Exception(
                f"Can't retrieve filter with name '{filter_name}' for data type: '{data_type}': input fields for operation '{op.operation_id}' don't match."
            )
        if len(inp_match) > 1:
            if "value" in inp_match:
                inp_match = ["value"]
            elif data_type in inp_match:
                inp_match = [data_type]
            else:
                raise Exception(
                    f"Can't retrieve filter with name '{filter_name}' for data type: '{data_type}', operation '{op.operation_id}' has multiple potential input fields: {', '.join(inp_match)}."
                )

        input_field = inp_match[0]

        outp_match = []
        for output_name, schema in op.outputs_schema.items():
            # TODO: check lineage/profiles
            if schema.type == data_type:
                outp_match.append(output_name)

        if not outp_match:
            raise Exception(
                f"Can't retrieve filter with name '{filter_name}' for data type: '{data_type}': output fields for operation '{op.operation_id}' don't match."
            )
        if len(outp_match) > 1:
            if "value" in outp_match:
                outp_match = ["value"]
            elif data_type in outp_match:
                outp_match = [data_type]
            else:
                raise Exception(
                    f"Can't retrieve filter with name '{filter_name}' for data type: '{data_type}', operation '{op.operation_id}' has multiple potential output fields: {', '.join(outp_match)}."
                )

        output_field = outp_match[0]
        filter = Filter(
            operation=op,
            input_name=input_field,
            output_name=output_field,
            data_type=data_type,
        )
        return filter

    def assemble_filter_pipeline_config(
        self,
        data_type: str,
        filters: Union[str, Iterable[str], Mapping[str, str]],
        endpoint: Union[None, Manifest, str] = None,
        endpoint_input_field: Union[str, None] = None,
        endpoint_step_id: Union[str, None] = None,
        extra_input_aliases: Union[None, Mapping[str, str]] = None,
        extra_output_aliases: Union[None, Mapping[str, str]] = None,
    ) -> PipelineConfig:
        """
        Assemble a (pipeline) module config to filter values of a specific data type.

        Optionally, a module that uses the filtered dataset as input can be specified.

        # TODO: document filter names
        For the 'filters' argument, the accepted inputs are:
        - a string, in which case a single-step pipeline will be created, with the string referencing the operation id or filter
        - a list of strings: in which case a multi-step pipeline will be created, the step_ids will be calculated automatically
        - a map of string pairs: the keys are step ids, the values operation ids or filter names

        Arguments:
        ---------
            data_type: the type of the data to filter
            filters: a list of operation ids or filter names (and potentiall step_ids if type is a mapping)
            endpoint: optional module to put as last step in the created pipeline
            endpoing_input_field: field name of the input that will receive the filtered value
            endpoint_step_id: id to use for the endpoint step (module type name will be used if not provided)
            extra_output_aliases: extra output aliases to add to the pipeline config

        Returns:
        -------
            the (pipeline) module configuration of the filter pipeline
        """
        steps: List[Mapping[str, Any]] = []
        last_filter_id: Union[str, None] = None
        last_filter_output_name: Union[str, None] = None
        input_aliases: Dict[str, str] = {}
        output_aliases: Dict[str, str] = {}

        if isinstance(filters, str):
            filters = {filters: filters}

        if not isinstance(filters, Mapping):
            _filters = {}
            _step_ids: List[str] = []
            for filter_name in filters:
                step_id = find_free_id(stem=filter_name, current_ids=_step_ids)
                _filters[step_id] = filter_name
            filters = _filters

        for filter_name, step_id in filters.items():
            if not input_aliases:
                input_aliases[f"{filter_name}.value"] = "value"
            filter = self.get_filter(data_type=data_type, filter_name=filter_name)
            step_data = {
                "module_type": filter.operation.operation_id,
                "step_id": step_id,
            }
            if last_filter_id:
                step_data["input_links"] = {
                    filter.input_name: f"{last_filter_id}.{last_filter_output_name}"
                }
            last_filter_id = step_id
            last_filter_output_name = filter.output_name
            steps.append(step_data)
            output_aliases[f"{step_id}.value"] = f"{step_id}__filtered"

        output_aliases[f"{last_filter_id}.{last_filter_output_name}"] = "filtered_value"

        doc = f"Auto generated filter operation ({'->'.join(filters.keys())}) for type '{data_type}'"

        if endpoint:
            endpoint_module = self._kiara.module_registry.create_module(
                manifest=endpoint
            )
            if endpoint_input_field is None:
                matches = []
                for field_name, schema in endpoint_module.inputs_schema.items():
                    # TODO: check profiles/lineage
                    if schema.type == data_type:
                        matches.append(field_name)
                if not matches:
                    raise Exception(
                        f"Can't assemble filter operation: no potential input field of type {data_type} for endpoint module found."
                    )
                elif len(matches) > 1:
                    raise Exception(
                        f"Can't assemble filter operation: multiple potential input fields of type {data_type} for endpoint module found: {', '.join(matches)}"
                    )
                endpoint_input_field = matches[0]

            if not endpoint_step_id:
                endpoint_step_id = find_free_id(
                    stem=endpoint_module.module_type_name, current_ids=filters.values()
                )
            step_data = {
                "module_type": endpoint_module.module_type_name,
                "module_config": endpoint_module.config.model_dump(),
                "step_id": endpoint_step_id,
            }
            step_data["input_links"] = {
                endpoint_input_field: {f"{last_filter_id}.{last_filter_output_name}"}
            }
            # for field_name in endpoint_module.output_names:
            #     output_aliases[f"{endpoint_step_id}.{field_name}"] = f"endpoint__{field_name}"
            doc = f"{doc}, feeding into endpoing module '{endpoint_module.module_type_name}'."
            steps.append(step_data)
        else:
            doc = f"{doc}."

        if extra_output_aliases:
            for k, v in extra_output_aliases.items():
                output_aliases[k] = v

        if extra_input_aliases:
            input_aliases.update(extra_input_aliases)
            # raise NotImplementedError("Extra input aliases not supported yet.")

        pipeline_config = PipelineConfig.from_config(
            pipeline_name="_filter_pipeline",
            data={
                "steps": steps,
                "input_aliases": input_aliases,
                "output_aliases": output_aliases,
                "doc": doc,
            },
        )

        return pipeline_config

    def create_filter_operation(
        self,
        data_type: str,
        filters: Union[Iterable[str], Mapping[str, str]],
        endpoint: Union[None, Manifest, str] = None,
        endpoint_input_field: Union[str, None] = None,
        endpoint_step_id: Union[str, None] = None,
    ) -> Operation:

        pipeline_config = self.assemble_filter_pipeline_config(
            data_type=data_type,
            filters=filters,
            endpoint=endpoint,
            endpoint_input_field=endpoint_input_field,
            endpoint_step_id=endpoint_step_id,
        )

        manifest = Manifest(
            module_type="pipeline", module_config=pipeline_config.model_dump()
        )
        module = self._kiara.module_registry.create_module(manifest=manifest)

        op_details = PipelineOperationDetails.create_operation_details(
            operation_id=module.config.pipeline_name,
            pipeline_inputs_schema=module.inputs_schema,
            pipeline_outputs_schema=module.outputs_schema,
            pipeline_config=module.config,
        )
        operation = Operation(
            module_type=manifest.module_type,
            module_config=manifest.module_config,
            operation_id=op_details.operation_id,
            operation_details=op_details,
            module_details=KiaraModuleInstance.from_module(module),
            metadata={},
            doc=pipeline_config.doc,
        )
        return operation


# kiara\kiara\src\kiara\operations\included_core_operations\import_data.py
# -*- coding: utf-8 -*-
from typing import TYPE_CHECKING, ClassVar, Iterable, Mapping, Union

import structlog
from pydantic import Field

from kiara.models.module.operation import (
    BaseOperationDetails,
    OperationConfig,
)
from kiara.models.values.value_schema import ValueSchema
from kiara.operations import OperationType

if TYPE_CHECKING:
    from kiara.modules import KiaraModule

logger = structlog.getLogger()


class ImportDataOpDetails(BaseOperationDetails):

    source_type: str = Field(description="The type of the value to be created.")
    target_type: str = Field(description="The result type.")
    optional_args: Mapping[str, ValueSchema] = Field(description="Optional arguments.")

    # def retrieve_inputs_schema(self) -> ValueSetSchema:
    #
    #     result: Dict[str, Union[ValueSchema, Dict[str, Any]]] = {
    #         self.source_type: {"type": self.source_type, "doc": "The source value."},
    #     }
    #     for field, schema in self.optional_args.items():
    #         if field in result.keys():
    #             raise Exception(
    #                 f"Can't create 'create_from' operation '{self.source_type}' -> '{self.target_type}': duplicate input field '{field}'."
    #             )
    #         result[field] = schema
    #     return result
    #
    # def retrieve_outputs_schema(self) -> ValueSetSchema:
    #
    #     return {
    #         self.target_type: {"type": self.target_type, "doc": "The result value."}
    #     }


class ImportDataOperationType(OperationType[ImportDataOpDetails]):

    _operation_type_name: ClassVar[str] = "import_data"

    def _calculate_op_id(self, source_type: str, target_type: str):

        if source_type == "any":
            operation_id = f"import.{target_type}"
        else:
            operation_id = f"create.{target_type}.from.{source_type}"

        return operation_id

    def retrieve_included_operation_configs(
        self,
    ) -> Iterable[Union[Mapping, OperationConfig]]:

        result: Iterable[Union[Mapping, OperationConfig]] = []
        return result
        # for name, module_cls in self._kiara.module_type_classes.items():
        #     if not hasattr(module_cls, "retrieve_supported_create_combinations"):
        #         continue
        #
        #     try:
        #         supported_combinations = module_cls.retrieve_supported_create_combinations()  # type: ignore
        #         for sup_comb in supported_combinations:
        #             source_type = sup_comb["source_type"]
        #             target_type = sup_comb["target_type"]
        #             func = sup_comb["func"]
        #
        #             if source_type not in self._kiara.data_type_names:
        #                 logger.debug(
        #                     "ignore.operation_config",
        #                     module_type=name,
        #                     reason=f"Source type '{source_type}' not registered.",
        #                 )
        #                 continue
        #             if target_type not in self._kiara.data_type_names:
        #                 logger.debug(
        #                     "ignore.operation_config",
        #                     module_type=name,
        #                     reason=f"Target type '{target_type}' not registered.",
        #                 )
        #                 continue
        #             if not hasattr(module_cls, func):
        #                 logger.debug(
        #                     "ignore.operation_config",
        #                     module_type=name,
        #                     reason=f"Specified create function '{func}' not available.",
        #                 )
        #                 continue
        #
        #             mc = {"source_type": source_type, "target_type": target_type}
        #             # TODO: check whether module config actually supports those, for now, only 'CreateFromModule' subtypes are supported
        #             _func = getattr(module_cls, func)
        #             doc = DocumentationMetadataModel.from_function(_func)
        #
        #             oc = ManifestOperationConfig(
        #                 module_type=name, module_config=mc, doc=doc
        #             )
        #             op_id = self._calculate_op_id(
        #                 source_type=source_type, target_type=target_type
        #             )
        #             result[op_id] = oc
        #     except Exception as e:
        #         log_exception(e)
        #         logger.debug(
        #             "ignore.create_operation_instance", module_type=name, reason=e
        #         )
        #         continue
        #
        # return result.values()

    def check_matching_operation(
        self, module: "KiaraModule"
    ) -> Union[ImportDataOpDetails, None]:

        return None

        # if not isinstance(module, CreateFromModule):
        #     return None
        #
        # source_type = None
        # for field_name, schema in module.inputs_schema.items():
        #     if field_name == schema.type:
        #         if source_type is not None:
        #             logger.debug(
        #                 "ignore.operation",
        #                 operation_type="create_from",
        #                 reason=f"more than one possible target type field: {field_name}",
        #             )
        #             return None
        #         source_type = field_name
        #
        # if source_type is None:
        #     return None
        #
        # target_type = None
        # for field_name, schema in module.outputs_schema.items():
        #     if field_name == schema.type:
        #         if target_type is not None:
        #             logger.debug(
        #                 "ignore.operation",
        #                 operation_type="create_from",
        #                 reason=f"more than one possible target type field: {field_name}",
        #             )
        #             return None
        #         target_type = field_name
        #
        # if target_type is None:
        #     return None
        #
        # op_id = self._calculate_op_id(source_type=source_type, target_type=target_type)
        #
        # if (
        #     "any" in self._kiara.type_registry.get_type_lineage(target_type)
        #     and target_type != "any"
        # ):
        #     is_internal = False
        # else:
        #     is_internal = True
        #
        # optional = {}
        # for field, schema in module.inputs_schema.items():
        #     if field == source_type:
        #         continue
        #     optional[field] = schema
        #
        # details = {
        #     "module_inputs_schema": module.inputs_schema,
        #     "module_outputs_schema": module.outputs_schema,
        #     "operation_id": op_id,
        #     "source_type": source_type,
        #     "target_type": target_type,
        #     "optional_args": optional,
        #     "is_internal_operation": is_internal,
        # }
        #
        # result = ImportDataOpDetails.create_operation_details(**details)
        # return result


# kiara\kiara\src\kiara\operations\included_core_operations\metadata.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

from typing import ClassVar, Iterable, Mapping, Type, Union

from pydantic import Field

from kiara.models.module.operation import (
    BaseOperationDetails,
    Operation,
    OperationConfig,
)
from kiara.models.values.value_metadata import ValueMetadata
from kiara.modules import KiaraModule
from kiara.operations import OperationType
from kiara.registries.models import ModelRegistry


class ExtractMetadataDetails(BaseOperationDetails):
    """A model that contains information needed to describe an 'extract_metadata' operation."""

    data_type: str = Field(
        description="The data type this metadata operation can be used with."
    )
    metadata_key: str = Field(description="The metadata key.")
    input_field_name: str = Field(description="The input field name.")
    result_field_name: str = Field(description="The result field name.")

    # def retrieve_inputs_schema(self) -> ValueSetSchema:
    #     return {
    #         "value": {
    #             "type": self.data_type,
    #             "doc": f"The {self.data_type} value to extract metadata from.",
    #         }
    #     }
    #
    # def retrieve_outputs_schema(self) -> ValueSetSchema:
    #
    #     return {"value_metadata": {"type": "value_metadata", "doc": "The metadata."}}
    #
    # def create_module_inputs(self, inputs: Mapping[str, Any]) -> Mapping[str, Any]:
    #     return {self.input_field_name: inputs["value"]}
    #
    # def create_operation_outputs(self, outputs: ValueMap) -> ValueMap:
    #
    #     return outputs


class ExtractMetadataOperationType(OperationType[ExtractMetadataDetails]):
    """
    An operation that extracts metadata of a specific type from value data.

    For a module profile to be picked up by this operation type, it needs to have:
    - exactly one input field
    - that input field must have the same name as its value type, or be 'value'
    - exactly one output field, whose field name is called 'value_metadata', and where the value has the type 'internal_model'
    """

    _operation_type_name: ClassVar[str] = "extract_metadata"

    def retrieve_included_operation_configs(
        self,
    ) -> Iterable[Union[Mapping, OperationConfig]]:

        model_registry = ModelRegistry.instance()
        all_models = model_registry.get_models_of_type(ValueMetadata)

        result = []
        for model_id, model_cls_info in all_models.item_infos.items():
            model_cls: Type[ValueMetadata] = model_cls_info.python_class.get_class()  # type: ignore
            metadata_key = model_cls._metadata_key  # type: ignore
            data_types = model_cls.retrieve_supported_data_types()
            if isinstance(data_types, str):
                data_types = [data_types]
            for data_type in data_types:

                config = {
                    "module_type": "value.extract_metadata",
                    "module_config": {
                        "data_type": data_type,
                        "kiara_model_id": model_cls._kiara_model_id,  # type: ignore
                    },
                    "doc": f"Extract '{metadata_key}' metadata for value type '{data_type}'.",
                }
                result.append(config)

        return result

    def check_matching_operation(
        self, module: "KiaraModule"
    ) -> Union[ExtractMetadataDetails, None]:

        if len(module.outputs_schema) != 1:
            return None
        if (
            "value_metadata" not in module.outputs_schema
            or module.outputs_schema["value_metadata"].type != "internal_model"
        ):
            return None
        if len(module.inputs_schema) != 1:
            return None

        input_field_name = next(iter(module.inputs_schema.keys()))
        input_schema = module.inputs_schema.get(input_field_name)
        assert input_schema is not None
        if input_field_name not in (input_schema.type, "value"):
            return None

        data_type_name = module.inputs_schema["value"].type
        model_id: str = module.get_config_value("kiara_model_id")

        registry = ModelRegistry.instance()
        metadata_model_cls = registry.get_model_cls(
            kiara_model_id=model_id, required_subclass=ValueMetadata
        )

        metadata_key = metadata_model_cls._metadata_key  # type: ignore

        if data_type_name == "any":
            op_id = f"extract.{metadata_key}.metadata"
        else:
            op_id = f"extract.{metadata_key}.metadata.from.{data_type_name}"

        details: ExtractMetadataDetails = (
            ExtractMetadataDetails.create_operation_details(
                module_inputs_schema=module.inputs_schema,
                module_outputs_schema=module.outputs_schema,
                operation_id=op_id,
                data_type=data_type_name,
                metadata_key=metadata_key,
                input_field_name=input_field_name,
                result_field_name="value_metadata",
                is_internal_operation=True,
            )
        )

        return details

    def get_operations_for_data_type(self, data_type: str) -> Mapping[str, Operation]:
        """
        Return all available metadata extract operations for the provided type (and it's parent types).

        Arguments:
        ---------
            data_type: the value type

        Returns:
        -------
            a mapping with the metadata type as key, and the operation as value
        """
        lineage = set(
            self._kiara.type_registry.get_type_lineage(data_type_name=data_type)
        )

        result = {}

        for op_id, op in self.operations.items():
            op_details = self.retrieve_operation_details(op)
            included = op_details.data_type in lineage
            if not included:
                continue
            metadata_key = op_details.metadata_key
            if metadata_key in result:
                raise Exception(
                    f"Duplicate metadata operations for type '{metadata_key}'."
                )

            result[metadata_key] = op

        return result


# kiara\kiara\src\kiara\operations\included_core_operations\pipeline.py
# -*- coding: utf-8 -*-
import os
from typing import TYPE_CHECKING, Any, ClassVar, Dict, Iterable, Mapping, Union

import structlog
from pydantic import Field, PrivateAttr
from ruamel.yaml import YAML

from kiara.models.module.operation import (
    OperationConfig,
    OperationDetails,
    OperationSchema,
    PipelineOperationConfig,
)
from kiara.models.module.pipeline import PipelineConfig
from kiara.models.values.value_schema import ValueSchema
from kiara.modules import KiaraModule
from kiara.modules.included_core_modules.pipeline import PipelineModule
from kiara.operations import OperationType
from kiara.utils.class_loading import find_all_kiara_pipeline_paths
from kiara.utils.pipelines import find_pipeline_data_in_paths

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)


if TYPE_CHECKING:
    from kiara.context import Kiara

logger = structlog.getLogger()
yaml = YAML(typ="safe")


class PipelineOperationDetails(OperationDetails):

    pipeline_inputs_schema: Mapping[str, ValueSchema] = Field(
        description="The input schema for the pipeline."
    )
    pipeline_outputs_schema: Mapping[str, ValueSchema] = Field(
        description="The output schema for the pipeline."
    )
    pipeline_config: PipelineConfig = Field(description="The pipeline config.")
    _op_schema: OperationSchema = PrivateAttr(default=None)

    def get_operation_schema(self) -> OperationSchema:

        if self._op_schema is not None:
            return self._op_schema

        self._op_schema = OperationSchema(
            alias=self.operation_id,
            inputs_schema=self.pipeline_inputs_schema,
            outputs_schema=self.pipeline_outputs_schema,
        )
        return self._op_schema

    # def create_module_inputs(self, inputs: Mapping[str, Any]) -> Mapping[str, Any]:
    #     return inputs
    #
    # def create_operation_outputs(self, outputs: ValueMap) -> Mapping[str, Value]:
    #     return outputs


class PipelineOperationType(OperationType[PipelineOperationDetails]):

    _operation_type_name: ClassVar[str] = "pipeline"

    def __init__(self, kiara: "Kiara", op_type_name: str) -> None:

        super().__init__(kiara=kiara, op_type_name=op_type_name)
        self._pipelines: Union[None, Mapping[str, Mapping[str, Any]]] = None

    @property
    def pipeline_data(self) -> Mapping[str, Mapping[str, Any]]:

        if self._pipelines is not None:
            return self._pipelines

        ignore_errors = False
        pipeline_paths: Dict[str, Union[Dict[str, Any], None]] = (
            find_all_kiara_pipeline_paths(skip_errors=ignore_errors)
        )

        for ep in self._kiara.context_config.extra_pipelines:
            ep = os.path.realpath(ep)
            if ep not in pipeline_paths.keys():
                pipeline_paths[ep] = None

        return find_pipeline_data_in_paths(pipeline_paths)

    def retrieve_included_operation_configs(
        self,
    ) -> Iterable[Union[Mapping, OperationConfig]]:

        op_configs = []
        for pipeline_name, pipeline_data in self.pipeline_data.items():
            pipeline_config: Dict[str, Any] = dict(pipeline_data["data"])
            pipeline_id = pipeline_config.pop("pipeline_name", None)
            doc = pipeline_config.get("doc", None)
            pipeline_metadata = pipeline_data["metadata"]

            op_details = PipelineOperationConfig(
                pipeline_name=pipeline_id,
                pipeline_config=pipeline_config,
                doc=doc,
                metadata=pipeline_metadata,
            )
            op_configs.append(op_details)
        return op_configs

    def check_matching_operation(
        self, module: "KiaraModule"
    ) -> Union[PipelineOperationDetails, None]:

        if isinstance(module, PipelineModule):

            op_details: PipelineOperationDetails = (
                PipelineOperationDetails.create_operation_details(
                    operation_id=module.config.pipeline_name,
                    pipeline_inputs_schema=module.inputs_schema,
                    pipeline_outputs_schema=module.outputs_schema,
                    pipeline_config=module.config,
                )
            )
            return op_details
        else:
            return None


# kiara\kiara\src\kiara\operations\included_core_operations\pretty_print.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

from typing import ClassVar, Dict, Iterable, Mapping, Union

from pydantic import Field

from kiara.models.documentation import DocumentationMetadataModel
from kiara.models.module.operation import (
    BaseOperationDetails,
    ManifestOperationConfig,
    Operation,
    OperationConfig,
)
from kiara.modules import KiaraModule
from kiara.modules.included_core_modules.pretty_print import PrettyPrintModule
from kiara.operations import OperationType
from kiara.utils import log_message


class PrettyPrintDetails(BaseOperationDetails):

    source_type: str = Field(description="The type of the value to be rendered.")
    target_type: str = Field(description="The type of the render result.")

    # def retrieve_inputs_schema(self) -> ValueSetSchema:
    #
    #     return {
    #         "value": {"type": "any", "doc": "The value to persist."},
    #         "render_type": {
    #             "type": "string",
    #             "doc": "The render target/type of render output.",
    #         },
    #         "render_config": {
    #             "type": "dict",
    #             "doc": "A value type specific configuration for how to render the data.",
    #             "optional": True,
    #         },
    #     }
    #
    # def retrieve_outputs_schema(self) -> ValueSetSchema:
    #
    #     return {"rendered_value": {"type": "any", "doc": "The rendered value."}}
    #
    # def create_module_inputs(self, inputs: Mapping[str, Any]) -> Mapping[str, Any]:
    #
    #     return {
    #         self.source_type: inputs["value"],
    #         "render_config": inputs.get("render_config", None),
    #     }
    #
    # def create_operation_outputs(self, outputs: ValueMap) -> Mapping[str, Value]:
    #     return {"rendered_value": outputs.get_value_obj("rendered_value")}


class PrettyPrintOperationType(OperationType[PrettyPrintDetails]):
    """
    An operation that takes a value, and renders into a format that can be printed for output..

    For a module profile to be picked up by this operation type, it needs to have:
    - exactly one output field named "rendered_value"
    - exactly two input fields, one of them named after the type it supports, and the other called 'render_config', of type 'dict'
    """

    _operation_type_name: ClassVar[str] = "pretty_print"

    def _calculate_op_id(self, source_type: str, target_type: str):

        if source_type == "any":
            operation_id = f"pretty_print.as.{target_type}"
        else:
            operation_id = f"pretty_print.{source_type}.as.{target_type}"

        return operation_id

    def retrieve_included_operation_configs(
        self,
    ) -> Iterable[Union[Mapping, OperationConfig]]:

        result = {}
        for name, module_cls in self._kiara.module_type_classes.items():

            if not issubclass(module_cls, PrettyPrintModule):
                continue

            for (
                source_type,
                target_type,
            ) in module_cls.retrieve_supported_render_combinations():
                if source_type not in self._kiara.data_type_names:
                    log_message("ignore.operation_config", operation_type="pretty_print", module_type=module_cls._module_type_name, source_type=source_type, target_type=target_type, reason=f"Source type '{source_type}' not registered.")  # type: ignore
                    continue
                if target_type not in self._kiara.data_type_names:
                    log_message(
                        "ignore.operation_config",
                        operation_type="pretty_print",
                        module_type=module_cls._module_type_name,
                        source_type=source_type,  # type: ignore
                        target_type=target_type,
                        reason=f"Target type '{target_type}' not registered.",
                    )
                    continue
                func_name = f"pretty_print__{source_type}__as__{target_type}"
                attr = getattr(module_cls, func_name)
                doc = DocumentationMetadataModel.from_function(attr)
                mc = {"source_type": source_type, "target_type": target_type}
                oc = ManifestOperationConfig(
                    module_type=name, module_config=mc, doc=doc
                )
                op_id = self._calculate_op_id(
                    source_type=source_type, target_type=target_type
                )
                result[op_id] = oc

        for data_type_name, data_type_class in self._kiara.data_type_classes.items():
            for attr in data_type_class.__dict__.keys():
                if not attr.startswith("pretty_print_as__"):
                    continue

                target_type = attr[17:]
                if target_type not in self._kiara.data_type_names:
                    log_message(
                        "operation_config.ignore",
                        operation_type="pretty_print",
                        source_type=data_type_name,
                        target_type=target_type,
                        reason=f"Target type '{target_type}' not registered.",
                    )  # type: ignore

                # TODO: inspect signature?
                doc = DocumentationMetadataModel.from_string(
                    f"Pretty print a {data_type_name} value as a {target_type}."
                )
                mc = {
                    "source_type": data_type_name,
                    "target_type": target_type,
                }
                oc = ManifestOperationConfig(
                    module_type="pretty_print.value", module_config=mc, doc=doc
                )
                result[f"_type_{data_type_name}__{target_type}"] = oc
        return result.values()

    def check_matching_operation(
        self, module: "KiaraModule"
    ) -> Union[PrettyPrintDetails, None]:

        details = self.extract_details(module)

        if details is None:
            return None
        else:
            return details

    def extract_details(self, module: "KiaraModule") -> Union[PrettyPrintDetails, None]:

        if len(module.inputs_schema) != 2 or len(module.outputs_schema) != 1:
            return None

        if "rendered_value" not in module.outputs_schema.keys():
            return None
        target_type = module.outputs_schema["rendered_value"].type

        if "value" not in module.inputs_schema.keys():
            return None
        if "render_config" not in module.inputs_schema.keys():
            return None

        input_field_match = "value"

        input_field_type = module.inputs_schema[input_field_match].type

        operation_id = self._calculate_op_id(
            source_type=input_field_type, target_type=target_type
        )

        details = {
            "module_inputs_schema": module.inputs_schema,
            "module_outputs_schema": module.outputs_schema,
            "operation_id": operation_id,
            "source_type": input_field_type,
            "target_type": target_type,
            "is_internal_operation": True,
        }

        result: PrettyPrintDetails = PrettyPrintDetails.create_operation_details(
            **details
        )
        return result

    def get_target_types_for(self, source_type: str) -> Mapping[str, Operation]:

        # TODO: support for sub-types
        result: Dict[str, Operation] = {}
        for operation in self.operations.values():
            details = self.retrieve_operation_details(operation)

            if details.source_type == source_type:
                target_type = details.target_type
                if target_type in result.keys():
                    raise Exception(
                        f"More than one operation for pretty_print combination '{source_type}'/'{target_type}', this is not supported (for now)."
                    )
                result[target_type] = operation

        return result

    def get_operation_for_render_combination(
        self, source_type: str, target_type: str
    ) -> Operation:

        type_lineage = self._kiara.type_registry.get_type_lineage(
            data_type_name=source_type
        )

        for st in type_lineage:
            target_types = self.get_target_types_for(source_type=st)
            if not target_types:
                continue
            if target_type not in target_types.keys():
                raise Exception(
                    f"No operation that produces '{target_type}' for source type: {st}."
                )
            result = target_types[target_type]
            return result

        raise Exception(f"No pretty_print opration(s) for source type: {source_type}.")


# kiara\kiara\src\kiara\operations\included_core_operations\render_data.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

from typing import ClassVar, Dict, Iterable, Mapping, Union

import structlog
from pydantic import Field

from kiara.models.module.operation import (
    BaseOperationDetails,
    Operation,
    OperationConfig,
)
from kiara.modules import KiaraModule
from kiara.operations import OperationType

logger = structlog.getLogger()


class RenderDataDetails(BaseOperationDetails):
    """A model that contains information needed to describe an 'extract_metadata' operation."""

    source_data_type: str = Field(description="The data type that will be rendered.")
    target_data_type: str = Field(description="The rendered data type.")


class RenderDataOperationType(OperationType[RenderDataDetails]):
    """An operation that renders data (and metadata) associated with a value."""

    _operation_type_name: ClassVar[str] = "render_data"

    def _calculate_op_id(cls, source_type: str, target_type: str):

        if source_type == "any":
            operation_id = f"render.as.{target_type}"
        else:
            operation_id = f"render.{source_type}.as.{target_type}"

        return operation_id

    def retrieve_included_operation_configs(
        self,
    ) -> Iterable[Union[Mapping, OperationConfig]]:

        # result = {}
        return []
        # for name, module_cls in self._kiara.module_type_classes.items():
        #
        #     if not issubclass(module_cls, RenderValueModule):
        #         continue
        #
        #     for (
        #         source_type,
        #         target_type,
        #     ) in module_cls.retrieve_supported_render_combinations():
        #         if source_type not in self._kiara.data_type_names:
        #             log_message("ignore.operation_config", operation_type="render_value", module_type=module_cls._module_type_name, source_type=source_type, target_type=target_type, reason=f"Source type '{source_type}' not registered.")  # type: ignore
        #             continue
        #         if target_type not in self._kiara.data_type_names:
        #             log_message(
        #                 "ignore.operation_config",
        #                 operation_type="render_value",
        #                 module_type=module_cls._module_type_name,
        #                 source_type=source_type,  # type: ignore
        #                 target_type=target_type,
        #                 reason=f"Target type '{target_type}' not registered.",
        #             )
        #             continue
        #         func_name = f"render__{source_type}__as__{target_type}"
        #         attr = getattr(module_cls, func_name)
        #         doc = DocumentationMetadataModel.from_function(attr)
        #         mc = {"source_type": source_type, "target_type": target_type}
        #         oc = ManifestOperationConfig(
        #             module_type=name, module_config=mc, doc=doc
        #         )
        #         op_id = self._calculate_op_id(
        #             source_type=source_type, target_type=target_type
        #         )
        #         result[op_id] = oc
        #
        # for data_type_name, data_type_class in self._kiara.data_type_classes.items():
        #     for attr in data_type_class.__dict__.keys():
        #         if not attr.startswith("render_as__"):
        #             continue
        #
        #         target_type = attr[11:]
        #         if target_type not in self._kiara.data_type_names:
        #             log_message(
        #                 "operation_config.ignore",
        #                 operation_type="render_value",
        #                 source_type=data_type_name,
        #                 target_type=target_type,
        #                 reason=f"Target type '{target_type}' not registered.",
        #             )  # type: ignore
        #
        #         # TODO: inspect signature?
        #         doc = DocumentationMetadataModel.from_string(
        #             f"Render a '{data_type_name}' value as a {target_type}."
        #         )
        #         mc = {
        #             "source_type": data_type_name,
        #             "target_type": target_type,
        #         }
        #         oc = ManifestOperationConfig(
        #             module_type="render.value", module_config=mc, doc=doc
        #         )
        #
        #         result[f"_type_{data_type_name}_{target_type}"] = oc
        #
        # return result.values()

    def check_matching_operation(
        self, module: "KiaraModule"
    ) -> Union[RenderDataDetails, None]:

        if len(module.inputs_schema) != 2:
            return None

        if len(module.outputs_schema) != 1:
            return None

        if "value" not in module.inputs_schema.keys():
            return None

        if (
            "render_config" not in module.inputs_schema.keys()
            or module.inputs_schema["render_config"].type != "dict"
        ):
            return None

        if (
            "render_value_result" not in module.outputs_schema.keys()
            or module.outputs_schema["render_value_result"].type
            != "render_value_result"
        ):
            return None

        source_type = module.inputs_schema["value"].type
        target_type = module.get_config_value("target_type")

        if source_type == "any":
            op_id = f"render.as.{target_type}"
        else:
            op_id = f"render.{source_type}.as.{target_type}"

        details: RenderDataDetails = RenderDataDetails.create_operation_details(
            module_inputs_schema=module.inputs_schema,
            module_outputs_schema=module.outputs_schema,
            operation_id=op_id,
            source_data_type=source_type,
            target_data_type=target_type,
            is_internal_operation=True,
        )

        return details

    def get_render_operations_for_source_type(
        self, source_type: str
    ) -> Mapping[str, Operation]:
        """
        Return all render operations for the specified data type.

        Arguments:
        ---------
            source_type: the data type to render

        Returns:
        -------
            a mapping with the target type as key, and the operation as value
        """
        if source_type not in self._kiara.data_type_names:
            source_type = "any"

        lineage = self._kiara.type_registry.get_type_lineage(data_type_name=source_type)

        result: Dict[str, Operation] = {}

        for data_type in lineage:

            for op_id, op in self.operations.items():
                op_details = self.retrieve_operation_details(op)
                match = op_details.source_data_type == data_type
                if not match:
                    continue
                target_type = op_details.target_data_type
                if target_type in result.keys():
                    continue
                result[target_type] = op

        return result

    def get_render_operations_for_target_type(
        self, target_type: str
    ) -> Mapping[str, Operation]:
        """
        Return all render operations that renders to the specified data type.

        Arguments:
        ---------
            target_type: the result data type

        Returns:
        -------
            a mapping with the source type as key, and the operation as value
        """
        # TODO: consider type lineages

        if target_type not in self._kiara.data_type_names:
            raise Exception(f"Invalid target data type: {target_type}")

        result: Dict[str, Operation] = {}

        for op_id, op in self.operations.items():
            op_details = self.retrieve_operation_details(op)
            match = op_details.target_data_type == target_type
            if not match:
                continue
            source_type = op_details.source_data_type
            if source_type in result.keys():
                continue

            result[source_type] = op

        return result

    def get_render_operation(
        self, source_type: str, target_type: str
    ) -> Union[Operation, None]:

        all_ops = self.get_render_operations_for_source_type(source_type=source_type)
        return all_ops.get(target_type, None)


# kiara\kiara\src\kiara\operations\included_core_operations\render_value.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

from typing import ClassVar, Dict, Iterable, Mapping, Union

import structlog
from pydantic import Field

from kiara.models.documentation import DocumentationMetadataModel
from kiara.models.module.operation import (
    BaseOperationDetails,
    ManifestOperationConfig,
    Operation,
    OperationConfig,
)
from kiara.modules import KiaraModule
from kiara.modules.included_core_modules.render_value import RenderValueModule
from kiara.operations import OperationType
from kiara.utils import log_message

logger = structlog.getLogger()


class RenderValueDetails(BaseOperationDetails):
    """A model that contains information needed to describe an 'extract_metadata' operation."""

    source_data_type: str = Field(description="The data type that will be rendered.")
    target_data_type: str = Field(description="The rendered data type.")


class RenderValueOperationType(OperationType[RenderValueDetails]):
    """
    An operation that renders a value.

    A 'render_value' operation typically is named follwing the pattern:

    ```
     'render.<source_type>.as.<target_type>'
    ```

    It has 2 inputs:
      - 'value': the value to render
      - 'reneer_config' a target type-specific configuration dict

    And one output:
      - `render_value_result`: using internal type [RenderValueResultDataType][kiara.data_types.included_core_types.internal.render_value.RenderValueResultDataType]

    """

    _operation_type_name: ClassVar[str] = "render_value"

    def _calculate_op_id(cls, source_type: str, target_type: str):

        if source_type == "any":
            operation_id = f"render.as.{target_type}"
        else:
            operation_id = f"render.{source_type}.as.{target_type}"

        return operation_id

    def retrieve_included_operation_configs(
        self,
    ) -> Iterable[Union[Mapping, OperationConfig]]:

        result = {}
        for name, module_cls in self._kiara.module_type_classes.items():

            if not issubclass(module_cls, RenderValueModule):
                continue

            for (
                source_type,
                target_type,
            ) in module_cls.retrieve_supported_render_combinations():
                if source_type not in self._kiara.data_type_names:
                    log_message("ignore.operation_config", operation_type="render_value", module_type=module_cls._module_type_name, source_type=source_type, target_type=target_type, reason=f"Source type '{source_type}' not registered.")  # type: ignore
                    continue
                if target_type not in self._kiara.data_type_names:
                    log_message(
                        "ignore.operation_config",
                        operation_type="render_value",
                        module_type=module_cls._module_type_name,
                        source_type=source_type,  # type: ignore
                        target_type=target_type,
                        reason=f"Target type '{target_type}' not registered.",
                    )
                    continue
                func_name = f"render__{source_type}__as__{target_type}"
                attr = getattr(module_cls, func_name)
                doc = DocumentationMetadataModel.from_function(attr)
                mc = {"source_type": source_type, "target_type": target_type}
                oc = ManifestOperationConfig(
                    module_type=name, module_config=mc, doc=doc
                )
                op_id = self._calculate_op_id(
                    source_type=source_type, target_type=target_type
                )
                result[op_id] = oc

        for data_type_name, data_type_class in self._kiara.data_type_classes.items():
            for attr in data_type_class.__dict__.keys():
                if not attr.startswith("render_as__"):
                    continue

                target_type = attr[11:]
                if target_type not in self._kiara.data_type_names:
                    log_message(
                        "operation_config.ignore",
                        operation_type="render_value",
                        source_type=data_type_name,
                        target_type=target_type,
                        reason=f"Target type '{target_type}' not registered.",
                    )  # type: ignore

                # TODO: inspect signature?
                doc = DocumentationMetadataModel.from_string(
                    f"Render a '{data_type_name}' value as a {target_type}."
                )
                mc = {
                    "source_type": data_type_name,
                    "target_type": target_type,
                }
                oc = ManifestOperationConfig(
                    module_type="render.value", module_config=mc, doc=doc
                )

                result[f"_type_{data_type_name}_{target_type}"] = oc

        return result.values()

    def check_matching_operation(
        self, module: "KiaraModule"
    ) -> Union[RenderValueDetails, None]:

        if len(module.inputs_schema) != 2:
            return None

        if len(module.outputs_schema) != 1:
            return None

        if "value" not in module.inputs_schema.keys():
            return None

        if (
            "render_config" not in module.inputs_schema.keys()
            or module.inputs_schema["render_config"].type != "dict"
        ):
            return None

        if (
            "render_value_result" not in module.outputs_schema.keys()
            or module.outputs_schema["render_value_result"].type
            != "render_value_result"
        ):
            return None

        source_type = module.inputs_schema["value"].type
        target_type = module.get_config_value("target_type")

        if source_type == "any":
            op_id = f"render.as.{target_type}"
        else:
            op_id = f"render.{source_type}.as.{target_type}"

        details: RenderValueDetails = RenderValueDetails.create_operation_details(
            module_inputs_schema=module.inputs_schema,
            module_outputs_schema=module.outputs_schema,
            operation_id=op_id,
            source_data_type=source_type,
            target_data_type=target_type,
            is_internal_operation=True,
        )

        return details

    def get_render_operations_for_source_type(
        self, source_type: str
    ) -> Mapping[str, Operation]:
        """
        Return all render operations for the specified data type.

        Arguments:
        ---------
            source_type: the data type to render

        Returns:
        -------
            a mapping with the target type as key, and the operation as value
        """
        if source_type not in self._kiara.data_type_names:
            source_type = "any"

        lineage = self._kiara.type_registry.get_type_lineage(data_type_name=source_type)

        result: Dict[str, Operation] = {}

        for data_type in lineage:

            for op_id, op in self.operations.items():
                op_details = self.retrieve_operation_details(op)
                match = op_details.source_data_type == data_type
                if not match:
                    continue
                target_type = op_details.target_data_type
                if target_type in result.keys():
                    continue
                result[target_type] = op

        return result

    def get_render_operations_for_target_type(
        self, target_type: str
    ) -> Mapping[str, Operation]:
        """
        Return all render operations that renders to the specified data type.

        Arguments:
        ---------
            target_type: the result data type

        Returns:
        -------
            a mapping with the source type as key, and the operation as value
        """
        # TODO: consider type lineages

        if target_type not in self._kiara.data_type_names:
            raise Exception(f"Invalid target data type: {target_type}")

        result: Dict[str, Operation] = {}

        for op_id, op in self.operations.items():
            op_details = self.retrieve_operation_details(op)
            match = op_details.target_data_type == target_type
            if not match:
                continue
            source_type = op_details.source_data_type
            if source_type in result.keys():
                continue

            result[source_type] = op

        return result

    def get_render_operation(
        self, source_type: str, target_type: str
    ) -> Union[Operation, None]:

        all_ops = self.get_render_operations_for_source_type(source_type=source_type)
        return all_ops.get(target_type, None)


# kiara\kiara\src\kiara\operations\included_core_operations\serialize.py
# -*- coding: utf-8 -*-

# Copyright (c) 2021, University of Luxembourg / DHARPA project
# Copyright (c) 2021, Markus Binsteiner
#
# Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

from typing import TYPE_CHECKING, Any, ClassVar, Dict, Iterable, List, Mapping, Union

from pydantic import Field

from kiara.models.documentation import DocumentationMetadataModel
from kiara.models.module.operation import (
    BaseOperationDetails,
    ManifestOperationConfig,
    Operation,
    OperationConfig,
)
from kiara.operations import OperationType
from kiara.utils import log_message

if TYPE_CHECKING:
    from kiara.modules import KiaraModule


class DeSerializeDetails(BaseOperationDetails):

    value_type: str = Field(
        "The name of the input field for the serialized version of the value."
    )
    value_input_field: str = Field(
        "The name of the input field for the serialized version of the value."
    )
    object_output_field: str = Field(
        description="The (output) field name containing the deserialized python class."
    )
    serialization_profile: str = Field(
        description="The name for the serialization profile used on the source value."
    )
    target_profile: str = Field(description="The target profile name.")
    # target_class: PythonClass = Field(
    #     description="The python class of the result object."
    # )

    # def retrieve_inputs_schema(self) -> ValueSetSchema:
    #
    #     return {"value": {"type": self.value_type, "doc": "The value to de-serialize."}}
    #
    # def retrieve_outputs_schema(self) -> ValueSetSchema:
    #
    #     return {
    #         "python_object": {
    #             "type": "python_object",
    #             "doc": "The de-serialized python object instance.",
    #         }
    #     }
    #
    # def create_module_inputs(self, inputs: Mapping[str, Any]) -> Mapping[str, Any]:
    #
    #     result = {self.value_input_field: inputs["value"]}
    #     return result
    #
    # def create_operation_outputs(self, outputs: ValueMap) -> Mapping[str, Value]:
    #
    #     return outputs


class DeSerializeOperationType(OperationType[DeSerializeDetails]):
    """
    An operation that takes a value, and serializes it into the format suitable to the [`serialized_value`][kiara.data_types.included_core_types.SeriailzedValue] value type.

    For a module profile to be picked up by this operation type, it needs to have:
    - exactly one output field of type `serialized_value`
    - either one of (in this order):
      - exactly one input field
      - one input field where the field name equals the type name
      - an input field called 'value'
    """

    _operation_type_name: ClassVar[str] = "deserialize"

    def retrieve_included_operation_configs(
        self,
    ) -> Iterable[Union[Mapping, OperationConfig]]:
        result = []
        for name, module_cls in self._kiara.module_type_classes.items():

            if not hasattr(module_cls, "retrieve_serialized_value_type"):
                continue
            if not hasattr(module_cls, "retrieve_supported_target_profiles"):
                continue
            if not hasattr(module_cls, "retrieve_supported_serialization_profile"):
                continue

            try:
                value_type = module_cls.retrieve_serialized_value_type()  # type: ignore
            except TypeError:
                raise Exception(
                    f"Can't retrieve source value type for deserialization module '{module_cls.__name__}'. This is most likely a bug, maybe you are missing a '@classmethod' annotation on the 'retrieve_source_value_type' method?"
                )
            try:
                serialization_profile = module_cls.retrieve_supported_serialization_profile()  # type: ignore
            except TypeError:
                raise Exception(
                    f"Can't retrieve supported serialization profiles for deserialization module '{module_cls.__name__}'. This is most likely a bug, maybe you are missing a '@classmethod' annotation on the 'retrieve_supported_serialization_profile' method?"
                )

            try:
                target_profiles = module_cls.retrieve_supported_target_profiles()  # type: ignore
            except TypeError:
                raise Exception(
                    f"Can't retrieve supported target profile for deserialization module '{module_cls.__name__}'. This is most likely a bug, maybe you are missing a '@classmethod' annotation on the 'retrieve_supported_target_profile' method?"
                )

            for _profile_name, cls in target_profiles.items():
                func_name = f"to__{_profile_name}"
                attr = getattr(module_cls, func_name)
                doc = DocumentationMetadataModel.from_function(attr)
                mc = {
                    "value_type": value_type,
                    "target_profile": _profile_name,
                    "serialization_profile": serialization_profile,
                    # "target_class": PythonClass.from_class(cls),
                }
                oc = ManifestOperationConfig(
                    module_type=name, module_config=mc, doc=doc
                )
                result.append(oc)

        return result

    def check_matching_operation(
        self, module: "KiaraModule"
    ) -> Union[DeSerializeDetails, None]:

        details = self.extract_details(module)

        if details is None:
            return None
        else:
            return details

    def extract_details(self, module: "KiaraModule") -> Union[DeSerializeDetails, None]:

        result_field_name = None
        for field_name, schema in module.outputs_schema.items():
            if schema.type != "python_object":
                continue
            else:
                if result_field_name is not None:
                    log_message(
                        "ignore.operation",
                        reason=f"found more than one potential result value field: {result_field_name} -- {field_name}'",
                        module_type=module.module_type_name,
                    )
                    continue
                else:
                    result_field_name = field_name

        if not result_field_name:
            return None

        input_field_name = None
        for field_name, schema in module.inputs_schema.items():
            if field_name != schema.type:
                continue
            if input_field_name is not None:
                log_message(
                    "ignore.operation",
                    reason=f"found more than one potential result value field: {result_field_name} -- {field_name}'",
                    module_type=module.module_type_name,
                )
                continue
            else:
                input_field_name = field_name

        if not input_field_name:
            return None

        try:
            value_type = module.config.get("value_type")
            target_profile = module.config.get("target_profile")
            serialization_profile = module.config.get("serialization_profile")
            # target_class = module.config.get("target_class")
        except Exception as e:
            log_message(
                "ignore.operation",
                reason=str(e),
                module_type=module.module_type_name,
            )
            return None

        if value_type not in self._kiara.type_registry.get_data_type_names(
            include_profiles=True
        ):
            log_message(
                "ignore.operation",
                reason=f"Invalid value type: {value_type}",
                module_type=module.module_type_name,
            )
            return None

        if input_field_name == "any":
            operation_id = "deserialize.value"
        else:
            operation_id = f"deserialize.{input_field_name}.as.{target_profile}"

        details: Dict[str, Any] = {
            "module_inputs_schema": module.inputs_schema,
            "module_outputs_schema": module.outputs_schema,
            "operation_id": operation_id,
            "value_type": input_field_name,
            "value_input_field": input_field_name,
            "object_output_field": result_field_name,
            "target_profile": target_profile,
            "serialization_profile": serialization_profile,
            # "target_class": target_class,
            "is_internal_operation": True,
        }

        result = DeSerializeDetails(**details)
        return result

    def find_deserialization_operations_for_type(
        self, type_name: str
    ) -> List[Operation]:

        lineage = self._kiara.type_registry.get_type_lineage(type_name)
        result = []
        for data_type in lineage:
            match = []
            for op in self.operations.values():
                details = self.retrieve_operation_details(op)
                if details.value_type == data_type:
                    match.append(op)

            result.extend(match)

        return result

    def find_deserialzation_operation_for_type_and_profile(
        self, type_name: str, serialization_profile: str
    ) -> List[Operation]:

        lineage = self._kiara.type_registry.get_type_lineage(type_name)
        serialize_ops: List[Operation] = []
        for data_type in lineage:
            match = []
            op = None
            for op in self.operations.values():
                details = self.retrieve_operation_details(op)
                if (
                    details.value_type == data_type
                    and details.serialization_profile == serialization_profile
                ):
                    match.append(op)

            if match:
                if len(match) > 1:
                    assert op is not None
                    raise Exception(
                        f"Multiple deserialization operations found for data type '{type_name}' and serialization profile '{serialization_profile}'. This is not supported (yet)."
                    )
                serialize_ops.append(match[0])

        return serialize_ops


# kiara\kiara\src\kiara\operations\included_core_operations\__init__.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

from typing import TYPE_CHECKING, ClassVar, Dict, Iterable, List, Mapping, Type, Union

import structlog
from pydantic import Field, PrivateAttr

from kiara.exceptions import KiaraException
from kiara.models.documentation import DocumentationMetadataModel
from kiara.models.module.operation import (
    ManifestOperationConfig,
    OperationConfig,
    OperationDetails,
    OperationSchema,
)
from kiara.models.values.value_schema import ValueSchema
from kiara.modules import KiaraModule
from kiara.operations import OperationType

if TYPE_CHECKING:
    from multiformats import CID

    from kiara.context import Kiara

logger = structlog.getLogger()


class CustomModuleOperationDetails(OperationDetails):
    @classmethod
    def create_from_module(cls, module: KiaraModule):

        return CustomModuleOperationDetails(
            operation_id=module.module_type_name,
            module_inputs_schema=module.inputs_schema,
            module_outputs_schema=module.outputs_schema,
        )

    module_inputs_schema: Mapping[str, ValueSchema] = Field(
        description="The input schemas of the module."
    )
    module_outputs_schema: Mapping[str, ValueSchema] = Field(
        description="The output schemas of the module."
    )
    _op_schema: OperationSchema = PrivateAttr(default=None)

    def get_operation_schema(self) -> OperationSchema:

        if self._op_schema is not None:
            return self._op_schema

        self._op_schema = OperationSchema(
            alias=self.operation_id,
            inputs_schema=self.module_inputs_schema,
            outputs_schema=self.module_outputs_schema,
        )
        return self._op_schema


class CustomModuleOperationType(OperationType[CustomModuleOperationDetails]):

    _operation_type_name: ClassVar[str] = "custom_module"

    def __init__(self, kiara: "Kiara", op_type_name: str):
        self._included_operations_cache: Dict[type, List["ManifestOperationConfig"]] = (
            {}
        )
        self._included_operations_lookup_cache: Dict[type, Dict["CID", str]] = {}

        super().__init__(kiara=kiara, op_type_name=op_type_name)

    def _retrieve_included_operations(
        self, module_cls: Type[KiaraModule]
    ) -> List["ManifestOperationConfig"]:

        if self._included_operations_cache.get(module_cls, None) is not None:
            return self._included_operations_cache.get(module_cls)  # type: ignore

        from kiara.models.module.operation import ManifestOperationConfig

        this_module_type_name: str = module_cls._module_type_name  # type: ignore

        doc = None
        cache: List[ManifestOperationConfig] = []
        lookup_cache: Dict["CID", str] = {}

        op_ids: List[str] = []

        if hasattr(module_cls, "retrieve_included_operations"):
            manifests = module_cls.retrieve_included_operations()  # type: ignore
            for op_id, op in manifests.items():

                if op_id in op_ids:
                    raise KiaraException(
                        msg=f"Included operation '{op_id}' invalid.",
                        reason="Duplicate operation id.",
                    )
                op_ids.append(op_id)

                if isinstance(op, Mapping):
                    mtn = op.get("module_type", None)
                    if not mtn:
                        op = dict(op)
                        op["module_type"] = this_module_type_name
                        if "doc" not in op.keys():
                            if doc is None:
                                doc = DocumentationMetadataModel.from_class_doc(
                                    module_cls
                                )
                            op["doc"] = doc
                    elif not mtn != this_module_type_name:
                        raise KiaraException(
                            msg=f"Included operation '{op_id}' invalid.",
                            reason=f"module_type must be empty or set to the name '{this_module_type_name}'.",
                        )
                    mopc = ManifestOperationConfig(**op)
                elif not isinstance(op, ManifestOperationConfig):
                    raise KiaraException(
                        msg=f"Included operation '{op_id}' invalid.",
                        reason="Must be a Mapping or ManifestOperationConfig instance.",
                    )
                else:
                    mopc = op

                cache.append(mopc)
                resolved = self._kiara.module_registry.resolve_manifest(
                    mopc.get_manifest()
                )
                mopc._manifest_cache = resolved
                lookup_cache[resolved.manifest_cid] = op_id

        if (
            this_module_type_name not in op_ids
            and not module_cls._config_cls.requires_config()
        ):
            doc = DocumentationMetadataModel.from_class_doc(module_cls)
            mopc = ManifestOperationConfig(module_type=this_module_type_name, doc=doc)
            resolved = self._kiara.module_registry.resolve_manifest(mopc.get_manifest())
            mopc._manifest_cache = resolved

            cache.append(mopc)
            lookup_cache[resolved.manifest_cid] = this_module_type_name

        self._included_operations_cache[module_cls] = cache
        self._included_operations_lookup_cache[module_cls] = lookup_cache
        return self._included_operations_cache[module_cls]

    def retrieve_included_operation_configs(
        self,
    ) -> Iterable[Union[Mapping, OperationConfig]]:

        result = []
        for name, module_cls in self._kiara.module_type_classes.items():
            configs = self._retrieve_included_operations(module_cls=module_cls)
            result.extend(configs)

        return result

    def check_matching_operation(
        self, module: "KiaraModule"
    ) -> Union[CustomModuleOperationDetails, None]:

        op_id: Union[str, None] = None
        if not module.is_pipeline():
            manifest_cid = module.manifest.manifest_cid
            op_id = self._included_operations_lookup_cache[module.__class__].get(
                manifest_cid
            )

        if not op_id:
            return None

        is_internal = module.characteristics.is_internal
        # inputs_map = {k: k for k in module.inputs_schema.keys()}
        # outputs_map = {k: k for k in module.outputs_schema.keys()}
        op_details: CustomModuleOperationDetails = (
            CustomModuleOperationDetails.create_operation_details(
                operation_id=op_id,
                module_inputs_schema=module.inputs_schema,
                module_outputs_schema=module.outputs_schema,
                is_internal_operation=is_internal,
            )
        )
        return op_details


# kiara\kiara\src\kiara\processing\synchronous.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import uuid

from kiara.models.values.value import ValueMap, ValueMapWritable
from kiara.modules import KiaraModule
from kiara.processing import JobLog, JobStatus, ModuleProcessor, ProcessorConfig

try:
    pass
except Exception:
    pass


class SynchronousProcessorConfig(ProcessorConfig):

    pass


class SynchronousProcessor(ModuleProcessor):
    def _add_processing_task(
        self,
        job_id: uuid.UUID,
        module: "KiaraModule",
        inputs: ValueMap,
        outputs: ValueMapWritable,
        job_log: JobLog,
    ):

        self.job_status_updated(job_id=job_id, status=JobStatus.STARTED)
        try:
            module.process_step(inputs=inputs, outputs=outputs, job_log=job_log)
            # output_wrap._sync()
            self.job_status_updated(job_id=job_id, status=JobStatus.SUCCESS)
        except Exception as e:
            self.job_status_updated(job_id=job_id, status=e)

    def _wait_for(self, *job_ids: uuid.UUID):

        # jobs will always be finished, since we were waiting for them in the 'process' method
        return


# kiara\kiara\src\kiara\processing\__init__.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import abc
import uuid
from typing import TYPE_CHECKING, Any, Dict, List, Literal, Protocol, Set, Union

import structlog
from pydantic import BaseModel

from kiara.exceptions import KiaraException, KiaraProcessingException
from kiara.models.module.jobs import (
    ActiveJob,
    JobConfig,
    JobLog,
    JobRecord,
    JobStatus,
)
from kiara.models.values.value import (
    ValueMap,
    ValueMapReadOnly,
    ValueMapWritable,
    ValuePedigree,
)
from kiara.modules import KiaraModule
from kiara.registries.ids import ID_REGISTRY
from kiara.utils import get_dev_config, is_develop, log_exception
from kiara.utils.dates import get_current_time_incl_timezone

if TYPE_CHECKING:
    from kiara.context import Kiara

log = structlog.getLogger()


class JobStatusListener(Protocol):
    def job_status_changed(
        self,
        job_id: uuid.UUID,
        old_status: Union[JobStatus, None],
        new_status: JobStatus,
    ):
        pass


class ProcessorConfig(BaseModel):

    module_processor_type: Literal["synchronous", "multi-threaded"] = "synchronous"


class ModuleProcessor(abc.ABC):
    def __init__(self, kiara: "Kiara"):

        self._kiara: Kiara = kiara
        self._created_jobs: Dict[uuid.UUID, Dict[str, Any]] = {}
        self._running_job_details: Dict[uuid.UUID, Dict[str, Any]] = {}
        self._active_jobs: Dict[uuid.UUID, ActiveJob] = {}
        self._failed_jobs: Dict[uuid.UUID, ActiveJob] = {}
        self._finished_jobs: Dict[uuid.UUID, ActiveJob] = {}
        self._output_refs: Dict[uuid.UUID, ValueMapWritable] = {}
        self._job_records: Dict[uuid.UUID, JobRecord] = {}
        self._auto_save_jobs: Set[uuid.UUID] = set()

        self._listeners: List[JobStatusListener] = []

    def _send_job_event(
        self,
        job_id: uuid.UUID,
        old_status: Union[JobStatus, None],
        new_status: JobStatus,
    ):

        for listener in self._listeners:
            listener.job_status_changed(
                job_id=job_id, old_status=old_status, new_status=new_status
            )

    def register_job_status_listener(self, listener: JobStatusListener):

        self._listeners.append(listener)

    def get_job(self, job_id: uuid.UUID) -> ActiveJob:

        if job_id in self._active_jobs.keys():
            return self._active_jobs[job_id]
        elif job_id in self._finished_jobs.keys():
            return self._finished_jobs[job_id]
        elif job_id in self._failed_jobs.keys():
            return self._failed_jobs[job_id]
        else:
            raise Exception(f"No job with id '{job_id}' registered.")

    def get_job_status(self, job_id: uuid.UUID) -> JobStatus:

        job = self.get_job(job_id=job_id)
        return job.status

    def get_job_record(self, job_id: uuid.UUID) -> JobRecord:

        if job_id in self._job_records.keys():
            return self._job_records[job_id]
        else:
            raise Exception(f"No job record for job with id '{job_id}' registered.")

    def create_job(
        self, job_config: JobConfig, auto_save_result: bool = False
    ) -> uuid.UUID:

        environments = {
            env.model_type_id: str(env.instance_cid)
            for env in self._kiara.current_environments.values()
        }

        result_pedigree = ValuePedigree(
            kiara_id=self._kiara.id,
            module_type=job_config.module_type,
            module_config=job_config.module_config,
            inputs=job_config.inputs,
            environments=environments,
        )

        module = self._kiara.module_registry.create_module(manifest=job_config)
        unique_result_values = module.characteristics.unique_result_values

        outputs = ValueMapWritable.create_from_schema(
            kiara=self._kiara,
            schema=module.outputs_schema,
            pedigree=result_pedigree,
            unique_value_ids=unique_result_values,
        )
        job_id: uuid.UUID = ID_REGISTRY.generate(kiara_id=self._kiara.id)
        job_log = JobLog()

        job = ActiveJob(
            job_id=job_id, job_config=job_config, job_log=job_log, results=None
        )
        ID_REGISTRY.update_metadata(job_id, obj=job)
        job.job_log.add_log("job created")

        job_details: Dict[str, Any] = {
            "job_config": job_config,
            "job": job,
            "module": module,
            "outputs": outputs,
        }
        job_details["pipeline_metadata"] = job_config.pipeline_metadata

        self._created_jobs[job_id] = job_details

        self._send_job_event(
            job_id=job_id, old_status=None, new_status=JobStatus.CREATED
        )

        if is_develop():

            dev_settings = get_dev_config()

            if dev_settings.log.log_pre_run and (
                not module.characteristics.is_internal
                or dev_settings.log.pre_run.internal_modules
            ):

                is_pipeline_step = job_config.pipeline_metadata is not None
                if is_pipeline_step:
                    if dev_settings.log.pre_run.pipeline_steps:
                        step_id = job_config.pipeline_metadata.step_id  # type: ignore
                        assert step_id is not None
                        title = (
                            f"Pre-run information for pipeline step: [i]{step_id}[/i]"
                        )
                    else:
                        title = None
                else:
                    title = f"Pre-run information for module: [i]{module.module_type_name}[/i]"

                if title:
                    from kiara.utils.debug import create_module_preparation_table
                    from kiara.utils.develop import log_dev_message

                    table = create_module_preparation_table(
                        kiara=self._kiara,
                        job_config=job_config,
                        job_id=job_id,
                        module=module,
                    )
                    log_dev_message(table, title=title)

        if auto_save_result:
            self._auto_save_jobs.add(job_id)

        return job_id

    def queue_job(self, job_id: uuid.UUID) -> ActiveJob:

        job_details = self._created_jobs.pop(job_id)
        self._running_job_details[job_id] = job_details
        job_config: JobConfig = job_details.get("job_config")  # type: ignore

        job: ActiveJob = job_details.get("job")  # type: ignore
        module: KiaraModule = job_details.get("module")  # type: ignore
        outputs: ValueMapWritable = job_details.get("outputs")  # type: ignore

        self._active_jobs[job_id] = job  # type: ignore
        self._output_refs[job_id] = outputs  # type: ignore

        input_values = self._kiara.data_registry.load_values(job_config.inputs)

        if module.is_pipeline():
            module._set_job_registry(self._kiara.job_registry)  # type: ignore

        try:
            self._add_processing_task(
                job_id=job_id,
                module=module,
                inputs=input_values,
                outputs=outputs,
                job_log=job.job_log,
            )
            return job

        except Exception as e:
            msg = str(e)
            if not msg:
                msg = repr(e)
            job.error = msg

            if isinstance(e, KiaraProcessingException):
                e._module = module
                e._inputs = ValueMapReadOnly.create_from_ids(
                    data_registry=self._kiara.data_registry, **job_config.inputs
                )
                job._exception = e
                log_exception(e)
                raise e
            else:
                kpe = KiaraProcessingException(
                    e,
                    module=module,
                    inputs=ValueMapReadOnly.create_from_ids(
                        self._kiara.data_registry, **job_config.inputs
                    ),
                )
                job._exception = kpe
                log_exception(kpe)
                raise e

    def job_status_updated(
        self, job_id: uuid.UUID, status: Union[JobStatus, str, Exception]
    ):

        job = self._active_jobs.get(job_id, None)
        if job is None:
            raise Exception(
                f"Can't retrieve active job with id '{job_id}', no such job registered."
            )

        old_status = job.status

        result_values = None

        if status == JobStatus.SUCCESS:
            self._active_jobs.pop(job_id)
            job.job_log.add_log("job finished successfully")
            job.status = JobStatus.SUCCESS
            job.finished = get_current_time_incl_timezone()
            result_values = self._output_refs[job_id]
            try:
                result_values.sync_values()
                for field, val in result_values.items():
                    val.job_id = job_id

                value_ids = result_values.get_all_value_ids()
                job.results = value_ids
                job.job_log.percent_finished = 100
                job_record = JobRecord.from_active_job(
                    active_job=job, kiara=self._kiara
                )
                self._job_records[job_id] = job_record
                self._finished_jobs[job_id] = job
            except Exception as e:
                status = e
                job.job_log.add_log("job failed")
                job.status = JobStatus.FAILED
                job.finished = get_current_time_incl_timezone()
                msg = str(status)
                job.error = msg
                job._exception = status
                self._failed_jobs[job_id] = job

                log.debug(
                    "job.failed",
                    job_id=str(job.job_id),
                    msg=f"failed to sync job results: {job.error}",
                    module_type=job.job_config.module_type,
                )
                status = JobStatus.FAILED

        elif status == JobStatus.FAILED or isinstance(status, (str, Exception)):
            self._active_jobs.pop(job_id)
            job.job_log.add_log("job failed")
            job.status = JobStatus.FAILED
            job.finished = get_current_time_incl_timezone()
            if isinstance(status, str):
                job.error = status
            elif isinstance(status, Exception):
                msg = str(status)
                job.error = msg
                job._exception = status
            self._failed_jobs[job_id] = job
            log.debug(
                "job.failed",
                job_id=str(job.job_id),
                msg=job.error,
                module_type=job.job_config.module_type,
            )
            status = JobStatus.FAILED
        elif status == JobStatus.STARTED:
            job.job_log.add_log("job started")
            job.status = JobStatus.STARTED
            job.started = get_current_time_incl_timezone()
        else:
            raise ValueError(f"Invalid value for status: {status}")

        log.debug(
            "job.status_updated",
            old_status=old_status.value,
            new_status=job.status.value,
            job_id=str(job.job_id),
            module_type=job.job_config.module_type,
        )

        if status in [JobStatus.SUCCESS, JobStatus.FAILED]:
            if is_develop():
                dev_config = get_dev_config()
                if dev_config.log.log_post_run:
                    details = self._running_job_details[job_id]
                    module: KiaraModule = details["module"]
                    skip = False
                    if (
                        module.characteristics.is_internal
                        and not dev_config.log.post_run.internal_modules
                    ):
                        skip = True

                    pipeline_metadata = details.get("pipeline_metadata", None)
                    is_pipeline_step = pipeline_metadata is not None

                    if is_pipeline_step and not dev_config.log.post_run.pipeline_steps:
                        skip = True

                    if not skip:
                        if is_pipeline_step:
                            step_id = pipeline_metadata.step_id  # type: ignore
                            title = f"Post-run information for pipeline step: {step_id}"
                        else:
                            title = f"Post-run information for module: {module.module_type_name}"

                        from kiara.utils.debug import create_post_run_table
                        from kiara.utils.develop import log_dev_message

                        rendered = create_post_run_table(
                            kiara=self._kiara,
                            job=job,
                            module=module,
                            job_config=details["job_config"],
                        )
                        log_dev_message(rendered, title=title)

            self._running_job_details.pop(job_id)

        self._send_job_event(
            job_id=job_id, old_status=old_status, new_status=job.status
        )

        if status is JobStatus.SUCCESS:
            if job_id in self._auto_save_jobs:
                assert result_values is not None
                try:
                    for val in result_values.values():
                        self._kiara.data_registry.store_value(val)
                except Exception as e:
                    log_exception(e)
                    raise KiaraException(
                        msg=f"Failed to auto-save job results for job: {job_id}",
                        parent=e,
                    )

    def wait_for(self, *job_ids: uuid.UUID):
        """Wait for the jobs with the specified ids, also optionally sync their outputs with the pipeline value state."""
        self._wait_for(*job_ids)

        for job_id in job_ids:
            job = self._job_records.get(job_id, None)
            if job is None:
                _job = self._failed_jobs.get(job_id, None)
                if _job is None:
                    raise Exception(f"Can't find job with id: {job_id}")

    @abc.abstractmethod
    def _wait_for(self, *job_ids: uuid.UUID):
        pass

    @abc.abstractmethod
    def _add_processing_task(
        self,
        job_id: uuid.UUID,
        module: "KiaraModule",
        inputs: ValueMap,
        outputs: ValueMapWritable,
        job_log: JobLog,
    ) -> str:
        pass


# kiara\kiara\src\kiara\registries\__init__.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import abc
import os
import uuid
from pathlib import Path
from typing import (
    TYPE_CHECKING,
    Any,
    Dict,
    Generic,
    Iterable,
    Mapping,
    Type,
    TypeVar,
    Union,
)

import structlog
from pydantic import BaseModel, ConfigDict, Field, RootModel, field_validator

from kiara.defaults import (
    ARCHIVE_NAME_MARKER,
    CHUNK_COMPRESSION_TYPE,
    DEFAULT_CHUNK_COMPRESSION,
)
from kiara.utils import log_message

try:
    from typing import Literal  # type: ignore
except ImportError:
    from typing_extensions import Literal  # type: ignore
try:
    from typing import Self  # type: ignore
except ImportError:
    from typing_extensions import Self  # type: ignore

if TYPE_CHECKING:
    from kiara.context import Kiara


class ArchiveConfig(BaseModel, abc.ABC):
    @classmethod
    @abc.abstractmethod
    def create_new_store_config(cls, store_base_path: str, **kwargs) -> Self:
        raise NotImplementedError(
            f"Store config type '{cls}' does not implement 'create_new_config'."
        )

    model_config = ConfigDict()

    # @abc.abstractmethod
    # def get_archive_id(self) -> uuid.UUID:
    #     raise NotImplementedError(
    #         f"Store config type '{self.__class__.__name__}' does not implement 'get_archive_id'."
    #     )


ARCHIVE_CONFIG_CLS = TypeVar("ARCHIVE_CONFIG_CLS", bound=ArchiveConfig)


logger = structlog.getLogger()


class ArchiveDetails(RootModel):
    root: Dict[str, Any]


class ArchiveMetadata(RootModel):
    root: Mapping[str, Any]

    def __iter__(self):
        return iter(self.root)

    def __getitem__(self, item):
        return self.root[item]

    def __setitem__(self, key, value):
        self.root[key] = value

    def get(self, key, default=None):
        return self.root.get(key, default)

    # archive_id: Union[uuid.UUID, None] = Field(
    #     description="The id of the stored archive.", default=None
    # )
    # custom_metadata: Dict[str, Any] = Field(
    #     description="Custom metadata for the archive.", default_factory=dict
    # )


NON_ARCHIVE_DETAILS = ArchiveDetails(root={})


class KiaraArchive(abc.ABC, Generic[ARCHIVE_CONFIG_CLS]):

    _config_cls: Type[ARCHIVE_CONFIG_CLS] = None  # type: ignore

    # @classmethod
    # def create_store_config_instance(
    #     cls, config: Union[ARCHIVE_CONFIG_CLS, BaseModel, Mapping[str, Any]]
    # ) -> "BaseArchive":
    #     """Create a store config instance from a config instance of a few different types."""
    #
    #     from kiara.context.config import KiaraArchiveConfig
    #
    #     if isinstance(config, cls._config_cls):
    #         config = config
    #     elif isinstance(config, KiaraArchiveConfig):
    #         config = cls._config_cls(**config.config)
    #     elif isinstance(config, BaseModel):
    #         config = cls._config_cls(**config.model_dump())
    #     elif isinstance(config, Mapping):
    #         config = cls._config_cls(**config)
    #
    #     return config

    # @classmethod
    # def is_valid_archive(cls, store_uri: str, **kwargs: Any) -> bool:
    #     return False

    @classmethod
    def _load_archive_config(
        cls, archive_uri: str, allow_write_access: bool, **kwargs
    ) -> Union[Dict[str, Any], None]:
        """Tries to assemble an archive config from an uri (and optional paramters).

        If the archive type supports the archive at the uri, then a valid config will be returned,
        otherwise 'None'.
        """

        return None

    @classmethod
    def load_archive_config(
        cls, archive_uri: str, allow_write_access: bool, **kwargs
    ) -> Union[Dict[str, Any], None]:

        log_message(
            "attempt_loading_existing_store",
            archive_uri=archive_uri,
            archive_type=cls.__name__,
        )

        return cls._load_archive_config(
            archive_uri=archive_uri, allow_write_access=allow_write_access, **kwargs
        )

    @classmethod
    def create_new_store_config(
        cls, store_base_path: str, **kwargs
    ) -> ARCHIVE_CONFIG_CLS:

        log_message(
            "create_new_store",
            store_base_path=store_base_path,
            store_type=cls.__name__,
        )

        Path(store_base_path).mkdir(parents=True, exist_ok=True)

        archive_config: ARCHIVE_CONFIG_CLS = cls._config_cls.create_new_store_config(
            store_base_path=store_base_path, **kwargs
        )
        return archive_config

    def __init__(
        self,
        archive_config: ARCHIVE_CONFIG_CLS,
        force_read_only: bool = False,
        archive_name: Union[str, None] = None,
    ):

        self._archive_instance_name: Union[str, None] = archive_name
        self._config: ARCHIVE_CONFIG_CLS = archive_config
        self._force_read_only: bool = force_read_only

        self._archive_metadata: Union[ArchiveMetadata, None] = None

    @property
    def archive_metadata(self) -> ArchiveMetadata:

        if self._archive_metadata is None:
            archive_metadata = self._retrieve_archive_metadata()
            self._archive_metadata = ArchiveMetadata(root=archive_metadata)

        return self._archive_metadata

    @classmethod
    @abc.abstractmethod
    def supported_item_types(cls) -> Iterable[str]:
        pass

    @classmethod
    @abc.abstractmethod
    def _is_writeable(cls) -> bool:
        pass

    @abc.abstractmethod
    def register_archive(self, kiara: "Kiara"):
        pass

    @abc.abstractmethod
    def _retrieve_archive_metadata(self) -> Mapping[str, Any]:
        """Retrieve metadata for the archive.

        Must contain at least one key 'archive_id', with a uuid-able value that
        uniquely identifies the archive.
        """

        raise NotImplementedError()

    def get_archive_metadata(self, key: str) -> Any:

        return self.archive_metadata.get(key, None)

    def set_archive_metadata_value(self, key: str, value: Any):

        if not self.is_writeable():
            raise Exception("Can't set metadata on read-only archive.")

        self._set_archive_metadata_value(key, value)
        self.archive_metadata[key] = value

    def _set_archive_metadata_value(self, key: str, value: Any):
        """Set custom metadata for the archive."""

        raise NotImplementedError(
            f"This archive type '{type(self.__class__)}' does not support setting metadata."
        )

    @property
    def archive_name(self) -> str:
        if self._archive_instance_name:
            return self._archive_instance_name

        alias = self.get_archive_metadata(ARCHIVE_NAME_MARKER)
        if not alias:
            alias = str(self.archive_id)
        self._archive_instance_name = alias
        return self._archive_instance_name  # type: ignore

    def is_force_read_only(self) -> bool:
        return self._force_read_only

    def set_force_read_only(self, force_read_only: bool):
        self._force_read_only = force_read_only

    def is_writeable(self) -> bool:
        if self._force_read_only:
            return False
        return self.__class__._is_writeable()

    # @abc.abstractmethod
    # def register_archive(self, kiara: "Kiara"):
    #     pass

    @property
    def archive_id(self) -> uuid.UUID:

        try:
            result = self.archive_metadata["archive_id"]
        except KeyError:
            raise Exception("Archive does not have an id metadata value set.")
        return uuid.UUID(result)

    @property
    def config(self) -> ARCHIVE_CONFIG_CLS:
        return self._config

    def get_archive_details(self) -> ArchiveDetails:
        return NON_ARCHIVE_DETAILS

    def delete_archive(self, archive_id: Union[uuid.UUID, None] = None):

        if archive_id != self.archive_id:
            raise Exception(
                f"Not deleting archive with id '{self.archive_id}': confirmation id '{archive_id}' does not match."
            )

        logger.info(
            "deleting.archive",
            archive_id=self.archive_id,
            item_types=self.supported_item_types(),
            archive_type=self.__class__.__name__,
        )
        self._delete_archive()

    @abc.abstractmethod
    def _delete_archive(self):
        pass

    def __hash__(self):

        return hash(self.__class__) + hash(self.archive_id)

    def __eq__(self, other):

        if self.__class__ != other.__class__:
            return False

        return self.archive_id == other.archive_id


class BaseArchive(KiaraArchive[ARCHIVE_CONFIG_CLS], Generic[ARCHIVE_CONFIG_CLS]):
    """A base class that can be used to implement a kiara archive."""

    def __init__(
        self,
        archive_name: str,
        archive_config: ARCHIVE_CONFIG_CLS,
        force_read_only: bool = False,
    ):

        super().__init__(
            archive_name=archive_name,
            archive_config=archive_config,
            force_read_only=force_read_only,
        )
        self._kiara: Union["Kiara", None] = None

    @classmethod
    def _is_writeable(cls) -> bool:
        return False

    @property
    def kiara_context(self) -> "Kiara":
        if self._kiara is None:
            raise Exception("Archive not registered into a kiara context yet.")
        return self._kiara

    def register_archive(self, kiara: "Kiara"):
        if self._kiara is not None:
            raise Exception("Archive already registered in a context.")
        self._kiara = kiara

    def _delete_archive(self):

        logger.info(
            "ignore.archive_delete_request",
            reason="not implemented/applicable",
            archive_id=self.archive_id,
            item_types=self.supported_item_types(),
            archive_type=self.__class__.__name__,
        )


class FileSystemArchiveConfig(ArchiveConfig):
    @classmethod
    def load_store_config(cls, store_uri: str, **kwargs) -> Self:
        raise NotImplementedError(
            f"Store config type '{cls}' does not implement 'create_config'."
        )

    @classmethod
    def create_new_store_config(
        cls, store_base_path: str, **kwargs
    ) -> "FileSystemArchiveConfig":

        store_id = str(uuid.uuid4())
        if "path" in kwargs:
            file_name = kwargs["path"]
        else:
            file_name = store_id

        archive_path = os.path.abspath(os.path.join(store_base_path, file_name))

        return FileSystemArchiveConfig(archive_path=archive_path)

    archive_path: str = Field(
        description="The path where the data for this archive is stored."
    )


class SqliteArchiveConfig(ArchiveConfig):
    @classmethod
    def create_new_store_config(
        cls, store_base_path: str, **kwargs
    ) -> "SqliteArchiveConfig":

        store_id = str(uuid.uuid4())

        if "file_name" in kwargs:
            file_name = kwargs["file_name"]
        else:
            file_name = f"{store_id}.sqlite"

        archive_path = os.path.abspath(os.path.join(store_base_path, file_name))

        if not os.path.exists(archive_path):
            Path(archive_path).parent.mkdir(exist_ok=True, parents=True)

        import sqlite3

        conn = sqlite3.connect(archive_path)

        # Create a cursor object
        c = conn.cursor()

        # Create table
        c.execute(
            """CREATE TABLE IF NOT EXISTS archive_metadata
                     (key text PRIMARY KEY , value text NOT NULL)"""
        )

        # Insert a row of data
        c.execute(
            "INSERT OR IGNORE INTO archive_metadata VALUES ('archive_id', ?)",
            (store_id,),
        )

        # Save (commit) the changes
        conn.commit()

        # Close the connection
        conn.close()

        use_wal_mode = kwargs.get("wal_mode", False)

        return SqliteArchiveConfig(
            sqlite_db_path=archive_path, use_wal_mode=use_wal_mode
        )

    sqlite_db_path: str = Field(
        description="The path where the data for this archive is stored."
    )
    use_wal_mode: bool = Field(
        description="Whether to use WAL mode for the SQLite database.", default=False
    )


class SqliteDataStoreConfig(SqliteArchiveConfig):
    @classmethod
    def create_new_store_config(
        cls, store_base_path: str, **kwargs
    ) -> "SqliteDataStoreConfig":

        store_id = str(uuid.uuid4())

        if "file_name" in kwargs:
            file_name = kwargs["file_name"]
        else:
            file_name = f"{store_id}.sqlite"

        default_chunk_compression = kwargs.get(
            "default_chunk_compression", DEFAULT_CHUNK_COMPRESSION
        )

        archive_path = os.path.abspath(os.path.join(store_base_path, file_name))

        if os.path.exists(archive_path):
            raise Exception(f"Archive path '{archive_path}' already exists.")

        Path(archive_path).parent.mkdir(exist_ok=True, parents=True)

        # Connect to the SQLite database (or create it if it doesn't exist)
        import sqlite3

        conn = sqlite3.connect(archive_path)

        # Create a cursor object
        c = conn.cursor()

        # Create table
        c.execute(
            """CREATE TABLE archive_metadata
                     (key text PRIMARY KEY , value text NOT NULL)"""
        )

        # Insert a row of data
        c.execute("INSERT INTO archive_metadata VALUES ('archive_id', ?)", (store_id,))

        # Save (commit) the changes
        conn.commit()

        # Close the connection
        conn.close()

        use_wal_mode = kwargs.get("wal_mode", False)

        return SqliteDataStoreConfig(
            sqlite_db_path=archive_path,
            default_chunk_compression=default_chunk_compression,
            use_wal_mode=use_wal_mode,
        )

    default_chunk_compression: Literal["none", "lz4", "zstd", "lzma"] = Field(  # type: ignore
        description="The default compression type to use for data in this store.",
        default=DEFAULT_CHUNK_COMPRESSION.ZSTD.name.lower(),  # type: ignore
    )

    @field_validator("default_chunk_compression", mode="before")
    def validate_compression(cls, v):

        if v is None:
            v = "none"
        elif isinstance(v, CHUNK_COMPRESSION_TYPE):
            v = v.name

        return v.lower()


# kiara\kiara\src\kiara\registries\aliases\archives.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import os
import shutil
import uuid
from pathlib import Path
from typing import Any, Mapping, Set, Union

import orjson

from kiara.registries import ARCHIVE_CONFIG_CLS, FileSystemArchiveConfig
from kiara.registries.aliases import AliasArchive, AliasStore
from kiara.utils.windows import fix_windows_longpath


class FileSystemAliasArchive(AliasArchive):

    _archive_type_name = "filesystem_alias_archive"
    _config_cls = FileSystemArchiveConfig  # type: ignore

    def __init__(
        self,
        archive_name: str,
        archive_config: ARCHIVE_CONFIG_CLS,
        force_read_only: bool = False,
    ):

        super().__init__(
            archive_name=archive_name,
            archive_config=archive_config,
            force_read_only=force_read_only,
        )

        self._base_path: Union[Path, None] = None

    def _retrieve_archive_metadata(self) -> Mapping[str, Any]:

        if not self.archive_metadata_path.is_file():
            _archive_metadata = {}
        else:
            _archive_metadata = orjson.loads(self.archive_metadata_path.read_bytes())

        archive_id = _archive_metadata.get("archive_id", None)
        if not archive_id:
            try:
                _archive_id = uuid.UUID(
                    self.alias_store_path.name
                )  # just to check if it's a valid UUID
                _archive_metadata["archive_id"] = str(_archive_id)
            except Exception:
                raise Exception(
                    f"Could not retrieve archive id for alias archive '{self.archive_name}'."
                )

        return _archive_metadata

    @property
    def alias_store_path(self) -> Path:

        if self._base_path is not None:
            return self._base_path

        self._base_path = Path(self.config.archive_path).absolute()  # type: ignore
        self._base_path = fix_windows_longpath(self._base_path)
        self._base_path.mkdir(parents=True, exist_ok=True)
        return self._base_path

    @property
    def archive_metadata_path(self) -> Path:
        return self.alias_store_path / "store_metadata.json"

    @property
    def aliases_path(self) -> Path:
        return self.alias_store_path / "aliases"

    @property
    def value_id_path(self) -> Path:
        return self.alias_store_path / "value_ids"

    def _delete_archive(self):
        shutil.rmtree(self.alias_store_path)

    def _translate_alias(self, alias: str) -> Path:

        if "." in alias:
            tokens = alias.split(".")
            alias_path = (
                self.aliases_path.joinpath(*tokens[0:-1]) / f"{tokens[-1]}.alias"
            )
        else:
            alias_path = self.aliases_path / f"{alias}.alias"
        return alias_path

    def _translate_alias_path(self, alias_path: Path) -> str:

        relative = (
            alias_path.absolute()
            .relative_to(self.aliases_path.absolute())
            .as_posix()[:-6]
        )

        relative = os.path.normpath(relative)

        if os.path.sep not in relative:
            alias = relative
        else:
            alias = ".".join(relative.split(os.path.sep))

        return alias

    def _translate_value_id(self, value_id: uuid.UUID) -> Path:

        tokens = str(value_id).split("-")
        value_id_path = (
            self.value_id_path.joinpath(*tokens[0:-1]) / f"{tokens[-1]}.value"
        )
        return value_id_path

    def _translate_value_path(self, value_path: Path) -> uuid.UUID:

        relative = (
            fix_windows_longpath(value_path.absolute())
            .relative_to(fix_windows_longpath(self.value_id_path.absolute()))
            .as_posix()[:-6]
        )
        relative = os.path.normpath(relative)
        value_id_str = "-".join(relative.split(os.path.sep))

        return uuid.UUID(value_id_str)

    def retrieve_all_aliases(self) -> Mapping[str, uuid.UUID]:

        all_aliases = self.aliases_path.rglob("*.alias")
        result = {}
        for alias_path in all_aliases:
            alias = self._translate_alias_path(alias_path=alias_path)
            value_id = self._find_value_id_for_alias_path(alias_path=alias_path)
            assert value_id is not None
            result[alias] = value_id

        return result

    def find_value_id_for_alias(self, alias: str) -> Union[uuid.UUID, None]:
        alias_path = self._translate_alias(alias)
        if not alias_path.exists():
            return None
        return self._find_value_id_for_alias_path(alias_path=alias_path)

    def _find_value_id_for_alias_path(self, alias_path: Path) -> Union[uuid.UUID, None]:

        resolved = alias_path.resolve()

        assert resolved.name.endswith(".value")

        value_id = self._translate_value_path(value_path=resolved)
        return value_id

    def find_aliases_for_value_id(self, value_id: uuid.UUID) -> Union[Set[str], None]:
        raise NotImplementedError()


class FileSystemAliasStore(FileSystemAliasArchive, AliasStore):

    _archive_type_name = "filesystem_alias_store"

    def register_aliases(self, value_id: uuid.UUID, *aliases: str):

        value_path = self._translate_value_id(value_id=value_id)
        value_path.parent.mkdir(parents=True, exist_ok=True)
        value_path.touch()

        for alias in aliases:
            alias_path = self._translate_alias(alias)
            alias_path.parent.mkdir(parents=True, exist_ok=True)
            if alias_path.exists():
                resolved = alias_path.resolve()
                if resolved == value_path:
                    continue
                alias_path.unlink()
            alias_path.symlink_to(value_path)


# kiara\kiara\src\kiara\registries\aliases\sqlite_store.py
# -*- coding: utf-8 -*-
import uuid
from pathlib import Path
from typing import Any, Dict, Mapping, Set, Union

from sqlalchemy import text
from sqlalchemy.engine import Engine

from kiara.defaults import (
    REQUIRED_TABLES_ALIAS_ARCHIVE,
    TABLE_NAME_ALIASES,
    TABLE_NAME_ARCHIVE_METADATA,
)
from kiara.registries import SqliteArchiveConfig
from kiara.registries.aliases import AliasArchive, AliasStore
from kiara.utils.dates import get_current_time_incl_timezone
from kiara.utils.db import create_archive_engine, delete_archive_db


class SqliteAliasArchive(AliasArchive):

    _archive_type_name = "sqlite_alias_archive"
    _config_cls = SqliteArchiveConfig

    @classmethod
    def _load_archive_config(
        cls, archive_uri: str, allow_write_access: bool, **kwargs
    ) -> Union[Dict[str, Any], None]:

        if allow_write_access:
            return None

        if not Path(archive_uri).is_file():
            return None

        import sqlite3

        con = sqlite3.connect(archive_uri)
        cursor = con.cursor()
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
        tables = {x[0] for x in cursor.fetchall()}
        con.close()

        required_tables = REQUIRED_TABLES_ALIAS_ARCHIVE

        if not required_tables.issubset(tables):
            return None

        # config = SqliteArchiveConfig(sqlite_db_path=store_uri)
        return {"sqlite_db_path": archive_uri}

    def __init__(
        self,
        archive_name: str,
        archive_config: SqliteArchiveConfig,
        force_read_only: bool = False,
    ):

        AliasArchive.__init__(
            self,
            archive_name=archive_name,
            archive_config=archive_config,  # type: ignore
            force_read_only=force_read_only,
        )
        self._db_path: Union[Path, None] = None
        self._cached_engine: Union[Engine, None] = None
        self._use_wal_mode: bool = archive_config.use_wal_mode
        # self._lock: bool = True

    def _retrieve_archive_metadata(self) -> Mapping[str, Any]:

        sql = text(f"SELECT key, value FROM {TABLE_NAME_ARCHIVE_METADATA}")

        with self.sqlite_engine.connect() as connection:
            result = connection.execute(sql)
            return {row[0]: row[1] for row in result}

    @property
    def sqlite_path(self):

        if self._db_path is not None:
            return self._db_path

        db_path = Path(self.config.sqlite_db_path).resolve()
        # self._db_path = fix_windows_longpath(db_path)
        self._db_path = db_path

        if self._db_path.exists():
            return self._db_path

        self._db_path.parent.mkdir(parents=True, exist_ok=True)
        return self._db_path

    # @property
    # def db_url(self) -> str:
    #     return f"sqlite:///{self.sqlite_path}"

    @property
    def sqlite_engine(self) -> "Engine":

        if self._cached_engine is not None:
            return self._cached_engine

        self._cached_engine = create_archive_engine(
            db_path=self.sqlite_path,
            force_read_only=self.is_force_read_only(),
            use_wal_mode=self._use_wal_mode,
        )

        create_table_sql = f"""
CREATE TABLE IF NOT EXISTS {TABLE_NAME_ALIASES} (
    alias TEXT PRIMARY KEY,
    value_id TEXT NOT NULL,
    alias_created TEXT NOT NULL
);
"""
        with self._cached_engine.begin() as connection:
            for statement in create_table_sql.split(";"):
                if statement.strip():
                    connection.execute(text(statement))

        # if self._lock:
        #     event.listen(self._cached_engine, "connect", _pragma_on_connect)
        return self._cached_engine

    def find_value_id_for_alias(self, alias: str) -> Union[uuid.UUID, None]:

        sql = text(f"SELECT value_id FROM {TABLE_NAME_ALIASES} WHERE alias = :alias")
        with self.sqlite_engine.connect() as connection:
            result = connection.execute(sql, {"alias": alias})
            row = result.fetchone()
            if row is None:
                return None
            return uuid.UUID(row[0])

    def find_aliases_for_value_id(self, value_id: uuid.UUID) -> Union[Set[str], None]:

        sql = text(f"SELECT alias FROM {TABLE_NAME_ALIASES} WHERE value_id = :value_id")
        with self.sqlite_engine.connect() as connection:
            result = connection.execute(sql, {"value_id": str(value_id)})
            return {row[0] for row in result}

    def retrieve_all_aliases(self) -> Union[Mapping[str, uuid.UUID], None]:

        sql = text(f"SELECT alias, value_id FROM {TABLE_NAME_ALIASES}")
        with self.sqlite_engine.connect() as connection:
            result = connection.execute(sql)
            return {row[0]: uuid.UUID(row[1]) for row in result}

    def _delete_archive(self):

        delete_archive_db(db_path=self.sqlite_path)


class SqliteAliasStore(SqliteAliasArchive, AliasStore):

    _archive_type_name = "sqlite_alias_store"

    @classmethod
    def _load_archive_config(
        cls, archive_uri: str, allow_write_access: bool, **kwargs
    ) -> Union[Dict[str, Any], None]:

        if not allow_write_access:
            return None

        if not Path(archive_uri).is_file():
            return None

        import sqlite3

        con = sqlite3.connect(archive_uri)

        cursor = con.cursor()
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
        tables = {x[0] for x in cursor.fetchall()}
        con.close()

        required_tables = REQUIRED_TABLES_ALIAS_ARCHIVE

        if not required_tables.issubset(tables):
            return None

        # config = SqliteArchiveConfig(sqlite_db_path=store_uri)
        return {"sqlite_db_path": archive_uri}

    def _set_archive_metadata_value(self, key: str, value: Any):
        """Set custom metadata for the archive."""

        sql = text(
            f"INSERT OR REPLACE INTO {TABLE_NAME_ARCHIVE_METADATA} (key, value) VALUES (:key, :value)"
        )
        with self.sqlite_engine.connect() as conn:
            params = {"key": key, "value": value}
            conn.execute(sql, params)
            conn.commit()

    def register_aliases(self, value_id: uuid.UUID, *aliases: str):

        alias_created = get_current_time_incl_timezone().isoformat()

        sql = text(
            f"INSERT OR REPLACE INTO {TABLE_NAME_ALIASES} (alias, value_id, alias_created) VALUES (:alias, :value_id, :alias_created)"
        )

        with self.sqlite_engine.connect() as connection:
            params = [
                {
                    "alias": alias,
                    "value_id": str(value_id),
                    "alias_created": alias_created,
                }
                for alias in aliases
            ]

            for param in params:
                connection.execute(sql, param)

            connection.commit()


# kiara\kiara\src\kiara\registries\aliases\__init__.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import abc
import uuid
from typing import (
    TYPE_CHECKING,
    Callable,
    Dict,
    Iterable,
    List,
    Mapping,
    NamedTuple,
    Set,
    Union,
)

import structlog

from kiara.defaults import (
    DEFAULT_ALIAS_STORE_MARKER,
    DEFAULT_STORE_MARKER,
    INVALID_ALIAS_NAMES,
)
from kiara.exceptions import KiaraException
from kiara.models.events.alias_registry import AliasArchiveAddedEvent
from kiara.registries import ArchiveDetails, BaseArchive
from kiara.registries.data import ValueLink

if TYPE_CHECKING:
    from kiara.context import Kiara

logger = structlog.getLogger()


class AliasArchive(BaseArchive):
    @classmethod
    def supported_item_types(cls) -> Iterable[str]:
        return ["alias"]

    @abc.abstractmethod
    def retrieve_all_aliases(self) -> Union[Mapping[str, uuid.UUID], None]:
        """
        Retrieve a list of all aliases registered in this archive.

        The result of this method can be 'None', for cases where the aliases are determined dynamically.
        In kiara, the result of this method is mostly used to improve performance when looking up an alias.

        Returns:
        -------
            a list of strings (the aliases), or 'None' if this archive does not support alias indexes.
        """

    @abc.abstractmethod
    def find_value_id_for_alias(self, alias: str) -> Union[uuid.UUID, None]:
        pass

    @abc.abstractmethod
    def find_aliases_for_value_id(self, value_id: uuid.UUID) -> Union[Set[str], None]:
        pass

    def get_archive_details(self) -> ArchiveDetails:
        all_aliases = self.retrieve_all_aliases()
        if all_aliases is not None:
            no_aliases = len(all_aliases)
            aliases = sorted(all_aliases.keys())
            details = {
                "no_aliases": no_aliases,
                "aliases": aliases,
                "dynamic_archive": False,
            }
        else:
            details = {"dynamic_archive": True}
        return ArchiveDetails(root=details)


class AliasStore(AliasArchive):
    @abc.abstractmethod
    def register_aliases(self, value_id: uuid.UUID, *aliases: str):
        pass

    @classmethod
    def _is_writeable(cls) -> bool:
        return True


class AliasItem(NamedTuple):
    full_alias: str
    rel_alias: str
    value_id: uuid.UUID
    alias_archive: str
    alias_archive_id: uuid.UUID


class AliasRegistry(object):
    """The registry that handles all alias-related operations.

    This registry is responsible for managing all alias archives and stores, and for providing a unified view of all
    of them.

    Aliase archives/stores can be 'mounted' at specific mountpoints, and aliases refering to them use the format

    <mountpoint>#<actual_alias>

    There is also a 'default' alias store, which is used when the alias provided does not contain a '#' indicating a
     mountpoint.
    """

    def __init__(self, kiara: "Kiara"):

        self._kiara: Kiara = kiara

        self._event_callback: Callable = self._kiara.event_registry.add_producer(self)

        self._alias_archives: Dict[str, AliasArchive] = {}
        """All registered archives/stores."""
        self._mountpoints: Dict[str, str] = {}
        """All registered mountpoints (key: mountpoint, value: archive_alias)."""

        self._default_alias_store: Union[str, None] = None
        """The alias of the store where new aliases are stored by default."""

        self._dynamic_stores: Union[List[str], None] = None

        self._cached_aliases: Union[Dict[str, AliasItem], None] = None
        self._cached_aliases_by_id: Union[Dict[uuid.UUID, Set[AliasItem]], None] = None

        self._cached_dynamic_aliases: Union[Dict[str, AliasItem], None] = None

    def register_archive(
        self,
        archive: AliasArchive,
        set_as_default_store: Union[bool, None] = None,
        mount_point: Union[str, None] = None,
    ) -> str:

        alias = archive.archive_name

        if not alias:
            raise Exception("Invalid alias archive alias: can't be empty.")

        if not mount_point:
            mount_point = archive.archive_name

        if "#" in mount_point:
            raise Exception(
                f"Can't register alias archive with mountpoint '{alias}': mountpoint is not allowed to contain a '#' character."
            )

        if ":" in mount_point:
            raise Exception(
                f"Can't register alias archive with mountpoint '{alias}': mountpoint is not allowed to contain a ':' character."
            )

        if alias in self._alias_archives.keys():
            raise Exception(f"Can't add store, alias '{alias}' already registered.")

        if mount_point:
            # if mount_point in self.aliases:
            #     raise Exception(
            #         f"Can't mount alias archive: mountpoint '{mount_point}' already in use as alias."
            #     )
            if mount_point in self._mountpoints.keys():
                raise Exception(f"Mountpoint '{mount_point}' already registered.")
            self._mountpoints[mount_point] = alias

        archive.register_archive(kiara=self._kiara)
        self._alias_archives[alias] = archive

        is_store = False
        is_default_store = False
        if archive.is_writeable():
            is_store = True
            if set_as_default_store and self._default_alias_store is not None:
                raise Exception(
                    f"Can't set alias store '{alias}' as default store: default store already set."
                )

            if self._default_alias_store is None:
                is_default_store = True
                self._default_alias_store = alias

        # TODO: add to cache if it already exists instead of invalidating, for performance reasons
        self._cached_aliases = None
        self._cached_aliases_by_id = None
        self._dynamic_stores = None
        self._cached_dynamic_aliases = None

        event = AliasArchiveAddedEvent(
            kiara_id=self._kiara.id,
            alias_archive_id=archive.archive_id,
            alias_archive_alias=alias,
            is_store=is_store,
            is_default_store=is_default_store,
            mount_point=mount_point,
        )
        self._event_callback(event)

        return alias

    @property
    def default_alias_store(self) -> str:

        if self._default_alias_store is None:
            raise Exception("No default alias store set (yet).")
        return self._default_alias_store

    @property
    def alias_archives(self) -> Mapping[str, AliasArchive]:
        return self._alias_archives

    def get_archive(
        self, archive_alias: Union[str, None, uuid.UUID] = None
    ) -> Union[AliasArchive, None]:
        if archive_alias in (None, DEFAULT_STORE_MARKER, DEFAULT_ALIAS_STORE_MARKER):
            archive_alias = self.default_alias_store
            if archive_alias is None:
                raise Exception("Can't retrieve default alias archive, none set (yet).")

        archive = self._alias_archives.get(archive_alias, None)  # type: ignore
        if archive is None:
            if isinstance(archive_alias, str):
                try:
                    archive_alias = uuid.UUID(archive_alias)
                except Exception:
                    pass
            for a in self._alias_archives.values():
                if a.archive_id == archive_alias:
                    return a
        return archive

    @property
    def all_aliases(self) -> Iterable[str]:

        return self.aliases.keys()

    @property
    def aliases_by_id(self) -> Mapping[uuid.UUID, Set[AliasItem]]:
        if self._cached_aliases_by_id is None:
            self.aliases
        return self._cached_aliases_by_id  # type: ignore

    @property
    def dynamic_aliases(self) -> Dict[str, AliasItem]:
        if self._cached_dynamic_aliases is None:
            self.aliases
        return self._cached_dynamic_aliases  # type: ignore

    @property
    def aliases(self) -> Mapping[str, AliasItem]:
        """Retrieve a map of all available aliases, context wide, with the registered archive aliases as values."""
        if self._cached_aliases is not None:
            return self._cached_aliases

        # TODO: multithreading lock
        all_aliases: Dict[str, AliasItem] = {}
        all_aliases_by_id: Dict[uuid.UUID, Set[AliasItem]] = {}
        dynamic_stores = []

        for archive_alias, archive in self._alias_archives.items():

            alias_map = archive.retrieve_all_aliases()
            if alias_map is None:
                dynamic_stores.append(archive_alias)
                continue
            for alias, v_id in alias_map.items():
                if archive_alias == self.default_alias_store:
                    final_alias = alias
                else:
                    final_alias = f"{archive_alias}#{alias}"

                if final_alias in all_aliases.keys():
                    raise Exception(
                        f"Inconsistent alias registry: alias '{final_alias}' available more than once."
                    )
                item = AliasItem(
                    full_alias=final_alias,
                    rel_alias=alias,
                    value_id=v_id,
                    alias_archive=archive_alias,
                    alias_archive_id=archive.archive_id,
                )
                all_aliases[final_alias] = item
                all_aliases_by_id.setdefault(v_id, set()).add(item)

        self._cached_aliases = {k: all_aliases[k] for k in sorted(all_aliases.keys())}
        self._cached_aliases_by_id = all_aliases_by_id
        self._dynamic_stores = dynamic_stores
        self._cached_dynamic_aliases = {}

        return self._cached_aliases

    @property
    def dynamic_stores(self) -> List[str]:
        if self._dynamic_stores is None:
            self.aliases
        return self._dynamic_stores  # type: ignore

    def find_value_id_for_alias(self, alias: str) -> Union[uuid.UUID, None]:
        """Find the value id for a given alias.

        This method will check all registered archives if they have the alias registered (under their respective mountpoints, if applicable), then it will check the archives that have dynamic aliases (i.e. they don't
        return a list of all available aliases, but 'None' if queried).

        Once found, the value will be stored in a cache for faster retrieval next time.
        """

        alias_item = self.aliases.get(alias, None)
        if alias_item is not None:
            return alias_item.value_id

        alias_item = self.dynamic_aliases.get(alias, None)
        if alias_item is not None:
            return alias_item.value_id

        if "#" not in alias:
            archive_alias: Union[str, None] = self.default_alias_store
            rest = alias
        else:
            mountpoint, rest = alias.split("#", maxsplit=1)
            archive_alias = self._mountpoints.get(mountpoint, None)

            if archive_alias is None:
                return None

        if archive_alias not in self.dynamic_stores:
            return None

        archive = self.get_archive(archive_alias=archive_alias)
        if archive is None:
            raise Exception(f"Invalid alias store: '{archive_alias}' not registered.")
        result_value_id = archive.find_value_id_for_alias(alias=rest)
        if result_value_id:
            alias_item = AliasItem(
                full_alias=alias,
                rel_alias=rest,
                value_id=result_value_id,
                alias_archive=archive_alias,
                alias_archive_id=archive.archive_id,
            )
            self.dynamic_aliases[alias] = alias_item
            return result_value_id
        else:
            return None

    def _get_value_id(self, value_id: Union[uuid.UUID, ValueLink, str]) -> uuid.UUID:
        """Convenience method to ensure a uuid.UUID type for a value id."""

        if not isinstance(value_id, uuid.UUID):
            # fallbacks for common mistakes, this should error out if not a Value or string.
            if hasattr(value_id, "value_id"):
                _value_id: Union[uuid.UUID, str] = value_id.value_id  # type: ignore
                if isinstance(_value_id, str):
                    _value_id = uuid.UUID(_value_id)
            else:
                try:
                    _value_id = uuid.UUID(
                        value_id  # type: ignore
                    )  # this should fail if not string or wrong string format
                except ValueError:
                    raise KiaraException(f"Could not resolve value id for: {value_id}")
        else:
            _value_id = value_id

        if not _value_id:
            raise Exception(f"Could not resolve id: {value_id}")
        return _value_id

    def find_aliases_for_value_id(
        self,
        value_id: Union[uuid.UUID, ValueLink, str],
        search_dynamic_archives: bool = False,
    ) -> Set[str]:
        """Finds all registered aliases for the provided value id.

        If 'search_dynamic_archives' is set to 'True', this method will also search all dynamic archives for the value id, which is not being done by default for performance reasons.
        """

        value_id = self._get_value_id(value_id=value_id)

        aliases = {a.full_alias for a in self.aliases_by_id.get(value_id, [])}

        if search_dynamic_archives:
            for archive_alias, archive in self._alias_archives.items():
                _aliases = archive.find_aliases_for_value_id(value_id=value_id)
                if _aliases:
                    for a in _aliases:
                        full_alias = f"{archive_alias}#{a}"
                        alias_item = AliasItem(
                            full_alias=full_alias,
                            rel_alias=a,
                            value_id=value_id,
                            alias_archive=archive_alias,
                            alias_archive_id=archive.archive_id,
                        )
                        self.dynamic_aliases[full_alias] = alias_item
                        aliases.add(full_alias)

        return aliases

    def register_aliases(
        self,
        value_id: Union[uuid.UUID, ValueLink, str],
        aliases: Union[str, Iterable[str]],
        allow_overwrite: bool = False,
        alias_store: Union[str, None] = None,
    ):

        value = self._kiara.data_registry.get_value(value=value_id)

        if alias_store in [DEFAULT_STORE_MARKER, DEFAULT_ALIAS_STORE_MARKER, None]:
            alias_store = self.default_alias_store

        if isinstance(aliases, str):
            aliases = [aliases]
        else:
            for alias in aliases:
                if not isinstance(alias, str):
                    raise KiaraException(
                        msg=f"Invalid alias: {alias}.",
                        details="Alias must be a string.",
                    )
                try:
                    uuid.UUID(alias)
                    raise KiaraException(
                        msg=f"Invalid alias name: {alias}.",
                        details="Alias can't be a UUID.",
                    )
                except Exception:
                    pass

        aliases_to_store: Dict[str, List[str]] = {}
        for alias in aliases:
            if "#" in alias:
                mountpoint, alias_name = alias.split("#", maxsplit=1)
                alias_store_alias = self._mountpoints.get(mountpoint, None)
                if alias_store_alias is None:
                    raise Exception(
                        f"Invalid mountpoint: '{mountpoint}' not registered."
                    )

                if alias_store and alias_store != alias_store_alias:
                    raise Exception(
                        f"Can't register alias '{alias}': conflicting alias store references '{alias_store}' != '{alias_store_alias}'."
                    )

                if alias_store:
                    alias_store_alias = alias_store

            else:
                if alias_store:
                    alias_store_alias = alias_store
                else:
                    alias_store_alias = self.default_alias_store
                alias_name = alias

            if alias_name in INVALID_ALIAS_NAMES:
                raise KiaraException(
                    msg=f"Invalid alias name: {alias}.",
                    details=f"The following names can't be used as alias: {', '.join(INVALID_ALIAS_NAMES)}.",
                )

            if "#" in alias_name:
                raise KiaraException(
                    msg=f"Invalid alias name: {alias}.",
                    details="Alias can't contain a '#' character.",
                )
            if ":" in alias_name:
                raise KiaraException(
                    msg=f"Invalid alias name: {alias}.",
                    details="Alias can't contain a ':' character.",
                )

            aliases_to_store.setdefault(alias_store_alias, []).append(alias_name)

        self.aliases  # noqu

        if not allow_overwrite:
            duplicates = []
            for alias in aliases:
                if alias in self.aliases.keys():
                    duplicates.append(alias)

            if duplicates:
                raise Exception(f"Aliases already registered: {duplicates}")

        for store_alias, aliases_for_store in aliases_to_store.items():

            store: AliasStore = self.get_archive(archive_alias=store_alias)  # type: ignore
            if store is None:
                raise Exception(f"Invalid alias store: '{store_alias}' not registered.")
            if not store.is_writeable():
                raise Exception(
                    f"Can't register aliases in store '{store_alias}': store is read-only."
                )

        for store_alias, aliases_for_store in aliases_to_store.items():

            store = self.get_archive(archive_alias=store_alias)  # type: ignore
            store.register_aliases(value.value_id, *aliases_for_store)

            for alias in aliases:
                alias_item = AliasItem(
                    full_alias=alias,
                    rel_alias=alias,
                    value_id=value.value_id,
                    alias_archive=store_alias,
                    alias_archive_id=store.archive_id,
                )

                if store_alias == self.default_alias_store:
                    actual_alias = alias
                else:
                    actual_alias = f"{store_alias}#{alias}"

                if actual_alias in self.aliases.keys():
                    logger.info("alias.replace", alias=actual_alias)
                    # raise NotImplementedError()

                self.aliases[actual_alias] = alias_item  # type: ignore
                self._cached_aliases_by_id.setdefault(value_id, set()).add(alias_item)  # type: ignore


#
# class PersistentValueAliasMap(AliasValueMap):
#     # def __init__(self, data_registry: "DataRegistry", engine: Engine, doc: Any = None):
#     #
#     #     self._data_registry: DataRegistry = data_registry
#     #     self._engine: Engine = engine
#     #     doc = DocumentationMetadataModel.create(doc)
#     #     v_doc = self._data_registry.register_data(
#     #         doc, schema=ValueSchema(type="doc"), pedigree=ORPHAN
#     #     )
#     #     super().__init__(alias="", version=0, value=v_doc)
#     #
#     #     self._load_all_aliases()
#     doc: Optional[DocumentationMetadataModel] = Field(
#         description="Description of the values this map contains."
#     )
#     _engine: Engine = PrivateAttr(default=None)
#
#     @root_validator(pre=True)
#     def _fill_defaults(cls, values):
#         if "values_schema" not in values.keys():
#             values["values_schema"] = {}
#
#         if "version" not in values.keys():
#             values["version"] = 0
#         else:
#             assert values["version"] == 0
#
#         return values
#
#     def _load_all_aliases(self):
#
#         with Session(bind=self._engine, future=True) as session:  # type: ignore
#
#             alias_a = aliased(AliasOrm)
#             alias_b = aliased(AliasOrm)
#
#             result = (
#                 session.query(alias_b)
#                 .join(
#                     alias_a,
#                     and_(
#                         alias_a.alias == alias_b.alias,
#                         alias_a.version < alias_b.version,
#                     ),
#                 )
#                 .where(alias_b.value_id != None)
#                 .order_by(func.length(alias_b.alias), alias_b.alias)
#             )
#
#             for r in result:
#                 value = self._data_registry.get_value(r.value_id)
#                 self.set_alias(r.alias, value=value)
#
#     def save(self, *aliases):
#
#         for alias in aliases:
#             self._persist(alias)
#
#     def _persist(self, alias: str):
#
#         return
#
#         with Session(bind=self._engine, future=True) as session:  # type: ignore
#
#             current = []
#             tokens = alias.split(".")
#             for token in tokens:
#                 current.append(token)
#                 current_path = ".".join(current)
#                 alias_map = self.get_alias(current_path)
#                 if alias_map.is_stored:
#                     continue
#
#                 value_id = None
#                 if alias_map.assoc_value:
#                     value_id = alias_map.assoc_value
#
#                 if value_id is None:
#                     continue
#                 alias_map_orm = AliasOrm(
#                     value_id=value_id,
#                     created=alias_map.created,
#                     version=alias_map.version,
#                     alias=current_path,
#                 )
#                 session.add(alias_map_orm)
#
#             session.commit()


# kiara\kiara\src\kiara\registries\data\__init__.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)
import abc
import copy
import uuid
from pathlib import Path
from typing import (
    TYPE_CHECKING,
    Any,
    Callable,
    Dict,
    Generator,
    Iterable,
    List,
    Mapping,
    Protocol,
    Sequence,
    Set,
    Tuple,
    Union,
)

import structlog
from rich.console import RenderableType

from kiara.data_types import DataType
from kiara.data_types.included_core_types import NoneType
from kiara.defaults import (
    DATA_ARCHIVE_DEFAULT_VALUE_MARKER,
    DEFAULT_DATA_STORE_MARKER,
    DEFAULT_STORE_MARKER,
    ENVIRONMENT_MARKER_KEY,
    INVALID_HASH_MARKER,
    NO_SERIALIZATION_MARKER,
    NONE_STORE_ID,
    NONE_VALUE_ID,
    NOT_SET_VALUE_ID,
    ORPHAN_PEDIGREE_OUTPUT_NAME,
    STRICT_CHECKS,
    SpecialValue,
)
from kiara.exceptions import (
    InvalidValuesException,
    KiaraException,
    NoSuchValueAliasException,
    NoSuchValueException,
    NoSuchValueIdException,
)
from kiara.interfaces.python_api.models.info import ValueInfo
from kiara.models.events.data_registry import (
    DataArchiveAddedEvent,
    ValueCreatedEvent,
    ValuePreStoreEvent,
    ValueRegisteredEvent,
    ValueStoredEvent,
)
from kiara.models.module.operation import Operation
from kiara.models.python_class import PythonClass
from kiara.models.values import DEFAULT_SCALAR_DATATYPE_CHARACTERISTICS, ValueStatus
from kiara.models.values.matchers import ValueMatcher
from kiara.models.values.value import (
    ORPHAN,
    DataTypeInfo,
    PersistedData,
    SerializationMetadata,
    SerializedData,
    Value,
    ValueMap,
    ValueMapReadOnly,
    ValuePedigree,
)
from kiara.models.values.value_schema import ValueSchema
from kiara.registries.data.data_store import DataArchive, DataStore
from kiara.registries.ids import ID_REGISTRY
from kiara.utils import log_exception, log_message
from kiara.utils.data import pretty_print_data
from kiara.utils.hashing import NONE_CID
from kiara.utils.stores import check_external_archive

if TYPE_CHECKING:
    from multiformats.varint import BytesLike

    from kiara.context import Kiara
    from kiara.models.module.destiny import Destiny
    from kiara.models.module.manifest import Manifest


logger = structlog.getLogger()


class ValueLink(Protocol):

    value_id: uuid.UUID


NONE_PERSISTED_DATA = PersistedData(
    data_type="none",
    data_type_config={},
    serialization_profile="none",
    metadata=SerializationMetadata(),
    hash_codec="sha2-256",
    archive_id=NONE_STORE_ID,
    chunk_id_map={},
)


class AliasResolver(abc.ABC):
    def __init__(self, kiara: "Kiara"):

        self._kiara: "Kiara" = kiara

    @abc.abstractmethod
    def resolve_alias(self, alias: str) -> uuid.UUID:
        pass


ARCHIVE_REF_TYPE_NAME = "archive"


class DefaultAliasResolver(AliasResolver):
    def __init__(self, kiara: "Kiara"):

        super().__init__(kiara=kiara)

    def resolve_alias(self, alias: str) -> uuid.UUID:

        # preprocessing alias
        if alias.endswith(".kiarchive"):
            alias = f"archive:{alias}"

        if ":" in alias:
            ref_type, rest = alias.split(":", maxsplit=1)

            if ref_type == "value":
                _value_id: Union[uuid.UUID, None] = uuid.UUID(rest)
            elif ref_type == "alias":
                _value_id = self._kiara.alias_registry.find_value_id_for_alias(
                    alias=rest
                )
                if _value_id is None:
                    raise NoSuchValueAliasException(
                        alias=rest,
                        msg=f"Can't retrive value for alias '{rest}': no such alias registered.",
                    )
            elif ref_type == ARCHIVE_REF_TYPE_NAME:

                if "#" in rest:
                    archive_ref, path_in_archive = rest.split("#", maxsplit=1)
                else:
                    archive_ref = rest
                    path_in_archive = None

                archives = check_external_archive(
                    archive=archive_ref, allow_write_access=False
                )

                if archives:
                    data_archive: DataArchive = archives.get("data", None)  # type: ignore
                    if data_archive:
                        self._kiara.data_registry.register_data_archive(data_archive)

                        if not path_in_archive:
                            default_value = data_archive.get_archive_metadata(
                                DATA_ARCHIVE_DEFAULT_VALUE_MARKER
                            )
                            if default_value is None:
                                raise NoSuchValueException(
                                    f"No default value found for uri: {alias}"
                                )
                            _value_id = uuid.UUID(default_value)
                        else:
                            from kiara.registries.aliases import AliasArchive

                            alias_archive: AliasArchive = archives.get("alias", None)  # type: ignore
                            if alias_archive:
                                _value_id = alias_archive.find_value_id_for_alias(
                                    alias=path_in_archive
                                )
                            else:
                                raise NoSuchValueException(
                                    msg=f"No alias archive found for '{archive_ref}'."
                                )
                    else:
                        raise NoSuchValueException(
                            "No data archive found in '{archive_ref}'."
                        )
                else:
                    raise NoSuchValueException(
                        msg=f"No archive found for '{archive_ref}'."
                    )

            else:
                raise Exception(
                    f"Can't retrieve value for '{alias}': invalid reference type '{ref_type}'."
                )
        else:
            _value_id = self._kiara.alias_registry.find_value_id_for_alias(alias)
            if _value_id is None:
                raise Exception(
                    f"Can't retrieve value for alias '{alias}': no such alias registered."
                )

        if _value_id is None:
            raise Exception(
                f"Can't retrieve value for alias '{alias}': no such alias registered."
            )
        return _value_id


class DataRegistry(object):
    def __init__(self, kiara: "Kiara"):

        self._kiara: Kiara = kiara

        self._event_callback: Callable = self._kiara.event_registry.add_producer(self)

        self._data_archives: Dict[str, DataArchive] = {}

        self._default_data_store: Union[str, None] = None
        self._registered_values: Dict[uuid.UUID, Value] = {}

        self._value_archive_lookup_map: Dict[uuid.UUID, str] = {}
        """A cached dict that stores which archives which value ids belong to."""

        self._values_by_hash: Dict[str, Set[uuid.UUID]] = {}

        self._cached_data: Dict[uuid.UUID, Any] = {}
        self._persisted_value_descs: Dict[uuid.UUID, Union[PersistedData, None]] = {}

        self._alias_resolver: AliasResolver = DefaultAliasResolver(kiara=self._kiara)

        # initialize special values
        special_value_cls = PythonClass.from_class(NoneType)
        data_type_info = DataTypeInfo(
            data_type_name="none",
            characteristics=DEFAULT_SCALAR_DATATYPE_CHARACTERISTICS,
            data_type_class=special_value_cls,
        )
        self._not_set_value: Value = Value(
            value_id=NOT_SET_VALUE_ID,
            kiara_id=self._kiara.id,
            value_schema=ValueSchema(
                type="none",
                default=SpecialValue.NOT_SET,
                is_constant=True,
                doc="Special value, indicating a field is not set.",  # type: ignore
            ),
            environment_hashes={},
            value_status=ValueStatus.NOT_SET,
            value_size=0,
            value_hash=INVALID_HASH_MARKER,
            pedigree=ORPHAN,
            pedigree_output_name="__void__",
            data_type_info=data_type_info,
        )
        self._not_set_value._data_registry = self
        self._cached_data[NOT_SET_VALUE_ID] = SpecialValue.NOT_SET
        self._registered_values[NOT_SET_VALUE_ID] = self._not_set_value
        self._persisted_value_descs[NOT_SET_VALUE_ID] = NONE_PERSISTED_DATA
        # self._env_cache: Dict[str, Dict[str, RuntimeEnvironment]] = {}

        self._none_value: Value = Value(
            value_id=NONE_VALUE_ID,
            kiara_id=self._kiara.id,
            value_schema=ValueSchema(
                type="none",
                default=SpecialValue.NO_VALUE,
                is_constant=True,
                doc="Special value, indicating a field is set with a 'none' value.",  # type: ignore
            ),
            environment_hashes={},
            value_status=ValueStatus.NONE,
            value_size=0,
            value_hash=str(NONE_CID),
            pedigree=ORPHAN,
            pedigree_output_name="__void__",
            data_type_info=data_type_info,
        )
        self._none_value._data_registry = self
        self._cached_data[NONE_VALUE_ID] = SpecialValue.NO_VALUE
        self._registered_values[NONE_VALUE_ID] = self._none_value
        self._persisted_value_descs[NONE_VALUE_ID] = NONE_PERSISTED_DATA

        self._cached_value_aliases: Dict[uuid.UUID, Dict[str, Union[Destiny, None]]] = (
            {}
        )

        self._destinies: Dict[uuid.UUID, Destiny] = {}
        self._destinies_by_value: Dict[uuid.UUID, Dict[str, Destiny]] = {}

    @property
    def kiara_id(self) -> uuid.UUID:
        return self._kiara.id

    @property
    def NOT_SET_VALUE(self) -> Value:
        return self._not_set_value

    @property
    def NONE_VALUE(self) -> Value:
        return self._none_value

    def retrieve_all_available_value_ids(self) -> Set[uuid.UUID]:

        result: Set[uuid.UUID] = set()
        for alias, store in self._data_archives.items():
            ids = store.value_ids
            if ids:
                result.update(ids)

        return result

    def register_data_archive(
        self,
        archive: DataArchive,
        set_as_default_store: Union[bool, None] = None,
    ) -> str:

        alias = archive.archive_name

        if not alias:
            raise Exception("Invalid data archive alias: can't be empty.")

        if alias in self._data_archives.keys():
            raise Exception(
                f"Can't add data archive, alias '{alias}' already registered."
            )

        archive.register_archive(kiara=self._kiara)

        self._data_archives[alias] = archive
        is_store = False
        is_default_store = False
        if isinstance(archive, DataStore):
            is_store = True

            if set_as_default_store and self._default_data_store is not None:
                raise Exception(
                    f"Can't set data store '{alias}' as default store: default store already set."
                )

            if self._default_data_store is None or set_as_default_store:
                is_default_store = True
                self._default_data_store = alias

        event = DataArchiveAddedEvent(
            kiara_id=self._kiara.id,
            data_archive_id=archive.archive_id,
            data_archive_alias=alias,
            is_store=is_store,
            is_default_store=is_default_store,
        )
        self._event_callback(event)

        return alias

    @property
    def default_data_store(self) -> str:
        if self._default_data_store is None:
            raise Exception("No default data store set.")
        return self._default_data_store

    @property
    def data_archives(self) -> Mapping[str, DataArchive]:
        return self._data_archives

    def get_archive(
        self, archive_id_or_alias: Union[None, uuid.UUID, str] = None
    ) -> DataArchive:

        if archive_id_or_alias in (
            None,
            DEFAULT_STORE_MARKER,
            DEFAULT_DATA_STORE_MARKER,
        ):
            archive_id_or_alias = self.default_data_store
            if archive_id_or_alias is None:
                raise Exception("Can't retrieve default data archive, none set (yet).")

        if isinstance(archive_id_or_alias, uuid.UUID):
            for archive in self._data_archives.values():
                if archive.archive_id == archive_id_or_alias:
                    return archive

            raise Exception(
                f"Can't retrieve archive with id '{archive_id_or_alias}': no archive with that id registered."
            )

        if archive_id_or_alias in self._data_archives.keys():
            return self._data_archives[archive_id_or_alias]
        else:
            try:
                _archive_id = uuid.UUID(archive_id_or_alias)
                for archive in self._data_archives.values():
                    if archive.archive_id == _archive_id:
                        return archive
                    raise Exception(
                        f"Can't retrieve archive with id '{archive_id_or_alias}': no archive with that id registered."
                    )
            except Exception:
                pass

        raise Exception(
            f"Can't retrieve archive with id '{archive_id_or_alias}': no archive with that id registered."
        )

    def find_store_id_for_value(self, value_id: uuid.UUID) -> Union[str, None]:

        if value_id in self._value_archive_lookup_map.keys():
            return self._value_archive_lookup_map[value_id]

        matches = []
        for store_id, store in self.data_archives.items():
            match = store.has_value(value_id=value_id)
            if match:
                matches.append(store_id)

        if len(matches) == 0:
            return None
        elif len(matches) > 1:
            raise Exception(
                f"Found value with id '{value_id}' in multiple archives, this is not supported (yet): {matches}"
            )

        self._value_archive_lookup_map[value_id] = matches[0]
        return matches[0]

    def get_value(self, value: Union[uuid.UUID, ValueLink, str, Path]) -> Value:
        _value_id = None

        if not isinstance(value, uuid.UUID):
            # fallbacks for common mistakes, this should error out if not a Value or string.
            if hasattr(value, "value_id"):
                _value_id: Union[uuid.UUID, str, None] = value.value_id  # type: ignore
                if isinstance(_value_id, str):
                    _value_id = uuid.UUID(_value_id)
            else:

                try:
                    _value_id = uuid.UUID(
                        value  # type: ignore
                    )  # this should fail if not string or wrong string format
                except ValueError:
                    _value_id = None

                if _value_id is None:
                    if isinstance(value, Path):
                        raise NotImplementedError()
                    if not isinstance(value, str):
                        raise Exception(
                            f"Can't retrieve value for '{value}': invalid type '{type(value)}'."
                        )
                    try:
                        _value_id = self._alias_resolver.resolve_alias(value)
                    except Exception as e:
                        log_exception(e)
                        raise e
        else:
            _value_id = value

        assert _value_id is not None

        if _value_id in self._registered_values.keys():
            _value = self._registered_values[_value_id]
            return _value

        default_store: DataArchive = self.get_archive(
            archive_id_or_alias=self.default_data_store
        )
        if not default_store.has_value(value_id=_value_id):

            matches = []
            for store_id, store in self.data_archives.items():
                match = store.has_value(value_id=_value_id)
                if match:
                    matches.append(store_id)

            if len(matches) == 0:
                raise NoSuchValueIdException(
                    value_id=_value_id, msg=f"No value registered with id: {value}"
                )
            elif len(matches) > 1:
                raise NoSuchValueIdException(
                    value_id=_value_id,
                    msg=f"Found value with id '{value}' in multiple archives, this is not supported (yet): {matches}",
                )
            store_that_has_it = matches[0]
        else:
            store_that_has_it = self.default_data_store

        self._value_archive_lookup_map[_value_id] = store_that_has_it

        stored_value = self.get_archive(store_that_has_it).retrieve_value(
            value_id=_value_id
        )
        stored_value._set_registry(self)
        stored_value._is_stored = True

        self._registered_values[_value_id] = stored_value
        return self._registered_values[_value_id]

    def _persist_environment(self, env_hash: str, store: Union[str, None]):

        # cached = self._env_cache.get(env_type, {}).get(env_hash, None)
        # if cached is not None:
        #     return

        environment = self._kiara.metadata_registry.retrieve_environment_item(env_hash)

        if not environment:
            raise KiaraException(
                f"Can't persist data environment with hash '{env_hash}': no such environment registered."
            )

        self._kiara.metadata_registry.register_metadata_item(
            key=ENVIRONMENT_MARKER_KEY, item=environment, store=store
        )
        # self._env_cache.setdefault(env_type, {})[env_hash] = environment

    def store_value(
        self,
        value: Union[ValueLink, uuid.UUID, str],
        data_store: Union[str, None] = None,
    ) -> Union[PersistedData, None]:
        """Store a value into a data store.

        If 'data_store' is not provided, the default data store is used. If the 'data_store' argument is of
        type uuid, the archive_id is used, if string, first it will be converted to an uuid, if that works,
        again, the archive_id is used, if not, the string is used as the archive alias.

        """

        _value = self.get_value(value)

        # first, persist environment information
        for env_hash in _value.pedigree.environments.values():

            self._persist_environment(env_hash, store=data_store)

        store: DataStore = self.get_archive(archive_id_or_alias=data_store)  # type: ignore
        if not store.is_writeable():
            if data_store:
                raise Exception(
                    f"Can't write value into store '{data_store}': not writable."
                )
            else:
                raise Exception("Can't write value into store: not writable.")

        _data_store = store.archive_name
        # make sure all property values are available
        if _value.pedigree != ORPHAN:
            for value_id in _value.pedigree.inputs.values():
                self.store_value(value=value_id, data_store=_data_store)

        if not store.has_value(_value.value_id):
            event = ValuePreStoreEvent(kiara_id=self._kiara.id, value=_value)
            self._event_callback(event)
            persisted_value = store.store_value(_value)
            _value._is_stored = True

            self._value_archive_lookup_map[_value.value_id] = _data_store
            self._persisted_value_descs[_value.value_id] = persisted_value
            property_values = _value.property_values

            for property, property_value in property_values.items():
                self.store_value(value=property_value, data_store=_data_store)

            store_required = True
        else:
            persisted_value = None
            store_required = False

        store_event = ValueStoredEvent(
            kiara_id=self._kiara.id, value=_value, storing_required=store_required
        )
        self._event_callback(store_event)

        if _value.job_id:
            self._kiara.job_registry.store_job_record(
                job_id=_value.job_id, store=data_store
            )

        return persisted_value

    def lookup_aliases(self, value: Union[Value, uuid.UUID]) -> Set[str]:

        if isinstance(value, Value):
            value = value.value_id

        return self._kiara.alias_registry.find_aliases_for_value_id(value_id=value)

    def create_value_info(self, value: Union[Value, uuid.UUID]) -> ValueInfo:

        if isinstance(value, uuid.UUID):
            value = self.get_value(value=value)

        value_info = ValueInfo.create_from_instance(kiara=self._kiara, instance=value)
        return value_info

    def find_values(self, matcher: ValueMatcher) -> Dict[uuid.UUID, Value]:

        matches: Dict[uuid.UUID, Value] = {}
        for store_id, store in self.data_archives.items():

            if matcher.in_data_archives and store_id not in matcher.in_data_archives:
                continue

            try:
                _matches = store.find_values(matcher=matcher)
                for value in _matches:
                    if value.value_id in matches.keys():
                        raise Exception(
                            f"Found value '{value.value_id}' multiple times, this is not supported yet."
                        )
                    self._value_archive_lookup_map[value.value_id] = store_id
                    value._set_registry(self)
                    value._is_stored = True
                    self._registered_values[value.value_id] = value
                    matches[value.value_id] = value
                    self._values_by_hash.setdefault(value.value_hash, set()).add(
                        value.value_id
                    )
            except NotImplementedError:
                log_message(
                    "store.feature.missing",
                    feature="find_value",
                    reasong=f"find_values not implemented for store: {store_id}",
                    solution="return all values",
                )
                all_value_ids = store.value_ids
                if all_value_ids is None:
                    continue
                for value_id in all_value_ids:
                    value = store.retrieve_value(value_id=value_id)
                    value._set_registry(self)
                    value._is_stored = True

                    self._registered_values[value.value_id] = value

                    match = matcher.is_match(value, kiara=self._kiara)
                    if match:
                        if value.value_id in matches.keys():
                            raise Exception(
                                f"Found value '{value.value_id}' multiple times, this is not supported yet."
                            )
                        matches[value.value_id] = value

        return matches

    def find_values_with_aliases(self, matcher: ValueMatcher) -> Dict[str, Value]:

        matcher = matcher.model_copy(update={"has_aliases": True})
        all_values = self.find_values(matcher)
        result = {}
        for value in all_values.values():
            aliases = self._kiara.alias_registry.find_aliases_for_value_id(
                value_id=value.value_id
            )
            for a in aliases:
                assert a not in result  # this is a bug
                result[a] = value

        return result

    def find_values_for_hash(
        self, value_hash: str, data_type_name: Union[str, None] = None
    ) -> Set[Value]:

        if data_type_name:
            raise NotImplementedError()

        stored = self._values_by_hash.get(value_hash, None)
        if stored is None:
            matches: Dict[uuid.UUID, List[str]] = {}
            for store_id, store in self.data_archives.items():
                value_ids = store.find_values_with_hash(
                    value_hash=value_hash, data_type_name=data_type_name
                )
                for v_id in value_ids:
                    matches.setdefault(v_id, []).append(store_id)

            stored = set()
            for v_id, store_ids in matches.items():
                if len(store_ids) > 1:
                    raise Exception(
                        f"Found multiple stores for value id '{v_id}', this is not supported (yet)."
                    )
                self._value_archive_lookup_map[v_id] = store_ids[0]
                stored.add(v_id)

            if stored:
                self._values_by_hash[value_hash] = stored

        return {self.get_value(value=v_id) for v_id in stored}

    # ==============================================================================================
    # destiny stuff

    def retrieve_destinies_for_value_from_archives(
        self, value_id: uuid.UUID, alias_filter: Union[str, None] = None
    ) -> Mapping[str, uuid.UUID]:

        if alias_filter:
            raise NotImplementedError()

        all_destinies: Dict[str, uuid.UUID] = {}
        for archive_id, archive in self._data_archives.items():
            destinies: Union[Mapping[str, uuid.UUID], None] = (
                archive.find_destinies_for_value(
                    value_id=value_id, alias_filter=alias_filter
                )
            )
            if not destinies:
                continue
            for k, v in destinies.items():
                if k in all_destinies.keys():
                    raise Exception(f"Duplicate destiny '{k}' for value '{value_id}'.")
                all_destinies[k] = v

        return all_destinies

    def get_destiny_aliases_for_value(
        self, value_id: uuid.UUID, alias_filter: Union[str, None] = None
    ) -> Iterable[str]:

        # TODO: cache the result of this

        if alias_filter is not None:
            raise NotImplementedError()

        aliases: Set[str] = set()
        aliases.update(
            self.retrieve_destinies_for_value_from_archives(value_id=value_id).keys()
        )

        # all_stores = self._all_values_store_map.get(value_id)
        # if all_stores:
        #     for prefix in all_stores:
        #         all_aliases = self._destiny_archives[
        #             prefix
        #         ].get_destiny_aliases_for_value(value_id=value_id)
        #         if all_aliases is not None:
        #             aliases.update((f"{prefix}.{a}" for a in all_aliases))

        current = self._destinies_by_value.get(value_id, None)
        if current:
            aliases.update(current.keys())

        return sorted(aliases)

    def register_destiny(
        self,
        destiny_alias: str,
        values: Dict[str, uuid.UUID],
        manifest: "Manifest",
        result_field_name: Union[str, None] = None,
    ) -> "Destiny":
        """
        Add a destiny for one (or in some rare cases several) values.

        A destiny alias must be unique for every one of the involved input values.
        """
        if not values:
            raise Exception("Can't add destiny, no values provided.")

        from kiara.models.module.destiny import Destiny

        destiny = Destiny.create_from_values(
            kiara=self._kiara,
            destiny_alias=destiny_alias,
            manifest=manifest,
            result_field_name=result_field_name,
            values=values,
        )

        for value_id in destiny.fixed_inputs.values():

            self._destinies[destiny.destiny_id] = destiny
            # TODO: store history?
            self._destinies_by_value.setdefault(value_id, {})[destiny_alias] = destiny
            self._cached_value_aliases.setdefault(value_id, {})[destiny_alias] = destiny

        return destiny

    def attach_destiny_as_property(
        self,
        destiny: Union[uuid.UUID, "Destiny"],
        field_names: Union[Iterable[str], None] = None,
    ):

        if field_names:
            raise NotImplementedError()

        if isinstance(destiny, uuid.UUID):
            destiny = self._destinies[destiny]

        values = self.load_values(destiny.fixed_inputs)

        already_stored: List[uuid.UUID] = []
        for v in values.values():
            if v.is_stored:
                already_stored.append(v.value_id)

        if already_stored:
            stored = (str(v) for v in already_stored)
            raise Exception(
                f"Can't attach destiny as property, value(s) already stored: {', '.join(stored)}"
            )

        if destiny.result_value_id is None:
            destiny.execute(kiara=self._kiara)

        for v in values.values():
            assert destiny.result_value_id is not None
            v.add_property(
                value_id=destiny.result_value_id,
                property_path=destiny.destiny_alias,
                add_origin_to_property_value=True,
            )

    def get_registered_destiny(
        self, value_id: uuid.UUID, destiny_alias: str
    ) -> "Destiny":

        destiny = self._destinies_by_value.get(value_id, {}).get(destiny_alias, None)
        if destiny is None:
            raise Exception(
                f"No destiny '{destiny_alias}' available for value '{value_id}'."
            )

        return destiny

    def register_data(
        self,
        data: Any,
        schema: Union[ValueSchema, str, None, Mapping[str, Any]] = None,
        pedigree: Union[ValuePedigree, None] = None,
        pedigree_output_name: Union[str, None] = None,
        reuse_existing: bool = True,
    ) -> Value:

        value, newly_created = self._create_value(
            data=data,
            schema=schema,
            pedigree=pedigree,
            pedigree_output_name=pedigree_output_name,
            reuse_existing=reuse_existing,
        )

        if newly_created:
            self._values_by_hash.setdefault(value.value_hash, set()).add(value.value_id)
            self._registered_values[value.value_id] = value
            self._cached_data[value.value_id] = data

            event = ValueRegisteredEvent(kiara_id=self._kiara.id, value=value)
            self._event_callback(event)

        return value

    def _find_existing_value(
        self, data: Any, schema: Union[ValueSchema, None]
    ) -> Tuple[
        Union[Value, None],
        DataType,
        Union[Any, None],
        Union[str, SerializedData],
        ValueStatus,
        str,
        int,
    ]:

        if schema is None:
            raise NotImplementedError()

        if isinstance(data, Value):

            if data.value_id in self._registered_values.keys():

                if data.is_set and data.is_serializable:
                    serialized: Union[str, SerializedData] = data.serialized_data
                else:
                    serialized = NO_SERIALIZATION_MARKER
                return (
                    data,
                    data.data_type,
                    None,
                    serialized,
                    data.value_status,
                    data.value_hash,
                    data.value_size,
                )

            raise NotImplementedError("Importing values not supported (yet).")
            # self._registered_values[data.value_id] = data
            # return data

        try:
            value = self.get_value(value=data)
            if value.is_serializable:
                serialized = value.serialized_data
            else:
                serialized = NO_SERIALIZATION_MARKER

            return (
                value,
                value.data_type,
                None,
                serialized,
                value.value_status,
                value.value_hash,
                value.value_size,
            )
        except NoSuchValueException as nsve:
            raise nsve
        except Exception:
            # TODO: differentiate between 'value not found' and other type of errors
            pass

        # no obvious matches, so we try to find data that has the same hash
        data_type = self._kiara.type_registry.retrieve_data_type(
            data_type_name=schema.type, data_type_config=schema.type_config
        )

        data, serialized, status, value_hash, value_size = data_type._pre_examine_data(
            data=data, schema=schema
        )

        existing_value: Union[Value, None] = None
        if value_hash != INVALID_HASH_MARKER:
            existing = self.find_values_for_hash(value_hash=value_hash)
            if existing:
                if len(existing) == 1:
                    existing_value = next(iter(existing))
                else:
                    skalars = []
                    for v in existing:
                        if v.data_type.characteristics.is_scalar:
                            skalars.append(v)

                    if len(skalars) == 1:
                        existing_value = skalars[0]
                    elif skalars:
                        orphans = []
                        for v in skalars:
                            if v.pedigree == ORPHAN:
                                orphans.append(v)

                        if len(orphans) == 1:
                            existing_value = orphans[0]

        if existing_value is not None:
            self._persisted_value_descs[existing_value.value_id] = None
            return (
                existing_value,
                data_type,
                data,
                serialized,
                status,
                value_hash,
                value_size,
            )

        return (None, data_type, data, serialized, status, value_hash, value_size)

    def _create_value(
        self,
        data: Any,
        schema: Union[None, str, ValueSchema, Mapping[str, Any]] = None,
        pedigree: Union[ValuePedigree, None] = None,
        pedigree_output_name: Union[str, None] = None,
        reuse_existing: bool = True,
    ) -> Tuple[Value, bool]:
        """
        Create a new value, or return an existing one that matches the incoming data or reference.

        Arguments:
        ---------
            data: the (raw) data, or a reference to an existing value


        Returns:
        -------
            a tuple containing of the value object, and a boolean indicating whether the value was newly created (True), or already existing (False)
        """

        if schema is None:
            raise NotImplementedError()
        elif isinstance(schema, str):
            schema = ValueSchema(type=schema)
        elif isinstance(schema, Mapping):
            schema = ValueSchema(**schema)
        elif not isinstance(schema, ValueSchema):
            raise Exception(
                f"Invalid schema type: {type(schema)}, expected: {ValueSchema}"
            )

        if schema.type not in self._kiara.data_type_names:
            raise Exception(
                f"Can't register data of type '{schema.type}': type not registered. Available types: {', '.join(self._kiara.data_type_names)}"
            )

        if data is SpecialValue.NOT_SET and schema.default is not SpecialValue.NOT_SET:
            if callable(schema.default):
                raise NotImplementedError()
                data = schema.default()
            else:
                data = copy.deepcopy(schema.default)

            reuse_existing = False

        data_type: Union[None, DataType] = None
        if reuse_existing and not isinstance(data, (Value, uuid.UUID, SpecialValue)):

            data_type = self._kiara.type_registry.retrieve_data_type(
                data_type_name=schema.type, data_type_config=schema.type_config
            )
            if data_type.characteristics.is_scalar:
                reuse_existing = False

        if data is None:
            data = SpecialValue.NO_VALUE
        elif isinstance(data, uuid.UUID):
            if data == NONE_VALUE_ID:
                data = SpecialValue.NO_VALUE
            elif data == NOT_SET_VALUE_ID:
                data = SpecialValue.NOT_SET

        if reuse_existing and data not in [SpecialValue.NO_VALUE, SpecialValue.NOT_SET]:
            (
                _existing,
                data_type,
                data,
                serialized,
                status,
                value_hash,
                value_size,
            ) = self._find_existing_value(data=data, schema=schema)

            if _existing is not None:
                # TODO: check pedigree
                return (_existing, False)
        else:
            if data_type is None:
                data_type = self._kiara.type_registry.retrieve_data_type(
                    data_type_name=schema.type, data_type_config=schema.type_config
                )

            (
                data,
                serialized,
                status,
                value_hash,
                value_size,
            ) = data_type._pre_examine_data(data=data, schema=schema)

        if pedigree is None:
            pedigree = ORPHAN

        if pedigree_output_name is None:
            if pedigree == ORPHAN:
                pedigree_output_name = ORPHAN_PEDIGREE_OUTPUT_NAME
            else:
                raise NotImplementedError()

        if not pedigree.is_resolved:
            pedigree = self._kiara.module_registry.resolve_manifest(pedigree)  # type: ignore

        v_id = ID_REGISTRY.generate(
            type="value", kiara_id=self._kiara.id, pre_registered=False
        )

        value, data = data_type.assemble_value(
            value_id=v_id,
            data=data,
            schema=schema,
            environment_hashes=self._kiara.environment_registry.environment_hashes,
            serialized=serialized,
            status=status,
            value_hash=value_hash,
            value_size=value_size,
            pedigree=pedigree,
            kiara_id=self._kiara.id,
            pedigree_output_name=pedigree_output_name,
        )

        ID_REGISTRY.update_metadata(v_id, obj=value)
        value._data_registry = self

        event = ValueCreatedEvent(kiara_id=self._kiara.id, value=value)
        self._event_callback(event)

        return (value, True)

    def retrieve_persisted_value_details(self, value_id: uuid.UUID) -> PersistedData:

        if (
            value_id in self._persisted_value_descs.keys()
            and self._persisted_value_descs[value_id] is not None
        ):
            persisted_details = self._persisted_value_descs[value_id]
            assert persisted_details is not None
        else:
            # now, the value_store map should contain this value_id
            store_id = self.find_store_id_for_value(value_id=value_id)
            if store_id is None:
                raise Exception(
                    f"Can't find store for persisted data of value: {value_id}"
                )

            store = self.get_archive(store_id)
            assert value_id in self._registered_values.keys()
            # self.get_value(value_id=value_id)
            persisted_details = store.retrieve_serialized_value(value=value_id)
            for c in persisted_details.chunk_id_map.values():
                c._data_registry = self._kiara.data_registry
            self._persisted_value_descs[value_id] = persisted_details

        return persisted_details

    # def _retrieve_bytes(
    #     self, chunk_id: str, as_link: bool = True
    # ) -> Union[str, bytes]:
    #
    #     # TODO: support multiple stores
    #     return self.get_archive().retrieve_chunk(chunk_id=chunk_id, as_link=as_link)

    def retrieve_serialized_value(
        self, value_id: uuid.UUID
    ) -> Union[SerializedData, None]:
        """Create a LoadConfig object from the details of the persisted version of this value."""
        pv = self.retrieve_persisted_value_details(value_id=value_id)
        if pv is None:
            return None

        return pv

    # def retrieve_chunk(
    #     self,
    #     chunk_id: str,
    #     archive_id: Union[uuid.UUID, None] = None,
    #     as_file: bool = True,
    #     symlink_ok: bool = True,
    # ) -> Union[str, "BytesLike"]:
    #
    #     if archive_id is None:
    #         raise NotImplementedError()
    #
    #     archive = self.get_archive(archive_id)
    #     chunk = archive.retrieve_chunk(chunk_id, as_file=as_file, symlink_ok=symlink_ok)
    #
    #     return chunk

    def retrieve_chunks(
        self,
        chunk_ids: Sequence[str],
        as_files: bool = True,
        symlink_ok: bool = True,
        archive_id: Union[uuid.UUID, None] = None,
    ) -> Generator[Union[str, "BytesLike"], None, None]:
        """Return the chunk content in the same order as the 'chunk_ids' argument.

        If 'as_files' is 'True', it will return strings representing paths to files containing the chunk data. If symlink_ok is also set to 'True', the returning Path could potentially be a symlink, which means the underlying function might not need to copy the file. In this case, you are responsible to not change the contents of the path, ever.

        If 'as_files' is 'False', BytesLike objects will be returned, containing the chunk data bytes directly.
        """

        if archive_id is None:
            raise NotImplementedError(
                "Can't retrieve chunks without specifying an archive."
            )

        archive = self.get_archive(archive_id)

        chunks = archive.retrieve_chunks(
            chunk_ids, as_files=as_files, symlink_ok=symlink_ok
        )

        return chunks
        # for chunk_id in chunk_ids:
        #     yield archive.retrieve_chunk(chunk_id)

    def retrieve_value_data(
        self, value: Union[uuid.UUID, Value], target_profile: Union[str, None] = None
    ) -> Any:

        if isinstance(value, uuid.UUID):
            value = self.get_value(value=value)

        if value.value_id in self._cached_data.keys():
            return self._cached_data[value.value_id]

        if value._serialized_data is None:
            serialized_data: Union[str, SerializedData] = (
                self.retrieve_persisted_value_details(value_id=value.value_id)
            )
            value._serialized_data = serialized_data
        else:
            serialized_data = value._serialized_data

        if isinstance(serialized_data, str):
            raise Exception(
                f"Can't retrieve serialized version of value '{value.value_id}', this is most likely a bug."
            )

        manifest = serialized_data.metadata.deserialize.get("python_object", None)
        if manifest is None:
            raise Exception(
                f"No deserialize operation found for data type: {value.data_type_name}"
            )

        module = self._kiara.module_registry.create_module(manifest=manifest)
        op = Operation.create_from_module(module=module)

        input_field_match: Union[str, None] = None

        if len(op.inputs_schema) == 1:
            input_field_match = next(iter(op.inputs_schema.keys()))
        else:
            for input_field, schema in op.inputs_schema.items():
                for dt in self._kiara.type_registry.get_type_lineage(
                    value.data_type_name
                ):
                    if schema.type == dt:
                        if input_field_match is not None:
                            raise Exception(
                                f"Can't determine input field for deserialization operation '{module.module_type_name}': multiple input fields with type '{input_field_match}'."
                            )
                        else:
                            input_field_match = input_field
                            break
                if input_field_match:
                    break

        if input_field_match is None:
            raise Exception(
                f"Can't determine input field for deserialization operation '{module.module_type_name}'."
            )

        result_field_match: Union[str, None] = None
        for result_field, schema in op.outputs_schema.items():
            if schema.type == "python_object":
                if result_field_match is not None:
                    raise Exception(
                        f"Can't determine result field for deserialization operation '{module.module_type_name}': multiple result fields with type 'python_object'."
                    )
                else:
                    result_field_match = result_field
        if result_field_match is None:
            raise Exception(
                f"Can't determine result field for deserialization operation '{module.module_type_name}'."
            )

        inputs = {input_field_match: value}

        result = op.run(kiara=self._kiara, inputs=inputs)
        python_object = result.get_value_data(result_field_match)

        # TODO: can we do without this?
        parsed = value.data_type.parse_python_obj(python_object)
        value.data_type._validate(parsed)

        self._cached_data[value.value_id] = parsed

        return parsed

    def load_values(
        self,
        values: Mapping[str, Union[uuid.UUID, None, str, ValueLink]],
        values_schema: Union[None, Mapping[str, ValueSchema]] = None,
    ) -> ValueMapReadOnly:

        value_items = {}
        if values_schema:

            schemas = values_schema
            for k in schemas.keys():
                if k not in values.keys():
                    value_items[k] = self.get_value(NOT_SET_VALUE_ID)
                else:
                    value_id = values[k]
                    if value_id is None:
                        value_id = NONE_VALUE_ID

                    value_items[k] = self.get_value(value_id)
        else:
            schemas = {}
            for field_name, value_id in values.items():
                if value_id is None:
                    value_id = NONE_VALUE_ID

                value = self.get_value(value=value_id)
                value_items[field_name] = value
                schemas[field_name] = value.value_schema

        return ValueMapReadOnly(value_items=value_items, values_schema=schemas)

    def load_data(
        self, values: Mapping[str, Union[uuid.UUID, None]]
    ) -> Mapping[str, Any]:

        result_values = self.load_values(values=values)
        return {k: v.data for k, v in result_values.items()}

    def create_valuemap(
        self, data: Mapping[str, Any], schema: Mapping[str, ValueSchema]
    ) -> ValueMap:
        """Extract a set of [Value][kiara.data.values.Value] from Python data and ValueSchemas."""
        input_details = {}

        for input_name, value_schema in schema.items():
            input_details[input_name] = {"schema": value_schema}

        leftover = set(data.keys())
        leftover.difference_update(input_details.keys())
        if leftover:
            if not STRICT_CHECKS:
                log_message("unused.inputs", input_names=leftover)
            else:
                raise Exception(
                    f"Can't create values instance, inputs contain unused/invalid fields: {', '.join(leftover)}"
                )

        values = {}
        failed = {}

        _resolved: Dict[str, Value] = {}
        _unresolved = {}
        for input_name, details in input_details.items():
            _d = data.get(input_name, SpecialValue.NOT_SET)
            if isinstance(_d, str):
                try:
                    _d = uuid.UUID(_d)
                except Exception:
                    if schema[input_name].type == "string" and not (
                        _d.startswith("alias:") or _d.startswith("value:")
                    ):
                        pass
                    else:
                        try:
                            _d = self._alias_resolver.resolve_alias(_d)
                        except Exception as e:
                            log_exception(e)

            if isinstance(_d, Value):
                _resolved[input_name] = _d
            elif isinstance(_d, uuid.UUID):
                _resolved[input_name] = self.get_value(_d)
            else:
                _unresolved[input_name] = _d

        for input_name, _value in _resolved.items():
            # TODO: validate values against schema
            values[input_name] = _value

        for input_name, _data in _unresolved.items():

            value_schema = input_details[input_name]["schema"]

            if input_name not in data.keys():
                value_data = SpecialValue.NOT_SET
            elif data[input_name] in [
                None,
                SpecialValue.NO_VALUE,
                SpecialValue.NOT_SET,
            ]:
                value_data = SpecialValue.NO_VALUE
            else:
                value_data = data[input_name]

            try:
                value = self.register_data(
                    data=value_data, schema=value_schema, reuse_existing=True
                )
                values[input_name] = value

            except Exception as e:

                log_exception(e)

                msg: Any = str(e)
                if not msg:
                    msg = e

                log_message("invalid.valueset", error_reason=msg, input_name=input_name)
                failed[input_name] = e

        if failed:
            msg = []
            for k, v in failed.items():
                _v = str(v)
                if not str(v):
                    _v = type(v).__name__
                msg.append(f"{k}: {_v}")
            raise InvalidValuesException(
                msg=f"Can't create values instance: {', '.join(msg)}",
                invalid_values={k: str(v) for k, v in failed.items()},
            )
        return ValueMapReadOnly(value_items=values, values_schema=schema)  # type: ignore

    def create_renderable(self, **config: Any) -> RenderableType:
        """Create a renderable for this module configuration."""
        from kiara.utils.output import create_renderable_from_values

        all_values = {str(i): v for i, v in self._registered_values.items()}

        table = create_renderable_from_values(values=all_values, config=config)
        return table

    def pretty_print_data(
        self,
        value_id: uuid.UUID,
        target_type="terminal_renderable",
        **render_config: Any,
    ) -> Any:

        assert isinstance(value_id, uuid.UUID)

        return pretty_print_data(
            kiara=self._kiara,
            value_id=value_id,
            target_type=target_type,
            **render_config,
        )


# kiara\kiara\src\kiara\registries\data\data_store\filesystem_store.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)
import shutil
import uuid
from enum import Enum
from io import BytesIO
from pathlib import Path
from typing import (
    TYPE_CHECKING,
    Any,
    Generator,
    Generic,
    Iterable,
    Mapping,
    Sequence,
    Set,
    Union,
)

import orjson
import structlog

from kiara.exceptions import KiaraException
from kiara.models.module.jobs import JobRecord
from kiara.models.values.value import (
    PersistedData,
    Value,
)
from kiara.registries import ARCHIVE_CONFIG_CLS, ArchiveDetails, FileSystemArchiveConfig
from kiara.registries.data.data_store import BaseDataStore, DataArchive
from kiara.utils import log_message
from kiara.utils.hashfs import HashAddress, HashFS
from kiara.utils.json import orjson_dumps
from kiara.utils.windows import fix_windows_longpath, fix_windows_symlink

if TYPE_CHECKING:
    from multiformats import CID
    from multiformats.varint import BytesLike

logger = structlog.getLogger()

VALUE_DETAILS_FILE_NAME = "value.json"


class EntityType(Enum):

    VALUE = "values"
    VALUE_DATA = "value_data"
    ENVIRONMENT = "environments"
    MANIFEST = "manifests"
    DESTINY_LINK = "destiny_links"


DEFAULT_HASHFS_DEPTH = 4
DEFAULT_HASHFS_WIDTH = 1
DEFAULT_HASH_FS_ALGORITHM = "sha256"


class FileSystemDataArchive(
    DataArchive[FileSystemArchiveConfig], Generic[ARCHIVE_CONFIG_CLS]
):
    """Data store that loads data from the local filesystem."""

    _archive_type_name = "filesystem_data_archive"
    _config_cls = FileSystemArchiveConfig  # type: ignore

    def __init__(
        self,
        archive_name: str,
        archive_config: FileSystemArchiveConfig,
        force_read_only: bool = False,
    ):

        super().__init__(
            archive_name=archive_name,
            archive_config=archive_config,
            force_read_only=force_read_only,
        )
        self._base_path: Union[Path, None] = None
        self._hashfs_path: Union[Path, None] = None
        self._hashfs: Union[HashFS, None] = None
        # self._archive_metadata: Union[Mapping[str, Any], None] = None

    def _retrieve_archive_metadata(self) -> Mapping[str, Any]:

        if not self.archive_metadata_path.is_file():
            _archive_metadata = {}
        else:
            _archive_metadata = orjson.loads(self.archive_metadata_path.read_bytes())

        archive_id = _archive_metadata.get("archive_id", None)
        if not archive_id:
            try:
                _archive_id = uuid.UUID(
                    self.data_store_path.name
                )  # just to test it's a valid uuid
                _archive_metadata["archive_id"] = str(_archive_id)
            except Exception:
                raise Exception(
                    f"Could not retrieve archive id for alias archive '{self.archive_name}'."
                )

        return _archive_metadata

    @property
    def archive_metadata_path(self) -> Path:
        return self.data_store_path / "store_metadata.json"

    def get_archive_details(self) -> ArchiveDetails:

        size = sum(
            f.stat().st_size for f in self.data_store_path.glob("**/*") if f.is_file()
        )
        all_values = self.value_ids

        if all_values is not None:
            _all_values = list(all_values)
            details = {
                "size": size,
                "no_values": len(_all_values),
                "value_ids": sorted((str(x) for x in _all_values)),
                "dynamic_archive": False,
            }
        else:
            # will probably never happen
            details = {"size": size, "dynamic_archive": True}
        return ArchiveDetails(root=details)

    @property
    def data_store_path(self) -> Path:

        if self._base_path is not None:
            return self._base_path

        self._base_path = Path(self.config.archive_path).absolute()  # type: ignore
        self._base_path = fix_windows_longpath(self._base_path)
        self._base_path.mkdir(parents=True, exist_ok=True)
        return self._base_path

    def _delete_archive(self):
        shutil.rmtree(self.data_store_path)

    @property
    def hash_fs_path(self) -> Path:

        if self._hashfs_path is None:
            self._hashfs_path = self.data_store_path / "hash_fs"
        return self._hashfs_path

    @property
    def hashfs(self) -> HashFS:

        if self._hashfs is None:
            self._hashfs = HashFS(
                self.hash_fs_path.as_posix(),
                depth=DEFAULT_HASHFS_DEPTH,
                width=DEFAULT_HASHFS_WIDTH,
                algorithm=DEFAULT_HASH_FS_ALGORITHM,
            )
        return self._hashfs

    def get_path(
        self,
        entity_type: Union[EntityType, None] = None,
        base_path: Union[Path, None] = None,
    ) -> Path:
        if base_path is None:
            if entity_type is None:
                result = self.data_store_path
            else:
                result = self.data_store_path / entity_type.value
        else:
            if entity_type is None:
                result = base_path
            else:
                result = base_path / entity_type.value

        result.mkdir(parents=True, exist_ok=True)
        return result

    # def _retrieve_environment_details(
    #     self, env_type: str, env_hash: str
    # ) -> Mapping[str, Any]:
    #
    #     base_path = self.get_path(entity_type=EntityType.ENVIRONMENT)
    #     env_details_file = base_path / f"{env_type}_{env_hash}.json"
    #
    #     if not env_details_file.exists():
    #         raise Exception(
    #             f"Can't load environment details, file does not exist: {env_details_file.as_posix()}"
    #         )
    #
    #     environment: Mapping[str, Any] = orjson.loads(env_details_file.read_text())
    #     return environment

    def retrieve_all_job_hashes(
        self,
        manifest_hash: Union[str, None] = None,
        inputs_hash: Union[str, None] = None,
    ) -> Iterable[str]:

        raise NotImplementedError()

    def _retrieve_record_for_job_hash(self, job_hash: str) -> JobRecord:

        raise NotImplementedError()

    # def find_matching_job_record(
    #     self, inputs_manifest: InputsManifest
    # ) -> Optional[JobRecord]:
    #
    #     manifest_hash = str(inputs_manifest.instance_cid)
    #     jobs_hash = inputs_manifest.job_hash
    #
    #     base_path = self.get_path(entity_type=EntityType.MANIFEST)
    #     manifest_folder = base_path / str(manifest_hash)
    #
    #     if not manifest_folder.exists():
    #         return None
    #
    #     manifest_file = manifest_folder / "manifest.json"
    #
    #     if not manifest_file.exists():
    #         raise Exception(
    #             f"No 'manifests.json' file for manifest with hash: {manifest_hash}"
    #         )
    #
    #     manifest_data = orjson.loads(manifest_file.read_text())
    #
    #     job_folder = manifest_folder / jobs_hash
    #
    #     if not job_folder.exists():
    #         return None
    #
    #     inputs_file_name = job_folder / "inputs.json"
    #     if not inputs_file_name.exists():
    #         raise Exception(
    #             f"No 'inputs.json' file for manifest/inputs hash-combo: {manifest_hash} / {jobs_hash}"
    #         )
    #
    #     inputs_data = {
    #         k: uuid.UUID(v)
    #         for k, v in orjson.loads(inputs_file_name.read_text()).items()
    #     }
    #
    #     outputs = {}
    #     for output_file in job_folder.glob("output__*.json"):
    #         full_output_name = output_file.name[8:]
    #         start_value_id = full_output_name.find("__value_id__")
    #         output_name = full_output_name[0:start_value_id]
    #         value_id_str = full_output_name[start_value_id + 12 : -5]
    #
    #         value_id = uuid.UUID(value_id_str)
    #         outputs[output_name] = value_id
    #
    #     job_id = ID_REGISTRY.generate(obj_type=JobRecord, desc="fake job id")
    #     job_record = JobRecord(
    #         job_id=job_id,
    #         module_type=manifest_data["module_type"],
    #         module_config=manifest_data["module_config"],
    #         inputs=inputs_data,
    #         outputs=outputs,
    #     )
    #     return job_record

    def _find_values_with_hash(
        self,
        value_hash: str,
        value_size: Union[int, None] = None,
        data_type_name: Union[str, None] = None,
    ) -> Set[uuid.UUID]:

        value_data_folder = self.get_path(entity_type=EntityType.VALUE_DATA)

        glob = f"*/{value_hash}/value_id__*.json"

        matches = list(value_data_folder.glob(glob))

        result = set()
        for match in matches:
            if not match.is_symlink():
                log_message(
                    f"Ignoring value_id file, not a symlink: {match.as_posix()}"
                )
                continue

            uuid_str = match.name[10:-5]
            value_id = uuid.UUID(uuid_str)
            result.add(value_id)

        return result

    def _find_destinies_for_value(
        self, value_id: uuid.UUID, alias_filter: Union[str, None] = None
    ) -> Union[Mapping[str, uuid.UUID], None]:

        destiny_dir = self.get_path(entity_type=EntityType.DESTINY_LINK)
        destiny_value_dir = destiny_dir / str(value_id)

        if not destiny_value_dir.exists():
            return None

        destinies = {}
        for alias_link in destiny_value_dir.glob("*.json"):
            assert alias_link.is_symlink()

            alias = alias_link.name[0:-5]
            resolved = alias_link.resolve()

            value_id_str = resolved.parent.name
            value_id = uuid.UUID(value_id_str)
            destinies[alias] = value_id

        return destinies

    def _retrieve_all_value_ids(
        self, data_type_name: Union[str, None] = None
    ) -> Iterable[uuid.UUID]:

        if data_type_name is not None:
            raise NotImplementedError()

        childs = self.get_path(entity_type=EntityType.VALUE).glob("*")
        folders = [uuid.UUID(x.name) for x in childs if x.is_dir()]
        return folders

    def has_value(self, value_id: uuid.UUID) -> bool:
        """
        Check whether the specific value_id is persisted in this data store.
        way to quickly determine whether a value id is valid for this data store.

        Arguments:
        ---------
            value_id: the id of the value to check.


        Returns:
        -------
            whether this data store contains the value with the specified id
        """
        base_path = (
            self.get_path(entity_type=EntityType.VALUE)
            / str(value_id)
            / VALUE_DETAILS_FILE_NAME
        )
        return base_path.is_file()

    def _retrieve_value_details(self, value_id: uuid.UUID) -> Mapping[str, Any]:

        base_path = (
            self.get_path(entity_type=EntityType.VALUE)
            / str(value_id)
            / VALUE_DETAILS_FILE_NAME
        )
        if not base_path.is_file():
            raise Exception(
                f"Can't retrieve details for value with id '{value_id}': no value with that id stored."
            )

        value_data: Mapping[str, Any] = orjson.loads(base_path.read_text())
        return value_data

    def _retrieve_serialized_value(self, value: Value) -> PersistedData:

        base_path = self.get_path(entity_type=EntityType.VALUE_DATA)
        data_dir = base_path / value.data_type_name / str(value.value_hash)

        serialized_value_file = data_dir / ".serialized_value.json"
        data = orjson.loads(serialized_value_file.read_text())

        return PersistedData(**data)

    def _retrieve_chunk(
        self,
        chunk_id: str,
        as_file: bool = True,
        symlink_ok: bool = True,
    ) -> Union[bytes, str]:

        addr = self.hashfs.get(chunk_id)
        if addr is None:
            raise KiaraException(f"Can't find chunk with id '{chunk_id}'")

        if as_file is True:
            result: str = addr.abspath
            return result
        elif as_file is False:
            return Path(addr.abspath).read_bytes()
        else:
            raise NotImplementedError()

    def retrieve_chunks(
        self,
        chunk_ids: Sequence[str],
        as_files: bool = True,
        symlink_ok: bool = True,
    ) -> Generator[Union["BytesLike", str], None, None]:

        for chunk_id in chunk_ids:
            yield self._retrieve_chunk(
                chunk_id, as_file=as_files, symlink_ok=symlink_ok
            )


class FilesystemDataStore(FileSystemDataArchive, BaseDataStore):
    """Data store that stores data as files on the local filesystem."""

    _archive_type_name = "filesystem_data_store"

    # def _persist_environment_details(
    #     self, env_type: str, env_hash: str, env_data: Mapping[str, Any]
    # ):
    #
    #     base_path = self.get_path(entity_type=EntityType.ENVIRONMENT)
    #     env_details_file = base_path / f"{env_type}_{env_hash}.json"
    #
    #     if not env_details_file.exists():
    #         env_details_file.write_text(orjson_dumps(env_data))

    def _persist_stored_value_info(self, value: Value, persisted_value: PersistedData):

        working_dir = self.get_path(entity_type=EntityType.VALUE_DATA)
        data_dir = working_dir / value.data_type_name / str(value.value_hash)
        sv_file = data_dir / ".serialized_value.json"
        data_dir.mkdir(exist_ok=True, parents=True)
        sv_file.write_text(persisted_value.model_dump_json())

    def _persist_value_details(self, value: Value):

        value_dir = self.get_path(entity_type=EntityType.VALUE) / str(value.value_id)

        if value_dir.exists():
            raise Exception(
                f"Can't persist value '{value.value_id}', value directory already exists: {value_dir}"
            )
        else:
            value_dir.mkdir(parents=True, exist_ok=False)

        value_file = value_dir / VALUE_DETAILS_FILE_NAME
        value_data = value.model_dump()
        value_file.write_text(orjson_dumps(value_data, option=orjson.OPT_NON_STR_KEYS))

    def _persist_destiny_backlinks(self, value: Value):

        destiny_dir = self.get_path(entity_type=EntityType.DESTINY_LINK)

        for value_id, backlink in value.destiny_backlinks.items():

            destiny_value_dir = destiny_dir / str(value_id)
            destiny_value_dir.mkdir(parents=True, exist_ok=True)
            destiny_file = destiny_value_dir / f"{backlink}.json"
            assert not destiny_file.exists()

            value_dir = self.get_path(entity_type=EntityType.VALUE) / str(
                value.value_id
            )
            value_file = value_dir / VALUE_DETAILS_FILE_NAME
            assert value_file.exists()

            fix_windows_symlink(value_file, destiny_file)

    def _persist_chunks(self, chunks: Mapping["CID", Union[str, BytesIO]]):

        for cid, chunk in chunks.items():
            self._persist_chunk(str(cid), chunk)

    def _persist_chunk(self, chunk_id: str, chunk: Union[str, BytesIO]):

        addr: HashAddress = self.hashfs.put_with_precomputed_hash(chunk, chunk_id)

        assert addr.id == chunk_id
        # return addr
        # chunk_ids.append(addr.id)

    # def _persist_value_data(self, value: Value) -> PersistedData:
    #
    #     serialized_value: SerializedData = value.serialized_data
    #
    #     chunk_id_map = {}
    #     for key in serialized_value.get_keys():
    #
    #         data_model = serialized_value.get_serialized_data(key)
    #
    #         if data_model.type == "chunk":  # type: ignore
    #             chunks: Iterable[Union[str, BytesIO]] = [BytesIO(data_model.chunk)]  # type: ignore
    #         elif data_model.type == "chunks":  # type: ignore
    #             chunks = (BytesIO(c) for c in data_model.chunks)  # type: ignore
    #         elif data_model.type == "file":  # type: ignore
    #             chunks = [data_model.file]  # type: ignore
    #         elif data_model.type == "files":  # type: ignore
    #             chunks = data_model.files  # type: ignore
    #         elif data_model.type == "inline-json":  # type: ignore
    #             chunks = [BytesIO(data_model.as_json())]  # type: ignore
    #         else:
    #             raise Exception(
    #                 f"Invalid serialized data type: {type(data_model)}. Available types: {', '.join(SERIALIZE_TYPES)}"
    #             )
    #
    #         chunk_ids = []
    #         for item in zip(serialized_value.get_cids_for_key(key), chunks):
    #             cid = item[0]
    #             _chunk = item[1]
    #             addr: HashAddress = self.hashfs.put_with_precomputed_hash(
    #                 _chunk, str(cid)
    #             )
    #             chunk_ids.append(addr.id)
    #
    #         scids = SerializedChunkIDs(
    #             chunk_id_list=chunk_ids,
    #             archive_id=self.archive_id,
    #             size=data_model.get_size(),
    #         )
    #         scids._data_registry = self.kiara_context.data_registry
    #         chunk_id_map[key] = scids
    #
    #     pers_value = PersistedData(
    #         archive_id=self.archive_id,
    #         chunk_id_map=chunk_id_map,
    #         data_type=serialized_value.data_type,
    #         data_type_config=serialized_value.data_type_config,
    #         serialization_profile=serialized_value.serialization_profile,
    #         metadata=serialized_value.metadata,
    #     )
    #
    #     return pers_value

    def _persist_value_pedigree(self, value: Value):

        manifest_hash = value.pedigree.instance_cid
        jobs_hash = value.pedigree.job_hash

        base_path = self.get_path(entity_type=EntityType.MANIFEST)
        manifest_folder = base_path / str(manifest_hash)
        manifest_folder.mkdir(parents=True, exist_ok=True)

        manifest_info_file = manifest_folder / "manifest.json"
        if not manifest_info_file.exists():
            manifest_info_file.write_text(value.pedigree.manifest_data_as_json())

        job_folder = manifest_folder / str(jobs_hash)

        job_folder.mkdir(parents=True, exist_ok=True)

        inputs_details_file_name = job_folder / "inputs.json"
        if not inputs_details_file_name.exists():
            inputs_details_file_name.write_text(orjson_dumps(value.pedigree.inputs))

        outputs_file_name = (
            job_folder
            / f"output__{value.pedigree_output_name}__value_id__{value.value_id}.json"
        )

        outputs_file_name = fix_windows_longpath(outputs_file_name)

        if outputs_file_name.exists():
            # if value.pedigree_output_name == "__void__":
            #     return
            # else:
            raise Exception(f"Can't write value '{value.value_id}': already exists.")
        else:
            outputs_file_name.touch()

        value_data_dir = (
            self.get_path(entity_type=EntityType.VALUE_DATA)
            / value.value_schema.type
            / str(value.value_hash)
        )
        target_file = value_data_dir / f"value_id__{value.value_id}.json"

        fix_windows_symlink(outputs_file_name, target_file)


# kiara\kiara\src\kiara\registries\data\data_store\sqlite_store.py
# -*- coding: utf-8 -*-
import os
import uuid
from io import BytesIO
from pathlib import Path
from typing import (
    TYPE_CHECKING,
    Any,
    Dict,
    Generator,
    Generic,
    Iterable,
    List,
    Mapping,
    Sequence,
    Set,
    Union,
)

import orjson
from sqlalchemy import text
from sqlalchemy.engine import Connection, Engine

from kiara.defaults import (
    CHUNK_CACHE_BASE_DIR,
    CHUNK_CACHE_DIR_DEPTH,
    CHUNK_CACHE_DIR_WIDTH,
    CHUNK_COMPRESSION_TYPE,
    REQUIRED_TABLES_DATA_ARCHIVE,
    TABLE_NAME_ARCHIVE_METADATA,
    TABLE_NAME_DATA_CHUNKS,
    TABLE_NAME_DATA_DESTINIES,
    TABLE_NAME_DATA_METADATA,
    TABLE_NAME_DATA_PEDIGREE,
    TABLE_NAME_DATA_SERIALIZATION_METADATA,
)
from kiara.models.values.value import PersistedData, Value
from kiara.registries import (
    ARCHIVE_CONFIG_CLS,
    ArchiveDetails,
    SqliteArchiveConfig,
    SqliteDataStoreConfig,
)
from kiara.registries.data import DataArchive
from kiara.registries.data.data_store import BaseDataStore
from kiara.utils.db import create_archive_engine, delete_archive_db
from kiara.utils.hashfs import shard

if TYPE_CHECKING:
    from multiformats import CID
    from multiformats.varint import BytesLike


class SqliteDataArchive(DataArchive[SqliteArchiveConfig], Generic[ARCHIVE_CONFIG_CLS]):

    _archive_type_name = "sqlite_data_archive"
    _config_cls = SqliteArchiveConfig

    @classmethod
    def _load_archive_config(
        cls, archive_uri: str, allow_write_access: bool, **kwargs
    ) -> Union[Dict[str, Any], None]:

        if allow_write_access:
            return None

        if not Path(archive_uri).is_file():
            return None

        import sqlite3

        con = sqlite3.connect(archive_uri)

        cursor = con.cursor()
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
        tables = {x[0] for x in cursor.fetchall()}
        con.close()

        required_tables = REQUIRED_TABLES_DATA_ARCHIVE

        if not required_tables.issubset(tables):
            return None

        # config = SqliteArchiveConfig(sqlite_db_path=store_uri)
        return {"sqlite_db_path": archive_uri}

    def __init__(
        self,
        archive_name: str,
        archive_config: SqliteArchiveConfig,
        force_read_only: bool = False,
    ):

        super().__init__(
            archive_name=archive_name,
            archive_config=archive_config,
            force_read_only=force_read_only,
        )
        self._db_path: Union[Path, None] = None
        self._cached_engine: Union[Engine, None] = None
        self._data_cache_dir = CHUNK_CACHE_BASE_DIR
        self._data_cache_dir.mkdir(parents=True, exist_ok=True, mode=0o700)

        self._cache_dir_depth = CHUNK_CACHE_DIR_DEPTH
        self._cache_dir_width = CHUNK_CACHE_DIR_WIDTH
        self._value_id_cache: Union[Iterable[uuid.UUID], None] = None
        self._use_wal_mode: bool = archive_config.use_wal_mode
        # self._lock: bool = True

    def _retrieve_archive_metadata(self) -> Mapping[str, Any]:

        sql = text(f"SELECT key, value FROM {TABLE_NAME_ARCHIVE_METADATA}")

        with self.sqlite_engine.connect() as connection:
            result = connection.execute(sql)
            return {row[0]: row[1] for row in result}

    # def _retrieve_archive_id(self) -> uuid.UUID:
    #     sql = text("SELECT value FROM archive_metadata WHERE key='archive_id'")
    #
    #     with self.sqlite_engine.connect() as connection:
    #         result = connection.execute(sql)
    #         row = result.fetchone()
    #         if row is None:
    #             raise Exception("No archive ID found in metadata")
    #         return uuid.UUID(row[0])

    @property
    def sqlite_path(self):

        if self._db_path is not None:
            return self._db_path

        db_path = Path(self.config.sqlite_db_path).resolve()
        # self._db_path = fix_windows_longpath(db_path)
        self._db_path = db_path

        if self._db_path.exists():
            return self._db_path

        self._db_path.parent.mkdir(parents=True, exist_ok=True)
        return self._db_path

    # @property
    # def db_url(self) -> str:
    #     return f"sqlite:///{self.sqlite_path}"

    def get_chunk_path(self, chunk_id: str) -> Path:

        chunk_id = chunk_id.replace("-", "")
        chunk_id = chunk_id.lower()

        prefix = chunk_id[0:5]
        rest = chunk_id[5:]

        paths = shard(rest, self._cache_dir_depth, self._cache_dir_width)

        chunk_path = Path(os.path.join(self._data_cache_dir, prefix, *paths))
        return chunk_path

    @property
    def sqlite_engine(self) -> "Engine":

        if self._cached_engine is not None:
            return self._cached_engine

        self._cached_engine = create_archive_engine(
            db_path=self.sqlite_path,
            force_read_only=self.is_force_read_only(),
            use_wal_mode=self._use_wal_mode,
        )

        create_table_sql = f"""
CREATE TABLE IF NOT EXISTS {TABLE_NAME_DATA_METADATA} (
    value_id TEXT PRIMARY KEY,
    value_hash TEXT NOT NULL,
    value_size INTEGER NOT NULL,
    value_created TEXT NOT NULL,
    data_type_name TEXT NOT NULL,
    value_metadata TEXT NOT NULL
);
CREATE TABLE IF NOT EXISTS {TABLE_NAME_DATA_SERIALIZATION_METADATA} (
    value_id TEXT PRIMARY KEY,
    value_hash TEXT NOT NULL,
    value_size INTEGER NOT NULL,
    data_type_name TEXT NOT NULL,
    persisted_value_metadata TEXT NOT NULL
);
CREATE TABLE IF NOT EXISTS {TABLE_NAME_DATA_CHUNKS} (
    chunk_id TEXT PRIMARY KEY,
    chunk_data BLOB NOT NULL,
    compression_type INTEGER NULL
);
CREATE TABLE IF NOT EXISTS {TABLE_NAME_DATA_PEDIGREE} (
    value_id TEXT NOT NULL PRIMARY KEY,
    pedigree TEXT NOT NULL
);
CREATE TABLE IF NOT EXISTS {TABLE_NAME_DATA_DESTINIES} (
    value_id TEXT NOT NULL,
    destiny_name TEXT NOT NULL
);
"""

        with self._cached_engine.begin() as connection:
            for statement in create_table_sql.split(";"):
                if statement.strip():
                    connection.execute(text(statement))

        # if self._lock:
        #     event.listen(self._cached_engine, "connect", _pragma_on_connect)
        return self._cached_engine

    def _retrieve_serialized_value(self, value: Value) -> PersistedData:

        value_id = str(value.value_id)
        sql = text(
            f"SELECT persisted_value_metadata FROM {TABLE_NAME_DATA_SERIALIZATION_METADATA} WHERE value_id = :value_id"
        )
        with self.sqlite_engine.connect() as conn:
            cursor = conn.execute(sql, {"value_id": value_id})
            result = cursor.fetchone()
            data = orjson.loads(result[0])
            return PersistedData(**data)

    def _retrieve_value_details(self, value_id: uuid.UUID) -> Mapping[str, Any]:

        sql = text(
            f"SELECT value_metadata FROM {TABLE_NAME_DATA_METADATA} WHERE value_id = :value_id"
        )
        params = {"value_id": str(value_id)}
        with self.sqlite_engine.connect() as conn:
            cursor = conn.execute(sql, params)
            result = cursor.fetchone()
            data: Mapping[str, Any] = orjson.loads(result[0])
            return data

    # def _retrieve_environment_details(
    #     self, env_type: str, env_hash: str
    # ) -> Mapping[str, Any]:
    #
    #     sql = text(
    #         "SELECT environment_data FROM environments_data WHERE environment_type = ? AND environment_hash = ?"
    #     )
    #     with self.sqlite_engine.connect() as conn:
    #         cursor = conn.execute(sql, (env_type, env_hash))
    #         result = cursor.fetchone()
    #         return result[0]  # type: ignore

    # def find_values(self, matcher: ValueMatcher) -> Iterable[Value]:
    #     raise NotImplementedError()

    def has_value(self, value_id: uuid.UUID) -> bool:
        """
        Check whether the specific value_id is persisted in this data store.

        Implementing classes are encouraged to override this method, and choose a suitable, implementation specific
        way to quickly determine whether a value id is valid for this data store.

        Arguments:
        ---------
            value_id: the id of the value to check.


        Returns:
        -------
            whether this data store contains the value with the specified id
        """

        sql_text = text(
            f"SELECT EXISTS(SELECT 1 FROM {TABLE_NAME_DATA_METADATA} WHERE value_id = :value_id)"
        )
        with self.sqlite_engine.connect() as conn:
            result = conn.execute(sql_text, {"value_id": str(value_id)}).scalar()
            return bool(result)

    def _retrieve_all_value_ids(
        self, data_type_name: Union[str, None] = None
    ) -> Union[None, Iterable[uuid.UUID]]:

        if self._value_id_cache is not None:
            return self._value_id_cache

        sql = text(f"SELECT value_id FROM {TABLE_NAME_DATA_METADATA}")
        with self.sqlite_engine.connect() as conn:
            cursor = conn.execute(sql)
            result = cursor.fetchall()
            result_set = {uuid.UUID(x[0]) for x in result}
            self._value_id_cache = result_set
            return result_set

    def retrieve_all_chunk_ids(self) -> Iterable[str]:

        sql = text(f"SELECT chunk_id FROM {TABLE_NAME_DATA_CHUNKS}")
        with self.sqlite_engine.connect() as conn:
            cursor = conn.execute(sql)
            result = cursor.fetchall()
            return {x[0] for x in result}

    def _find_values_with_hash(
        self,
        value_hash: str,
        value_size: Union[int, None] = None,
        data_type_name: Union[str, None] = None,
    ) -> Union[Set[uuid.UUID], None]:

        if value_size is not None:
            raise NotImplementedError()
        if data_type_name is not None:
            raise NotImplementedError()

        sql = text(
            f"SELECT value_id FROM {TABLE_NAME_DATA_METADATA} WHERE value_hash = :value_hash"
        )
        params = {
            "value_hash": value_hash,
        }
        with self.sqlite_engine.connect() as conn:
            cursor = conn.execute(sql, parameters=params)
            result = cursor.fetchall()
            return {uuid.UUID(x[0]) for x in result}

    def _find_destinies_for_value(
        self, value_id: uuid.UUID, alias_filter: Union[str, None] = None
    ) -> Union[Mapping[str, uuid.UUID], None]:

        sql = text(
            f"SELECT destiny_name FROM {TABLE_NAME_DATA_DESTINIES} WHERE value_id = :value_id"
        )
        params = {"value_id": str(value_id)}
        with self.sqlite_engine.connect() as conn:
            cursor = conn.execute(sql, params)
            result = cursor.fetchall()
            result_destinies = {x[0]: value_id for x in result}
            return result_destinies

    # def retrieve_chunk(
    #     self,
    #     chunk_id: str,
    #     as_file: Union[bool, str, None] = None,
    #     symlink_ok: bool = True,
    # ) -> Union[bytes, str]:
    #
    #     import lzma
    #
    #     import lz4.frame
    #     from zstandard import ZstdDecompressor
    #
    #     dctx = ZstdDecompressor()
    #
    #     if as_file:
    #         chunk_path = self.get_chunk_path(chunk_id)
    #
    #         if chunk_path.exists():
    #             return chunk_path.as_posix()
    #
    #     sql = text(
    #         "SELECT chunk_data, compression_type FROM values_data WHERE chunk_id = :chunk_id"
    #     )
    #     params = {"chunk_id": chunk_id}
    #     with self.sqlite_engine.connect() as conn:
    #         cursor = conn.execute(sql, params)
    #         result_bytes = cursor.fetchone()
    #
    #     chunk_data: Union[str, bytes] = result_bytes[0]
    #     compression_type = result_bytes[1]
    #     if compression_type not in (None, 0):
    #         if CHUNK_COMPRESSION_TYPE(compression_type) == CHUNK_COMPRESSION_TYPE.ZSTD:
    #             chunk_data = dctx.decompress(chunk_data)
    #         elif (
    #             CHUNK_COMPRESSION_TYPE(compression_type) == CHUNK_COMPRESSION_TYPE.LZMA
    #         ):
    #             chunk_data = lzma.decompress(chunk_data)
    #         elif CHUNK_COMPRESSION_TYPE(compression_type) == CHUNK_COMPRESSION_TYPE.LZ4:
    #             chunk_data = lz4.frame.decompress(chunk_data)
    #         else:
    #             raise ValueError(f"Unsupported compression type: {compression_type}")
    #
    #     if not as_file:
    #         return chunk_data
    #
    #     chunk_path.parent.mkdir(parents=True, exist_ok=True, mode=0o700)
    #     with open(chunk_path, "wb") as file:
    #         file.write(chunk_data)
    #
    #     return chunk_path.as_posix()

    def retrieve_chunks(
        self,
        chunk_ids: Sequence[str],
        as_files: Union[bool, None] = None,
        symlink_ok: bool = True,
    ) -> Generator[Union["BytesLike", str], None, None]:

        import lzma

        import lz4.frame
        from zstandard import ZstdDecompressor

        dctx = ZstdDecompressor()

        MAX_CHUNKS_SQL = 50

        with self.sqlite_engine.connect() as conn:

            def retrieve_missing_chunks(
                missing_ids: List[str],
            ) -> Generator[Union["BytesLike", str], None, None]:

                id_list_str = ", ".join("'" + item + "'" for item in missing_ids)
                sql = text(
                    f"""SELECT chunk_id, chunk_data, compression_type FROM {TABLE_NAME_DATA_CHUNKS}
                    WHERE
                        chunk_id in ({id_list_str})
                    ORDER BY
                      CASE chunk_id
                        {"".join([f"WHEN '{id}' THEN {i} " for i, id in enumerate(missing_ids)])}
                      END
                    """
                )

                result = conn.execute(sql)
                for row in result:
                    result_chunk_id = row[0]
                    chunk_data = row[1]
                    compression_type = row[2]
                    if compression_type not in (None, 0):
                        if (
                            CHUNK_COMPRESSION_TYPE(compression_type)
                            == CHUNK_COMPRESSION_TYPE.ZSTD
                        ):
                            chunk_data = dctx.decompress(chunk_data)
                        elif (
                            CHUNK_COMPRESSION_TYPE(compression_type)
                            == CHUNK_COMPRESSION_TYPE.LZMA
                        ):
                            chunk_data = lzma.decompress(chunk_data)
                        elif (
                            CHUNK_COMPRESSION_TYPE(compression_type)
                            == CHUNK_COMPRESSION_TYPE.LZ4
                        ):
                            chunk_data = lz4.frame.decompress(chunk_data)
                        else:
                            raise ValueError(
                                f"Unsupported compression type: {compression_type}"
                            )

                    chunk_id = missing_ids.pop(0)
                    assert result_chunk_id == chunk_id

                    if not as_files:
                        yield chunk_data
                    else:
                        chunk_path = self.get_chunk_path(chunk_id)
                        chunk_path.parent.mkdir(parents=True, exist_ok=True, mode=0o700)
                        with open(chunk_path, "wb") as file:
                            file.write(chunk_data)
                        yield chunk_path.as_posix()

            missing_chunk_ids: List[str] = []

            for idx, chunk_id in enumerate(chunk_ids):

                if as_files:
                    chunk_path = self.get_chunk_path(chunk_id)

                    if chunk_path.exists():
                        path = chunk_path.as_posix()

                        if missing_chunk_ids:
                            for chunk in retrieve_missing_chunks(missing_chunk_ids):
                                yield chunk
                            assert not missing_chunk_ids

                        yield path
                        continue

                missing_chunk_ids.append(chunk_id)
                if len(missing_chunk_ids) >= MAX_CHUNKS_SQL:
                    for chunk in retrieve_missing_chunks(missing_chunk_ids):
                        yield chunk
                    assert not missing_chunk_ids

            if missing_chunk_ids:
                for chunk in retrieve_missing_chunks(missing_chunk_ids):
                    yield chunk
                assert not missing_chunk_ids

    def _delete_archive(self):

        delete_archive_db(db_path=self.sqlite_path)

    def get_archive_details(self) -> ArchiveDetails:

        size = self.sqlite_path.stat().st_size
        all_values = self.value_ids

        if all_values is not None:
            _all_values = list(all_values)
            details = {
                "no_values": len(_all_values),
                "value_ids": sorted((str(x) for x in _all_values)),
                "dynamic_archive": False,
                "size": size,
            }
        else:
            # will probably never happen
            details = {"dynamic_archive": True, "size": size}

        return ArchiveDetails(root=details)


class SqliteDataStore(SqliteDataArchive[SqliteDataStoreConfig], BaseDataStore):

    _archive_type_name = "sqlite_data_store"
    _config_cls = SqliteDataStoreConfig

    @classmethod
    def _load_archive_config(
        cls, archive_uri: str, allow_write_access: bool, **kwargs
    ) -> Union[Dict[str, Any], None]:

        if not allow_write_access:
            return None

        if not Path(archive_uri).is_file():
            return None

        import sqlite3

        con = sqlite3.connect(archive_uri)

        cursor = con.cursor()
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")

        tables = {x[0] for x in cursor.fetchall()}
        con.close()

        required_tables = REQUIRED_TABLES_DATA_ARCHIVE

        if not required_tables.issubset(tables):
            return None

        # config = SqliteArchiveConfig(sqlite_db_path=archive_uri)
        return {"sqlite_db_path": archive_uri}

    def _set_archive_metadata_value(self, key: str, value: Any):
        """Set custom metadata for the archive."""

        sql = text(
            f"INSERT OR REPLACE INTO {TABLE_NAME_ARCHIVE_METADATA} (key, value) VALUES (:key, :value)"
        )
        with self.sqlite_engine.connect() as conn:
            params = {"key": key, "value": value}
            conn.execute(sql, params)
            conn.commit()

    # def _persist_environment_details(
    #     self, env_type: str, env_hash: str, env_data: Mapping[str, Any]
    # ):
    #
    #     sql = text(
    #         "INSERT OR IGNORE INTO environments (environment_type, environment_hash, environment_data) VALUES (:environment_type, :environment_hash, :environment_data)"
    #     )
    #     env_data_json = orjson_dumps(env_data)
    #     with self.sqlite_engine.connect() as conn:
    #         params = {
    #             "environment_type": env_type,
    #             "environment_hash": env_hash,
    #             "environment_data": env_data_json,
    #         }
    #         conn.execute(sql, params)
    #         conn.commit()
    #     # print(env_type)
    #     # print(env_hash)
    #     # print(env_data_json)
    #     # raise NotImplementedError()

    # def _persist_value_data(self, value: Value) -> PersistedData:
    #
    #     serialized_value: SerializedData = value.serialized_data
    #     dbg(serialized_value.model_dump())
    #     dbg(serialized_value.get_keys())
    #
    #     raise NotImplementedError()

    def _persist_chunks(self, chunks: Mapping["CID", Union[str, BytesIO]]):

        all_chunk_ids = self.retrieve_all_chunk_ids()

        with self.sqlite_engine.connect() as conn:

            for chunk_id, chunk in chunks.items():
                cid_str = str(chunk_id)
                if cid_str in all_chunk_ids:
                    continue
                self._persist_chunk(conn, cid_str, chunk)

            conn.commit()

    def _persist_chunk(
        self, conn: Connection, chunk_id: str, chunk: Union[str, BytesIO]
    ):

        import lzma

        import lz4.frame
        from zstandard import ZstdCompressor

        cctx = ZstdCompressor()

        # sql = text(
        #     "SELECT EXISTS(SELECT 1 FROM values_data WHERE chunk_id = :chunk_id)"
        # )
        # with self.sqlite_engine.connect() as conn:
        #     result = conn.execute(sql, {"chunk_id": chunk_id}).scalar()
        #     if result:
        #         return

        if isinstance(chunk, str):
            with open(chunk, "rb") as file:
                file_data = file.read()
                bytes_io = BytesIO(file_data)
        else:
            bytes_io = chunk

        compression_type = CHUNK_COMPRESSION_TYPE[
            self.config.default_chunk_compression.upper()  # type: ignore
        ]

        if compression_type == CHUNK_COMPRESSION_TYPE.NONE:
            final_bytes = bytes_io.getvalue()
        elif compression_type == CHUNK_COMPRESSION_TYPE.ZSTD:
            bytes_io.seek(0)
            data = bytes_io.read()
            final_bytes = cctx.compress(data)
        elif compression_type == CHUNK_COMPRESSION_TYPE.LZMA:
            final_bytes = lzma.compress(bytes_io.getvalue())
        elif compression_type == CHUNK_COMPRESSION_TYPE.LZ4:
            bytes_io.seek(0)
            data = bytes_io.read()
            final_bytes = lz4.frame.compress(data)
        else:
            raise ValueError(
                f"Unsupported compression type: {self.config.default_chunk_compression}"
            )

        compression_type_value = (
            compression_type.value
            if compression_type is not CHUNK_COMPRESSION_TYPE.NONE
            else None
        )
        sql = text(
            f"INSERT INTO {TABLE_NAME_DATA_CHUNKS} (chunk_id, chunk_data, compression_type) VALUES (:chunk_id, :chunk_data, :compression_type)"
        )
        params = {
            "chunk_id": chunk_id,
            "chunk_data": final_bytes,
            "compression_type": compression_type_value,
        }

        conn.execute(sql, params)
        # conn.commit()

    def _persist_stored_value_info(self, value: Value, persisted_value: PersistedData):

        self._value_id_cache = None

        value_id = str(value.value_id)
        value_hash = value.value_hash
        value_size = value.value_size
        data_type_name = value.data_type_name

        metadata = persisted_value.model_dump_json()

        sql = text(
            f"INSERT INTO {TABLE_NAME_DATA_SERIALIZATION_METADATA} (value_id, value_hash, value_size, data_type_name, persisted_value_metadata) VALUES (:value_id, :value_hash, :value_size, :data_type_name, :metadata)"
        )

        with self.sqlite_engine.connect() as conn:
            params = {
                "value_id": value_id,
                "value_hash": value_hash,
                "value_size": value_size,
                "data_type_name": data_type_name,
                "metadata": metadata,
            }
            conn.execute(sql, params)
            conn.commit()

    def _persist_value_details(self, value: Value):

        value_id = str(value.value_id)
        value_hash = value.value_hash
        value_size = value.value_size
        data_type_name = value.data_type_name

        value_created = value.value_created.isoformat()

        metadata = value.model_dump_json()

        sql = text(
            f"INSERT INTO {TABLE_NAME_DATA_METADATA} (value_id, value_hash, value_size, value_created, data_type_name, value_metadata) VALUES (:value_id, :value_hash, :value_size, :value_created, :data_type_name, :metadata)"
        )
        with self.sqlite_engine.connect() as conn:
            params = {
                "value_id": value_id,
                "value_hash": value_hash,
                "value_size": value_size,
                "value_created": value_created,
                "data_type_name": data_type_name,
                "metadata": metadata,
            }
            conn.execute(sql, params)
            conn.commit()

    def _persist_destiny_backlinks(self, value: Value):

        value_id = str(value.value_id)

        with self.sqlite_engine.connect() as conn:

            for destiny_value_id, destiny_name in value.destiny_backlinks.items():

                sql = text(
                    f"INSERT INTO {TABLE_NAME_DATA_DESTINIES} (value_id, destiny_name) VALUES (:value_id, :destiny_name)"
                )
                params = {
                    "value_id": value_id,
                    "destiny_name": destiny_name,
                }
                conn.execute(sql, params)

            conn.commit()

    def _persist_value_pedigree(self, value: Value):

        value_id = str(value.value_id)
        pedigree = value.pedigree.manifest_data_as_json()

        sql = text(
            f"INSERT INTO {TABLE_NAME_DATA_PEDIGREE} (value_id, pedigree) VALUES (:value_id, :pedigree)"
        )
        with self.sqlite_engine.connect() as conn:
            params = {"value_id": value_id, "pedigree": pedigree}
            conn.execute(sql, params)
            conn.commit()


# kiara\kiara\src\kiara\registries\data\data_store\__init__.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import abc
import typing
import uuid
from io import BytesIO
from typing import (
    TYPE_CHECKING,
    Any,
    Dict,
    Generator,
    Iterable,
    Mapping,
    Sequence,
    Set,
    Union,
)

import structlog
from rich.console import RenderableType

from kiara.models.values.matchers import ValueMatcher
from kiara.models.values.value import (
    SERIALIZE_TYPES,
    PersistedData,
    SerializedChunkIDs,
    SerializedData,
    Value,
    ValuePedigree,
)
from kiara.models.values.value_schema import ValueSchema
from kiara.registries import ARCHIVE_CONFIG_CLS, BaseArchive
from kiara.utils.dates import get_earliest_time_incl_timezone

if TYPE_CHECKING:
    from multiformats import CID
    from multiformats.varint import BytesLike

logger = structlog.getLogger()


class DataArchive(BaseArchive[ARCHIVE_CONFIG_CLS], typing.Generic[ARCHIVE_CONFIG_CLS]):
    """Base class for data archiv implementationss."""

    @classmethod
    def supported_item_types(cls) -> Iterable[str]:
        """This archive type only supports storing data."""

        return ["data"]

    def __init__(
        self,
        archive_name: str,
        archive_config: ARCHIVE_CONFIG_CLS,
        force_read_only: bool = False,
    ):

        super().__init__(
            archive_name=archive_name,
            archive_config=archive_config,
            force_read_only=force_read_only,
        )

        self._env_cache: Dict[str, Dict[str, Mapping[str, Any]]] = {}
        self._value_cache: Dict[uuid.UUID, Value] = {}
        self._persisted_value_cache: Dict[uuid.UUID, PersistedData] = {}
        self._value_hash_index: Dict[str, Set[uuid.UUID]] = {}

    def retrieve_serialized_value(
        self, value: Union[uuid.UUID, Value]
    ) -> PersistedData:
        """Retrieve a 'PersistedData' instance from a value id or value instance."""

        if isinstance(value, Value):
            value_id: uuid.UUID = value.value_id
            _value: Union[Value, None] = value
        else:
            value_id = value
            _value = None

        if value_id in self._persisted_value_cache.keys():
            return self._persisted_value_cache[value_id]

        if _value is None:
            _value = self.retrieve_value(value_id)

        assert _value is not None

        persisted_value = self._retrieve_serialized_value(value=_value)
        self._persisted_value_cache[_value.value_id] = persisted_value
        return persisted_value

    @abc.abstractmethod
    def _retrieve_serialized_value(self, value: Value) -> PersistedData:
        """Retrieve a 'PersistedData' instance from a value instance.

        This method basically implements the store-specific logic to serialize/deserialize the value data to/from disk.

        Raise an exception if the value is not persisted in this archive.
        """

    def retrieve_value(self, value_id: uuid.UUID) -> Value:
        """Retrieve the value for the specified value_id.

        Looks up the value in the cache first, and if not found, calls the '_retrieve_value_details' method to retrieve

        Raises an exception if the value is not persisted in this archive.
        """

        cached = self._value_cache.get(value_id, None)
        if cached is not None:
            return cached

        value_data = self._retrieve_value_details(value_id=value_id)

        value_schema = ValueSchema(**value_data["value_schema"])
        # data_type = self._kiara.get_value_type(
        #         data_type=value_schema.type, data_type_config=value_schema.type_config
        #     )

        value_created = value_data.get("value_created", None)
        if value_created is None:
            value_created = get_earliest_time_incl_timezone()

        pedigree = ValuePedigree(**value_data["pedigree"])

        job_id_str = value_data.get("job_id", None)
        # TODO: check for this to be not-Null at some stage, once we can be sure it's always set (after release)
        if job_id_str is not None:
            job_id: Union[None, uuid.UUID] = uuid.UUID(job_id_str)
        else:
            job_id = None

        value = Value(
            value_id=value_data["value_id"],
            kiara_id=self.kiara_context.id,
            job_id=job_id,
            value_schema=value_schema,
            value_created=value_created,
            value_status=value_data["value_status"],
            value_size=value_data["value_size"],
            value_hash=value_data["value_hash"],
            environment_hashes=value_data.get("environment_hashes", {}),
            pedigree=pedigree,
            pedigree_output_name=value_data["pedigree_output_name"],
            data_type_info=value_data["data_type_info"],
            property_links=value_data["property_links"],
            destiny_backlinks=value_data["destiny_backlinks"],
        )

        self._value_cache[value_id] = value
        return self._value_cache[value_id]

    @abc.abstractmethod
    def _retrieve_value_details(self, value_id: uuid.UUID) -> Mapping[str, Any]:
        """Retrieve the value details for the specified value_id from disk.

        This method basically implements the store-specific logic to retrieve the value details from disk.

        """

    @property
    def value_ids(self) -> Union[None, Iterable[uuid.UUID]]:
        return self._retrieve_all_value_ids()

    @abc.abstractmethod
    def _retrieve_all_value_ids(
        self, data_type_name: Union[str, None] = None
    ) -> Union[None, Iterable[uuid.UUID]]:
        """Retrieve all value ids from the store.

        In the case that _retrieve_all_value_ids returns 'None', the store does not support statically determined value ids and the 'find_values' method(s) needs to be used to retrieve values. Also, 'has_value' can be used to test whether a specific value_id is stored in the archive.
        """

    def has_value(self, value_id: uuid.UUID) -> bool:
        """
        Check whether the specific value_id is persisted in this data store.

        Implementing classes are encouraged to override this method, and choose a suitable, implementation specific
        way to quickly determine whether a value id is valid for this data store.

        Arguments:
        ---------
            value_id: the id of the value to check.


        Returns:
        -------
            whether this data store contains the value with the specified id
        """
        all_value_ids = self.value_ids
        if all_value_ids is None:
            return False
        return value_id in all_value_ids

    # def retrieve_environment_details(
    #     self, env_type: str, env_hash: str
    # ) -> Mapping[str, Any]:
    #     """
    #     Retrieve the environment details with the specified type and hash.
    #
    #     The environment is stored by the data store as a dictionary, including it's schema, not as the actual Python model.
    #     This is to make sure it can still be loaded later on, in case the Python model has changed in later versions.
    #     """
    #     cached = self._env_cache.get(env_type, {}).get(env_hash, None)
    #     if cached is not None:
    #         return cached
    #
    #     env = self._retrieve_environment_details(env_type=env_type, env_hash=env_hash)
    #     self._env_cache.setdefault(env_type, {})[env_hash] = env
    #     return env
    #
    # @abc.abstractmethod
    # def _retrieve_environment_details(
    #     self, env_type: str, env_hash: str
    # ) -> Mapping[str, Any]:
    #     """Retrieve the environment details with the specified type and hash.
    #
    #     Each store needs to implement this so environemnt details related to a value can be retrieved later on. Since in most cases the environment details will not change, a lookup is more efficient than having to store the full information with each value.
    #     """

    def find_values(self, matcher: ValueMatcher) -> Iterable[Value]:
        raise NotImplementedError()

    def find_values_with_hash(
        self,
        value_hash: str,
        value_size: Union[int, None] = None,
        data_type_name: Union[str, None] = None,
    ) -> Set[uuid.UUID]:
        """Find all values that have data that match the specifid hash.

        If the data type name is specified, only values of that type are considered, which should speed up the search. Same with 'value_size'. But both filters are not implemented yet.
        """

        # if data_type_name is not None:
        #     raise NotImplementedError()
        #
        # if value_size is not None:
        #     raise NotImplementedError()

        if value_hash in self._value_hash_index.keys():
            value_ids: Union[Set[uuid.UUID], None] = self._value_hash_index[value_hash]
        else:
            value_ids = self._find_values_with_hash(
                value_hash=value_hash, data_type_name=data_type_name
            )
            if value_ids is None:
                value_ids = set()
            self._value_hash_index[value_hash] = value_ids

        assert value_ids is not None

        # TODO: if data_type_name or value_size are specified, validate the results?

        return value_ids

    @abc.abstractmethod
    def _find_values_with_hash(
        self,
        value_hash: str,
        value_size: Union[int, None] = None,
        data_type_name: Union[str, None] = None,
    ) -> Union[Set[uuid.UUID], None]:
        """Find all values that have data that match the specifid hash.

        If the data type name is specified, only values of that type are considered, which should speed up the search. Same with 'value_size'.
        This needs to be implemented in the implementing store though, and might or might not be used.
        """

    def find_destinies_for_value(
        self, value_id: uuid.UUID, alias_filter: Union[str, None] = None
    ) -> Union[Mapping[str, uuid.UUID], None]:
        """Find all destinies for the specified value id.

        TODO: explain destinies, and when they would be used.

        For now, you can just return 'None' in your implementation.
        """

        return self._find_destinies_for_value(
            value_id=value_id, alias_filter=alias_filter
        )

    @abc.abstractmethod
    def _find_destinies_for_value(
        self, value_id: uuid.UUID, alias_filter: Union[str, None] = None
    ) -> Union[Mapping[str, uuid.UUID], None]:
        """Find all destinies for the specified value id.

        TODO: explain destinies, and when they would be used.

        For now, you can just return 'None' in your implementation.
        """

    # @abc.abstractmethod
    # def retrieve_chunk(
    #     self,
    #     chunk_id: str,
    #     as_file: Union[bool, None] = None,
    #     symlink_ok: bool = True,
    # ) -> Union["BytesLike", str]:
    #     """Retrieve the chunk with the specified id.
    #
    #     If 'as_file' is specified, the chunk is written to a file, and the file path is returned. Otherwise, the chunk is returned as 'bytes'.
    #     """

    @abc.abstractmethod
    def retrieve_chunks(
        self,
        chunk_ids: Sequence[str],
        as_files: bool = True,
        symlink_ok: bool = True,
    ) -> Generator[Union["BytesLike", str], None, None]:
        """Retrieve a generator with all the specified chunks.

        If 'as_files' is specified, the chunks are written to a file, and the file path is returned. Otherwise, the chunk is returned as 'bytes'.
        """


class DataStore(DataArchive):
    @classmethod
    def _is_writeable(cls) -> bool:
        return True

    @abc.abstractmethod
    def store_value(self, value: Value) -> PersistedData:
        """
        "Store the value, its data and metadata into the store.

        Arguments:
        ---------
            value: the value to persist

        Returns:
        -------
            the load config that is needed to retrieve the value data later
        """


class BaseDataStore(DataStore):
    @abc.abstractmethod
    def _persist_stored_value_info(self, value: Value, persisted_value: PersistedData):
        """Store the details about the persisted data.

        This is used so an archive of this type can load the value data again later on. Value metadata is stored separately, later, using the '_persist_value_details' method.
        """

    @abc.abstractmethod
    def _persist_value_details(self, value: Value):
        """Persist the value details.

        Important details are:
         - value_id
         - value_hash
         - value_size
         - data_type_name
         - value_metadata
        """

    @abc.abstractmethod
    def _persist_value_pedigree(self, value: Value):
        """
        Create an internal link from a value to its pedigree (and pedigree details).

        This is so that the 'retrieve_job_record' can be used to prevent running the same job again, and the link of value
        to the job that produced it is preserved.
        """

    # @abc.abstractmethod
    # def _persist_environment_details(
    #     self, env_type: str, env_hash: str, env_data: Mapping[str, Any]
    # ):
    #     """Persist the environment details.
    #
    #     Each store type needs to store this for lookup purposes.
    #     """

    @abc.abstractmethod
    def _persist_destiny_backlinks(self, value: Value):
        """Persist the destiny backlinks."""

    def store_value(self, value: Value) -> PersistedData:

        logger.debug(
            "store.value",
            data_type=value.value_schema.type,
            value_id=value.value_id,
            value_hash=value.value_hash,
        )

        # # first, persist environment information
        # for env_type, env_hash in value.pedigree.environments.items():
        #     cached = self._env_cache.get(env_type, {}).get(env_hash, None)
        #     if cached is not None:
        #         continue
        #
        #     env = self.kiara_context.environment_registry.get_environment_for_cid(
        #         env_hash
        #     )
        #     self.persist_environment(env)

        # save the value data and metadata
        persisted_value = self._persist_value(value)
        self._persisted_value_cache[value.value_id] = persisted_value
        self._value_cache[value.value_id] = value
        self._value_hash_index.setdefault(value.value_hash, set()).add(value.value_id)

        # now link the output values to the manifest
        # then, make sure the manifest is persisted
        self._persist_value_pedigree(value=value)

        return persisted_value

    @abc.abstractmethod
    def _persist_chunks(self, chunks: Mapping["CID", BytesIO]):
        """Persist the specified chunk, and return the chunk id.

        If the chunk is a string, it represents a local file path, otherwise it is a BytesIO instance representing the actual data of the chunk.
        """

    def _persist_value_data(self, value: Value) -> PersistedData:

        serialized_value: SerializedData = value.serialized_data

        # dbg(serialized_value.model_dump())

        SIZE_LIMIT = 100000000

        chunk_id_map = {}
        chunks_to_persist: Dict[CID, BytesIO] = {}
        chunks_persisted: Set[CID] = set()
        current_size = 0
        for key in serialized_value.get_keys():

            data_model = serialized_value.get_serialized_data(key)

            if data_model.type == "chunk":  # type: ignore
                chunks: Iterable[BytesIO] = [BytesIO(data_model.chunk)]  # type: ignore
            elif data_model.type == "chunks":  # type: ignore
                chunks = (BytesIO(c) for c in data_model.chunks)  # type: ignore
            elif data_model.type == "file":  # type: ignore
                chunks = [data_model.file]  # type: ignore
            elif data_model.type == "files":  # type: ignore
                chunks = data_model.files  # type: ignore
            elif data_model.type == "inline-json":  # type: ignore
                chunks = [BytesIO(data_model.as_json())]  # type: ignore
            elif data_model.type == "chunk-ids":  # type: ignore
                # means this is already serialized in a different store
                data_model_instance: SerializedChunkIDs = data_model  # type: ignore
                chunks = (
                    BytesIO(x) for x in data_model_instance.get_chunks(as_files=False)  # type: ignore
                )

            else:
                raise Exception(
                    f"Invalid serialized data type: {type(data_model)}. Available types: {', '.join(SERIALIZE_TYPES)}"
                )

            cids = serialized_value.get_cids_for_key(key)
            chunk_iterable = zip(cids, chunks)
            chunks_to_persist.update(chunk_iterable)

            chunk_ids = [str(cid) for cid in cids]
            scids = SerializedChunkIDs(
                chunk_id_list=chunk_ids,
                archive_id=self.archive_id,
                size=data_model.get_size(),
            )
            scids._data_registry = self.kiara_context.data_registry
            chunk_id_map[key] = scids

            key_size = data_model.get_size()
            current_size += key_size
            # this is not super-exact, because the actual size of all chunks to be persisted is not known
            # since some of them might be filtered out, should be good enough to not let the memory blow up too much
            if current_size > SIZE_LIMIT:
                self._persist_chunks(
                    chunks={
                        k: v
                        for k, v in chunks_to_persist.items()
                        if k not in chunks_persisted
                    }
                )
                chunks_persisted.update(chunks_to_persist.keys())
                chunks_to_persist = {}
                current_size = 0

        if chunks_to_persist:
            self._persist_chunks(
                chunks={
                    k: v
                    for k, v in chunks_to_persist.items()
                    if k not in chunks_persisted
                }
            )

        pers_value = PersistedData(
            archive_id=self.archive_id,
            chunk_id_map=chunk_id_map,
            data_type=serialized_value.data_type,
            data_type_config=serialized_value.data_type_config,
            serialization_profile=serialized_value.serialization_profile,
            metadata=serialized_value.metadata,
        )

        return pers_value

    def _persist_value(self, value: Value) -> PersistedData:

        # TODO: check if value id is already persisted?
        if value.is_set:
            persisted_value_info: PersistedData = self._persist_value_data(value=value)
            if not persisted_value_info:
                raise Exception(
                    "Can't write persisted value info, no load config returned when persisting value."
                )
            if not isinstance(persisted_value_info, PersistedData):
                raise Exception(
                    f"Can't write persisted value info, invalid result type '{type(persisted_value_info)}' when persisting value."
                )
        else:
            persisted_value_info = PersistedData(
                archive_id=self.archive_id,
                data_type=value.data_type_name,
                serialization_profile="none",
                data_type_config=value.data_type_config,
                chunk_id_map={},
            )

        self._persist_stored_value_info(
            value=value, persisted_value=persisted_value_info
        )
        self._persist_value_details(value=value)
        # TODO: re-enable?
        if value.destiny_backlinks:
            self._persist_destiny_backlinks(value=value)

        return persisted_value_info

    # def persist_environment(self, environment: RuntimeEnvironment):
    #     """
    #     Persist the specified environment.
    #
    #     The environment is stored as a dictionary, including it's schema, not as the actual Python model.
    #     This is to make sure it can still be loaded later on, in case the Python model has changed in later versions.
    #     """
    #     env_type = environment.get_environment_type_name()
    #     env_hash = str(environment.instance_cid)
    #
    #     env = self._env_cache.get(env_type, {}).get(env_hash, None)
    #     if env is not None:
    #         return
    #
    #     env_data = environment.as_dict_with_schema()
    #     self._persist_environment_details(
    #         env_type=env_type, env_hash=env_hash, env_data=env_data
    #     )
    #     self._env_cache.setdefault(env_type, {})[env_hash] = env_data

    def create_renderable(self, **config: Any) -> RenderableType:
        """Create a renderable for this module configuration."""
        from kiara.utils.output import create_renderable_from_values

        all_values = {}
        all_value_ids = self.value_ids
        if all_value_ids:
            for value_id in all_value_ids:

                value = self.kiara_context.data_registry.get_value(value_id)
                all_values[str(value_id)] = value
            table = create_renderable_from_values(values=all_values, config=config)

            return table
        else:
            return "Data archive does not support statically determined ids."


# class PersistenceMgmt(object):
#     def __init__(self, kiara: "Kiara"):
#
#         self._kiara: Kiara = kiara
#         self._engine: Engine = self._kiara._engine
#
#     # def _serialize_value(self, value: Value) -> SerializedValueModel:
#     #
#     #     serialize_op_type = self._kiara.operations_mgmt.operation_types.get("serialize", None)
#     #     if serialize_op_type is None:
#     #         raise Exception("Can't serialize value, 'serialize' operation type not available.")
#     #
#     #     s_value = self._kiara.operations_mgmt.apply_operation(operation_type="serialize", value=value)
#     #     serialized: SerializedValueModel = s_value.serialized_value.data
#     #
#     #     return serialized
#
#     def _persist_value_data(self, value: Value) -> LoadConfig:
#
#         persist_op_type = self._kiara.operations_mgmt.operation_types.get("persist_value", None)
#         if persist_op_type is None:
#             raise Exception("Can't persist value, 'persist_value' operation type not available.")
#
#         op_type: PersistValueOperationType = self._kiara.operations_mgmt.get_operation_type("persist_value")  # type: ignore
#         op = op_type.get_operation_for_value_type(value.value_schema.type)
#         base_path = os.path.join(self._kiara.context_config.data_directory, str(value.value_id))
#         base_name = "value"
#
#         result = op.run(kiara=self._kiara, inputs={"value": value, "target": base_path, "base_name": base_name})
#
#         return result.get_value_data("load_config")
#
#     def _persist_value_in_session(self, value: Value, session: Session) -> Value:
#
#         if value.data is None:
#             raise NotImplementedError()
#
#         persisted = self._persist_value_data(value)
#         return None
#
#         data_type: DataType = self._kiara.get_value_type(
#             data_type=value.value_schema.type, data_type_config=value.value_schema.type_config
#         )
#
#         size = value.value_size
#         hash = value.value_hash
#
#         value_type_orm = (
#             session.query(ValueTypeOrm)
#             .filter_by(
#                 type_config_hash=data_type.value_type_hash,
#                 type_name=data_type.data_type_name,
#             )
#             .first()
#         )
#         if value_type_orm is None:
#             value_type_orm = ValueTypeOrm(
#                 type_config_hash=data_type.value_type_hash,
#                 type_name=data_type.data_type_name,
#                 type_config=data_type.type_config.dict(),
#             )
#             session.add(value_type_orm)
#             session.commit()
#
#         value_orm = (
#             session.query(Value)
#             .filter_by(
#                 value_hash=hash,
#                 value_size=size,
#                 data_type_name=data_type.data_type_name,
#             )
#             .first()
#         )
#         if value_orm is None:
#             value_id = uuid.uuid4()
#             value_orm = Value(
#                 global_id=value_id,
#                 data_type_name=data_type.data_type_name,
#                 value_size=size,
#                 value_hash=hash,
#                 value_type_id=value_type_orm.id,
#             )
#             session.add(value_orm)
#             session.commit()
#
#         return value_orm
#
#     def persist_value(self, value: Value):
#
#         with Session(bind=self._engine, future=True) as session:
#             value_orm = self._persist_value_in_session(value=value, session=session)
#
#         return value_orm
#
#     def persist_values(self, **values: Value):
#
#         orm_values = {}
#         with Session(bind=self._engine, future=True) as session:
#
#             for field, value in values.items():
#
#                 value_orm = self._persist_value_in_session(value=value, session=session)
#                 orm_values[field] = value_orm
#
#         return orm_values
#
#     def persist_module(self, module: "KiaraModule") -> ManifestOrm:
#
#         with Session(bind=self._engine, future=True) as session:
#             m = (
#                 session.query(ManifestOrm)
#                 .filter_by(module_config_hash=module.module_instance_hash)
#                 .first()
#             )
#
#             if m is None:
#                 mc = ManifestOrm(
#                     module_type=module.module_type_id,
#                     module_config_hash=module.module_instance_hash,
#                     module_config=module.render_config.dict(),
#                     is_idempotent=module.is_idempotent,
#                 )
#                 session.add(mc)
#                 session.commit()
#
#         return m


# kiara\kiara\src\kiara\registries\environment\__init__.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)


import inspect
from typing import TYPE_CHECKING, Any, Dict, Iterable, Mapping, Type, Union

from pydantic import BaseModel, Field, create_model
from rich import box
from rich.table import Table

from kiara.models.runtime_environment import RuntimeEnvironment, logger
from kiara.utils import _get_all_subclasses, is_debug, to_camel_case

if TYPE_CHECKING:
    pass


class EnvironmentRegistry(object):

    _instance = None

    @classmethod
    def instance(cls) -> "EnvironmentRegistry":
        """The default *kiara* context. In most cases, it's recommended you create and manage your own, though."""
        if cls._instance is None:
            cls._instance = EnvironmentRegistry()
        return cls._instance

    def __init__(self) -> None:

        self._environments: Union[Dict[str, RuntimeEnvironment], None] = None
        self._environment_hashes: Union[Dict[str, Mapping[str, str]], None] = None

        self._full_env_model: Union[BaseModel, None] = None

        # self._kiara: Kiara = kiara

    def get_environment_for_cid(self, env_cid: str) -> RuntimeEnvironment:

        envs = [
            env
            for env in self.environments.values()
            if str(env.instance_cid) == env_cid
        ]
        if len(envs) == 0:
            raise Exception(f"No environment with id '{env_cid}' available.")
        elif len(envs) > 1:
            raise Exception(
                f"Multipe environments with id '{env_cid}' available. This is most likely a bug."
            )
        return envs[0]

    def has_environment(self, env_cid: str) -> bool:

        for env in self.environments.values():
            if str(env.instance_cid) == env_cid:
                return True
        return False

    @property
    def environment_hashes(self) -> Mapping[str, Mapping[str, str]]:

        if self._environment_hashes is not None:
            return self._environment_hashes

        result = {}
        for env_name, env in self.environments.items():
            result[env_name] = env.env_hashes

        self._environment_hashes = result
        return self._environment_hashes

    @property
    def environments(self) -> Mapping[str, RuntimeEnvironment]:
        """Return all environments in this kiara runtime context."""
        if self._environments is not None:
            return self._environments

        import kiara.models.runtime_environment.kiara
        import kiara.models.runtime_environment.operating_system  # nowa
        import kiara.models.runtime_environment.python  # noqa

        subclasses: Iterable[Type[RuntimeEnvironment]] = _get_all_subclasses(
            RuntimeEnvironment  # type: ignore
        )
        envs = {}
        for sc in subclasses:
            if inspect.isabstract(sc):
                if is_debug():
                    logger.warning("class_loading.ignore_subclass", subclass=sc)
                else:
                    logger.debug("class_loading.ignore_subclass", subclass=sc)

            name = sc.get_environment_type_name()
            envs[name] = sc.create_environment_model()

        self._environments = {k: envs[k] for k in sorted(envs.keys())}
        return self._environments

    @property
    def full_model(self) -> BaseModel:
        """A model containing all environment data, incl. schemas and hashes of each sub-environment."""
        if self._full_env_model is not None:
            return self._full_env_model

        attrs = {k: (v.__class__, ...) for k, v in self.environments.items()}

        models = {}
        hashes = {}
        schemas = {}

        for k, v in attrs.items():
            name = to_camel_case(f"{k}_environment")
            k_cls: Type[RuntimeEnvironment] = create_model(
                name,
                __base__=v[0],
                metadata_hash=(
                    str,
                    Field(
                        description="The hash for this metadata (excl. this and the 'metadata_schema' field)."
                    ),
                ),
                metadata_schema=(
                    str,
                    Field(
                        description="JsonSchema describing this metadata (excl. this and the 'metadata_hash' field)."
                    ),
                ),
            )
            models[k] = (
                k_cls,
                Field(description=f"Metadata describing the {k} environment."),
            )
            schemas[k] = v[0].schema_json()
            hashes[k] = self.environments[k].instance_cid

        cls: Type[BaseModel] = create_model("KiaraRuntimeInfo", **models)  # type: ignore
        data = {}
        for k2, v2 in self.environments.items():
            d = v2.model_dump()
            assert "metadata_hash" not in d.keys()
            assert "metadata_schema" not in d.keys()
            d["metadata_hash"] = str(hashes[k2])
            d["metadata_schema"] = schemas[k]
            data[k2] = d
        model = cls(**data)  # type: ignore
        self._full_env_model = model
        return self._full_env_model

    def create_renderable(self, **config: Any):

        full_details = config.get("full_details", False)

        table = Table(show_header=True, box=box.SIMPLE)
        table.add_column("environment key", style="b")
        table.add_column("details")

        for env_name, env in self.environments.items():
            renderable = env.create_renderable(summary=not full_details)
            table.add_row(env_name, renderable)

        return table


# kiara\kiara\src\kiara\registries\events\metadata.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

from typing import TYPE_CHECKING, Any, Iterable

from kiara.models.events import KiaraEvent
from kiara.models.values.value import Value
from kiara.operations.included_core_operations.metadata import (
    ExtractMetadataDetails,
    ExtractMetadataOperationType,
)

if TYPE_CHECKING:
    from kiara.context import Kiara


class CreateMetadataDestinies(object):
    def __init__(self, kiara: "Kiara"):

        self._kiara: Kiara = kiara
        self._skip_internal_types: bool = True

    def supported_event_types(self) -> Iterable[str]:
        return ["value_created", "value_registered"]

    def handle_events(self, *events: KiaraEvent) -> Any:

        for event in events:
            if event.get_event_type() == "value_created":  # type: ignore
                if event.value.is_set:  # type: ignore
                    self.attach_metadata(event.value)  # type: ignore

        for event in events:
            if event.get_event_type() == "value_registered":  # type: ignore
                self.resolve_all_metadata(event.value)  # type: ignore

    def attach_metadata(self, value: Value):

        assert not value.is_stored

        if self._skip_internal_types:

            if value.value_schema.type == "any":
                return
            lineage = self._kiara.type_registry.get_type_lineage(
                value.value_schema.type
            )
            if "any" not in lineage:
                return

        op_type: ExtractMetadataOperationType = self._kiara.operation_registry.get_operation_type("extract_metadata")  # type: ignore
        operations = op_type.get_operations_for_data_type(value.value_schema.type)
        for metadata_key, op in operations.items():
            op_details: ExtractMetadataDetails = op.operation_details  # type: ignore
            input_field_name = op_details.input_field_name
            result_field_name = op_details.result_field_name

            # self._kiara.destiny_registry.add_destiny(
            #     destiny_alias=f"metadata.{metadata_key}",
            #     values={input_field_name: value.value_id},
            #     manifest=op,
            #     result_field_name=result_field_name,
            # )
            self._kiara.data_registry.register_destiny(
                destiny_alias=f"metadata.{metadata_key}",
                values={input_field_name: value.value_id},
                manifest=op,
                result_field_name=result_field_name,
            )

    def resolve_all_metadata(self, value: Value):

        if self._skip_internal_types:

            lineage = self._kiara.type_registry.get_type_lineage(
                value.value_schema.type
            )
            if "any" not in lineage:
                return

        assert not value.is_stored

        # aliases = self._kiara.destiny_registry.get_destiny_aliases_for_value(
        #     value_id=value.value_id
        # )

        aliases = self._kiara.data_registry.get_destiny_aliases_for_value(
            value_id=value.value_id
        )

        for alias in aliases:
            destiny = self._kiara.data_registry.get_registered_destiny(
                value_id=value.value_id, destiny_alias=alias
            )
            # self._kiara.destiny_registry.resolve_destiny(destiny)
            # self._kiara.destiny_registry.attach_as_property(destiny)
            self._kiara.data_registry.attach_destiny_as_property(destiny)


# kiara\kiara\src\kiara\registries\events\registry.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import fnmatch
import uuid
from functools import partial
from typing import TYPE_CHECKING, Callable, Dict, Iterable, List

from kiara.models.events import KiaraEvent
from kiara.registries.events import AsyncEventListener, EventListener, EventProducer
from kiara.registries.ids import ID_REGISTRY

if TYPE_CHECKING:
    from kiara.context import Kiara


class AllEvents(KiaraEvent):
    pass


class EventRegistry(object):
    def __init__(self, kiara: "Kiara"):

        self._kiara: Kiara = kiara
        self._producers: Dict[uuid.UUID, EventProducer] = {}
        self._listeners: Dict[uuid.UUID, EventListener] = {}
        self._subscriptions: Dict[uuid.UUID, List[str]] = {}

    def add_producer(self, producer: EventProducer) -> Callable:

        producer_id = ID_REGISTRY.generate(
            obj=producer, comment="adding event producer"
        )
        func = partial(self.handle_events, producer_id)
        return func

    def add_listener(self, listener, *subscriptions: str):

        if not subscriptions:
            _subscriptions = ["*"]
        else:
            _subscriptions = list(subscriptions)

        listener_id = ID_REGISTRY.generate(
            obj=listener, comment="adding event listener"
        )
        self._listeners[listener_id] = listener
        self._subscriptions[listener_id] = _subscriptions

    def _matches_subscription(
        self, events: Iterable[KiaraEvent], subscriptions: Iterable[str]
    ) -> Iterable[KiaraEvent]:

        result = []
        for subscription in subscriptions:
            for event in events:
                match = fnmatch.filter([event.get_event_type()], subscription)
                if match:
                    result.append(event)

        return result

    def handle_events(self, producer_id: uuid.UUID, *events: KiaraEvent):

        event_targets: Dict[uuid.UUID, List[KiaraEvent]] = {}

        for l_id, listener in self._listeners.items():
            matches = self._matches_subscription(
                events=events, subscriptions=self._subscriptions[l_id]
            )
            if matches:
                event_targets.setdefault(l_id, []).extend(matches)

        responses = {}
        for l_id, l_events in event_targets.items():
            listener = self._listeners[l_id]
            response = listener.handle_events(*l_events)
            responses[l_id] = response

        for l_id, response in responses.items():
            if response is None:
                continue

            a_listener: AsyncEventListener = self._listeners[l_id]  # type: ignore
            if not hasattr(a_listener, "wait_for_processing"):
                raise Exception(
                    "Can't wait for processing of event for listener: listener does not provide 'wait_for_processing' method."
                )
            a_listener.wait_for_processing(response)


# kiara\kiara\src\kiara\registries\events\__init__.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

from typing import Any, Protocol

from kiara.models.events import KiaraEvent


class EventListener(Protocol):
    def handle_events(self, *events: KiaraEvent) -> Any:
        pass


class AsyncEventListener(Protocol):
    def wait_for_processing(self, processing_id: Any):
        pass


class EventProducer(Protocol):

    pass

    # def suppoerted_event_types(self) -> Iterable[Type[KiaraEvent]]:
    #     pass


# kiara\kiara\src\kiara\registries\ids\__init__.py
# -*- coding: utf-8 -*-
import os

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)
import uuid
from typing import Any, Dict, Type, Union
from weakref import WeakValueDictionary

import structlog
from fasteners import InterProcessLock

from kiara.defaults import KIARA_MAIN_CONTEXT_LOCKS_PATH
from kiara.utils import is_debug, is_develop

logger = structlog.getLogger()


class NO_TYPE_MARKER(object):
    pass


class IdRegistry(object):
    def __init__(self) -> None:
        self._ids: Dict[uuid.UUID, Dict[Type, Dict[str, Any]]] = {}
        self._objs: Dict[uuid.UUID, WeakValueDictionary[Type, Any]] = {}

        self._process_context_locks: Dict[uuid.UUID, InterProcessLock] = {}

    def lock_context(self, context_id: uuid.UUID) -> bool:

        if context_id not in self._process_context_locks.keys():
            lock = InterProcessLock(
                os.path.join(
                    KIARA_MAIN_CONTEXT_LOCKS_PATH, f"context_{context_id}.lock"
                )
            )
            self._process_context_locks[context_id] = lock
        else:
            lock = self._process_context_locks[context_id]

        aquired: bool = lock.acquire(blocking=False)
        return aquired

    def unlock_context(self, context_id: uuid.UUID):

        if context_id not in self._process_context_locks.keys():
            return

        lock = self._process_context_locks[context_id]
        if lock.acquired:
            lock.release()

    def generate(
        self,
        id: Union[uuid.UUID, None] = None,
        obj_type: Union[Type, None] = None,
        obj: Union[Any, None] = None,
        **metadata: Any,
    ) -> uuid.UUID:

        if id is None:
            id = uuid.uuid4()

        if is_debug() or is_develop():

            # logger.debug("generate.id", id=id, metadata=metadata)
            if obj_type is None:
                if obj:
                    obj_type = obj.__class__
                else:
                    obj_type = NO_TYPE_MARKER
            self._ids.setdefault(id, {}).setdefault(obj_type, {}).update(metadata)
            if obj:
                self._objs.setdefault(id, WeakValueDictionary())[obj_type] = obj

        return id

    def update_metadata(
        self,
        id: uuid.UUID,
        obj_type: Union[Type, None] = None,
        obj: Union[Any, None] = None,
        **metadata,
    ):

        if not is_debug() and not is_develop():
            return

        if obj_type is None:
            if obj:
                obj_type = obj.__class__
            else:
                obj_type = NO_TYPE_MARKER
        self._ids.setdefault(id, {}).setdefault(obj_type, {}).update(metadata)
        if obj:
            self._objs.setdefault(id, WeakValueDictionary())[obj_type] = obj


ID_REGISTRY = IdRegistry()


# kiara\kiara\src\kiara\registries\jobs\__init__.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import abc
import uuid
from datetime import datetime
from typing import TYPE_CHECKING, Any, Dict, Iterable, List, Mapping, Type, Union

import structlog
from bidict import bidict
from rich.console import Group

from kiara.defaults import (
    DEFAULT_DATA_STORE_MARKER,
    DEFAULT_JOB_STORE_MARKER,
    DEFAULT_STORE_MARKER,
    ENVIRONMENT_MARKER_KEY,
)
from kiara.exceptions import FailedJobException, KiaraException
from kiara.models.events import KiaraEvent
from kiara.models.events.job_registry import (
    JobArchiveAddedEvent,
    JobRecordPreStoreEvent,
    JobRecordStoredEvent,
)
from kiara.models.module.jobs import (
    ActiveJob,
    JobConfig,
    JobMatcher,
    JobRecord,
    JobStatus,
)
from kiara.models.module.manifest import InputsManifest, Manifest
from kiara.models.values.value import ValueMap, ValueMapReadOnly
from kiara.processing import ModuleProcessor
from kiara.processing.synchronous import SynchronousProcessor
from kiara.registries.jobs.job_store import JobArchive, JobStore
from kiara.utils import get_dev_config, is_develop

if TYPE_CHECKING:
    from kiara.context import Kiara
    from kiara.context.runtime_config import JobCacheStrategy
    from kiara.models.runtime_environment import RuntimeEnvironment

logger = structlog.getLogger()

MANIFEST_SUB_PATH = "manifests"


class ExistingJobMatcher(abc.ABC):
    def __init__(self, kiara: "Kiara"):

        self._kiara: Kiara = kiara

    @abc.abstractmethod
    def find_existing_job(
        self, inputs_manifest: InputsManifest
    ) -> Union[JobRecord, None]:
        pass


class NoneExistingJobMatcher(ExistingJobMatcher):
    def find_existing_job(
        self, inputs_manifest: InputsManifest
    ) -> Union[JobRecord, None]:
        return None


class ValueIdExistingJobMatcher(ExistingJobMatcher):
    def find_existing_job(
        self, inputs_manifest: InputsManifest
    ) -> Union[JobRecord, None]:

        matches = []

        for store_id, archive in self._kiara.job_registry.job_archives.items():

            match = archive.retrieve_record_for_job_hash(
                job_hash=inputs_manifest.job_hash
            )
            if match:
                matches.append(match)

        if len(matches) == 0:
            return None
        elif len(matches) > 1:
            raise Exception(
                f"Multiple stores have a record for inputs manifest '{inputs_manifest}', this is not supported (yet)."
            )

        job_record = matches[0]
        job_record._is_stored = True

        return job_record


class DataHashExistingJobMatcher(ExistingJobMatcher):
    def find_existing_job(
        self, inputs_manifest: InputsManifest
    ) -> Union[JobRecord, None]:

        matches = []

        ignore_internal = True
        if ignore_internal:

            module = self._kiara.module_registry.create_module(inputs_manifest)
            if module.characteristics.is_internal:
                return None

        for store_id, archive in self._kiara.job_registry.job_archives.items():

            match = archive.retrieve_record_for_job_hash(
                job_hash=inputs_manifest.job_hash
            )
            if match:
                matches.append(match)

        if len(matches) > 1:
            raise Exception(
                f"Multiple stores have a record for inputs manifest '{inputs_manifest}', this is not supported (yet)."
            )

        elif len(matches) == 1:

            job_record = matches[0]
            job_record._is_stored = True

            return job_record

        inputs_data_cid, contains_invalid = inputs_manifest.calculate_inputs_data_cid(
            data_registry=self._kiara.data_registry
        )

        inputs_data_hash = str(inputs_data_cid)

        matching_records = []
        for store_id, archive in self._kiara.job_registry.job_archives.items():
            _matches = archive.retrieve_all_job_hashes(
                manifest_hash=inputs_manifest.manifest_hash
            )
            for _match in _matches:
                _job_record = archive.retrieve_record_for_job_hash(_match)
                assert _job_record is not None
                if _job_record.inputs_data_hash == inputs_data_hash:
                    matching_records.append(_job_record)

        if not matching_records:
            return None
        elif len(matches) > 1:
            raise Exception(
                f"Multiple stores have a record for inputs manifest '{inputs_manifest}', this is not supported (yet)."
            )
        else:
            return matching_records[0]


class JobRegistry(object):
    def __init__(self, kiara: "Kiara"):

        self._kiara: Kiara = kiara

        self._job_matcher_cache: Dict[JobCacheStrategy, ExistingJobMatcher] = {}

        self._active_jobs: bidict[str, uuid.UUID] = bidict()
        self._failed_jobs: Dict[str, uuid.UUID] = {}
        self._finished_jobs: Dict[str, uuid.UUID] = {}
        self._archived_records: Dict[uuid.UUID, JobRecord] = {}

        self._processor: ModuleProcessor = SynchronousProcessor(kiara=self._kiara)
        self._processor.register_job_status_listener(self)
        self._job_archives: Dict[str, JobArchive] = {}
        self._default_job_store: Union[str, None] = None

        self._event_callback = self._kiara.event_registry.add_producer(self)

        self._env_cache: Dict[str, Dict[str, RuntimeEnvironment]] = {}

        # default_archive = FileSystemJobStore.create_from_kiara_context(self._kiara)
        # self.register_job_archive(default_archive, store_alias=DEFAULT_STORE_MARKER)

        # default_file_store = self._kiara.data_registry.get_archive(DEFAULT_STORE_MARKER)
        # self.register_job_archive(default_file_store, store_alias="default_data_store")  # type: ignore

    @property
    def job_matcher(self) -> ExistingJobMatcher:

        from kiara.context.runtime_config import JobCacheStrategy

        strategy = self._kiara.runtime_config.job_cache
        if is_develop():
            dev_config = get_dev_config()
            if not dev_config.job_cache:
                logger.debug(
                    "disable.job_cache",
                    reason="dev mode enabled and 'disable_job_cache' is set.",
                )
                strategy = JobCacheStrategy.no_cache

        job_matcher = self._job_matcher_cache.get(strategy, None)
        if job_matcher is None:
            if strategy == JobCacheStrategy.no_cache:
                job_matcher = NoneExistingJobMatcher(kiara=self._kiara)
            elif strategy == JobCacheStrategy.value_id:
                job_matcher = ValueIdExistingJobMatcher(kiara=self._kiara)
            elif strategy == JobCacheStrategy.data_hash:
                job_matcher = DataHashExistingJobMatcher(kiara=self._kiara)
            else:
                raise Exception(f"Job cache strategy not implemented: {strategy}")
            self._job_matcher_cache[strategy] = job_matcher

        return job_matcher

    def suppoerted_event_types(self) -> Iterable[Type[KiaraEvent]]:

        return [JobArchiveAddedEvent, JobRecordPreStoreEvent, JobRecordStoredEvent]

    def register_job_archive(self, archive: JobArchive) -> str:

        alias = archive.archive_name

        if not alias:
            raise Exception("Invalid job archive alias: can't be empty.")

        if alias in self._job_archives.keys():
            raise Exception(
                f"Can't register job store, store id already registered: {alias}."
            )

        archive.register_archive(self._kiara)
        self._job_archives[alias] = archive

        is_store = False
        is_default_store = False
        if isinstance(archive, JobStore):
            is_store = True
            if self._default_job_store is None:
                self._default_job_store = alias

        event = JobArchiveAddedEvent(
            kiara_id=self._kiara.id,
            job_archive_id=archive.archive_id,
            job_archive_alias=alias,
            is_store=is_store,
            is_default_store=is_default_store,
        )
        self._event_callback(event)

        return alias

    @property
    def default_job_store(self) -> str:

        if self._default_job_store is None:
            raise Exception("No default job store set (yet).")
        return self._default_job_store  # type: ignore

    def get_archive(self, store_id: Union[str, None, uuid.UUID] = None) -> JobArchive:

        if store_id in [
            None,
            "",
            DEFAULT_DATA_STORE_MARKER,
            DEFAULT_JOB_STORE_MARKER,
            DEFAULT_STORE_MARKER,
        ]:
            if self.default_job_store is None:
                raise Exception("Can't retrieve deafult job archive, none set (yet).")
            _store_id: str = self.default_job_store

        elif not isinstance(store_id, str):
            raise NotImplementedError(
                "Can't retrieve job archive by (uu)id or other type (yet)."
            )
        else:
            _store_id = store_id

        return self._job_archives[_store_id]

    @property
    def job_archives(self) -> Mapping[str, JobArchive]:
        return self._job_archives

    def job_status_changed(
        self,
        job_id: uuid.UUID,
        old_status: Union[JobStatus, None],
        new_status: JobStatus,
    ):

        # print(f"JOB STATUS CHANGED: {job_id} - {old_status} - {new_status.value}")
        if job_id in self._active_jobs.values() and new_status is JobStatus.FAILED:
            job_hash = self._active_jobs.inverse.pop(job_id)
            self._failed_jobs[job_hash] = job_id
        elif job_id in self._active_jobs.values() and new_status is JobStatus.SUCCESS:
            job_hash = self._active_jobs.inverse.pop(job_id)

            job_record = self._processor.get_job_record(job_id)

            self._finished_jobs[job_hash] = job_id
            self._archived_records[job_id] = job_record

    def _persist_environment(self, env_type: str, env_hash: str):

        cached = self._env_cache.get(env_type, {}).get(env_hash, None)
        if cached is not None:
            return

        environment = self._kiara.metadata_registry.retrieve_environment_item(env_hash)

        if not environment:
            raise KiaraException(
                f"Can't persist job environment for with hash '{env_hash}': no such environment registered."
            )

        self._kiara.metadata_registry.register_metadata_item(
            key=ENVIRONMENT_MARKER_KEY, item=environment
        )
        self._env_cache.setdefault(env_type, {})[env_hash] = environment

    def store_job_record(self, job_id: uuid.UUID, store: Union[str, None] = None):

        # TODO: allow to store job record to external store

        job_record = self.get_job_record(job_id=job_id)

        for env_type, env_hash in job_record.environment_hashes.items():
            self._persist_environment(env_type, env_hash)

        if job_record._is_stored:
            logger.debug(
                "ignore.store.job_record", reason="already stored", job_id=str(job_id)
            )
            return

        store: JobStore = self.get_archive(store)  # type: ignore
        if not isinstance(store, JobStore):
            raise Exception("Can't store job record to archive: not writable.")

        logger.debug(
            "store.job_record",
            job_hash=job_record.job_hash,
            module_type=job_record.module_type,
        )

        pre_store_event = JobRecordPreStoreEvent(
            kiara_id=self._kiara.id, job_record=job_record
        )
        self._event_callback(pre_store_event)

        store.store_job_record(job_record)

        stored_event = JobRecordStoredEvent(
            kiara_id=self._kiara.id, job_record=job_record
        )
        self._event_callback(stored_event)

    def get_job_record_in_session(self, job_id: uuid.UUID) -> JobRecord:

        return self._processor.get_job_record(job_id)

    def get_job_record(self, job_id: uuid.UUID) -> JobRecord:

        if job_id in self._archived_records.keys():
            return self._archived_records[job_id]

        try:
            job_record = self._processor.get_job_record(job_id=job_id)
            return job_record
        except Exception:
            pass

        # try:
        #     job = self._processor.get_job(job_id=job_id)
        #     if job is not None:
        #         if job.status == JobStatus.FAILED:
        #             return None
        # except Exception:
        #     pass

        all_job_records = self.retrieve_all_job_records()
        for r in all_job_records.values():
            if r.job_id == job_id:
                return r

        # raise a FailedJobException if the job is in the failed jobs list
        self.get_active_job(job_id=job_id)

        # this should never happen
        raise KiaraException("Can't find job record with id: {job_id}")

    def find_job_records(self, matcher: JobMatcher) -> Mapping[uuid.UUID, JobRecord]:

        all_records: List[JobRecord] = []
        for archive in self.job_archives.values():

            _job_records = archive.retrieve_matching_job_records(matcher=matcher)
            all_records.extend(_job_records)

        # TODO: check for duplicates and mismatching datetimes
        all_jobs_sorted = {
            job.job_id: job
            for job in sorted(
                all_records,
                key=lambda item: item.job_submitted,
                reverse=True,
            )
        }

        return all_jobs_sorted

    def retrieve_all_job_record_ids(self) -> List[uuid.UUID]:
        """Retrieve a list of all available job record ids, sorted from latest to earliest."""

        all_records: Dict[uuid.UUID, datetime] = {}
        for archive in self.job_archives.values():
            all_record_ids = archive.retrieve_all_job_ids()
            # TODO: check for duplicates and mismatching datetimes
            all_records.update(all_record_ids)

        all_ids_sorted = [
            uuid
            for uuid, _ in sorted(
                all_records.items(), key=lambda item: item[1], reverse=True
            )
        ]

        return all_ids_sorted

    def retrieve_all_job_records(self) -> Mapping[uuid.UUID, JobRecord]:
        """Retrieves all job records from all job archives.

        Returns:
            a map of job-id/job-record pairs, sorted by job submission time, from latest to earliest
        """

        all_records: Dict[uuid.UUID, JobRecord] = {}
        for archive in self.job_archives.values():
            all_record_ids = archive.retrieve_all_job_ids().keys()
            for r in all_record_ids:
                if r in all_records.keys():
                    continue

                job_record = archive.retrieve_record_for_job_id(r)
                assert job_record is not None
                all_records[r] = job_record

        all_records_sorted = dict(
            sorted(
                all_records.items(),
                key=lambda item: item[1].job_submitted,
                reverse=True,
            )
        )

        return all_records_sorted

    def find_job_record_for_manifest(
        self, inputs_manifest: InputsManifest
    ) -> Union[uuid.UUID, None]:
        """
        Check if a job with same inputs manifest already ran some time before.

        Arguments:
        ---------
            inputs_manifest: the manifest incl. inputs

        Returns:
        -------
            'None' if no such job exists, a (uuid) job-id if the job is currently running or has run in the past
        """
        log = logger.bind(module_type=inputs_manifest.module_type)
        if inputs_manifest.job_hash in self._active_jobs.keys():
            log.debug("job.use_running")
            return self._active_jobs[inputs_manifest.job_hash]

        if inputs_manifest.job_hash in self._finished_jobs.keys():
            job_id = self._finished_jobs[inputs_manifest.job_hash]
            return job_id

        module = self._kiara.module_registry.create_module(manifest=inputs_manifest)
        if not module.characteristics.is_idempotent:
            log.debug(
                "skip.job_matching",
                reason="module is not idempotent",
                module_type=inputs_manifest.module_type,
            )
            return None

        job_record = self.job_matcher.find_existing_job(inputs_manifest=inputs_manifest)

        if job_record is None:
            return None

        self._finished_jobs[inputs_manifest.job_hash] = job_record.job_id
        self._archived_records[job_record.job_id] = job_record
        log.debug(
            "job.found_cached_record",
            job_id=str(job_record.job_id),
            job_hash=inputs_manifest.job_hash,
            module_type=inputs_manifest.module_type,
        )

        return job_record.job_id

    def prepare_job_config(
        self, manifest: Manifest, inputs: Mapping[str, Any]
    ) -> JobConfig:
        """Prepare a JobConfig instance from the manifest and inputs.

        This involves creating (and therefor validating) a module instance, as well as making sure the inputs are valid.
        """

        module = self._kiara.module_registry.create_module(manifest=manifest)

        job_config = JobConfig.create_from_module(
            data_registry=self._kiara.data_registry, module=module, inputs=inputs
        )

        return job_config

    def execute(
        self,
        manifest: Manifest,
        inputs: Mapping[str, Any],
        wait: bool = False,
    ) -> uuid.UUID:
        """Prepare a job config, then execute it."""

        job_config = self.prepare_job_config(manifest=manifest, inputs=inputs)
        return self.execute_job(job_config, wait=wait)

    def execute_job(
        self, job_config: JobConfig, wait: bool = False, auto_save_result=False
    ) -> uuid.UUID:
        """Execute the job specified by the job config.

        Arguments:
            job_config: the job config
            wait: whether to wait for the job to finish
            auto_save_result: whether to automatically save the job's outputs to the data registry once the job finished successfully
        """

        # from kiara.models.metadata import CommentMetadata
        # if "comment" not in job_metadata.keys():
        #     raise KiaraException("You need to provide a 'comment' for the job.")
        #
        # comment = job_metadata.get("comment")
        # if not isinstance(comment, str):
        #     raise KiaraException("The 'comment' must be a string.")
        #
        # comment_metadata = CommentMetadata(comment=comment)
        # self.context.metadata_registry.register_metadata_item(
        #     key="comment", item=comment_metadata, force=False, store=None
        # )

        if job_config.module_type != "pipeline":
            log = logger.bind(
                module_type=job_config.module_type,
                module_config=job_config.module_config,
                inputs={k: str(v) for k, v in job_config.inputs.items()},
                job_hash=job_config.job_hash,
            )
        else:
            pipeline_name = job_config.module_config.get("pipeline_name", "n/a")
            log = logger.bind(
                module_type=job_config.module_type,
                pipeline_name=pipeline_name,
                inputs={k: str(v) for k, v in job_config.inputs.items()},
                job_hash=job_config.job_hash,
            )

        stored_job = self.find_job_record_for_manifest(inputs_manifest=job_config)

        is_pipeline_step = False if job_config.pipeline_metadata is None else True
        if is_pipeline_step:
            pipeline_step_id: Union[None, str] = job_config.pipeline_metadata.step_id  # type: ignore
            pipeline_id: Union[None, uuid.UUID] = job_config.pipeline_metadata.pipeline_id  # type: ignore
        else:
            pipeline_step_id = None
            pipeline_id = None

        if stored_job is not None:
            log.debug(
                "job.use_cached",
                job_id=str(stored_job),
                module_type=job_config.module_type,
            )
            if is_develop():

                module = self._kiara.module_registry.create_module(manifest=job_config)
                if is_pipeline_step:
                    title = f"Using cached pipeline step: {pipeline_step_id}"
                else:
                    title = f"Using cached job for: {module.module_type_name}"

                from kiara.utils.debug import create_module_preparation_table
                from kiara.utils.develop import log_dev_message

                stored_job_record = self.get_job_record(stored_job)
                if stored_job_record is None:
                    raise Exception(
                        f"Can't retrieve job record for job with id '{stored_job}'."
                    )

                table = create_module_preparation_table(
                    kiara=self._kiara,
                    job_config=job_config,
                    job_id=stored_job_record.job_id,
                    module=module,
                )
                include = ["job_hash", "inputs_id_hash", "input_ids_hash", "outputs"]
                table_job_record = stored_job_record.create_renderable(include=include)
                panel = Group(table, table_job_record)
                log_dev_message(panel, title=title)

            # TODO: in this case, and if 'auto_save_result' is True, we should also verify the outputs are saved?

            return stored_job

        dbg_data = {
            "module_type": job_config.module_type,
            "is_pipeline_step": is_pipeline_step,
        }
        if is_pipeline_step:
            dbg_data["step_id"] = pipeline_step_id
            dbg_data["pipeline_id"] = str(pipeline_id)

        log.debug("job.execute", **dbg_data)

        job_id = self._processor.create_job(
            job_config=job_config, auto_save_result=auto_save_result
        )
        self._active_jobs[job_config.job_hash] = job_id

        try:
            self._processor.queue_job(job_id=job_id)
        except Exception as e:
            log.error("error.queue_job", job_id=job_id)
            raise e

        if wait:
            self._processor.wait_for(job_id)

        return job_id

    def get_active_job(self, job_id: uuid.UUID) -> ActiveJob:

        if job_id in self._active_jobs.keys() or job_id in self._failed_jobs.keys():
            return self._processor.get_job(job_id)
        else:
            if job_id in self._archived_records.keys():
                raise Exception(
                    f"Can't retrieve active job with id '{job_id}': job is archived."
                )
            elif job_id in self._processor._failed_jobs.keys():
                job = self._processor.get_job(job_id)
                msg = job.error
                if not msg and job._exception:
                    msg = str(job._exception)
                    if not msg:
                        msg = repr(job._exception)
                raise FailedJobException(job=job, msg=msg, parent=job._exception)
            else:
                raise Exception(f"Can't retrieve job with id '{job_id}': no such job.")

    def get_job(self, job_id: uuid.UUID) -> ActiveJob:
        return self._processor.get_job(job_id=job_id)

    def get_job_status(self, job_id: uuid.UUID) -> JobStatus:

        if job_id in self._archived_records.keys():
            return JobStatus.SUCCESS
        elif job_id in self._failed_jobs.values():
            return JobStatus.FAILED

        return self._processor.get_job_status(job_id=job_id)

    def wait_for(self, *job_id: uuid.UUID):
        not_finished = (j for j in job_id if j not in self._archived_records.keys())
        if not_finished:
            self._processor.wait_for(*not_finished)

    def retrieve_result(self, job_id: uuid.UUID) -> ValueMapReadOnly:

        if job_id not in self._archived_records.keys():
            self._processor.wait_for(job_id)

        if job_id in self._archived_records.keys():
            job_record = self._archived_records[job_id]
            results = self._kiara.data_registry.load_values(job_record.outputs)
            return results
        elif job_id in self._failed_jobs.values():
            j = self._processor.get_job(job_id=job_id)
            exception = FailedJobException(job=j, parent=j._exception)
            raise exception
        else:
            raise Exception(f"Could not find job with id: {job_id}")

    def execute_and_retrieve(
        self, manifest: Manifest, inputs: Mapping[str, Any]
    ) -> ValueMap:

        job_id = self.execute(manifest=manifest, inputs=inputs, wait=True)
        results = self.retrieve_result(job_id=job_id)
        return results


# kiara\kiara\src\kiara\registries\jobs\job_store\filesystem_store.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)
import datetime
import shutil
import uuid
from pathlib import Path
from typing import Any, Dict, Generator, Iterable, Mapping, Union

import orjson
import structlog

from kiara.models.module.jobs import JobMatcher, JobRecord
from kiara.registries import ArchiveDetails, FileSystemArchiveConfig
from kiara.registries.jobs import MANIFEST_SUB_PATH, JobArchive, JobStore
from kiara.utils.windows import fix_windows_longpath

log = structlog.getLogger()


class FileSystemJobArchive(JobArchive):

    _archive_type_name = "filesystem_job_archive"
    _config_cls = FileSystemArchiveConfig  # type: ignore

    @classmethod
    def supported_item_types(cls) -> Iterable[str]:
        return ["job_record"]

    def __init__(
        self,
        archive_name: str,
        archive_config: FileSystemArchiveConfig,
        force_read_only: bool = False,
    ):

        super().__init__(
            archive_name=archive_name,
            archive_config=archive_config,
            force_read_only=force_read_only,
        )
        self._base_path: Union[Path, None] = None

    def get_archive_details(self) -> ArchiveDetails:

        size = sum(
            f.stat().st_size for f in self.job_store_path.glob("**/*") if f.is_file()
        )
        return ArchiveDetails(root={"size": size})

    def _retrieve_archive_metadata(self) -> Mapping[str, Any]:

        if not self.archive_metadata_path.is_file():
            _archive_metadata = {}
        else:
            _archive_metadata = orjson.loads(self.archive_metadata_path.read_bytes())

        archive_id = _archive_metadata.get("archive_id", None)
        if not archive_id:
            try:
                _archive_id = uuid.UUID(
                    self.job_store_path.name
                )  # just to check if it's a valid uuid
                _archive_metadata["archive_id"] = str(_archive_id)
            except Exception:
                raise Exception(
                    f"Could not retrieve archive id for alias archive '{self.archive_name}'."
                )

        return _archive_metadata

    @property
    def archive_metadata_path(self) -> Path:
        return self.job_store_path / "store_metadata.json"

    @property
    def job_store_path(self) -> Path:

        if self._base_path is not None:
            return self._base_path

        self._base_path = Path(self.config.archive_path).absolute()  # type: ignore
        self._base_path = fix_windows_longpath(self._base_path)
        self._base_path.mkdir(parents=True, exist_ok=True)
        return self._base_path

    def _delete_archive(self) -> None:

        shutil.rmtree(self.job_store_path)

    def retrieve_all_job_hashes(
        self,
        manifest_hash: Union[str, None] = None,
        inputs_id_hash: Union[str, None] = None,
        inputs_data_hash: Union[str, None] = None,
    ) -> Iterable[str]:

        base_path = self.job_store_path / MANIFEST_SUB_PATH
        if not manifest_hash:
            if not inputs_id_hash:
                records = base_path.glob("*/*/*.job_record")
            else:
                records = base_path.glob(f"*/{inputs_id_hash}/*.job_record")
        else:
            if not inputs_id_hash:
                records = base_path.glob(f"{manifest_hash}/*/*.job_record")
            else:
                records = base_path.glob(
                    f"{manifest_hash}/{inputs_id_hash}/*.job_record"
                )

        result = []
        for record in records:
            result.append(record.name[0:-11])
        return result

    def _retrieve_record_for_job_hash(self, job_hash: str) -> Union[JobRecord, None]:

        base_path = self.job_store_path / MANIFEST_SUB_PATH
        records = list(base_path.glob(f"*/*/{job_hash}.job_record"))

        if not records:
            return None

        assert len(records) == 1
        details_file = records[0]

        details_content = details_file.read_text()
        details: Dict[str, Any] = orjson.loads(details_content)

        job_record = JobRecord(**details)
        job_record._is_stored = True
        return job_record

    def _retrieve_all_job_ids(self) -> Mapping[uuid.UUID, datetime.datetime]:

        raise NotImplementedError()

    def _retrieve_matching_job_records(
        self, matcher: JobMatcher
    ) -> Generator[JobRecord, None, None]:

        raise NotImplementedError()

    def _retrieve_record_for_job_id(self, job_id: uuid.UUID) -> Union[JobRecord, None]:

        raise NotImplementedError()

    # def find_matching_job_record(
    #     self, inputs_manifest: InputsManifest
    # ) -> Optional[JobRecord]:
    #
    #     manifest_hash = inputs_manifest.manifest_cid
    #     jobs_hash = inputs_manifest.job_hash
    #
    #     base_path = self.job_store_path / MANIFEST_SUB_PATH
    #     manifest_folder = base_path / str(manifest_hash)
    #
    #     if not manifest_folder.exists():
    #         return None
    #
    #     manifest_file = manifest_folder / "manifest.json"
    #
    #     if not manifest_file.exists():
    #         raise Exception(
    #             f"No 'manifests.json' file for manifest with hash: {manifest_hash}"
    #         )
    #
    #     details_folder = manifest_folder / str(jobs_hash)
    #     if not details_folder.exists():
    #         return None
    #
    #     details_file_name = details_folder / "details.json"
    #     if not details_file_name.exists():
    #         raise Exception(
    #             f"No 'inputs.json' file for manifest/inputs hash-combo: {manifest_hash} / {jobs_hash}"
    #         )
    #
    #     details_content = details_file_name.read_text()
    #     details: Dict[str, Any] = orjson.loads(details_content)
    #
    #     job_record = JobRecord(**details)
    #     job_record._is_stored = True
    #     return job_record


class FileSystemJobStore(FileSystemJobArchive, JobStore):

    _archive_type_name = "filesystem_job_store"

    def store_job_record(self, job_record: JobRecord):

        manifest_cid = job_record.manifest_cid

        input_ids_hash = job_record.input_ids_hash

        base_path = self.job_store_path / MANIFEST_SUB_PATH
        manifest_folder = base_path / str(manifest_cid)

        manifest_folder.mkdir(parents=True, exist_ok=True)

        manifest_info_file = manifest_folder / "manifest.json"
        if not manifest_info_file.exists():
            manifest_info_file.write_text(job_record.manifest_data_as_json())

        job_folder = manifest_folder / input_ids_hash
        job_folder = fix_windows_longpath(job_folder)
        job_folder.mkdir(parents=True, exist_ok=True)

        job_details_file = job_folder / f"{job_record.job_hash}.job_record"
        job_details_file = fix_windows_longpath(job_details_file)

        exists = False
        if job_details_file.exists():
            exists = True
            # TODO: check details match? or overwrite
            file_m_time = datetime.datetime.fromtimestamp(
                job_details_file.stat().st_mtime
            ).timestamp()
            archive = job_folder / ".archive"
            archive.mkdir(parents=True, exist_ok=True)
            backup = archive / f"{job_details_file.name}.{file_m_time}"
            log.debug(
                "overwrite.store_job_record",
                reason="job record already exists",
                job_hash=job_record.job_hash,
                new_path=backup.as_posix(),
            )
            shutil.move(job_details_file.as_posix(), backup)

        job_details_file.write_text(job_record.model_dump_json())

        for output_name, output_v_id in job_record.outputs.items():

            outputs_file_name = (
                job_folder / f"output__{output_name}__value_id__{output_v_id}.json"
            )

            if outputs_file_name.exists() and not exists:
                # if value.pedigree_output_name == "__void__":
                #     return
                # else:
                raise Exception(f"Can't write value '{output_v_id}': already exists.")
            else:
                outputs_file_name.touch()


# kiara\kiara\src\kiara\registries\jobs\job_store\sqlite_store.py
# -*- coding: utf-8 -*-
import uuid
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, Generator, Iterable, Mapping, Union

import orjson
from sqlalchemy import text
from sqlalchemy.engine import Engine

from kiara.defaults import (
    REQUIRED_TABLES_JOB_ARCHIVE,
    TABLE_NAME_ARCHIVE_METADATA,
    TABLE_NAME_JOB_RECORDS,
)
from kiara.models.module.jobs import JobMatcher, JobRecord
from kiara.registries import ArchiveDetails, SqliteArchiveConfig
from kiara.registries.jobs import JobArchive, JobStore
from kiara.utils.db import create_archive_engine, delete_archive_db


class SqliteJobArchive(JobArchive):

    _archive_type_name = "sqlite_job_archive"
    _config_cls = SqliteArchiveConfig

    @classmethod
    def _load_archive_config(
        cls, archive_uri: str, allow_write_access: bool, **kwargs
    ) -> Union[Dict[str, Any], None]:

        if allow_write_access:
            return None

        if not Path(archive_uri).is_file():
            return None

        import sqlite3

        con = sqlite3.connect(archive_uri)

        cursor = con.cursor()
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
        tables = {x[0] for x in cursor.fetchall()}
        con.close()

        required_tables = REQUIRED_TABLES_JOB_ARCHIVE

        if not required_tables.issubset(tables):
            return None

        # config = SqliteArchiveConfig(sqlite_db_path=store_uri)
        return {"sqlite_db_path": archive_uri}

    def __init__(
        self,
        archive_name: str,
        archive_config: SqliteArchiveConfig,
        force_read_only: bool = False,
    ):

        super().__init__(
            archive_name=archive_name,
            archive_config=archive_config,
            force_read_only=force_read_only,
        )
        self._db_path: Union[Path, None] = None
        self._cached_engine: Union[Engine, None] = None
        self._use_wal_mode: bool = archive_config.use_wal_mode
        # self._lock: bool = True

    # def _retrieve_archive_id(self) -> uuid.UUID:
    #     sql = text("SELECT value FROM archive_metadata WHERE key='archive_id'")
    #
    #     with self.sqlite_engine.connect() as connection:
    #         result = connection.execute(sql)
    #         row = result.fetchone()
    #         if row is None:
    #             raise Exception("No archive ID found in metadata")
    #         return uuid.UUID(row[0])

    def _retrieve_archive_metadata(self) -> Mapping[str, Any]:

        sql = text(f"SELECT key, value FROM {TABLE_NAME_ARCHIVE_METADATA}")

        with self.sqlite_engine.connect() as connection:
            result = connection.execute(sql)
            return {row[0]: row[1] for row in result}

    @property
    def sqlite_path(self):

        if self._db_path is not None:
            return self._db_path

        db_path = Path(self.config.sqlite_db_path).resolve()
        # self._db_path = fix_windows_longpath(db_path)
        self._db_path = db_path

        if self._db_path.exists():
            return self._db_path

        self._db_path.parent.mkdir(parents=True, exist_ok=True)
        return self._db_path

    # @property
    # def db_url(self) -> str:
    #     return f"sqlite:///{self.sqlite_path}"

    @property
    def sqlite_engine(self) -> "Engine":

        if self._cached_engine is not None:
            return self._cached_engine

        self._cached_engine = create_archive_engine(
            db_path=self.sqlite_path,
            force_read_only=self.is_force_read_only(),
            use_wal_mode=self._use_wal_mode,
        )

        create_table_sql = f"""
CREATE TABLE IF NOT EXISTS {TABLE_NAME_JOB_RECORDS} (
    job_id TEXT PRIMARY KEY,
    job_hash TEXT TEXT NOT NULL,
    job_submitted TEXT NOT NULL,
    manifest_hash TEXT NOT NULL,
    input_ids_hash TEXT NOT NULL,
    inputs_data_hash TEXT NOT NULL,
    job_metadata TEXT NOT NULL
);
"""

        with self._cached_engine.begin() as connection:
            for statement in create_table_sql.split(";"):
                if statement.strip():
                    connection.execute(text(statement))

        # if self._lock:
        #     event.listen(self._cached_engine, "connect", _pragma_on_connect)
        return self._cached_engine

    def _retrieve_record_for_job_hash(self, job_hash: str) -> Union[JobRecord, None]:

        sql = text(
            f"SELECT job_metadata FROM {TABLE_NAME_JOB_RECORDS} WHERE job_hash = :job_hash"
        )
        params = {"job_hash": job_hash}

        with self.sqlite_engine.connect() as connection:
            result = connection.execute(sql, params)
            row = result.fetchone()
            if not row:
                return None

            job_record_json = row[0]
            job_record_data = orjson.loads(job_record_json)
            job_record = JobRecord(**job_record_data)
            return job_record

    def _retrieve_all_job_ids(self) -> Mapping[uuid.UUID, datetime]:
        """
        Retrieve a list of all job record ids in the archive.
        """

        sql = text(
            f"SELECT job_id, job_submitted FROM {TABLE_NAME_JOB_RECORDS} ORDER BY job_submitted DESC;"
        )

        with self.sqlite_engine.connect() as connection:
            result = connection.execute(sql)
            return {uuid.UUID(row[0]): datetime.fromisoformat(row[1]) for row in result}

    def _retrieve_record_for_job_id(self, job_id: uuid.UUID) -> Union[JobRecord, None]:

        sql = text(
            f"SELECT job_metadata FROM {TABLE_NAME_JOB_RECORDS} WHERE job_id = :job_id"
        )

        params = {"job_id": str(job_id)}

        with self.sqlite_engine.connect() as connection:
            result = connection.execute(sql, params)
            row = result.fetchone()
            if not row:
                return None

            job_record_json = row[0]
            job_record_data = orjson.loads(job_record_json)
            job_record = JobRecord(**job_record_data)
            return job_record

    def _retrieve_matching_job_records(
        self, matcher: JobMatcher
    ) -> Generator[JobRecord, None, None]:

        query_conditions = []
        params: Dict[str, Any] = {}
        if matcher.job_ids:
            query_conditions.append("job_id IN :job_ids")
            params["job_ids"] = (str(x) for x in matcher.job_ids)

        if not matcher.allow_internal:
            cond = "json_extract(job_metadata, '$.is_internal') = 0"
            query_conditions.append(cond)

        if matcher.earliest:
            cond = "job_submitted >= :earliest"
            query_conditions.append(cond)
            params["earliest"] = matcher.earliest.isoformat()

        if matcher.latest:
            cond = "job_submitted <= :latest"
            query_conditions.append(cond)
            params["latest"] = matcher.latest.isoformat()

        if matcher.operation_inputs:
            raise NotImplementedError(
                "Job matcher 'operation_inputs' not implemented yet"
            )

        if matcher.produced_outputs:
            raise NotImplementedError(
                "Job matcher 'produced_outputs' not implemented yet"
            )

        sql_query = f"SELECT job_id, job_metadata FROM {TABLE_NAME_JOB_RECORDS}"
        if query_conditions:
            sql_query += " WHERE "

            for query_cond in query_conditions:
                sql_query += "( " + query_cond + " ) AND "

            sql_query = sql_query[:-5] + ";"

        sql = text(sql_query)

        with self.sqlite_engine.connect() as connection:
            result = connection.execute(sql, params)
            for row in result:
                # job_id = uuid.UUID(row[0])
                job_metadata = orjson.loads(row[1])
                job_record = JobRecord(**job_metadata)
                yield job_record

        return

    def retrieve_all_job_hashes(
        self,
        manifest_hash: Union[str, None] = None,
        inputs_id_hash: Union[str, None] = None,
        inputs_data_hash: Union[str, None] = None,
    ) -> Iterable[str]:

        if not manifest_hash:
            if not inputs_id_hash:
                sql = text(f"SELECT job_hash FROM {TABLE_NAME_JOB_RECORDS}")
                params = {}
            else:
                sql = text(
                    f"SELECT job_hash FROM {TABLE_NAME_JOB_RECORDS} WHERE inputs_hash = :inputs_hash"
                )
                params = {"inputs_hash": inputs_id_hash}
        else:
            if not inputs_id_hash:
                sql = text(
                    f"SELECT job_hash FROM {TABLE_NAME_JOB_RECORDS} WHERE manifest_hash = :manifest_hash"
                )
                params = {"manifest_hash": manifest_hash}
            else:
                sql = text(
                    f"SELECT job_hash FROM {TABLE_NAME_JOB_RECORDS} WHERE manifest_hash = :manifest_hash AND inputs_hash = :inputs_hash"
                )
                params = {"manifest_hash": manifest_hash, "inputs_hash": inputs_id_hash}

        with self.sqlite_engine.connect() as connection:
            result = connection.execute(sql, params)
            return {row[0] for row in result}

    def _delete_archive(self):

        delete_archive_db(db_path=self.sqlite_path)

    def get_archive_details(self) -> ArchiveDetails:

        all_job_records_sql = text(f"SELECT COUNT(*) FROM {TABLE_NAME_JOB_RECORDS}")

        with self.sqlite_engine.connect() as connection:
            result = connection.execute(all_job_records_sql)
            job_count = result.fetchone()[0]

            details = {"no_job_records": job_count, "dynamic_archive": False}
            return ArchiveDetails(**details)


class SqliteJobStore(SqliteJobArchive, JobStore):

    _archive_type_name = "sqlite_job_store"

    @classmethod
    def _load_archive_config(
        cls, archive_uri: str, allow_write_access: bool, **kwargs
    ) -> Union[Dict[str, Any], None]:

        if not allow_write_access:
            return None

        if not Path(archive_uri).is_file():
            return None

        import sqlite3

        con = sqlite3.connect(archive_uri)

        cursor = con.cursor()
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
        tables = {x[0] for x in cursor.fetchall()}
        con.close()

        required_tables = REQUIRED_TABLES_JOB_ARCHIVE

        if not required_tables.issubset(tables):
            return None

        # config = SqliteArchiveConfig(sqlite_db_path=store_uri)
        return {"sqlite_db_path": archive_uri}

    def store_job_record(self, job_record: JobRecord):

        job_hash = job_record.job_hash
        manifest_hash = job_record.manifest_hash
        input_ids_hash = job_record.input_ids_hash
        inputs_data_hash = job_record.inputs_data_hash

        job_record_json = job_record.model_dump_json()

        job_submitted = job_record.job_submitted.isoformat()

        sql = text(
            f"INSERT OR IGNORE INTO {TABLE_NAME_JOB_RECORDS}(job_id, job_submitted, job_hash, manifest_hash, input_ids_hash, inputs_data_hash, job_metadata) VALUES (:job_id, :job_submitted, :job_hash, :manifest_hash, :input_ids_hash, :inputs_data_hash, :job_metadata)"
        )
        params = {
            "job_id": str(job_record.job_id),
            "job_submitted": job_submitted,
            "job_hash": job_hash,
            "manifest_hash": manifest_hash,
            "input_ids_hash": input_ids_hash,
            "inputs_data_hash": inputs_data_hash,
            "job_metadata": job_record_json,
        }

        with self.sqlite_engine.connect() as connection:
            connection.execute(sql, params)

            connection.commit()

    def _set_archive_metadata_value(self, key: str, value: Any):
        """Set custom metadata for the archive."""

        sql = text(
            f"INSERT OR REPLACE INTO {TABLE_NAME_ARCHIVE_METADATA} (key, value) VALUES (:key, :value)"
        )
        with self.sqlite_engine.connect() as conn:
            params = {"key": key, "value": value}
            conn.execute(sql, params)
            conn.commit()


# kiara\kiara\src\kiara\registries\jobs\job_store\__init__.py
# -*- coding: utf-8 -*-
import abc
import uuid
from datetime import datetime
from typing import Generator, Iterable, Mapping, Union

from kiara.models.module.jobs import JobMatcher, JobRecord
from kiara.registries import BaseArchive


class JobArchive(BaseArchive):
    # @abc.abstractmethod
    # def find_matching_job_record(
    #     self, inputs_manifest: InputsManifest
    # ) -> Optional[JobRecord]:
    #     pass

    @classmethod
    def supported_item_types(cls) -> Iterable[str]:
        return ["job_record"]

    @abc.abstractmethod
    def retrieve_all_job_hashes(
        self,
        manifest_hash: Union[str, None] = None,
        inputs_id_hash: Union[str, None] = None,
        inputs_data_hash: Union[str, None] = None,
    ) -> Iterable[str]:
        """
        Retrieve a list of all job record hashes (cids) that match the given filter arguments.

        A job record hash includes information about the module type used in the job, the module configuration, as well as input field names and value ids for the values used in those inputs.

        If the job archive retrieves its jobs in a dynamic way, this will return 'None'.
        """

    @abc.abstractmethod
    def _retrieve_all_job_ids(self) -> Mapping[uuid.UUID, datetime]:
        """
        Retrieve a list of all job record ids in the archive, along with when they where submitted.
        """

    def retrieve_all_job_ids(self) -> Mapping[uuid.UUID, datetime]:
        """Retrieve a list of all job ids in the archive, along with when they where submitted."""
        return self._retrieve_all_job_ids()

    @abc.abstractmethod
    def _retrieve_record_for_job_id(self, job_id: uuid.UUID) -> Union[JobRecord, None]:
        pass

    def retrieve_record_for_job_id(self, job_id: uuid.UUID) -> Union[JobRecord, None]:
        job_record = self._retrieve_record_for_job_id(job_id=job_id)
        return job_record

    @abc.abstractmethod
    def _retrieve_record_for_job_hash(self, job_hash: str) -> Union[JobRecord, None]:
        pass

    def retrieve_record_for_job_hash(self, job_hash: str) -> Union[JobRecord, None]:

        job_record = self._retrieve_record_for_job_hash(job_hash=job_hash)
        return job_record

    def retrieve_matching_job_records(
        self, matcher: JobMatcher
    ) -> Generator[JobRecord, None, None]:
        return self._retrieve_matching_job_records(matcher=matcher)

    @abc.abstractmethod
    def _retrieve_matching_job_records(
        self, matcher: JobMatcher
    ) -> Generator[JobRecord, None, None]:
        pass


class JobStore(JobArchive):
    @classmethod
    def _is_writeable(cls) -> bool:
        return True

    @abc.abstractmethod
    def store_job_record(self, job_record: JobRecord):
        pass


# kiara\kiara\src\kiara\registries\metadata\__init__.py
# -*- coding: utf-8 -*-
import uuid
from typing import (
    TYPE_CHECKING,
    Any,
    Callable,
    Dict,
    Generator,
    List,
    Literal,
    Mapping,
    Tuple,
    Union,
)

from pydantic import Field, field_validator

from kiara.defaults import (
    DEFAULT_DATA_STORE_MARKER,
    DEFAULT_METADATA_STORE_MARKER,
    DEFAULT_STORE_MARKER,
)
from kiara.exceptions import KiaraException
from kiara.models import KiaraModel
from kiara.models.events import RegistryEvent
from kiara.models.metadata import CommentMetadata, KiaraMetadata
from kiara.registries.metadata.metadata_store import MetadataArchive, MetadataStore

if TYPE_CHECKING:
    from kiara.context import Kiara
    from kiara.models.runtime_environment import RuntimeEnvironment


class MetadataMatcher(KiaraModel):
    """An object describing requirements metadata items should satisfy in order to be included in a query result."""

    @classmethod
    def create_matcher(cls, **match_options: Any):
        m = MetadataMatcher(**match_options)
        return m

    # metadata_item_keys: Union[None, List[str]] = Field(
    #     description="The metadata item key to match (if provided).", default=None
    # )
    reference_item_types: Union[None, List[str]] = Field(
        description="A 'reference_item_type' a metadata item is referenced from.",
        default=None,
    )
    reference_item_keys: Union[None, List[str]] = Field(
        description="A 'reference_item_key' a metadata item is referenced from.",
        default=None,
    )
    reference_item_ids: Union[None, List[str]] = Field(
        description="An list of ids that a metadata item is referenced from.",
        default=None,
    )

    @field_validator(
        "reference_item_types",
        "reference_item_keys",
        "reference_item_ids",
        mode="before",
    )
    @classmethod
    def validate_reference_item_ids(cls, v):

        if v is None:
            return None
        elif isinstance(v, str):
            return [v]
        elif isinstance(v, uuid.UUID):
            return [str(v)]
        else:
            v = set(v)
            result = [str(x) for x in v]
            return result


class MetadataArchiveAddedEvent(RegistryEvent):

    event_type: Literal["metadata_archive_added"] = "metadata_archive_added"
    metadata_archive_id: uuid.UUID = Field(
        description="The unique id of this metadata archive."
    )
    metadata_archive_alias: str = Field(
        description="The alias this metadata archive was added as."
    )
    is_store: bool = Field(
        description="Whether this archive supports write operations (aka implements the 'MetadataStore' interface)."
    )
    is_default_store: bool = Field(
        description="Whether this store acts as default store."
    )


class MetadataRegistry(object):
    def __init__(self, kiara: "Kiara"):

        self._kiara: Kiara = kiara
        self._event_callback: Callable = self._kiara.event_registry.add_producer(self)

        self._metadata_archives: Dict[str, MetadataArchive] = {}
        self._default_metadata_store: Union[str, None] = None

        # self._env_registry: EnvironmentRegistry = self._kiara.environment_registry

    @property
    def kiara_id(self) -> uuid.UUID:
        return self._kiara.id

    def register_metadata_archive(
        self,
        archive: MetadataArchive,
        set_as_default_store: Union[bool, None] = None,
    ) -> str:

        alias = archive.archive_name

        if not alias:
            raise Exception("Invalid data archive alias: can't be empty.")

        if alias in self._metadata_archives.keys():
            raise Exception(
                f"Can't add data archive, alias '{alias}' already registered."
            )

        archive.register_archive(kiara=self._kiara)

        self._metadata_archives[alias] = archive
        is_store = False
        is_default_store = False
        if isinstance(archive, MetadataStore):
            is_store = True

            if set_as_default_store and self._default_metadata_store is not None:
                raise Exception(
                    f"Can't set data store '{alias}' as default store: default store already set."
                )

            if self._default_metadata_store is None or set_as_default_store:
                is_default_store = True
                self._default_metadata_store = alias

        event = MetadataArchiveAddedEvent(
            kiara_id=self._kiara.id,
            metadata_archive_id=archive.archive_id,
            metadata_archive_alias=alias,
            is_store=is_store,
            is_default_store=is_default_store,
        )
        self._event_callback(event)

        return alias

    @property
    def default_metadata_store(self) -> str:
        if self._default_metadata_store is None:
            raise Exception("No default metadata store set.")
        return self._default_metadata_store

    @property
    def metadata_archives(self) -> Mapping[str, MetadataArchive]:
        return self._metadata_archives

    def get_archive(
        self, archive_id_or_alias: Union[None, uuid.UUID, str] = None
    ) -> MetadataArchive:

        if archive_id_or_alias in (
            None,
            DEFAULT_STORE_MARKER,
            DEFAULT_METADATA_STORE_MARKER,
            DEFAULT_DATA_STORE_MARKER,
        ):
            archive_id_or_alias = self.default_metadata_store
            if archive_id_or_alias is None:
                raise Exception(
                    "Can't retrieve default metadata archive, none set (yet)."
                )

        if isinstance(archive_id_or_alias, uuid.UUID):
            for archive in self._metadata_archives.values():
                if archive.archive_id == archive_id_or_alias:
                    return archive

            raise Exception(
                f"Can't retrieve metadata archive with id '{archive_id_or_alias}': no archive with that id registered."
            )

        if archive_id_or_alias in self._metadata_archives.keys():
            return self._metadata_archives[archive_id_or_alias]
        else:
            try:
                _archive_id = uuid.UUID(archive_id_or_alias)
                for archive in self._metadata_archives.values():
                    if archive.archive_id == _archive_id:
                        return archive
                    raise Exception(
                        f"Can't retrieve archive with id '{archive_id_or_alias}': no archive with that id registered."
                    )
            except Exception:
                pass

        raise Exception(
            f"Can't retrieve archive with id '{archive_id_or_alias}': no archive with that id registered."
        )

    def find_metadata_items(
        self, matcher: MetadataMatcher
    ) -> Generator[Tuple[Any, ...], None, None]:

        mounted_store: MetadataArchive = self.get_archive()

        return mounted_store.find_matching_metadata_items(matcher=matcher)

    def retrieve_environment_item(self, env_cid: str) -> "RuntimeEnvironment":

        if self._kiara.environment_registry.has_environment(env_cid):
            environment = self._kiara.environment_registry.get_environment_for_cid(
                env_cid
            )
        else:
            _environment = self.retrieve_metadata_item_with_hash(item_hash=env_cid)
            if _environment is None:
                raise KiaraException(
                    f"No environment with id '{env_cid}' available in metadata store."
                )

            from kiara.models.runtime_environment import RuntimeEnvironment

            if isinstance(_environment, RuntimeEnvironment):
                environment = _environment
            else:
                raise KiaraException(
                    f"Invalid environment item with id '{env_cid}' available in metadata store."
                )

        return environment

    def retrieve_metadata_item_with_hash(
        self, item_hash: str, store: Union[str, uuid.UUID, None] = None
    ) -> Union[KiaraMetadata, None]:
        """Retrieves a metadata item by its hash."""

        if store:
            mounted_archive: MetadataStore = self.get_archive(archive_id_or_alias=store)  # type: ignore
            result = mounted_archive.find_metadata_item_with_hash(item_hash=item_hash)
        else:
            mounted_archive: MetadataStore = self.get_archive(archive_id_or_alias=store)  # type: ignore
            result = mounted_archive.find_metadata_item_with_hash(item_hash=item_hash)
            if not result:
                for archive in self.metadata_archives.values():

                    result = archive.find_metadata_item_with_hash(item_hash=item_hash)
                    if result:
                        break

        if result is None:
            return None

        model_type_id, data = result
        model_cls = self._kiara.kiara_model_registry.get_model_cls(
            kiara_model_id=model_type_id, required_subclass=KiaraMetadata
        )

        model_instance = model_cls(**data)
        return model_instance  # type: ignore

    def retrieve_metadata_item(
        self,
        key: str,
        reference_item_type: Union[str, None] = None,
        reference_item_key: Union[str, None] = None,
        reference_item_id: Union[str, None] = None,
        store: Union[str, uuid.UUID, None] = None,
    ) -> Union[KiaraMetadata, None]:
        """Retrieves a metadata item."""

        if store:
            mounted_store: MetadataStore = self.get_archive(archive_id_or_alias=store)  # type: ignore
            result = mounted_store.retrieve_metadata_item(
                metadata_item_key=key,
                reference_type=reference_item_type,
                reference_key=reference_item_key,
                reference_id=reference_item_id,
            )
        else:
            mounted_store: MetadataStore = self.get_archive(archive_id_or_alias=store)  # type: ignore
            result = mounted_store.retrieve_metadata_item(
                metadata_item_key=key,
                reference_type=reference_item_type,
                reference_key=reference_item_key,
                reference_id=reference_item_id,
            )
            if not result:

                for archive in self.metadata_archives.values():
                    result = archive.retrieve_metadata_item(
                        metadata_item_key=key,
                        reference_type=reference_item_type,
                        reference_key=reference_item_key,
                        reference_id=reference_item_id,
                    )
                    if result:
                        break

        if result is None:
            return None

        model_type_id, data = result
        model_cls = self._kiara.kiara_model_registry.get_model_cls(
            kiara_model_id=model_type_id, required_subclass=KiaraMetadata
        )

        model_instance = model_cls(**data)
        return model_instance  # type: ignore

    def register_metadata_item(
        self,
        key: str,
        item: KiaraMetadata,
        reference_item_type: Union[str, None] = None,
        reference_item_key: Union[str, None] = None,
        reference_item_id: Union[str, None] = None,
        replace_existing_references: bool = False,
        allow_multiple_references: bool = False,
        store: Union[str, uuid.UUID, None] = None,
    ) -> uuid.UUID:

        mounted_store: MetadataStore = self.get_archive(archive_id_or_alias=store)  # type: ignore

        result = mounted_store.store_metadata_item(
            key=key,
            item=item,
            reference_item_type=reference_item_type,
            reference_item_key=reference_item_key,
            reference_item_id=reference_item_id,
            replace_existing_references=replace_existing_references,
            allow_multiple_references=allow_multiple_references,
        )
        return result

    def register_job_metadata_items(
        self,
        job_id: uuid.UUID,
        items: Mapping[str, Any],
        store: Union[str, uuid.UUID, None] = None,
        reference_item_key: Union[str, None] = None,
        replace_existing_references: bool = True,
        allow_multiple_references: bool = False,
    ) -> None:

        for key, value in items.items():

            _reference_item_key = None
            if isinstance(value, str):
                value = CommentMetadata(comment=value)
                if not reference_item_key:
                    _reference_item_key = "comment"
                else:
                    _reference_item_key = reference_item_key
            elif isinstance(value, CommentMetadata):
                _reference_item_key = "comment"
            elif not isinstance(value, KiaraMetadata):
                raise Exception(f"Invalid metadata value for key '{key}': {value}")

            if not _reference_item_key:
                _reference_item_key = value._kiara_model_id

            self.register_metadata_item(
                key=key,
                item=value,
                reference_item_type="job",
                reference_item_key=_reference_item_key,
                reference_item_id=str(job_id),
                store=store,
                replace_existing_references=replace_existing_references,
                allow_multiple_references=allow_multiple_references,
            )

    def retrieve_job_metadata_items(self, job_id: uuid.UUID):

        pass

    def retrieve_job_metadata_item(
        self, job_id: uuid.UUID, key: str, store: Union[str, uuid.UUID, None] = None
    ) -> Union[KiaraMetadata, None]:

        return self.retrieve_metadata_item(
            key=key,
            reference_item_type="job",
            reference_item_key="comment",
            reference_item_id=str(job_id),
            store=store,
        )


# kiara\kiara\src\kiara\registries\metadata\metadata_store\sqlite_store.py
# -*- coding: utf-8 -*-
import uuid
from pathlib import Path
from typing import (
    Any,
    Dict,
    Generator,
    Iterable,
    Mapping,
    Tuple,
    Union,
)

import orjson
from sqlalchemy import text
from sqlalchemy.engine import Engine

from kiara.defaults import (
    REQUIRED_TABLES_METADATA,
    TABLE_NAME_ARCHIVE_METADATA,
    TABLE_NAME_METADATA,
    TABLE_NAME_METADATA_REFERENCES,
    TABLE_NAME_METADATA_SCHEMAS,
)
from kiara.exceptions import KiaraException
from kiara.registries import ArchiveDetails, SqliteArchiveConfig
from kiara.registries.metadata import MetadataArchive, MetadataMatcher, MetadataStore
from kiara.utils.dates import get_current_time_incl_timezone
from kiara.utils.db import create_archive_engine, delete_archive_db


class SqliteMetadataArchive(MetadataArchive):

    _archive_type_name = "sqlite_metadata_archive"
    _config_cls = SqliteArchiveConfig

    @classmethod
    def _load_archive_config(
        cls, archive_uri: str, allow_write_access: bool, **kwargs
    ) -> Union[Dict[str, Any], None]:

        if allow_write_access:
            return None

        if not Path(archive_uri).is_file():
            return None

        import sqlite3

        con = sqlite3.connect(archive_uri)

        cursor = con.cursor()
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
        tables = {x[0] for x in cursor.fetchall()}
        con.close()

        if not REQUIRED_TABLES_METADATA.issubset(tables):
            return None

        # config = SqliteArchiveConfig(sqlite_db_path=store_uri)
        return {"sqlite_db_path": archive_uri}

    def __init__(
        self,
        archive_name: str,
        archive_config: SqliteArchiveConfig,
        force_read_only: bool = False,
    ):

        super().__init__(
            archive_name=archive_name,
            archive_config=archive_config,
            force_read_only=force_read_only,
        )
        self._db_path: Union[Path, None] = None
        self._cached_engine: Union[Engine, None] = None
        self._use_wal_mode: bool = archive_config.use_wal_mode

        # self._lock: bool = True

    # def _retrieve_archive_id(self) -> uuid.UUID:
    #     sql = text("SELECT value FROM archive_metadata WHERE key='archive_id'")
    #
    #     with self.sqlite_engine.connect() as connection:
    #         result = connection.execute(sql)
    #         row = result.fetchone()
    #         if row is None:
    #             raise Exception("No archive ID found in metadata")
    #         return uuid.UUID(row[0])

    def _retrieve_archive_metadata(self) -> Mapping[str, Any]:

        sql = text(f"SELECT key, value FROM {TABLE_NAME_ARCHIVE_METADATA}")

        with self.sqlite_engine.connect() as connection:
            result = connection.execute(sql)
            return {row[0]: row[1] for row in result}

    @property
    def sqlite_path(self):

        if self._db_path is not None:
            return self._db_path

        db_path = Path(self.config.sqlite_db_path).resolve()
        # self._db_path = fix_windows_longpath(db_path)
        self._db_path = db_path

        if self._db_path.exists():
            return self._db_path

        self._db_path.parent.mkdir(parents=True, exist_ok=True)
        return self._db_path

    # @property
    # def db_url(self) -> str:
    #     return f"sqlite:///{self.sqlite_path}"

    @property
    def sqlite_engine(self) -> "Engine":

        if self._cached_engine is not None:
            return self._cached_engine

        self._cached_engine = create_archive_engine(
            db_path=self.sqlite_path,
            force_read_only=self.is_force_read_only(),
            use_wal_mode=self._use_wal_mode,
        )

        create_table_sql = f"""
CREATE TABLE IF NOT EXISTS {TABLE_NAME_METADATA_SCHEMAS} (
    model_schema_hash TEXT PRIMARY KEY,
    model_type_id TEXT NOT NULL,
    model_schema TEXT NOT NULL
);
CREATE TABLE IF NOT EXISTS {TABLE_NAME_METADATA} (
    metadata_item_id TEXT PRIMARY KEY,
    metadata_item_created TEXT NOT NULL,
    metadata_item_key TEXT NOT NULL,
    metadata_item_hash TEXT NOT NULL,
    model_type_id TEXT NOT NULL,
    model_schema_hash TEXT NOT NULL,
    metadata_value TEXT NOT NULL,
    FOREIGN KEY (model_schema_hash) REFERENCES metadata_schemas (model_schema_hash),
    UNIQUE (metadata_item_key, metadata_item_hash)
);
CREATE TABLE IF NOT EXISTS {TABLE_NAME_METADATA_REFERENCES} (
    reference_item_type TEXT NOT NULL,
    reference_item_key TEXT NOT NULL,
    reference_item_id TEXT NOT NULL,
    reference_created TEXT NOT NULL,
    metadata_item_id TEXT NOT NULL,
    FOREIGN KEY (metadata_item_id) REFERENCES metadata (metadata_item_id),
    UNIQUE (reference_item_type, reference_item_key, reference_item_id, metadata_item_id, reference_created)
);
"""

        with self._cached_engine.begin() as connection:
            for statement in create_table_sql.split(";"):
                if statement.strip():
                    connection.execute(text(statement))

        # if self._lock:
        #     event.listen(self._cached_engine, "connect", _pragma_on_connect)
        return self._cached_engine

    def _retrieve_metadata_item_with_hash(
        self, item_hash: str, key: Union[str, None] = None
    ) -> Union[Tuple[str, Mapping[str, Any]], None]:

        if not key:
            sql = text(
                f"""
                SELECT m.model_type_id, m.metadata_value
                FROM {TABLE_NAME_METADATA} m
                WHERE m.metadata_item_hash = :item_hash
            """
            )
        else:
            sql = text(
                f"""
                SELECT m.model_type_id, m.metadata_value
                FROM {TABLE_NAME_METADATA} m
                WHERE m.metadata_item_hash = :item_hash AND m.metadata_item_key = :key
            """
            )

        with self.sqlite_engine.connect() as connection:
            params = {"item_hash": item_hash}
            if key:
                params["key"] = key
            result = connection.execute(sql, params)
            row = result.fetchall()
            if not row:
                return None

            if len(row) > 1:
                msg = (
                    f"Multiple ({len(row)}) metadata items found for hash '{item_hash}'"
                )
                if key:
                    msg += f" and key '{key}'"
                msg += "."
                raise KiaraException(msg)

            data_str = row[0][1]
            data = orjson.loads(data_str)

            return (row[0][0], data)

    def _find_matching_metadata_and_ref_items(
        self,
        matcher: "MetadataMatcher",
        metadata_item_result_fields: Union[Iterable[str], None] = None,
        reference_item_result_fields: Union[Iterable[str], None] = None,
    ) -> Generator[Tuple[Any, ...], None, None]:

        # find all metadata items first

        if not metadata_item_result_fields:
            metadata_fields_str = "m.*"
        else:
            metadata_fields_str = ", ".join(
                (f"m.{x}" for x in metadata_item_result_fields)
            )

        metadata_fields_str += ", :result_type as result_type"

        sql_string = f"SELECT {metadata_fields_str} FROM {TABLE_NAME_METADATA} m "
        conditions = []
        params = {"result_type": "metadata_item"}

        ref_query = False
        if (
            matcher.reference_item_types
            or matcher.reference_item_keys
            or matcher.reference_item_ids
        ):
            ref_query = True
            sql_string += (
                "JOIN metadata_references r ON m.metadata_item_id = r.metadata_item_id"
            )

        # if matcher.metadata_item_keys:
        #     conditions.append("m.metadata_item_key in :metadata_item_keys")
        #     params["metadata_item_key"] = matcher.metadata_item_keys

        if matcher.reference_item_ids:
            assert ref_query
            in_clause = []
            for idx, item_id in enumerate(matcher.reference_item_ids):
                params[f"ri_id_{idx}"] = item_id
                in_clause.append(f":ri_id_{idx}")
            in_clause_str = ", ".join(in_clause)
            conditions.append(f"r.reference_item_id IN ({in_clause_str})")
            # params["reference_item_ids"] = tuple(matcher.reference_item_ids)

        if matcher.reference_item_types:
            assert ref_query
            in_clause = []
            for idx, item_type in enumerate(matcher.reference_item_types):
                params[f"ri_type_{idx}"] = item_type
                in_clause.append(f":ri_type_{idx}")
            in_clause_str = ", ".join(in_clause)
            conditions.append(f"r.reference_item_type IN ({in_clause_str})")
            # params["reference_item_types"] = tuple(matcher.reference_item_types)

        if matcher.reference_item_keys:
            assert ref_query
            in_clause = []
            for idx, item_key in enumerate(matcher.reference_item_keys):
                params[f"ri_key_{idx}"] = item_key
                in_clause.append(f":ri_key_{idx}")
            in_clause_str = ", ".join(in_clause)
            conditions.append(f"r.reference_item_key IN ({in_clause_str})")
            # params["reference_item_keys"] = tuple(matcher.reference_item_keys)

        if conditions:
            sql_string += " WHERE"
            for cond in conditions:
                sql_string += f" {cond} AND"

            sql_string = sql_string[:-4]
        sql = text(sql_string)

        # ... now construct the query to find the reference items (if applicable)
        if not reference_item_result_fields:
            reference_fields_str = "r.*"
        else:
            reference_fields_str = ", ".join(
                (f"r.{x}" for x in reference_item_result_fields)
            )

        ref_sql_string = f"SELECT {reference_fields_str}, :result_type as result_type FROM metadata_references r"
        ref_params = {"result_type": "metadata_ref_item"}
        ref_conditions = []

        if matcher.reference_item_ids:
            assert ref_query
            in_clause = []
            for idx, item_id in enumerate(matcher.reference_item_ids):
                ref_params[f"ri_id_{idx}"] = item_id
                in_clause.append(f":ri_id_{idx}")
            ref_conditions.append(f"r.reference_item_id IN ({in_clause_str})")
            # ref_params["reference_item_ids"] = tuple(matcher.reference_item_ids)

        if matcher.reference_item_types:
            assert ref_query
            in_clause = []
            for idx, item_type in enumerate(matcher.reference_item_types):
                ref_params[f"ri_type_{idx}"] = item_type
                in_clause.append(f":ri_type_{idx}")
            in_clause_str = ", ".join(in_clause)
            ref_conditions.append(f"r.reference_item_type IN ({in_clause_str})")
            # ref_params["reference_item_types"] = tuple(matcher.reference_item_types)

        if matcher.reference_item_keys:
            assert ref_query
            in_clause = []
            for idx, item_key in enumerate(matcher.reference_item_keys):
                ref_params[f"ri_key_{idx}"] = item_key
                in_clause.append(f":ri_key_{idx}")
            in_clause_str = ", ".join(in_clause)
            ref_conditions.append(f"r.reference_item_key IN ({in_clause_str})")
            # ref_params["reference_item_keys"] = tuple(matcher.reference_item_keys)

        if ref_conditions:
            ref_sql_string += " WHERE"
            for cond in ref_conditions:
                ref_sql_string += f" {cond} AND"

            ref_sql_string = ref_sql_string[:-4]

        ref_sql = text(ref_sql_string)

        with self.sqlite_engine.connect() as connection:
            result = connection.execute(sql, params)
            for row in result:
                yield row

            result = connection.execute(ref_sql, ref_params)
            for row in result:
                yield row

    def _retrieve_referenced_metadata_item_data(
        self, key: str, reference_type: str, reference_key: str, reference_id: str
    ) -> Union[Tuple[str, Mapping[str, Any]], None]:

        sql = text(
            f"""
            SELECT m.model_type_id, m.metadata_value
            FROM {TABLE_NAME_METADATA} m
            JOIN {TABLE_NAME_METADATA_REFERENCES} r ON m.metadata_item_id = r.metadata_item_id
            WHERE r.reference_item_type = :reference_type AND r.reference_item_key = :reference_key AND r.reference_item_id = :reference_id and m.metadata_item_key = :key
        """
        )

        with self.sqlite_engine.connect() as connection:
            parmas = {
                "reference_type": reference_type,
                "reference_key": reference_key,
                "reference_id": reference_id,
                "key": key,
            }
            result = connection.execute(sql, parmas)
            row = result.fetchall()
            if not row:
                return None

            if len(row) > 1:
                msg = f"Multiple ({len(row)}) metadata items found for key '{key}'"
                if reference_type:
                    msg += f" and reference type '{reference_type}'"
                if reference_id:
                    msg += f" and reference id '{reference_id}'"
                msg += "."
                raise KiaraException(msg)

            data_str = row[0][1]
            data = orjson.loads(data_str)

            return (row[0][0], data)

    def _delete_archive(self):

        delete_archive_db(db_path=self.sqlite_path)

    def get_archive_details(self) -> ArchiveDetails:

        all_metadata_items_sql = text(f"SELECT COUNT(*) FROM {TABLE_NAME_METADATA}")
        all_references_sql = text(
            f"SELECT COUNT(*) FROM {TABLE_NAME_METADATA_REFERENCES}"
        )

        with self.sqlite_engine.connect() as connection:
            result = connection.execute(all_metadata_items_sql)
            metadata_count = result.fetchone()[0]

            result = connection.execute(all_references_sql)
            reference_count = result.fetchone()[0]

            details = {
                "no_metadata_items": metadata_count,
                "no_references": reference_count,
                "dynamic_archive": False,
            }
            return ArchiveDetails(**details)


class SqliteMetadataStore(SqliteMetadataArchive, MetadataStore):

    _archive_type_name = "sqlite_metadata_store"

    @classmethod
    def _load_archive_config(
        cls, archive_uri: str, allow_write_access: bool, **kwargs
    ) -> Union[Dict[str, Any], None]:

        if not allow_write_access:
            return None

        if not Path(archive_uri).is_file():
            return None

        import sqlite3

        con = sqlite3.connect(archive_uri)

        cursor = con.cursor()
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
        tables = {x[0] for x in cursor.fetchall()}
        con.close()

        if not REQUIRED_TABLES_METADATA.issubset(tables):
            return None

        # config = SqliteArchiveConfig(sqlite_db_path=store_uri)
        return {"sqlite_db_path": archive_uri}

    def _set_archive_metadata_value(self, key: str, value: Any):
        """Set custom metadata for the archive."""

        sql = text(
            f"INSERT OR REPLACE INTO {TABLE_NAME_ARCHIVE_METADATA} (key, value) VALUES (:key, :value)"
        )
        with self.sqlite_engine.connect() as conn:
            params = {"key": key, "value": value}
            conn.execute(sql, params)
            conn.commit()

    def _store_metadata_schema(
        self, model_schema_hash: str, model_type_id: str, model_schema: str
    ):

        sql = text(
            f"INSERT OR IGNORE INTO {TABLE_NAME_METADATA_SCHEMAS} (model_schema_hash, model_type_id, model_schema) VALUES (:model_schema_hash, :model_type_id, :model_schema)"
        )
        params = {
            "model_schema_hash": model_schema_hash,
            "model_type_id": model_type_id,
            "model_schema": model_schema,
        }
        with self.sqlite_engine.connect() as conn:
            conn.execute(sql, params)
            conn.commit()

    def _store_metadata_item(
        self,
        key: str,
        value_json: str,
        value_hash: str,
        model_type_id: str,
        model_schema_hash: str,
    ) -> uuid.UUID:

        from kiara.registries.ids import ID_REGISTRY

        metadata_item_created = get_current_time_incl_timezone().isoformat()

        sql = text(
            f"INSERT OR IGNORE INTO {TABLE_NAME_METADATA} (metadata_item_id, metadata_item_created, metadata_item_key, metadata_item_hash, model_type_id, model_schema_hash, metadata_value) VALUES (:metadata_item_id, :metadata_item_created, :metadata_item_key, :metadata_item_hash, :model_type_id, :model_schema_hash, :metadata_value)"
        )

        metadata_item_id = ID_REGISTRY.generate(
            comment="new provisional metadata item id"
        )

        params = {
            "metadata_item_id": str(metadata_item_id),
            "metadata_item_created": metadata_item_created,
            "metadata_item_key": key,
            "metadata_item_hash": value_hash,
            "model_type_id": model_type_id,
            "model_schema_hash": model_schema_hash,
            "metadata_value": value_json,
        }

        query_metadata_id = text(
            f"SELECT metadata_item_id FROM {TABLE_NAME_METADATA} WHERE metadata_item_key = :metadata_item_key AND metadata_item_hash = :metadata_item_hash"
        )
        query_metadata_params = {
            "metadata_item_key": key,
            "metadata_item_hash": value_hash,
        }

        with self.sqlite_engine.connect() as conn:
            conn.execute(sql, params)
            result = conn.execute(query_metadata_id, query_metadata_params)
            metadata_item_id = uuid.UUID(result.fetchone()[0])
            conn.commit()

        return metadata_item_id

    def _store_metadata_reference(
        self,
        reference_item_type: str,
        reference_item_key: str,
        reference_item_id: str,
        metadata_item_id: str,
        replace_existing_references: bool = False,
        allow_multiple_references: bool = False,
    ) -> None:

        if not replace_existing_references:
            raise NotImplementedError(
                "not replacing existing metadata references is not yet supported"
            )

        else:

            sql_replace = text(
                f"DELETE FROM {TABLE_NAME_METADATA_REFERENCES} WHERE reference_item_type = :reference_item_type AND reference_item_key = :reference_item_key AND reference_item_id = :reference_item_id"
            )
            sql_replace_params = {
                "reference_item_type": reference_item_type,
                "reference_item_key": reference_item_key,
                "reference_item_id": reference_item_id,
            }

            metadata_reference_created = get_current_time_incl_timezone().isoformat()
            sql_insert = text(
                f"INSERT INTO {TABLE_NAME_METADATA_REFERENCES} (reference_item_type, reference_item_key, reference_item_id, reference_created, metadata_item_id) VALUES (:reference_item_type, :reference_item_key, :reference_item_id, :reference_created, :metadata_item_id)"
            )
            sql_insert_params = {
                "reference_item_type": reference_item_type,
                "reference_item_key": reference_item_key,
                "reference_item_id": reference_item_id,
                "reference_created": metadata_reference_created,
                "metadata_item_id": metadata_item_id,
            }
            with self.sqlite_engine.connect() as conn:
                conn.execute(sql_replace, sql_replace_params)
                conn.execute(sql_insert, sql_insert_params)
                conn.commit()

    def _store_metadata_and_ref_items(
        self, items: Generator[Tuple[Any, ...], None, None]
    ):

        insert_metadata_sql = text(
            f"INSERT OR IGNORE INTO {TABLE_NAME_METADATA} (metadata_item_id, metadata_item_created, metadata_item_key, metadata_item_hash, model_type_id, model_schema_hash, metadata_value) VALUES (:metadata_item_id, :metadata_item_created, :metadata_item_key, :metadata_item_hash, :model_type_id, :model_schema_hash, :metadata_value)"
        )

        insert_ref_sql = text(
            f"INSERT OR IGNORE INTO {TABLE_NAME_METADATA_REFERENCES} (reference_item_type, reference_item_key, reference_item_id, reference_created, metadata_item_id) VALUES (:reference_item_type, :reference_item_key, :reference_item_id, :reference_created, :metadata_item_id)"
        )

        batch_size = 100

        with self.sqlite_engine.connect() as conn:

            metadata_items = []
            ref_items = []

            for item in items:
                if item.result_type == "metadata_item":  # type: ignore
                    metadata_items.append(item._asdict())  # type: ignore
                elif item.result_type == "metadata_ref_item":  # type: ignore
                    ref_items.append(item._asdict())  # type: ignore
                else:
                    raise KiaraException(f"Unknown result type '{item.result_type}'")  # type: ignore

                if len(metadata_items) >= batch_size:
                    conn.execute(insert_metadata_sql, metadata_items)
                    metadata_items.clear()
                if len(ref_items) >= batch_size:
                    conn.execute(insert_ref_sql, ref_items)
                    ref_items.clear()

            if metadata_items:
                conn.execute(insert_metadata_sql, metadata_items)
            if ref_items:
                conn.execute(insert_ref_sql, ref_items)

            conn.commit()


# kiara\kiara\src\kiara\registries\metadata\metadata_store\__init__.py
# -*- coding: utf-8 -*-
import abc
import json
import uuid
from typing import (
    TYPE_CHECKING,
    Any,
    Dict,
    Generator,
    Generic,
    Iterable,
    Mapping,
    Tuple,
    Union,
)

from kiara.exceptions import KiaraException
from kiara.models.metadata import KiaraMetadata
from kiara.registries import ARCHIVE_CONFIG_CLS, BaseArchive

if TYPE_CHECKING:
    from kiara.registries.metadata import MetadataMatcher


class MetadataArchive(BaseArchive[ARCHIVE_CONFIG_CLS], Generic[ARCHIVE_CONFIG_CLS]):
    """Base class for data archiv implementationss."""

    @classmethod
    def supported_item_types(cls) -> Iterable[str]:
        """This archive type only supports storing data."""

        return ["metadata"]

    def __init__(
        self,
        archive_name: str,
        archive_config: ARCHIVE_CONFIG_CLS,
        force_read_only: bool = False,
    ):

        super().__init__(
            archive_name=archive_name,
            archive_config=archive_config,
            force_read_only=force_read_only,
        )
        self._schema_stored_cache: Dict[str, Any] = {}
        self._schema_stored_item: Dict[str, uuid.UUID] = {}

    def find_metadata_item_with_hash(
        self, item_hash: str, key: Union[str, None] = None
    ) -> Union[Tuple[str, Mapping[str, Any]], None]:
        """Return the key of the metadata item with the specified hash."""

        return self._retrieve_metadata_item_with_hash(item_hash=item_hash, key=key)

    @abc.abstractmethod
    def _retrieve_metadata_item_with_hash(
        self, item_hash: str, key: Union[str, None] = None
    ) -> Union[Tuple[str, Mapping[str, Any]], None]:
        pass

    def find_matching_metadata_items(
        self,
        matcher: "MetadataMatcher",
        metadata_item_result_fields: Union[Iterable[str], None] = None,
        reference_item_result_fields: Union[Iterable[str], None] = None,
    ) -> Generator[Tuple[Any, ...], None, None]:

        return self._find_matching_metadata_and_ref_items(
            matcher=matcher,
            metadata_item_result_fields=metadata_item_result_fields,
            reference_item_result_fields=reference_item_result_fields,
        )

    @abc.abstractmethod
    def _find_matching_metadata_and_ref_items(
        self,
        matcher: "MetadataMatcher",
        metadata_item_result_fields: Union[Iterable[str], None] = None,
        reference_item_result_fields: Union[Iterable[str], None] = None,
    ) -> Generator[Tuple[Any, ...], None, None]:
        pass

    def retrieve_metadata_item(
        self,
        metadata_item_key: str,
        reference_type: Union[str, None] = None,
        reference_key: Union[str, None] = None,
        reference_id: Union[str, None] = None,
    ) -> Union[Tuple[str, Mapping[str, Any]], None]:
        """Return the model type and model data for the specified metadata item.

        If more than one item matches, an exception is raised.

        Arguments:
            metadata_item_key: The key of the metadata item to retrieve.
            reference_type: The type of the referenced item.
            reference_key: The key of the referenced item.
            reference_id: The id of the referenced item.
        """

        if reference_id and (not reference_type or not reference_key):
            raise ValueError(
                "If reference_id is set, reference_key & reference_type must be set as well."
            )
        if reference_type and reference_key:
            if reference_id is None:
                raise KiaraException(
                    msg="reference_id must set also if reference_type is set."
                )
            result = self._retrieve_referenced_metadata_item_data(
                key=metadata_item_key,
                reference_type=reference_type,
                reference_key=reference_key,
                reference_id=reference_id,
            )
            if result is None:
                return None
            else:
                return result
        else:
            raise NotImplementedError(
                "Retrieving metadata item without reference not implemented yet."
            )

    @abc.abstractmethod
    def _retrieve_referenced_metadata_item_data(
        self, key: str, reference_type: str, reference_key: str, reference_id: str
    ) -> Union[Tuple[str, Mapping[str, Any]], None]:
        """Return the model type id and model data for the specified referenced metadata item."""


class MetadataStore(MetadataArchive):
    def __init__(
        self,
        archive_name: str,
        archive_config: ARCHIVE_CONFIG_CLS,
        force_read_only: bool = False,
    ):

        super().__init__(
            archive_name=archive_name,
            archive_config=archive_config,
            force_read_only=force_read_only,
        )

    @classmethod
    def _is_writeable(cls) -> bool:
        return True

    @abc.abstractmethod
    def _store_metadata_schema(
        self, model_schema_hash: str, model_type_id: str, model_schema: str
    ):
        """Store the metadata schema for the specified model."""

    def store_metadata_item(
        self,
        key: str,
        item: KiaraMetadata,
        reference_item_type: Union[str, None] = None,
        reference_item_key: Union[str, None] = None,
        reference_item_id: Union[str, None] = None,
        replace_existing_references: bool = False,
        allow_multiple_references: bool = False,
        store: Union[str, uuid.UUID, None] = None,
    ) -> uuid.UUID:
        """Store a metadata item into the store.

        If `reference_item_type` and `reference_item_id` are set, the stored metadata item will
        be linked to the stored metadata item, to enable lokoups later on.
        """

        if store:
            raise NotImplementedError(
                "Cannot store metadata item with store, not implemented yet."
            )

        # TODO: check if already stored
        model_type = item.model_type_id
        model_schema_hash = str(item.get_schema_cid())

        if model_schema_hash not in self._schema_stored_cache.keys():

            model_item_schema = item.model_json_schema()
            model_item_schema_str = json.dumps(model_item_schema)

            self._store_metadata_schema(
                model_schema_hash=model_schema_hash,
                model_type_id=model_type,
                model_schema=model_item_schema_str,
            )
            self._schema_stored_cache[model_schema_hash] = model_item_schema

        # data = item.model_dump()
        data_json = item.model_dump_json()
        data_hash = str(item.instance_cid)

        metadata_item_id = self._schema_stored_item.get(data_hash, None)
        if not metadata_item_id:

            metadata_item_id = self._store_metadata_item(
                key=key,
                value_json=data_json,
                value_hash=data_hash,
                model_type_id=model_type,
                model_schema_hash=model_schema_hash,
            )
            self._schema_stored_item[data_hash] = metadata_item_id

        if (reference_item_id and not reference_item_type) or (
            reference_item_type and not reference_item_id
        ):
            raise ValueError(
                "If reference_item_id is set, reference_item_type must be set as well."
            )

        if reference_item_type:
            assert reference_item_id is not None
            assert reference_item_key is not None
            self._store_metadata_reference(
                reference_item_type=reference_item_type,
                reference_item_key=reference_item_key,
                reference_item_id=reference_item_id,
                metadata_item_id=str(metadata_item_id),
                replace_existing_references=replace_existing_references,
                allow_multiple_references=allow_multiple_references,
            )

        return metadata_item_id

    @abc.abstractmethod
    def _store_metadata_reference(
        self,
        reference_item_type: str,
        reference_item_key: str,
        reference_item_id: str,
        metadata_item_id: str,
        replace_existing_references: bool = False,
        allow_multiple_references: bool = False,
    ) -> None:
        pass

    @abc.abstractmethod
    def _store_metadata_item(
        self,
        key: str,
        value_json: str,
        value_hash: str,
        model_type_id: str,
        model_schema_hash: str,
    ) -> uuid.UUID:
        pass

    def store_metadata_and_ref_items(
        self, items: Generator[Tuple[Any, ...], None, None]
    ):

        return self._store_metadata_and_ref_items(items)

    @abc.abstractmethod
    def _store_metadata_and_ref_items(
        self, items: Generator[Tuple[Any, ...], None, None]
    ):

        pass


# kiara\kiara\src\kiara\registries\models\__init__.py
# -*- coding: utf-8 -*-
from typing import TYPE_CHECKING, Dict, Type, Union

import orjson

from kiara.defaults import KIARA_MODEL_DATA_KEY, KIARA_MODEL_ID_KEY
from kiara.exceptions import KiaraException
from kiara.interfaces.python_api.models.info import KiaraModelClassesInfo
from kiara.models import KiaraModel

if TYPE_CHECKING:
    pass


class ModelRegistry(object):

    _instance = None

    @classmethod
    def instance(cls) -> "ModelRegistry":
        """
        The default ModelRegistry instance.

        Can be a simgleton because it only contains data that is determined by the current Python environment.
        """
        if cls._instance is None:
            cls._instance = ModelRegistry()
        return cls._instance

    def __init__(self) -> None:

        self._all_models: Union[KiaraModelClassesInfo, None] = None
        self._models_per_package: Dict[str, KiaraModelClassesInfo] = {}
        self._sub_models: Dict[Type[KiaraModel], KiaraModelClassesInfo] = {}

    @property
    def all_models(self) -> KiaraModelClassesInfo:

        if self._all_models is not None:
            return self._all_models

        self._all_models = KiaraModelClassesInfo.find_kiara_models()
        return self._all_models

    def get_model_cls(
        self,
        kiara_model_id: str,
        required_subclass: Union[Type[KiaraModel], None] = None,
    ) -> Type[KiaraModel]:

        model_info = self.all_models.item_infos.get(kiara_model_id, None)
        if model_info is None:
            if required_subclass:
                available = self.get_models_of_type(
                    required_subclass
                ).item_infos.values()
            else:
                available = self.all_models.item_infos.values()

            msg = f"Can't retrieve model class for id '{kiara_model_id}': id not registered."

            if available:
                msg = f"{msg}\n\nAvailable models:\n"
                for model in available:
                    msg += f"\n - {model.type_name}"

            raise KiaraException(msg=msg)

        cls = model_info.python_class.get_class()  # type: ignore
        if required_subclass:
            if not issubclass(cls, required_subclass):
                raise Exception(
                    f"Can't retrieve sub model of '{required_subclass.__name__}' with id '{kiara_model_id}': exists, but not the required subclass."
                )

        return cls  # type: ignore

    def get_models_for_package(self, package_name: str) -> KiaraModelClassesInfo:

        if package_name in self._models_per_package.keys():
            return self._models_per_package[package_name]

        temp = {}
        for key, info in self.all_models.item_infos.items():
            if info.context.labels.get("package") == package_name:
                temp[key] = info

        group = KiaraModelClassesInfo(
            group_title=f"kiara_models.{package_name}", item_infos=temp  # type: ignore
        )

        self._models_per_package[package_name] = group
        return group

    def get_models_of_type(self, model_type: Type[KiaraModel]) -> KiaraModelClassesInfo:

        if model_type in self._sub_models.keys():
            return self._sub_models[model_type]

        sub_classes = {}
        for model_id, type_info in self.all_models.item_infos.items():
            cls: Type[KiaraModel] = type_info.python_class.get_class()  # type: ignore

            if issubclass(cls, model_type):
                sub_classes[model_id] = type_info

        classes = KiaraModelClassesInfo(
            group_title=f"{model_type.__name__}-submodels", item_infos=sub_classes
        )
        self._sub_models[model_type] = classes
        return classes

    def create_instance_from_json(self, json_data: str) -> KiaraModel:

        data = orjson.loads(json_data)

        model_id = data.get(KIARA_MODEL_ID_KEY, None)
        if model_id is None:
            raise KiaraException(
                "Can't create model instance from JSON: no kiara model id."
            )

        cls = self.get_model_cls(model_id)

        model_data = data.get(KIARA_MODEL_DATA_KEY, None)
        if model_data is None:
            raise KiaraException(
                "Can't create model instance from JSON: no model data."
            )

        return cls(**model_data)


# kiara\kiara\src\kiara\registries\modules\__init__.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

"""Base module for code that handles the import and management of [KiaraModule][kiara.module.KiaraModule] sub-classes."""

from typing import TYPE_CHECKING, Dict, Iterable, Mapping, Type, Union

import structlog
from multiformats import CID

from kiara.exceptions import InvalidManifestException
from kiara.interfaces.python_api.models.info import ModuleTypeInfo, ModuleTypesInfo
from kiara.models.module.manifest import Manifest
from kiara.utils import is_debug

if TYPE_CHECKING:
    from kiara.context import Kiara
    from kiara.modules import KiaraModule

logget = structlog.getLogger()


class ModuleRegistry(object):
    def __init__(self, kiara: "Kiara"):

        self._kiara: Kiara = kiara

        self._cached_modules: Dict[str, Dict[CID, KiaraModule]] = {}

        from kiara.utils.class_loading import find_all_kiara_modules

        module_classes = find_all_kiara_modules()

        self._module_classes: Mapping[str, Type[KiaraModule]] = {}
        self._module_class_metadata: Dict[str, ModuleTypeInfo] = {}

        for k, v in module_classes.items():
            self._module_classes[k] = v

    @property
    def module_types(self) -> Mapping[str, Type["KiaraModule"]]:
        return self._module_classes

    def get_module_class(self, module_type: str) -> Type["KiaraModule"]:

        cls = self._module_classes.get(module_type, None)
        if cls is None:
            raise InvalidManifestException(
                f"No module of type '{module_type}' available.",
                module_type=module_type,
                available_module_types=self._module_classes.keys(),
            )
        return cls

    def get_module_type_names(self) -> Iterable[str]:
        return self._module_classes.keys()

    def get_module_type_metadata(self, type_name: str) -> ModuleTypeInfo:

        md = self._module_class_metadata.get(type_name, None)
        if md is None:
            md = ModuleTypeInfo.create_from_type_class(
                type_cls=self.get_module_class(module_type=type_name), kiara=self._kiara
            )
            self._module_class_metadata[type_name] = md
        return self._module_class_metadata[type_name]

    def get_context_metadata(
        self, alias: Union[str, None] = None, only_for_package: Union[str, None] = None
    ) -> ModuleTypesInfo:

        result = {}
        for type_name in self.module_types.keys():
            md = self.get_module_type_metadata(type_name=type_name)
            if only_for_package:
                if md.context.labels.get("package") == only_for_package:
                    result[type_name] = md
            else:
                result[type_name] = md

        return ModuleTypesInfo(group_title=alias, item_infos=result)  # type: ignore

    def resolve_manifest(self, manifest: Manifest) -> Manifest:
        """Returns a cloned manifest with resolved module config."""

        if manifest.is_resolved:
            return manifest.model_copy()

        m_cls = self.get_module_class(manifest.module_type)

        try:
            resolved = m_cls._resolve_module_config(**manifest.module_config)
            resolved_dict = resolved.model_dump()
            manifest_clone = manifest.model_copy(
                update={"module_config": resolved_dict, "is_resolved": True}
            )
            return manifest_clone

        except Exception as e:
            if is_debug():
                import traceback

                traceback.print_exc()

            raise InvalidManifestException(
                f"Error while resolving module config for module '{manifest.module_type}': {e}",
                module_type=manifest.module_type,
                module_config=manifest.module_config,
                parent=e,
            )

    def create_module(self, manifest: Union[Manifest, str]) -> "KiaraModule":
        """
        Create a [KiaraModule][kiara.module.KiaraModule] object from a module configuration.

        Arguments:
        ---------
            manifest: the module configuration
        """
        if isinstance(manifest, str):
            manifest = Manifest(module_type=manifest, module_config={})

        m_cls: Type[KiaraModule] = self.get_module_class(manifest.module_type)

        if not manifest.is_resolved:
            manifest = self.resolve_manifest(manifest)

        if self._cached_modules.setdefault(manifest.module_type, {}).get(
            manifest.instance_cid, None
        ):
            return self._cached_modules[manifest.module_type][manifest.instance_cid]

        if manifest.module_type in self.get_module_type_names():
            kiara_module = m_cls(module_config=manifest.module_config)
            kiara_module._manifest_cache = self.resolve_manifest(manifest)

        else:
            raise Exception(
                f"Invalid module type '{manifest.module_type}'. Available type names: {', '.join(self.get_module_type_names())}"
            )

        return kiara_module


# kiara\kiara\src\kiara\registries\operations\__init__.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import json
import os.path
import sys
from pathlib import Path
from typing import (
    TYPE_CHECKING,
    Any,
    Dict,
    Iterable,
    List,
    Mapping,
    Set,
    Type,
    TypeVar,
    Union,
)

import structlog
from rich.console import Group, RenderableType
from ruamel.yaml import YAML

from kiara.exceptions import (
    InvalidOperationException,
    NoSuchOperationException,
)
from kiara.interfaces.python_api.models.info import (
    OperationTypeClassesInfo,
    OperationTypeInfo,
)
from kiara.models.module.manifest import Manifest
from kiara.models.module.operation import (
    ManifestOperationConfig,
    Operation,
    OperationConfig,
    PipelineOperationConfig,
)
from kiara.models.module.pipeline import PipelineConfig
from kiara.models.module.pipeline.pipeline import Pipeline
from kiara.models.python_class import KiaraModuleInstance
from kiara.operations import OperationType
from kiara.utils import is_develop, log_exception, log_message
from kiara.utils.cli import terminal_print
from kiara.utils.output import extract_renderable
from kiara.utils.pipelines import find_pipeline_data_in_paths

if TYPE_CHECKING:
    from kiara.context import Kiara

logger = structlog.getLogger()
yaml = YAML(typ="safe")


OP_TYPE = TypeVar("OP_TYPE", bound=OperationType)


class OperationRegistry(object):
    def __init__(
        self,
        kiara: "Kiara",
        operation_type_classes: Union[Mapping[str, Type[OperationType]], None] = None,
    ):

        self._kiara: "Kiara" = kiara

        self._operation_type_classes: Union[Dict[str, Type["OperationType"]], None] = (
            None
        )

        if operation_type_classes is not None:
            self._operation_type_classes = dict(operation_type_classes)

        self._operation_type_metadata: Dict[str, OperationTypeInfo] = {}

        self._operation_types: Union[Dict[str, OperationType], None] = None

        self._operations: Union[Dict[str, Operation], None] = None
        self._operations_by_type: Union[Dict[str, List[str]], None] = None

        self._module_map: Union[Dict[str, Dict[str, Any]], None] = None

        self._invalid_operations: Dict[str, Any] = {}

    @property
    def is_initialized(self) -> bool:

        return self._operations is not None

    def get_module_map(self) -> Mapping[str, Mapping[str, Any]]:

        if not self.is_initialized:
            raise Exception(
                "Can't retrieve module map: operations not initialized yet."
            )

        if self._module_map is not None:
            return self._module_map

        module_map = {}
        for k, v in self.operations.items():
            module_map[k] = {
                "module_type": v.module_type,
                "module_config": v.module_config,
            }
        self._module_map = module_map
        return self._module_map

    @property
    def operation_types(self) -> Mapping[str, OperationType]:

        if self._operation_types is not None:
            return self._operation_types

        # TODO: support op type config
        _operation_types = {}
        for op_name, op_cls in self.operation_type_classes.items():
            try:
                _operation_types[op_name] = op_cls(
                    kiara=self._kiara, op_type_name=op_name
                )
            except Exception as e:
                log_exception(e)
                logger.debug("ignore.operation_type", operation_name=op_name, reason=e)

        self._operation_types = _operation_types
        return self._operation_types

    def get_operation_type(self, op_type: Union[str, Type[OP_TYPE]]) -> OP_TYPE:

        if not isinstance(op_type, str):
            try:
                op_type = op_type._operation_type_name  # type: ignore
            except Exception:
                raise ValueError(
                    f"Can't retrieve operation type, invalid input type '{type(op_type)}'."
                )

        if op_type not in self.operation_types.keys():
            raise Exception(
                f"No operation type '{op_type}' registered. Available operation types: {', '.join(self.operation_types.keys())}."
            )

        return self.operation_types[op_type]  # type: ignore

    def get_type_metadata(self, type_name: str) -> OperationTypeInfo:

        md = self._operation_type_metadata.get(type_name, None)
        if md is None:
            md = OperationTypeInfo.create_from_type_class(
                kiara=self._kiara, type_cls=self.operation_type_classes[type_name]
            )
            self._operation_type_metadata[type_name] = md
        return self._operation_type_metadata[type_name]

    def get_context_metadata(
        self, alias: Union[str, None] = None, only_for_package: Union[str, None] = None
    ) -> OperationTypeClassesInfo:

        result = {}
        for type_name in self.operation_type_classes.keys():
            md = self.get_type_metadata(type_name=type_name)
            if only_for_package:
                if md.context.labels.get("package") == only_for_package:
                    result[type_name] = md
            else:
                result[type_name] = md

        return OperationTypeClassesInfo(group_title=alias, item_infos=result)  # type: ignore

    @property
    def operation_type_classes(
        self,
    ) -> Mapping[str, Type["OperationType"]]:

        if self._operation_type_classes is not None:
            return self._operation_type_classes

        from kiara.utils.class_loading import find_all_operation_types

        self._operation_type_classes = find_all_operation_types()
        return self._operation_type_classes

    # @property
    # def operation_ids(self) -> List[str]:
    #     return list(self.profiles.keys())

    @property
    def operation_ids(self) -> Iterable[str]:
        return self.operations.keys()

    @property
    def operations(self) -> Mapping[str, Operation]:

        if self._operations is not None:
            return self._operations

        all_op_configs: Set[OperationConfig] = set()
        for op_type in self.operation_types.values():
            included_ops = op_type.retrieve_included_operation_configs()
            for op in included_ops:
                if isinstance(op, Mapping):
                    op = ManifestOperationConfig(**op)
                all_op_configs.add(op)

        for data_type in self._kiara.data_type_classes.values():
            if hasattr(data_type, "retrieve_included_operations"):
                included_ops = data_type.retrieve_included_operations()
                for op in included_ops:
                    if isinstance(op, Mapping):
                        op = ManifestOperationConfig(**op)
                    all_op_configs.add(op)

        operations: Dict[str, Operation] = {}
        operations_by_type: Dict[str, List[str]] = {}

        deferred_module_names: Dict[str, List[OperationConfig]] = {}

        # first iteration
        for op_config in all_op_configs:

            try:

                if isinstance(op_config, PipelineOperationConfig):
                    for mt in op_config.required_module_types:
                        if mt not in self._kiara.module_type_names:
                            deferred_module_names.setdefault(mt, []).append(op_config)
                    deferred_module_names.setdefault(
                        op_config.pipeline_name, []
                    ).append(op_config)
                    continue

            except Exception as e:
                details: Dict[str, Any] = {}
                module_id = op_config.retrieve_module_type(kiara=self._kiara)
                details["module_id"] = module_id
                if module_id == "pipeline":
                    details["pipeline_name"] = op_config.pipeline_name  # type: ignore
                msg: Union[str, Exception] = str(e)
                if not msg:
                    msg = e
                details["details"] = msg
                logger.error("invalid.operation", **details)
                self._invalid_operations[op_config.pipeline_name] = details  # type: ignore
                log_exception(e)
                continue

            try:

                module_type = op_config.retrieve_module_type(kiara=self._kiara)
                if module_type not in self._kiara.module_type_names:
                    deferred_module_names.setdefault(module_type, []).append(op_config)
                else:
                    module_config = op_config.retrieve_module_config(kiara=self._kiara)

                    manifest = Manifest(
                        module_type=module_type, module_config=module_config
                    )
                    ops = self._create_operations(manifest=manifest, doc=op_config.doc)

                    for op_type_name, _op in ops.items():
                        if _op.operation_id in operations.keys():
                            logger.debug(
                                "duplicate_operation_id",
                                op_id=_op.operation_id,
                                left_module=operations[_op.operation_id].module_type,
                                right_module=_op.module_type,
                            )
                            raise Exception(
                                f"Duplicate operation id: {_op.operation_id}"
                            )
                        operations[_op.operation_id] = _op
                        operations_by_type.setdefault(op_type_name, []).append(
                            _op.operation_id
                        )
            except Exception as e:
                details = {}
                module_id = op_config.retrieve_module_type(kiara=self._kiara)
                details["module_id"] = module_id
                if module_id == "pipeline":
                    details["pipeline_name"] = op_config.pipeline_name  # type: ignore
                msg = str(e)
                if not msg:
                    msg = e
                details["details"] = msg
                logger.error("invalid.operation", **details)
                log_exception(e)
                continue

        error_details = {}
        while deferred_module_names:

            deferred_length = len(deferred_module_names)

            remove_deferred_names = set()

            for missing_op_id in deferred_module_names.keys():
                if missing_op_id in operations.keys():
                    remove_deferred_names.add(missing_op_id)
                    continue

                for op_config in deferred_module_names[missing_op_id]:

                    try:

                        if isinstance(op_config, PipelineOperationConfig):

                            if all(
                                mt in self._kiara.module_type_names
                                or mt in operations.keys()
                                for mt in op_config.required_module_types
                            ):

                                module_map = {}
                                for mt in op_config.required_module_types:
                                    if mt in operations.keys():
                                        module_map[mt] = {
                                            "module_type": operations[mt].module_type,
                                            "module_config": operations[
                                                mt
                                            ].module_config,
                                        }
                                op_config.module_map.update(module_map)
                                module_config = op_config.retrieve_module_config(
                                    kiara=self._kiara
                                )

                                manifest = Manifest(
                                    module_type="pipeline",
                                    module_config=module_config,
                                )
                                ops = self._create_operations(
                                    manifest=manifest,
                                    doc=op_config.doc,
                                    metadata=op_config.metadata,
                                )

                            else:
                                missing = (
                                    mt
                                    for mt in op_config.required_module_types
                                    if mt not in self._kiara.module_type_names
                                    and mt not in operations.keys()
                                )
                                raise Exception(
                                    f"Can't find all required module types when processing pipeline '{missing_op_id}': {', '.join(missing)}"
                                )

                        else:
                            raise NotImplementedError(
                                f"Invalid type: {type(op_config)}"
                            )
                            # module_type = op_config.retrieve_module_type(kiara=self._kiara)
                            # module_config = op_config.retrieve_module_config(kiara=self._kiara)
                            #
                            # # TODO: merge dicts instead of update?
                            # new_module_config = dict(base_config)
                            # new_module_config.update(module_config)
                            #
                            # manifest = Manifest(module_type=operation.module_type,
                            #                       module_config=new_module_config)

                        for op_type_name, _op in ops.items():

                            if _op.operation_id in operations.keys():
                                raise Exception(
                                    f"Duplicate operation id: {_op.operation_id}"
                                )

                            operations[_op.operation_id] = _op
                            operations_by_type.setdefault(op_type_name, []).append(
                                _op.operation_id
                            )
                            assert _op.operation_id == op_config.pipeline_name

                        for _op_id in deferred_module_names.keys():
                            if op_config in deferred_module_names[_op_id]:
                                deferred_module_names[_op_id].remove(op_config)
                    except Exception as e:
                        details = {}
                        module_id = op_config.retrieve_module_type(kiara=self._kiara)
                        details["module_id"] = module_id
                        try:
                            details["module_config"] = op_config.retrieve_module_config(
                                kiara=self._kiara
                            )
                        except Exception as xe:
                            details["module_config"] = str(xe)
                        if module_id == "pipeline":
                            details["pipeline_name"] = op_config.pipeline_name  # type: ignore

                        msg = str(e)
                        if not msg:
                            msg = e
                        details["details"] = msg
                        error_details[missing_op_id] = details
                        exc_info = sys.exc_info()
                        details["parent"] = exc_info[1]

                        continue

            for name, dependencies in deferred_module_names.items():
                if not dependencies:
                    remove_deferred_names.add(name)

            for rdn in remove_deferred_names:
                deferred_module_names.pop(rdn)

            if len(deferred_module_names) == deferred_length:

                for mn in deferred_module_names:
                    if mn in operations.keys():
                        continue
                    details = error_details.get(missing_op_id, {"details": "-- n/a --"})
                    exception = details.get("parent", None)
                    if exception:
                        log_exception(exception)

                    self._invalid_operations[mn] = details
                    log_message(f"invalid.operation.{mn}", operation_id=mn, **details)
                break

        self._operations = {}
        for missing_op_id in sorted(operations.keys()):
            self._operations[missing_op_id] = operations[missing_op_id]

        self._operations_by_type = {}
        for op_type_name in sorted(operations_by_type.keys()):
            self._operations_by_type.setdefault(
                op_type_name, sorted(operations_by_type[op_type_name])
            )

        return self._operations

    def register_pipelines(self, *paths: Union[str, Path]) -> Dict[str, Operation]:
        """
        Register pipelines from one or more paths.

        Args:
        ----
            *paths: one or more paths to load pipelines from.
        """
        pipeline_data = find_pipeline_data_in_paths(
            {k if isinstance(k, str) else k.as_posix(): {} for k in paths}
        )
        duplicates = set()
        for op_id in pipeline_data.keys():
            if op_id in self.operations.keys():
                duplicates.add(op_id)

        if duplicates:
            raise Exception(
                "Can't register pipelines from the provided path(s), duplicate operation ids found: "
                + ", ".join(sorted(duplicates))
            )

        ops = {}
        for op_id, op_data in pipeline_data.items():
            # TODO: what to do with the additional data, like source and source type?
            try:
                op = self.register_pipeline(data=op_data["data"], operation_id=op_id)
            except Exception as e:
                log_message("invalid.pipeline", pipeline_id=op_id, reason=str(e))
                if is_develop():
                    renderables: List[RenderableType] = []
                    renderables.append("")
                    renderables.append(extract_renderable(e))
                    renderables.append("")
                    label = f"[red]Invalid Pipeline [/red][i]'{op_id}'[/i]"
                    terminal_print(Group(*renderables), in_panel=label)
                # log_exception(e)
                continue
            ops[op.operation_id] = op
        return ops

    def register_pipeline(
        self,
        data: Union[Path, str, Mapping[str, Any]],
        operation_id: Union[str, None] = None,
    ) -> Operation:

        if isinstance(data, Path):
            if not data.is_file():
                raise Exception(
                    f"Can't register operation from path '{data.as_posix()}: path is not a file."
                )

            pipeline_config = PipelineConfig.from_file(
                data.as_posix(), kiara=self._kiara, pipeline_name=operation_id
            )
        elif isinstance(data, Mapping):

            pipeline_config = PipelineConfig.from_config(
                pipeline_name=operation_id, data=data, kiara=self._kiara
            )
        elif isinstance(data, str):
            if os.path.isfile((os.path.realpath(data))):
                pipeline_config = PipelineConfig.from_file(
                    data, kiara=self._kiara, pipeline_name=operation_id
                )
            else:
                config_data = None
                try:
                    config_data = json.loads(data)
                except Exception:
                    try:
                        config_data = yaml.load(data)
                    except Exception:
                        pass
                if config_data:
                    pipeline_config = PipelineConfig.from_config(
                        pipeline_name=operation_id, data=config_data, kiara=self._kiara
                    )
                else:
                    raise Exception(
                        f"Can't register pipeline with id '{operation_id}': can't parse data as file path, json or yaml."
                    )
        else:
            raise Exception(
                f"Can't register pipeline with id '{operation_id}': invalid type '{type(data)}' for pipeline data: {type(data)}"
            )

        _operation_id = pipeline_config.pipeline_name
        if operation_id:
            assert _operation_id == operation_id

        if _operation_id in self.operation_ids:
            raise Exception(
                f"Can't register pipeline with id '{_operation_id}': operation id already in use."
            )

        manifest = Manifest(
            module_type="pipeline", module_config=pipeline_config.model_dump()
        )
        module = self._kiara.module_registry.create_module(manifest)

        from kiara.operations.included_core_operations.pipeline import (
            PipelineOperationDetails,
        )

        op_details = PipelineOperationDetails.create_operation_details(
            operation_id=module.config.pipeline_name,
            pipeline_inputs_schema=module.inputs_schema,
            pipeline_outputs_schema=module.outputs_schema,
            pipeline_config=module.config,
        )

        metadata: Dict[str, Any] = {}
        operation = Operation(
            module_type=manifest.module_type,
            module_config=manifest.module_config,
            operation_id=_operation_id,
            operation_details=op_details,
            module_details=KiaraModuleInstance.from_module(module),
            metadata=metadata,
            doc=pipeline_config.doc,
        )

        pc: PipelineConfig = module.config
        # make sure the pipeline can be created
        Pipeline(structure=pc.structure, kiara=self._kiara)

        operation._module = module
        assert self._operations is not None
        self._operations[_operation_id] = operation
        current_pipelines = self.operations_by_type.get("pipeline", None)
        if not current_pipelines:
            current_pipelines = []
            self._operations_by_type["pipeline"] = current_pipelines  # type: ignore

        current_pipelines.append(_operation_id)  # type: ignore
        assert self._operations_by_type is not None
        # self._operations_by_type["pipeline"] = sorted(current_pipelines)

        logger.debug("pipeline.registered", operation_id=_operation_id)
        return operation

    def _create_operations(
        self,
        manifest: Manifest,
        doc: Any,
        metadata: Union[Mapping[str, Any], None] = None,
    ) -> Dict[str, Operation]:

        module = self._kiara.module_registry.create_module(manifest)
        op_types = {}

        if metadata is None:
            metadata = {}

        for op_name, op_type in self.operation_types.items():

            op_details = op_type.check_matching_operation(module=module)
            if not op_details:
                continue

            operation = Operation(
                module_type=manifest.module_type,
                module_config=manifest.module_config,
                operation_id=op_details.operation_id,
                operation_details=op_details,
                module_details=KiaraModuleInstance.from_module(module),
                metadata=metadata,
                doc=doc,
            )
            operation._module = module

            op_types[op_name] = operation

        return op_types

    def get_operation(self, operation_id: str) -> Operation:

        if operation_id not in self.operation_ids:
            if operation_id in self._invalid_operations.keys():
                raise InvalidOperationException(self._invalid_operations[operation_id])
            else:
                raise NoSuchOperationException(
                    operation_id=operation_id,
                    available_operations=sorted(self.operation_ids),
                )

        op = self.operations[operation_id]
        return op

    def find_all_operation_types(self, operation_id: str) -> Set[str]:

        result = set()
        for op_type, ops in self.operations_by_type.items():
            if operation_id in ops:
                result.add(op_type)

        return result

    @property
    def operations_by_type(self) -> Mapping[str, Iterable[str]]:

        if self._operations_by_type is None:
            self.operations
        return self._operations_by_type  # type: ignore

    def find_operation_id(self, manifest: Manifest) -> Union[str, None]:

        for op in self.operations.values():
            if manifest.manifest_cid == op.manifest_cid:
                return op.operation_id

        return None


# kiara\kiara\src\kiara\registries\rendering\__init__.py
# -*- coding: utf-8 -*-
import os
from functools import partial
from typing import TYPE_CHECKING, Any, Dict, Iterable, List, Mapping, Type, Union

import mistune
import structlog
from jinja2 import (
    BaseLoader,
    Environment,
    FileSystemLoader,
    PackageLoader,
    PrefixLoader,
    Template,
    TemplateNotFound,
    select_autoescape,
)

from kiara.defaults import SpecialValue
from kiara.exceptions import KiaraException
from kiara.operations.included_core_operations.render_value import (
    RenderValueOperationType,
)
from kiara.renderers import KiaraRenderer
from kiara.renderers.jinja import BaseJinjaRenderer, JinjaEnv
from kiara.utils import log_exception, log_message
from kiara.utils.class_loading import find_all_kiara_renderers
from kiara.utils.values import extract_raw_value

if TYPE_CHECKING:
    from kiara.context import Kiara
    from kiara.models import KiaraModel

logger = structlog.getLogger()


def render_model_filter(render_registry: "RenderRegistry", instance: "KiaraModel"):

    template = render_registry.get_template("kiara/render/models/model_data.html")
    rendered = template.render(instance=instance)
    return rendered


def boolean_filter(data: bool):

    return "yes" if data else "no"


def default_filter(data: Any):

    if data in [None, SpecialValue.NO_VALUE, SpecialValue.NOT_SET]:
        return ""
    elif callable(data):
        return str(data())
    else:
        return str(data)


def render_markdown(markdown: mistune.Markdown, markdown_str: str):
    return markdown(markdown_str)


class RenderRegistry(object):
    """A registry collecting all Renderer types/objects that are available to render Value objects or internal kiara models."""

    _instance = None

    def __init__(self, kiara: "Kiara") -> None:

        self._kiara: Kiara = kiara

        self._renderer_types: Union[Mapping[str, Type[KiaraRenderer]], None] = None
        self._registered_renderers: Dict[str, KiaraRenderer] = {}

        self._template_pkg_loaders: Union[None, Dict[str, PackageLoader]] = None
        self._template_folders: Union[None, Dict[str, FileSystemLoader]] = None

        self._template_loader: Union[None, PrefixLoader] = None
        self._default_jinja_env: Union[None, Environment] = None

    def register_renderer_cls(self, renderer_cls: Type[KiaraRenderer]):

        try:
            self.register_renderer(renderer_type=renderer_cls)
        except Exception as e:
            log_message(
                "ignore.renderer",
                error=e,
                renderer_cls=renderer_cls,
                reason="can't initiate default renderer instance",
            )

        if hasattr(renderer_cls, "_renderer_profiles"):

            try:
                profiles = renderer_cls._renderer_profiles  # type: ignore
                if callable(profiles):
                    profiles = profiles()
                for config in profiles.values():  # type: ignore
                    try:
                        self.register_renderer(renderer_cls, config)  # type: ignore
                    except Exception as e:
                        log_exception(e)
                        log_message(
                            "ignore.renderer.profile",
                            error=e,
                            renderer_cls=renderer_cls,
                            config=config,
                        )
            except Exception as xe:
                log_exception(xe)
                log_message(
                    "ignore.renderer.profiles", error=xe, renderer_cls=renderer_cls
                )

        from kiara.renderers.included_renderers.value import ValueRenderer

        if renderer_cls == ValueRenderer:
            target_types = set()
            op_type: RenderValueOperationType = self._kiara.operation_registry.get_operation_type("render_value")  # type: ignore
            for op in op_type.operations.values():
                details = op_type.retrieve_operation_details(op)
                target_type = details.target_data_type
                target_types.add(target_type)

            for target_type in target_types:
                self.register_renderer(
                    renderer_type=ValueRenderer,
                    renderer_config={"target_type": target_type},
                )

    def register_renderer(
        self,
        renderer_type: Union[str, Type[KiaraRenderer]],
        renderer_config: Union[Mapping[str, Any], None] = None,
    ):

        if isinstance(renderer_type, str):
            renderer_cls = self.renderer_types.get(renderer_type, None)
        else:
            renderer_cls = renderer_type

        if renderer_cls is None:
            raise Exception(f"No renderer found for type: {renderer_type}.")

        if renderer_config is None:
            renderer_config = {}
        else:
            renderer_config = dict(renderer_config)

        if BaseJinjaRenderer in renderer_cls.mro():
            if "env" not in renderer_config:
                default_env = JinjaEnv()
                renderer_config["env"] = default_env

            assert renderer_config["env"]._render_registry is None
            renderer_config["env"]._render_registry = self

        renderer = renderer_cls(kiara=self._kiara, renderer_config=renderer_config)
        alias = renderer.get_renderer_alias()
        if alias in self._registered_renderers.keys():
            raise Exception(
                f"Can't register renderer, duplicate renderer alias: {alias}"
            )

        self._registered_renderers[alias] = renderer

    @property
    def renderer_types(self) -> Mapping[str, Type[KiaraRenderer]]:

        if self._renderer_types is not None:
            return self._renderer_types

        self._renderer_types = find_all_kiara_renderers()
        for value in self._renderer_types.values():
            self.register_renderer_cls(value)
        return self._renderer_types

    @property
    def default_jinja_environment(self) -> Environment:

        return self.retrieve_jinja_env()

    @property
    def template_loaders(self) -> Mapping[str, BaseLoader]:

        if self._template_pkg_loaders is not None:
            return self._template_pkg_loaders

        template_pkg_loaders = {}
        template_pkg_loaders["kiara"] = PackageLoader(
            package_name="kiara", package_path="resources/templates/render"
        )

        from importlib_metadata import entry_points

        for entry_point in entry_points(group="kiara.plugin"):

            try:
                template_pkg_loaders[entry_point.value] = PackageLoader(
                    package_name=entry_point.value, package_path="resources/templates"
                )
            except ValueError:
                # means no templates directory exists
                pass

        self._template_pkg_loaders = template_pkg_loaders
        self._template_loader = None
        return self._template_pkg_loaders

    @property
    def template_folders(self) -> Mapping[str, FileSystemLoader]:

        if self._template_folders is not None:
            return self._template_folders

        self._template_folders = {}
        return self._template_folders

    def retrieve_jinja_env(self, template_base: Union[str, None] = None) -> Environment:

        if not template_base:
            if self._default_jinja_env is not None:
                return self._default_jinja_env
            loader: BaseLoader = self.template_loader
        else:

            if template_base in self.template_folders.keys():
                loader = self.template_folders[template_base]
            elif template_base in self.template_loaders.keys():
                loader = self.template_loaders[template_base]
            else:
                msg = "Available template bases:\n\n"
                bases = sorted(
                    list(self.template_folders.keys())
                    + list(self.template_loaders.keys())
                )
                for base in bases:
                    msg += f" - {base}\n"
                raise KiaraException(
                    f"No template base found for: {template_base}", details=msg
                )

        env = Environment(loader=loader, autoescape=select_autoescape())

        env.filters["render_model"] = partial(render_model_filter, self)
        env.filters["render_bool"] = boolean_filter
        env.filters["render_default"] = default_filter
        try:
            markdown = mistune.create_markdown()
        except Exception:
            # depends on version of mistune that is installed
            markdown = mistune.Markdown()
        env.filters["markdown"] = partial(render_markdown, markdown)
        env.filters["extract_raw_data"] = partial(extract_raw_value, self._kiara)

        if not template_base:
            self._default_jinja_env = env

        return env

    @property
    def registered_renderers(self) -> Iterable[KiaraRenderer]:

        # make sure all the renderers are registered
        self.renderer_types
        return self._registered_renderers.values()

    def retrieve_renderers_for_source_type(
        self, source_type: str
    ) -> List[KiaraRenderer]:

        result = []
        for renderer in self.registered_renderers:
            if source_type in renderer.retrieve_supported_render_sources():
                result.append(renderer)
        return result

    def retrieve_renderers_for_source_target_combination(
        self, source_type: str, target_type: str
    ) -> List[KiaraRenderer]:

        result = []

        for renderer in self.registered_renderers:
            if (
                source_type in renderer.retrieve_supported_render_sources()
                and target_type in renderer.retrieve_supported_render_targets()
            ):
                result.append(renderer)
        return result

    def render(
        self,
        source_type: str,
        item: Any,
        target_type: str,
        render_config: Union[Mapping[str, Any], None] = None,
    ) -> Any:

        renderers = self.retrieve_renderers_for_source_target_combination(
            source_type, target_type
        )
        if not renderers:
            raise Exception(
                f"No renderer(s) available for rendering '{source_type}' to '{target_type}'."
            )

        if len(renderers) > 1:
            raise Exception(
                f"Multiple renderers available for rendering '{source_type}' to '{target_type}': {renderers}. This is not implemented yet."
            )

        renderer_instance = next(iter(renderers))
        if render_config is None:
            render_config = {}
        rc = renderer_instance.__class__._inputs_schema(**render_config)

        return renderer_instance.render(item, render_config=rc)

    @property
    def template_loader(self) -> PrefixLoader:

        if self._template_loader is not None:
            return self._template_loader

        loaders: Dict[str, BaseLoader] = dict(self.template_loaders)
        loaders.update(self.template_folders)

        self._template_loader = PrefixLoader(loaders)
        return self._template_loader

    def register_template_folder(self, alias: str, path: str):

        if alias in self.template_folders.keys():
            raise Exception(f"Duplicate template alias: {alias}")
        if alias in self.template_loaders.keys():
            raise Exception(f"Duplicate template alias: {alias}")
        if not os.path.isdir(path):
            raise Exception(f"Template path doesn't exist or is not a folder: {path}")

        self.template_folders[alias] = FileSystemLoader(path)  # type: ignore
        self._template_loader = None

    def register_template_pkg_location(self, alias: str, pkg_name: str, path: str):

        if alias in self.template_loaders.keys():
            raise Exception(f"Duplicate template alias: {alias}")
        if alias in self.template_folders.keys():
            raise Exception(f"Duplicate template alias: {alias}")

        self.template_loaders[alias] = PackageLoader(  # type: ignore
            package_name=pkg_name, package_path=path
        )

        self._template_loader = None

    def get_template(
        self, name: str, template_base: Union[str, None] = None
    ) -> Template:
        env = self.retrieve_jinja_env(template_base=template_base)
        try:
            return env.get_template(name=name)
        except TemplateNotFound:
            available_templates = env.list_templates()
            if not available_templates:
                if template_base:
                    details = "No templates registered in default jinja environment."
                else:
                    details = f"No templates registered in jinja environment with template base: {template_base}."
            else:
                details = "Available templates:\n\n"
                for at in available_templates:
                    details += f"- {at}\n"

            raise KiaraException(f"Template not found: {name}", details=details)

    def get_template_names(self, template_base: Union[str, None] = None) -> List[str]:
        """List all available template names."""
        env = self.retrieve_jinja_env(template_base=template_base)
        result: List[str] = env.list_templates()
        return result


# kiara\kiara\src\kiara\registries\templates\__init__.py
# -*- coding: utf-8 -*-

import importlib
import os
import pkgutil
from functools import partial
from pathlib import Path
from typing import TYPE_CHECKING, Any, List, Mapping, Union

import mistune
import structlog
from jinja2 import (
    Environment,
    FileSystemLoader,
    PrefixLoader,
    Template,
    select_autoescape,
)

from kiara.defaults import SpecialValue
from kiara.utils import log_exception

if TYPE_CHECKING:
    from kiara.models import KiaraModel

logger = structlog.getLogger()


def render_model_filter(template_registry: "TemplateRegistry", instance: "KiaraModel"):

    template = template_registry.get_template("kiara/render/models/model_data.html")
    rendered = template.render(instance=instance)
    return rendered


def boolean_filter(data: bool):

    return "yes" if data else "no"


def default_filter(data: Any):

    if data in [None, SpecialValue.NO_VALUE, SpecialValue.NOT_SET]:
        return ""
    elif callable(data):
        return str(data())
    else:
        return str(data)


def render_markdown(markdown: mistune.Markdown, markdown_str: str):
    return markdown(markdown_str)


class TemplateRegistry(object):
    """
    A registry collecting all the (jinja) templates that are available in the current environment.

    Packages can register templates by specifying an entrypoint under 'kiara.templates', pointing to a Python module
    that has template files
    """

    _instance = None

    @classmethod
    def instance(cls) -> "TemplateRegistry":
        """
        The default *kiara* TemplateRegistry instance.

        Can be a simgleton because it only contains data that is determined by the current Python environment.
        """
        if cls._instance is None:
            cls._instance = TemplateRegistry()
        return cls._instance

    def __init__(self) -> None:

        self._template_dirs: Union[None, Mapping[str, Path]] = None
        self._template_loader: Union[None, PrefixLoader] = None
        self._environment: Union[None, Environment] = None

    @property
    def environment(self) -> Environment:

        if self._environment is not None:
            return self._environment

        self._environment = Environment(
            loader=self.template_loader, autoescape=select_autoescape()
        )
        self._environment.filters["render_model"] = partial(render_model_filter, self)
        self._environment.filters["render_bool"] = boolean_filter
        self._environment.filters["render_default"] = default_filter
        try:
            markdown = mistune.create_markdown()
        except Exception:
            markdown = mistune.Markdown()
        self._environment.filters["markdown"] = partial(render_markdown, markdown)
        return self._environment

    @property
    def template_dirs(self) -> Mapping[str, Path]:

        if self._template_dirs is not None:
            return self._template_dirs

        discovered_plugins = {}

        try:
            import kiara_plugin  # type: ignore

            plugin_modules_available = True
        except Exception:
            plugin_modules_available = False
            plugin_modules = []

        if plugin_modules_available:
            plugin_modules = [
                name
                for finder, name, ispkg in pkgutil.iter_modules(
                    kiara_plugin.__path__, kiara_plugin.__name__ + "."  # type: ignore
                )
            ] + [
                name
                for finder, name, ispkg in pkgutil.iter_modules()
                if name.startswith("kiara")
            ]

        for module_name in plugin_modules:  # type: ignore

            try:
                module = importlib.import_module(module_name)
                discovered_plugins[module_name] = module
            except Exception as e:
                log_exception(e)

        all_template_dirs = {}
        for plugin_name, module in discovered_plugins.items():
            if not module.__file__:
                logger.warning(
                    "skip.discovered_plugin", plugin_name=plugin_name, module=module
                )
                continue
            templates_folder = os.path.join(
                os.path.dirname(module.__file__), "resources", "templates"
            )
            if not os.path.isdir(templates_folder):
                continue
            all_template_dirs[plugin_name] = Path(templates_folder)
            logger.debug(
                "registered.templates_dir", package=plugin_name, path=templates_folder
            )

        self._template_dirs = all_template_dirs
        return self._template_dirs

    @property
    def template_loader(self) -> PrefixLoader:

        if self._template_loader is not None:
            return self._template_loader

        loaders = {}
        for plugin_name, path in self.template_dirs.items():
            loaders[plugin_name] = FileSystemLoader(searchpath=path)

        self._template_loader = PrefixLoader(loaders)
        return self._template_loader

    def get_template(self, name: str) -> Template:

        return self.environment.get_template(name=name)

    @property
    def template_names(self) -> List[str]:
        """List all available template names."""

        templates: List[str] = self.environment.list_templates()
        return templates

    def get_template_for_model_type(
        self,
        model_type: str,
        template_format: str = "html",
        use_generic_if_none: bool = False,
    ) -> Union[Template, None]:

        matches = [
            template_name
            for template_name in self.template_names
            if template_name.endswith(f"{model_type}.{template_format}")
        ]

        if not matches and use_generic_if_none:
            matches = [
                template_name
                for template_name in self.template_names
                if template_name.endswith(f"generic_model_info.{template_format}")
            ]

        if not matches:
            return None
        elif len(matches) > 1:
            raise Exception(
                f"Multiple templates found for model type '{model_type}' and format '{template_format}'. This is not supported yet."
            )

        return self.get_template(matches[0])


# kiara\kiara\src\kiara\registries\types\__init__.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

from typing import TYPE_CHECKING, Any, Dict, List, Mapping, Set, Type, Union

from bidict import bidict

from kiara.data_types import DataType
from kiara.defaults import KIARA_ROOT_TYPE_NAME
from kiara.exceptions import DataTypeUnknownException
from kiara.interfaces.python_api.models.info import (
    DataTypeClassesInfo,
    DataTypeClassInfo,
)
from kiara.utils.class_loading import find_all_data_types

if TYPE_CHECKING:
    import networkx as nx

    from kiara.context import Kiara


class TypeRegistry(object):
    def __init__(self, kiara: "Kiara"):

        self._kiara: Kiara = kiara
        self._data_types: Union[bidict[str, Type[DataType]], None] = None
        self._data_type_metadata: Dict[str, DataTypeClassInfo] = {}
        self._cached_data_type_objects: Dict[int, DataType] = {}
        # self._registered_python_classes: Dict[Type, typing.List[str]] = None  # type: ignore
        self._type_hierarchy: Union[nx.DiGraph, None] = None
        self._lineages_cache: Dict[str, List[str]] = {}

        self._type_profiles: Union[Dict[str, Mapping[str, Any]], None] = None

    def invalidate_types(self):

        self._data_types = None
        # self._registered_python_classes = None

    def retrieve_data_type(
        self,
        data_type_name: str,
        data_type_config: Union[Mapping[str, Any], None] = None,
    ) -> DataType:

        if data_type_config is None:
            data_type_config = {}
        else:
            data_type_config = dict(data_type_config)

        if data_type_name not in self.data_type_profiles.keys():
            raise Exception(f"Data type name not registered: {data_type_name}")

        data_type: str = self.data_type_profiles[data_type_name]["type_name"]
        type_config = self.data_type_profiles[data_type_name]["type_config"]

        if data_type_config:
            type_config = dict(type_config)
            type_config.update(data_type_config)

        cls = self.get_data_type_cls(type_name=data_type)

        hash = cls._calculate_data_type_hash(type_config)
        if hash in self._cached_data_type_objects.keys():
            return self._cached_data_type_objects[hash]

        result = cls(**type_config)
        assert result.data_type_hash == hash
        self._cached_data_type_objects[result.data_type_hash] = result
        return result

    @property
    def data_type_classes(self) -> bidict[str, Type[DataType]]:

        if self._data_types is not None:
            return self._data_types

        self._data_types = bidict(find_all_data_types())
        profiles: Dict[str, Mapping[str, Any]] = {
            dn: {"type_name": dn, "type_config": {}} for dn in self._data_types.keys()
        }

        for name, cls in self._data_types.items():
            cls_profiles = cls.retrieve_available_type_profiles()
            for profile_name, type_config in cls_profiles.items():
                if profile_name in profiles.keys():
                    raise Exception(f"Duplicate data type profile: {profile_name}")
                profiles[profile_name] = {"type_name": name, "type_config": type_config}

        self._type_profiles = profiles
        return self._data_types

    @property
    def data_type_profiles(self) -> Mapping[str, Mapping[str, Any]]:

        if self._type_profiles is None:
            self.data_type_classes
        assert self._type_profiles is not None
        return self._type_profiles

    @property
    def data_type_hierarchy(self) -> "nx.DiGraph":

        if self._type_hierarchy is not None:
            return self._type_hierarchy

        def recursive_base_find(cls: Type, current: Union[List[str], None] = None):

            if current is None:
                current = []

            for base in cls.__bases__:

                if base in self.data_type_classes.values():
                    current.append(self.data_type_classes.inverse[base])

                recursive_base_find(base, current=current)

            return current

        bases = {}
        for name, cls in self.data_type_classes.items():
            bases[name] = recursive_base_find(cls)

        for profile_name, details in self.data_type_profiles.items():

            if not details["type_config"]:
                continue
            if profile_name in bases.keys():
                raise Exception(
                    f"Invalid profile name '{profile_name}': shadowing data type. This is most likely a bug."
                )
            bases[profile_name] = [details["type_name"]]

        import networkx as nx

        hierarchy = nx.DiGraph()
        hierarchy.add_node(KIARA_ROOT_TYPE_NAME)

        for name, _bases in bases.items():
            profile_details = self.data_type_profiles[name]
            cls = self.data_type_classes[profile_details["type_name"]]
            hierarchy.add_node(name, cls=cls)
            if not _bases:
                hierarchy.add_edge(KIARA_ROOT_TYPE_NAME, name)
            else:
                # we only need the first parent, all others will be taken care of by the parent of the parent
                hierarchy.add_edge(_bases[0], name)

        self._type_hierarchy = hierarchy
        return self._type_hierarchy

    def get_sub_hierarchy(self, data_type: str):

        import networkx as nx

        graph: nx.DiGraph = self.data_type_hierarchy

        desc = nx.descendants(graph, data_type)
        desc.add(data_type)
        sub_graph = graph.subgraph(desc)
        return sub_graph

    def get_type_lineage(self, data_type_name: str) -> List[str]:
        """Returns the shortest path between the specified type and the root, in reverse direction starting from the specified type."""
        if data_type_name not in self.data_type_profiles.keys():
            raise DataTypeUnknownException(data_type=data_type_name)

        if data_type_name in self._lineages_cache.keys():
            return self._lineages_cache[data_type_name]

        import networkx as nx

        path = nx.shortest_path(
            self.data_type_hierarchy, KIARA_ROOT_TYPE_NAME, data_type_name
        )
        path.remove(KIARA_ROOT_TYPE_NAME)
        self._lineages_cache[data_type_name] = list(reversed(path))
        return self._lineages_cache[data_type_name]

    def get_sub_types(self, data_type_name: str) -> Set[str]:

        if data_type_name not in self.data_type_classes.keys():
            raise Exception(f"No data type '{data_type_name}' registered.")

        import networkx as nx

        desc: Set[str] = nx.descendants(self.data_type_hierarchy, data_type_name)
        return desc

    def is_profile(self, data_type_name: str) -> bool:

        type_config = self.data_type_profiles.get(data_type_name, {}).get(
            "type_config", None
        )
        return True if type_config else False

    def get_profile_parent(self, data_type_name: str) -> Union[None, bool]:
        """
        Return the parent data type of the specified data type (if that is indeed a profile name).

        If the specified data type is not a profile name, 'None' will be returned.
        """
        return self.data_type_profiles.get(data_type_name, {}).get("type_name", None)

    def get_associated_profiles(
        self, data_type_name: str
    ) -> Mapping[str, Mapping[str, Any]]:

        if data_type_name not in self.data_type_classes.keys():
            raise Exception(f"No data type '{data_type_name}' registered.")

        result = {}
        for profile_name, details in self.data_type_profiles.items():
            if (
                profile_name != data_type_name
                and data_type_name == details["type_name"]
            ):
                result[profile_name] = details

        return result

    def get_data_type_names(self, include_profiles: bool = False) -> List[str]:
        if include_profiles:
            return list(self.data_type_profiles.keys())
        else:
            return list(self.data_type_classes.keys())

    def get_data_type_cls(self, type_name: str) -> Type[DataType]:

        _type_details = self.data_type_profiles.get(type_name, None)
        if _type_details is None:
            raise Exception(
                f"No value type '{type_name}', available types: {', '.join(self.data_type_profiles.keys())}"
            )

        resolved_type_name: str = _type_details["type_name"]

        t = self.data_type_classes.get(resolved_type_name, None)
        if t is None:
            raise Exception(
                f"No value type '{type_name}', available types: {', '.join(self.data_type_profiles.keys())}"
            )
        return t

    def get_data_type_instance(
        self, type_name: str, type_config: Union[None, Mapping[str, Any]] = None
    ) -> DataType:

        cls = self.get_data_type_cls(type_name=type_name)
        if not type_config:
            obj = cls()
        else:
            obj = cls(**type_config)
        return obj

    def get_type_metadata(self, type_name: str) -> DataTypeClassInfo:

        md = self._data_type_metadata.get(type_name, None)
        if md is None:
            md = DataTypeClassInfo.create_from_type_class(
                type_cls=self.get_data_type_cls(type_name=type_name), kiara=self._kiara
            )
            self._data_type_metadata[type_name] = md
        return self._data_type_metadata[type_name]

    def get_context_metadata(
        self, alias: Union[str, None] = None, only_for_package: Union[str, None] = None
    ) -> DataTypeClassesInfo:

        result = {}
        for type_name in self.data_type_classes.keys():
            md = self.get_type_metadata(type_name=type_name)
            if only_for_package:
                if md.context.labels.get("package") == only_for_package:
                    result[type_name] = md
            else:
                result[type_name] = md

        _result = DataTypeClassesInfo(group_title=alias, item_infos=result)  # type: ignore
        _result._kiara = self._kiara
        return _result

    def is_internal_type(self, data_type_name: str) -> bool:

        if data_type_name not in self.data_type_profiles.keys():
            return False

        lineage = self.get_type_lineage(data_type_name=data_type_name)
        return "any" not in lineage


# kiara\kiara\src\kiara\registries\workflows\archives.py
# -*- coding: utf-8 -*-
import shutil
import uuid
from pathlib import Path
from typing import Any, Dict, Iterable, Mapping, Union

import orjson

from kiara.exceptions import NoSuchWorkflowException
from kiara.models.workflow import WorkflowMetadata, WorkflowState
from kiara.registries import ARCHIVE_CONFIG_CLS, FileSystemArchiveConfig
from kiara.registries.workflows import WorkflowArchive, WorkflowStore
from kiara.utils.windows import fix_windows_longpath


class FileSystemWorkflowArchive(WorkflowArchive):

    _archive_type_name = "filesystem_workflow_archive"
    _config_cls = FileSystemArchiveConfig  # type: ignore

    def __init__(
        self,
        archive_name: str,
        archive_config: ARCHIVE_CONFIG_CLS,
        force_read_only: bool = False,
    ):

        super().__init__(
            archive_name=archive_name,
            archive_config=archive_config,
            force_read_only=force_read_only,
        )

        self._base_path: Union[Path, None] = None
        self.alias_store_path.mkdir(parents=True, exist_ok=True)

    def _retrieve_archive_metadata(self) -> Mapping[str, Any]:

        if not self.archive_metadata_path.is_file():
            _archive_metadata = {}
        else:
            _archive_metadata = orjson.loads(self.archive_metadata_path.read_bytes())

        archive_id = _archive_metadata.get("archive_id", None)
        if not archive_id:
            try:
                _archive_id = uuid.UUID(self.workflow_store_path.name)
                _archive_metadata["archive_id"] = str(_archive_id)
            except Exception:
                raise Exception(
                    f"Could not retrieve archive id for alias archive '{self.archive_name}'."
                )

        return _archive_metadata

    @property
    def archive_metadata_path(self) -> Path:
        return self.workflow_store_path / "store_metadata.json"

    @property
    def workflow_store_path(self) -> Path:

        if self._base_path is not None:
            return self._base_path

        self._base_path = Path(self.config.archive_path).absolute()  # type: ignore
        self._base_path = fix_windows_longpath(self._base_path)
        self._base_path.mkdir(parents=True, exist_ok=True)
        return self._base_path

    @property
    def alias_store_path(self) -> Path:

        return self.workflow_store_path / "aliases"

    def _delete_archive(self):
        shutil.rmtree(self.workflow_store_path)

    @property
    def workflow_path(self) -> Path:
        return self.workflow_store_path / "workflows"

    @property
    def workflow_states_path(self) -> Path:
        return self.workflow_store_path / "states"

    def get_workflow_details_path(self, workflow_id: uuid.UUID) -> Path:

        return self.workflow_path / str(workflow_id) / "workflow.json"

    def get_alias_path(self, alias: str):

        return self.alias_store_path / f"{alias}.alias"

    def retrieve_all_workflow_aliases(self) -> Mapping[str, uuid.UUID]:

        all_aliases = self.alias_store_path.glob("*.alias")
        result: Dict[str, uuid.UUID] = {}
        for path in all_aliases:
            alias = path.name[0:-6]
            workflow_path = path.resolve()
            workflow_id = uuid.UUID(workflow_path.parent.name)
            if alias in result.keys():
                raise Exception(
                    f"Invalid internal state for workflow archive '{self.archive_id}': duplicate alias '{alias}'."
                )
            result[alias] = workflow_id

        return result

    def retrieve_all_workflow_ids(self) -> Iterable[uuid.UUID]:

        all_ids = self.workflow_path.glob("*")
        result = []
        for path in all_ids:
            workflow_id = uuid.UUID(path.name)
            result.append(workflow_id)
        return result

    def retrieve_workflow_metadata(self, workflow_id: uuid.UUID) -> WorkflowMetadata:

        workflow_path = self.get_workflow_details_path(workflow_id=workflow_id)
        if not workflow_path.exists():
            raise NoSuchWorkflowException(
                workflow=workflow_id,
                msg=f"Can't retrieve workflow with id '{workflow_id}': id does not exist.",
            )

        workflow_json = workflow_path.read_text()

        workflow_data = orjson.loads(workflow_json)
        workflow = WorkflowMetadata(**workflow_data)
        workflow._kiara = self.kiara_context

        return workflow

    def retrieve_workflow_state(self, workflow_state_id: str) -> WorkflowState:

        workflow_state_path = self.workflow_states_path / f"{workflow_state_id}.state"

        if not workflow_state_path.exists():
            raise Exception(f"No workflow state with id '{workflow_state_id}' exists.")

        _data = workflow_state_path.read_text()
        _json = orjson.loads(_data)
        # _json["pipeline_info"]["pipeline_structure"] = {
        #     "pipeline_config": _json["pipeline_info"]["pipeline_structure"][
        #         "pipeline_config"
        #     ]
        # }
        _state = WorkflowState(**_json)
        _state.pipeline_info._kiara = self.kiara_context
        _state._kiara = self.kiara_context
        return _state

    def retrieve_all_states_for_workflow(
        self, workflow_id: uuid.UUID
    ) -> Mapping[str, WorkflowState]:

        details = self.retrieve_workflow_metadata(workflow_id=workflow_id)

        result = {}
        for ws_id in details.workflow_history.values():
            ws_state = self.retrieve_workflow_state(workflow_state_id=ws_id)
            result[ws_id] = ws_state

        return result


class FileSystemWorkflowStore(FileSystemWorkflowArchive, WorkflowStore):

    _archive_type_name = "filesystem_workflow_store"

    def _register_workflow_metadata(self, workflow_metadata: WorkflowMetadata):

        workflow_path = self.get_workflow_details_path(
            workflow_id=workflow_metadata.workflow_id
        )

        if workflow_path.exists():
            raise Exception(
                f"Can't register workflow with id '{workflow_metadata.workflow_id}': id already registered."
            )

        workflow_path.parent.mkdir(parents=True, exist_ok=False)

        workflow_json = workflow_metadata.model_dump_json()
        workflow_path.write_text(workflow_json)

    def _update_workflow_metadata(self, workflow_metadata: WorkflowMetadata):

        workflow_path = self.get_workflow_details_path(
            workflow_id=workflow_metadata.workflow_id
        )

        if not workflow_path.exists():
            raise Exception(
                f"Can't update workflow with id '{workflow_metadata.workflow_id}': id not registered."
            )

        workflow_json = workflow_metadata.json(option=orjson.OPT_NON_STR_KEYS)
        workflow_path.write_text(workflow_json)

    def register_alias(self, workflow_id: uuid.UUID, alias: str, force: bool = False):

        alias_path = self.get_alias_path(alias=alias)
        if not force and alias_path.exists():
            raise Exception(
                f"Can't register workflow alias '{alias}': alias already registered."
            )
        elif alias_path.exists():
            alias_path.unlink()

        workflow_path = self.get_workflow_details_path(workflow_id=workflow_id)
        if not workflow_path.exists():
            raise Exception(
                f"Can't register workflow alias '{alias}': target id '{workflow_id}' not registered."
            )

        alias_path.symlink_to(workflow_path)

    def unregister_alias(self, alias: str) -> bool:

        alias_path = self.get_alias_path(alias=alias)

        if not alias_path.exists():
            return False

        alias_path.unlink()
        return True

    def add_workflow_state(self, workflow_state: WorkflowState):

        self.workflow_states_path.mkdir(exist_ok=True, parents=True)
        workflow_state_path = (
            self.workflow_states_path / f"{workflow_state.instance_id}.state"
        )

        workflow_state_json = workflow_state.model_dump_json()
        workflow_state_path.write_text(workflow_state_json)


# kiara\kiara\src\kiara\registries\workflows\sqlite_store.py
# -*- coding: utf-8 -*-
import uuid
from pathlib import Path
from typing import Iterable, Mapping, Union

from sqlalchemy import create_engine, text
from sqlalchemy.engine import Engine

from kiara.models.workflow import WorkflowMetadata, WorkflowState
from kiara.registries import SqliteArchiveConfig
from kiara.registries.workflows import WorkflowArchive, WorkflowStore
from kiara.utils.windows import fix_windows_longpath


class SqliteWorkflowArchive(WorkflowArchive[SqliteArchiveConfig]):

    _archive_type_name = "sqlite_workflow_archive"
    _config_cls = SqliteArchiveConfig

    def __init__(
        self,
        archive_name: str,
        archive_config: SqliteArchiveConfig,
        force_read_only: bool = False,
    ):

        super().__init__(
            archive_name=archive_name,
            archive_config=archive_config,
            force_read_only=force_read_only,
        )
        self._db_path: Union[Path, None] = None
        self._cached_engine: Union[Engine, None] = None
        # self._lock: bool = True

    def _retrieve_archive_id(self) -> uuid.UUID:
        sql = text("SELECT value FROM archive_metadata WHERE key='archive_id'")

        with self.sqlite_engine.connect() as connection:
            result = connection.execute(sql)
            row = result.fetchone()
            if row is None:
                raise Exception("No archive ID found in metadata")
            return uuid.UUID(row[0])

    @property
    def sqlite_path(self):

        if self._db_path is not None:
            return self._db_path

        db_path = Path(self.config.sqlite_db_path).resolve()
        self._db_path = fix_windows_longpath(db_path)

        if self._db_path.exists():
            return self._db_path

        self._db_path.parent.mkdir(parents=True, exist_ok=True)
        return self._db_path

    @property
    def db_url(self) -> str:
        return f"sqlite:///{self.sqlite_path}"

    @property
    def sqlite_engine(self) -> "Engine":

        if self._cached_engine is not None:
            return self._cached_engine

        # def _pragma_on_connect(dbapi_con, con_record):
        #     dbapi_con.execute("PRAGMA query_only = ON")

        self._cached_engine = create_engine(self.db_url, future=True)
        create_table_sql = """
CREATE TABLE IF NOT EXISTS job_records (
    job_hash TEXT PRIMARY KEY,
    manifest_hash TEXT NOT NULL,
    inputs_hash TEXT NOT NULL,
    job_metadata TEXT NOT NULL
);
"""

        with self._cached_engine.begin() as connection:
            for statement in create_table_sql.split(";"):
                if statement.strip():
                    connection.execute(text(statement))

        # if self._lock:
        #     event.listen(self._cached_engine, "connect", _pragma_on_connect)
        return self._cached_engine

    def retrieve_all_workflow_aliases(self) -> Mapping[str, uuid.UUID]:

        raise NotImplementedError()

    def retrieve_all_workflow_ids(self) -> Iterable[uuid.UUID]:

        raise NotImplementedError()

    def retrieve_workflow_metadata(self, workflow_id: uuid.UUID) -> WorkflowMetadata:

        raise NotImplementedError()

    def retrieve_workflow_state(self, workflow_state_id: str) -> WorkflowState:

        raise NotImplementedError()

    def retrieve_all_states_for_workflow(
        self, workflow_id: uuid.UUID
    ) -> Mapping[str, WorkflowState]:

        raise NotImplementedError()


class SqliteWorkflowStore(SqliteWorkflowArchive, WorkflowStore):

    _archive_type_name = "sqlite_workflow_store"

    def _register_workflow_metadata(self, workflow_metadata: WorkflowMetadata) -> None:

        raise NotImplementedError()

    def _update_workflow_metadata(self, workflow_metadata: WorkflowMetadata):

        raise NotImplementedError()

    def add_workflow_state(self, workflow_state: WorkflowState):

        raise NotImplementedError()

    def register_alias(self, workflow_id: uuid.UUID, alias: str):

        raise NotImplementedError()


# kiara\kiara\src\kiara\registries\workflows\__init__.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)
import abc
import datetime
import uuid
from typing import (
    TYPE_CHECKING,
    Callable,
    Dict,
    Generic,
    Iterable,
    List,
    Mapping,
    Union,
)

import structlog

from kiara.exceptions import NoSuchWorkflowException
from kiara.models.events.workflow_registry import WorkflowArchiveAddedEvent
from kiara.models.workflow import WorkflowMetadata, WorkflowState
from kiara.registries import ARCHIVE_CONFIG_CLS, BaseArchive
from kiara.registries.ids import ID_REGISTRY
from kiara.utils.dates import get_current_time_incl_timezone

if TYPE_CHECKING:
    from kiara.context import Kiara

logger = structlog.getLogger()


class WorkflowArchive(BaseArchive[ARCHIVE_CONFIG_CLS], Generic[ARCHIVE_CONFIG_CLS]):
    @classmethod
    def supported_item_types(cls) -> Iterable[str]:
        return ["workflow"]

    @abc.abstractmethod
    def retrieve_all_workflow_aliases(self) -> Mapping[str, uuid.UUID]:
        pass

    @abc.abstractmethod
    def retrieve_all_workflow_ids(self) -> Iterable[uuid.UUID]:
        pass

    @abc.abstractmethod
    def retrieve_workflow_metadata(self, workflow_id: uuid.UUID) -> WorkflowMetadata:
        pass

    # @abc.abstractmethod
    # def retrieve_workflow_states(
    #     self, workflow_id: uuid.UUID, filter: Union[WorkflowStateFilter, None] = None
    # ) -> Dict[str, WorkflowState]:
    #     pass

    @abc.abstractmethod
    def retrieve_workflow_state(self, workflow_state_id: str) -> WorkflowState:
        """
        Retrieve workflow state details for the provided state id.

        Arguments:
        ---------
            workflow_id: id of the workflow
            workflow_state_id: the id of the workflow state
        """

    @abc.abstractmethod
    def retrieve_all_states_for_workflow(
        self, workflow_id: uuid.UUID
    ) -> Mapping[str, WorkflowState]:
        """
        Retrieve workflow state details for the provided state id.

        Arguments:
        ---------
            workflow_id: id of the workflow
            workflow_state_id: the id of the workflow state
        """


class WorkflowStore(WorkflowArchive):
    @classmethod
    def _is_writeable(cls) -> bool:
        return True

    def register_workflow(
        self,
        workflow_metadata: WorkflowMetadata,
        workflow_aliases: Union[Iterable[str], None] = None,
    ):

        self._register_workflow_metadata(workflow_metadata=workflow_metadata)
        if workflow_aliases:
            if isinstance(workflow_aliases, str):
                workflow_aliases = [workflow_aliases]
            for workflow_alias in workflow_aliases:
                self.register_alias(
                    workflow_id=workflow_metadata.workflow_id, alias=workflow_alias
                )
        return workflow_metadata

    def unregister_alias(self, alias: str) -> bool:
        """
        Unregister a workflow alias.

        Returns:
        -------
            'True' if an alias existed and was unregistered, 'False' otherwise
        """
        raise NotImplementedError()

    def update_workflow_metadata(self, workflow_metadata: WorkflowMetadata):

        self._update_workflow_metadata(workflow_metadata=workflow_metadata)

    @abc.abstractmethod
    def _register_workflow_metadata(self, workflow_metadata: WorkflowMetadata):
        pass

    @abc.abstractmethod
    def _update_workflow_metadata(self, workflow_metadata: WorkflowMetadata):
        pass

    @abc.abstractmethod
    def add_workflow_state(self, workflow_state: WorkflowState):
        pass

    @abc.abstractmethod
    def register_alias(self, workflow_id: uuid.UUID, alias: str):
        pass


class WorkflowRegistry(object):
    def __init__(self, kiara: "Kiara"):

        self._kiara: Kiara = kiara
        self._event_callback: Callable = self._kiara.event_registry.add_producer(self)

        self._workflow_archives: Dict[str, WorkflowArchive] = {}
        """All registered archives/stores."""

        self._default_alias_store: Union[str, None] = None
        """The alias of the store where new aliases are stored by default."""

        self._all_aliases: Union[Dict[str, uuid.UUID], None] = None
        """All workflow aliases."""

        self._all_workflow_ids: Union[Dict[uuid.UUID, str], None] = None
        """All workflow ids, with store alias as values"""

        self._cached_workflow_metadata_items: Dict[uuid.UUID, WorkflowMetadata] = {}

    def register_archive(
        self,
        archive: WorkflowArchive,
        set_as_default_store: Union[bool, None] = None,
    ):
        alias = archive.archive_name
        if not alias:
            raise Exception("Invalid workflows archive alias: can't be empty.")

        if "." in alias:
            raise Exception(
                f"Can't register workflow archive with as '{alias}': registered name is not allowed to contain a '.' character (yet)."
            )

        if alias in self._workflow_archives.keys():
            raise Exception(
                f"Can't add store, workflow archive alias '{alias}' already registered."
            )

        archive.register_archive(kiara=self._kiara)

        self._workflow_archives[alias] = archive
        is_store = False
        is_default_store = False
        if isinstance(archive, WorkflowStore):
            is_store = True
            if set_as_default_store and self._default_alias_store is not None:
                raise Exception(
                    f"Can't set alias store '{alias}' as default store: default store already set."
                )

            if self._default_alias_store is None:
                is_default_store = True
                self._default_alias_store = alias

        event = WorkflowArchiveAddedEvent(
            kiara_id=self._kiara.id,
            workflow_archive_id=archive.archive_id,
            workflow_archive_alias=alias,
            is_store=is_store,
            is_default_store=is_default_store,
        )
        self._event_callback(event)

    @property
    def default_alias_store(self) -> str:

        if self._default_alias_store is None:
            raise Exception("No default alias store set (yet).")
        return self._default_alias_store

    @property
    def workflow_archives(self) -> Mapping[str, WorkflowArchive]:
        return self._workflow_archives

    def get_archive(
        self, archive_id: Union[str, None] = None
    ) -> Union[WorkflowArchive, None]:
        if archive_id is None:
            archive_id = self.default_alias_store
            if archive_id is None:
                raise Exception("Can't retrieve default alias archive, none set (yet).")

        archive = self._workflow_archives.get(archive_id, None)
        return archive

    def get_aliases(self, workflow_id: uuid.UUID) -> List[str]:
        """Return all aliases for the specified workflow id."""
        return [
            alias
            for alias, w_id in self.workflow_aliases.items()
            if w_id == workflow_id
        ]

    @property
    def workflow_aliases(self) -> Dict[str, uuid.UUID]:
        """Retrieve all registered workflow aliases."""
        if self._all_aliases is not None:
            return self._all_aliases

        all_workflows: Dict[str, uuid.UUID] = {}
        for archive_alias, archive in self._workflow_archives.items():
            workflow_map = archive.retrieve_all_workflow_aliases()
            for alias, w_id in workflow_map.items():
                if archive_alias == self.default_alias_store:
                    final_alias = alias
                else:
                    final_alias = f"{archive_alias}.{alias}"

                if final_alias in all_workflows.keys():
                    raise Exception(
                        f"Inconsistent alias registry: alias '{final_alias}' available more than once."
                    )
                all_workflows[final_alias] = w_id
        self._all_aliases = all_workflows
        return self._all_aliases

    @property
    def all_workflow_ids(self) -> Iterable[uuid.UUID]:

        if self._all_workflow_ids is not None:
            return self._all_workflow_ids.keys()

        all_ids: Dict[uuid.UUID, str] = {}
        for archive_alias, archive in self._workflow_archives.items():
            ids = archive.retrieve_all_workflow_ids()

            for _id in ids:
                assert _id not in all_ids.keys()
                all_ids[_id] = archive_alias

        self._all_workflow_ids = all_ids
        return self._all_workflow_ids.keys()

    def get_workflow_id(self, workflow_alias: str) -> uuid.UUID:

        workflow_id = self.workflow_aliases.get(workflow_alias, None)

        if workflow_id is None:
            raise NoSuchWorkflowException(
                workflow=workflow_alias,
                msg=f"Can't retrieve workflow with id/alias '{workflow_alias}': alias not registered.",
            )

        return workflow_id

    def get_workflow_metadata(
        self, workflow: Union[str, uuid.UUID]
    ) -> WorkflowMetadata:

        if isinstance(workflow, str):
            try:
                workflow_id = uuid.UUID(workflow)
            except Exception:
                workflow_id = self.get_workflow_id(workflow_alias=workflow)
        else:
            workflow_id = workflow

        if workflow_id in self._cached_workflow_metadata_items.keys():
            return self._cached_workflow_metadata_items[workflow_id]

        if self._all_workflow_ids is None:
            self.all_workflow_ids

        store_alias = self._all_workflow_ids[workflow_id]  # type: ignore
        store = self._workflow_archives[store_alias]

        workflow_details = store.retrieve_workflow_metadata(workflow_id=workflow_id)
        # workflow_metadata._kiara = self._kiara
        # workflow = Workflow(kiara=self._kiara, workflow_metadata=workflow_metadata)
        self._cached_workflow_metadata_items[workflow_id] = workflow_details

        # states = store.retrieve_workflow_states(workflow_id=workflow_id)
        # workflow._snapshots = states
        # workflow.load_state()

        return workflow_details

    def unregister_alias(self, alias: str) -> bool:
        """
        Unregister a workflow alias.

        Arguments:
        ---------
            alias: the alias

        Returns:
        -------
            whether an alias existed and was unregistered (True), or not (False)
        """
        try:
            self.get_workflow_id(workflow_alias=alias)
        except NoSuchWorkflowException:
            return False

        store_name = self.default_alias_store
        store: WorkflowStore = self.get_archive(archive_id=store_name)  # type: ignore

        result = store.unregister_alias(alias=alias)
        self.workflow_aliases.pop(alias)
        return result

    def register_workflow(
        self,
        workflow_metadata: Union[None, WorkflowMetadata, str] = None,
        workflow_aliases: Union[Iterable[str], None] = None,
    ) -> WorkflowMetadata:
        """
        Register a workflow.

        If no details are specified, a new WorkflowMetadata object will be created. If a string is provided, a new
        WorkflowMetadata object will be created that uses the string as documentation/description.

        Arguments:
        ---------
            workflow_metadata: the (optional) metadata of the workflow
            workflow_aliases: (optional) aliases to register the workflow under
        """
        if workflow_aliases:
            for workflow_alias in workflow_aliases:
                if workflow_alias in self.workflow_aliases.keys():
                    raise Exception(
                        f"Can't register workflow with alias '{workflow_alias}': alias already registered."
                    )

        store_name = self.default_alias_store
        store: WorkflowStore = self.get_archive(archive_id=store_name)  # type: ignore

        if workflow_metadata is None:
            _workflow_id = ID_REGISTRY.generate(comment="New workflow object.")
            workflow_metadata = WorkflowMetadata(workflow_id=_workflow_id)
            workflow_metadata._kiara = self._kiara
        elif isinstance(workflow_metadata, str):
            workflow_metadata = WorkflowMetadata(documentation=workflow_metadata)  # type: ignore
            workflow_metadata._kiara = self._kiara

        if self._all_workflow_ids is None:
            self.all_workflow_ids

        store.register_workflow(
            workflow_metadata=workflow_metadata, workflow_aliases=workflow_aliases
        )

        self._all_workflow_ids[workflow_metadata.workflow_id] = store_name  # type: ignore
        self._cached_workflow_metadata_items[workflow_metadata.workflow_id] = (
            workflow_metadata
        )

        if workflow_aliases:
            for workflow_alias in workflow_aliases:
                self._all_workflow_ids[workflow_metadata.workflow_id] = store_name  # type: ignore
                self.workflow_aliases[workflow_alias] = workflow_metadata.workflow_id

        return workflow_metadata

    def get_workflow_state(
        self,
        workflow_state_id: Union[str, None] = None,
        workflow: Union[None, uuid.UUID, str] = None,
    ) -> WorkflowState:

        if workflow is None and workflow_state_id is None:
            raise Exception(
                "Can't retrieve workflow state, neither workflow nor workflow state id specified."
            )

        if workflow:
            workflow_details = self.get_workflow_metadata(workflow=workflow)
            if workflow_state_id is None:
                workflow_state_id = workflow_details.current_state
            else:
                if workflow_state_id not in workflow_details.workflow_history.values():
                    raise Exception(
                        f"Can't retrieve workflow state '{workflow_state_id}' for workflow '{workflow}': state not registered for workflow."
                    )
        else:
            raise NotImplementedError()

        if workflow_state_id is None:
            raise Exception(
                f"Can't retrieve current workflow state, no state exists yet for workflow '{workflow}'."
            )

        if self._all_workflow_ids is None:
            self.all_workflow_ids
        archive_alias = self._all_workflow_ids[workflow_details.workflow_id]  # type: ignore

        archive = self.get_archive(archive_alias)
        if archive is None:
            raise Exception(
                f"Can't retrieve workflow archive '{archive_alias}', this is most likely a bug."
            )
        state = archive.retrieve_workflow_state(
            workflow_state_id=workflow_state_id,
        )
        state._kiara = self._kiara

        return state

    def get_all_states_for_workflow(
        self, workflow: Union[uuid.UUID, str]
    ) -> Mapping[str, WorkflowState]:

        workflow_details = self.get_workflow_metadata(workflow=workflow)

        if self._all_workflow_ids is None:
            self.all_workflow_ids
        archive_alias = self._all_workflow_ids[workflow_details.workflow_id]  # type: ignore

        archive = self.get_archive(archive_alias)
        if archive is None:
            raise Exception(
                f"Can't retrieve workflow archive '{archive_alias}', this is most likely a bug."
            )

        states = archive.retrieve_all_states_for_workflow(
            workflow_id=workflow_details.workflow_id
        )
        return states

    def add_workflow_state(
        self,
        workflow: Union[str, uuid.UUID],
        workflow_state: WorkflowState,
        timestamp: Union[None, datetime.datetime] = None,
        set_current: bool = True,
    ) -> WorkflowMetadata:

        workflow_details = self.get_workflow_metadata(workflow=workflow)

        if timestamp is None:
            timestamp = get_current_time_incl_timezone()

        if timestamp in workflow_details.workflow_history.keys():
            if (
                workflow_details.workflow_history[timestamp]
                != workflow_state.instance_id
            ):
                raise Exception(
                    f"Can't register workflow for timestamp '{timestamp}': timestamp already registered."
                )

        workflow_details.workflow_history[timestamp] = workflow_state.instance_id

        for field_name, value_id in workflow_state.inputs.items():
            self._kiara.data_registry.store_value(value=value_id)

        store_name = self.default_alias_store
        store: WorkflowStore = self.get_archive(archive_id=store_name)  # type: ignore

        store.add_workflow_state(workflow_state=workflow_state)
        if set_current:
            workflow_details.current_state = workflow_state.instance_id

        store.update_workflow_metadata(workflow_details)

        return workflow_details

    def update_workflow_metadata(self, workflow_metadata: WorkflowMetadata):

        store_name = self.default_alias_store
        store: WorkflowStore = self.get_archive(archive_id=store_name)  # type: ignore

        store.update_workflow_metadata(workflow_metadata=workflow_metadata)


# kiara\kiara\src\kiara\renderers\jinja.py
# -*- coding: utf-8 -*-
import abc
from typing import TYPE_CHECKING, Any, Generic, Mapping, Union

from jinja2 import Environment, Template
from pydantic import BaseModel, Field, PrivateAttr

from kiara.exceptions import KiaraException
from kiara.renderers import (
    INPUTS_SCHEMA,
    SOURCE_TYPE,
    KiaraRenderer,
    KiaraRendererConfig,
    RenderInputsSchema,
)

if TYPE_CHECKING:
    from kiara.context import Kiara
    from kiara.registries.rendering import RenderRegistry


class JinjaEnv(BaseModel):

    template_base: Union[str, None] = Field(
        description="The alias of the base loader to use. Defaults to a special loader that combines all template sources.",
        default=None,
    )

    _render_registry: "RenderRegistry" = PrivateAttr(default=None)

    @property
    def instance(self) -> Environment:

        return self._render_registry.retrieve_jinja_env(self.template_base)


class JinjaRndererConfig(KiaraRendererConfig):

    env: JinjaEnv = Field(description="The loader to use for the jinja environment.")


class BaseJinjaRenderer(
    KiaraRenderer[SOURCE_TYPE, INPUTS_SCHEMA, str, JinjaRndererConfig],
    Generic[SOURCE_TYPE, INPUTS_SCHEMA],
):

    _renderer_config_cls = JinjaRndererConfig

    def __init__(
        self,
        kiara: "Kiara",
        renderer_config: Union[None, Mapping[str, Any], KiaraRendererConfig] = None,
    ):

        self._jinja_env: Union[Environment, None] = None
        super().__init__(kiara=kiara, renderer_config=renderer_config)

    def get_jinja_env(self) -> Environment:

        if self._jinja_env is None:
            je = self.retrieve_jinja_env()
            je._render_registry = self._kiara.render_registry
            self._jinja_env = je.instance
        return self._jinja_env

    def retrieve_jinja_env(self) -> JinjaEnv:
        return JinjaEnv()

    @abc.abstractmethod
    def get_template(self, render_config: INPUTS_SCHEMA) -> Template:
        pass

    @abc.abstractmethod
    def assemble_render_inputs(
        self, instance: Any, render_config: INPUTS_SCHEMA
    ) -> Mapping[str, Any]:
        pass

    def _render(self, instance: SOURCE_TYPE, render_config: INPUTS_SCHEMA) -> str:

        template = self.get_template(render_config=render_config)
        if template is None:
            msg = "Available templates:\n"
            for template_name in self.get_jinja_env().list_templates():
                msg += f" - {template_name}\n"
            raise KiaraException(msg=f"Could not find requested template for renderer '{self.__class__._renderer_name}'", details=msg)  # type: ignore

        inputs = self.assemble_render_inputs(instance, render_config=render_config)
        rendered: str = template.render(**inputs)
        return rendered


class JinjaRenderInputsSchema(RenderInputsSchema):
    template: str = Field(description="The template to use for rendering.")


class JinjaRenderer(BaseJinjaRenderer[Any, JinjaRenderInputsSchema]):

    _renderer_name = "jinja"
    _inputs_schema_cls = JinjaRenderInputsSchema

    def get_template(self, render_config: JinjaRenderInputsSchema) -> Template:

        try:
            template = self.get_jinja_env().get_template(render_config.template)
            return template
        except Exception as e:
            raise Exception(f"Error loading template '{render_config.template}': {e}")


# kiara\kiara\src\kiara\renderers\__init__.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2022, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import abc
import inspect
from typing import (
    TYPE_CHECKING,
    Any,
    Generic,
    Iterable,
    List,
    Mapping,
    Set,
    Type,
    TypeVar,
    Union,
)

from jinja2 import TemplateNotFound

from kiara.exceptions import KiaraException
from kiara.models import KiaraModel
from kiara.models.documentation import DocumentationMetadataModel

if TYPE_CHECKING:
    from kiara.context import Kiara


class KiaraRendererConfig(KiaraModel):
    pass


class RenderInputsSchema(KiaraModel):
    pass


RENDERER_CONFIG = TypeVar("RENDERER_CONFIG", bound=KiaraRendererConfig)
SOURCE_TYPE = TypeVar("SOURCE_TYPE")
INPUTS_SCHEMA = TypeVar("INPUTS_SCHEMA", bound=RenderInputsSchema)
TARGET_TYPE = TypeVar("TARGET_TYPE")


class SourceTransformer(Generic[SOURCE_TYPE]):
    def __init__(self) -> None:
        self._doc: Union[DocumentationMetadataModel, None] = None

    @abc.abstractmethod
    def retrieve_supported_python_classes(self) -> Iterable[Type]:
        pass

    @abc.abstractmethod
    def validate_and_transform(self, source: Any) -> Union[SOURCE_TYPE, None]:
        pass

    @abc.abstractmethod
    def retrieve_supported_inputs_descs(self) -> Union[str, Iterable[str]]:
        pass


class NoOpSourceTransformer(SourceTransformer):
    def retrieve_supported_python_classes(self) -> Iterable[Type]:
        return [object]

    def validate_and_transform(self, source: Any) -> Any:
        return source

    def retrieve_supported_inputs_descs(self) -> Union[str, Iterable[str]]:
        return "any Python input, unchecked"


class KiaraRenderer(
    abc.ABC, Generic[SOURCE_TYPE, INPUTS_SCHEMA, TARGET_TYPE, RENDERER_CONFIG]
):

    _renderer_config_cls: Type[RENDERER_CONFIG] = KiaraRendererConfig  # type: ignore
    _inputs_schema: Type[INPUTS_SCHEMA] = RenderInputsSchema  # type: ignore

    def __init__(
        self,
        kiara: "Kiara",
        renderer_config: Union[None, Mapping[str, Any], KiaraRendererConfig] = None,
    ):

        self._kiara: "Kiara" = kiara
        self._source_transformers: Union[None, Iterable[SourceTransformer]] = None
        self._doc: Union[DocumentationMetadataModel, None] = None
        self._supported_inputs_desc: Union[None, Iterable[str]] = None

        if renderer_config is None:
            self._config: RENDERER_CONFIG = self.__class__._renderer_config_cls()
        elif isinstance(renderer_config, Mapping):
            self._config = self.__class__._renderer_config_cls(**renderer_config)
        elif not isinstance(renderer_config, self.__class__._renderer_config_cls):
            raise Exception(
                f"Can't create renderer instance, invalid config type: {type(renderer_config)}, must be: {self.__class__._renderer_config_cls.__name__}"
            )
        else:
            self._config = renderer_config

    @property
    def renderer_config(self) -> RENDERER_CONFIG:
        return self._config

    @property
    def supported_inputs_descs(self) -> Iterable[str]:

        if self._supported_inputs_desc is not None:
            return self._supported_inputs_desc

        transformers: List[str] = []
        for transformer in self.source_transformers:
            descs = transformer.retrieve_supported_inputs_descs()
            if isinstance(descs, str):
                descs = [descs]
            transformers.extend(descs)
        return transformers

    def retrieve_doc(self) -> Union[str, None]:
        return None

    @property
    def doc(self) -> DocumentationMetadataModel:
        if self._doc is not None:
            return self._doc

        doc = self.retrieve_doc()
        if doc is None:
            doc = self.__class__.__doc__
            if not doc:
                doc = ""
            doc = f"{inspect.cleandoc(doc)}\n\n"

        transformers_list = (
            "## Supported inputs:\n\nThis renderer supports the following inputs:\n\n"
        )
        for transformer in self.supported_inputs_descs:
            transformers_list += f"- {transformer}\n"

        doc = f"{doc}\n\n{transformers_list}"

        self._doc = DocumentationMetadataModel.create(doc)
        return self._doc

    @property
    def source_transformers(self) -> Iterable[SourceTransformer]:
        if self._source_transformers is None:
            self._source_transformers = self.retrieve_source_transformers()
        return self._source_transformers

    def retrieve_source_transformers(self) -> Iterable[SourceTransformer]:
        return [NoOpSourceTransformer()]

    def retrieve_supported_python_classes(self) -> Set[Type]:
        """Retrieve the set of Python classes that this renderer supports as inputs."""
        result: Set[Type] = set()
        for x in self.source_transformers:
            result.update(x.retrieve_supported_python_classes())
        return result

    def get_renderer_alias(self) -> str:
        return self.__class__._renderer_name  # type: ignore

    @abc.abstractmethod
    def retrieve_supported_render_sources(self) -> Union[Iterable[str], str]:
        pass

    @abc.abstractmethod
    def retrieve_supported_render_targets(self) -> Union[Iterable[str], str]:
        pass

    @abc.abstractmethod
    def _render(
        self, instance: SOURCE_TYPE, render_config: INPUTS_SCHEMA
    ) -> TARGET_TYPE:
        pass

    def _post_process(self, rendered: TARGET_TYPE) -> TARGET_TYPE:
        return rendered

    def render(self, instance: SOURCE_TYPE, render_config: INPUTS_SCHEMA) -> Any:

        transformed = None
        for transformer in self.source_transformers:
            try:

                for cls in transformer.retrieve_supported_python_classes():
                    if isinstance(instance, cls):
                        transformed = transformer.validate_and_transform(instance)
                        if transformed is not None:
                            break
            except Exception as e:
                raise KiaraException("Error transforming source object.", parent=e)

        if not transformed:
            raise Exception(f"Can't transform input object: {instance}.")

        try:
            rendered: TARGET_TYPE = self._render(
                instance=transformed, render_config=render_config
            )
        except Exception as e:

            if isinstance(e, TemplateNotFound):
                details = f"Template not found: {e}"
                raise KiaraException("Error while rendering item.", details=details)
            else:
                raise e

        try:
            post_processed: TARGET_TYPE = self._post_process(rendered=rendered)
        except Exception as e:
            raise KiaraException("Error post-processing rendered item.", parent=e)

        return post_processed


# kiara\kiara\src\kiara\renderers\included_renderers\archive.py
# -*- coding: utf-8 -*-
from pathlib import Path
from typing import (
    TYPE_CHECKING,
    Any,
    Iterable,
    Mapping,
    Set,
    Type,
    Union,
)

from jinja2 import Template

from kiara.interfaces.python_api.models.archive import KiArchive
from kiara.models.module.pipeline.pipeline import Pipeline
from kiara.renderers import (
    RenderInputsSchema,
    SourceTransformer,
)
from kiara.renderers.jinja import BaseJinjaRenderer, JinjaEnv

if TYPE_CHECKING:
    from kiara.context import Kiara


class KiArchiveTransformer(SourceTransformer):
    def __init__(self, kiara: "Kiara"):
        self._kiara: "Kiara" = kiara

        super().__init__()

    def retrieve_supported_python_classes(self) -> Iterable[Type]:
        return [KiArchive, str, Path]

    def retrieve_supported_inputs_descs(self) -> Union[str, Iterable[str]]:
        return [
            "a KiaraArchive instance",
            "a path to a a kiara archive file",
        ]

    def validate_and_transform(self, source: Any) -> Union["KiArchive", None]:

        if isinstance(source, (str, Path)):
            archive: Union[KiArchive, None] = KiArchive.load_kiarchive(
                kiara=self._kiara, path=source
            )
        elif isinstance(source, KiArchive):
            archive = source
        else:
            archive = None

        return archive


class ArchiveRendererHtml(BaseJinjaRenderer[Type[Pipeline], RenderInputsSchema]):
    """Renders archive information as a static html page.

    This is a placeholder for now.
    """

    _renderer_name = "archive_html"

    # _render_profiles: Mapping[str, Mapping[str, Any]] = {"html": {}}

    def retrieve_supported_render_sources(cls) -> str:
        return "archive"

    def retrieve_supported_render_targets(self) -> Union[Set[str], str]:
        return "html"

    def retrieve_source_transformers(self) -> Iterable[SourceTransformer]:
        return [KiArchiveTransformer(kiara=self._kiara)]

    def retrieve_jinja_env(self) -> JinjaEnv:

        jinja_env = JinjaEnv(template_base="kiara")
        return jinja_env

    def get_template(self, render_config: RenderInputsSchema) -> Template:

        return self.get_jinja_env().get_template("archive/static_page/page.html.j2")

    def assemble_render_inputs(
        self, instance: Any, render_config: RenderInputsSchema
    ) -> Mapping[str, Any]:

        inputs = {
            "archive": instance,
        }
        return inputs


# kiara\kiara\src\kiara\renderers\included_renderers\job.py
# -*- coding: utf-8 -*-
from pathlib import Path
from typing import (
    TYPE_CHECKING,
    Any,
    Iterable,
    Mapping,
    Type,
    Union,
)

from jinja2 import Template

from kiara.models.module.pipeline.pipeline import Pipeline
from kiara.renderers import (
    RenderInputsSchema,
    SourceTransformer,
)
from kiara.renderers.jinja import BaseJinjaRenderer, JinjaEnv

if TYPE_CHECKING:
    from kiara.api import JobDesc
    from kiara.context import Kiara


class JobDescTransformer(SourceTransformer):
    def __init__(self, kiara: "Kiara"):
        self._kiara: "Kiara" = kiara
        super().__init__()

    def retrieve_supported_python_classes(self) -> Iterable[Type]:
        from kiara.api import JobDesc

        return [JobDesc, str, Mapping, Path]

    def retrieve_supported_inputs_descs(self) -> Union[str, Iterable[str]]:
        return [
            "a job description instance",
            "a path to a a job description file",
            "the job description as a Python dict",
        ]

    def validate_and_transform(self, source: Any) -> Union["JobDesc", None]:
        from kiara.api import JobDesc

        if isinstance(source, JobDesc):
            return source
        elif isinstance(source, (Path, str)):
            return JobDesc.create_from_file(source)
        elif isinstance(source, Mapping):
            return JobDesc.create_from_data(data=source)
        else:
            return None


class JobDescPythonScriptRenderer(BaseJinjaRenderer["JobDesc", RenderInputsSchema]):
    """Renders a simple executable python script from a job description.

    ## Examples

    ### Terminal

    Example invoication from the command line (using [this](https://github.com/DHARPA-Project/kiara_plugin.tabular/blob/develop/examples/jobs/init.yaml) job description):

    ```
    kiara render --source-type job_desc --target-type python_script item init.yaml
    python tables_from_csv_files.py
    ```

    ### Python API

    Example usage from the Python API:

    ``` python
    from kiara.api import KiaraAPI

    kiara = KiaraAPI.instance()

    job_desc = "<path_to_job_desc>.yaml"

    rendered = kiara.render(job_desc, source_type="job_desc", target_type="python_script")
    print(f"# Rendered python script for job '{job_desc}':")
    print(rendered)
    ```

    """

    _renderer_name = "job_to_python_script"
    _inputs_schema = RenderInputsSchema

    def retrieve_supported_render_sources(self) -> str:
        return "job_desc"

    def retrieve_supported_render_targets(cls) -> Union[Iterable[str], str]:
        return "python_script"

    def retrieve_source_transformers(self) -> Iterable[SourceTransformer]:
        return [JobDescTransformer(kiara=self._kiara)]

    def retrieve_jinja_env(self) -> JinjaEnv:

        jinja_env = JinjaEnv(template_base="kiara")
        return jinja_env

    def get_template(self, render_config: RenderInputsSchema) -> Template:

        return self.get_jinja_env().get_template("pipeline/python_script.py.j2")

    def assemble_render_inputs(
        self, instance: Any, render_config: RenderInputsSchema
    ) -> Mapping[str, Any]:

        from kiara.utils.rendering import create_pipeline_render_inputs

        job_desc: JobDesc = instance

        pipeline = Pipeline.create_pipeline(
            kiara=self._kiara, pipeline=job_desc.operation
        )

        result = create_pipeline_render_inputs(pipeline, job_desc.inputs)
        return result


# kiara\kiara\src\kiara\renderers\included_renderers\pipeline.py
# -*- coding: utf-8 -*-
from typing import (
    TYPE_CHECKING,
    Any,
    Dict,
    Iterable,
    Literal,
    Mapping,
    MutableMapping,
    Set,
    Type,
    Union,
)

from jinja2 import Template
from pydantic import Field

from kiara.defaults import KIARA_DEFAULT_STAGES_EXTRACTION_TYPE
from kiara.models.module.pipeline import PipelineConfig
from kiara.models.module.pipeline.pipeline import Pipeline
from kiara.models.module.pipeline.structure import PipelineStructure
from kiara.renderers import (
    KiaraRenderer,
    KiaraRendererConfig,
    RenderInputsSchema,
    SourceTransformer,
)
from kiara.renderers.jinja import BaseJinjaRenderer, JinjaEnv
from kiara.utils.graphs import create_image

if TYPE_CHECKING:
    from kiara.context import Kiara


class PipelineTransformer(SourceTransformer):
    def __init__(self, kiara: "Kiara"):
        self._kiara: "Kiara" = kiara
        super().__init__()

    def retrieve_supported_python_classes(self) -> Iterable[Type]:
        return [PipelineConfig, Pipeline, PipelineStructure, str, Mapping]

    def retrieve_supported_inputs_descs(self) -> Union[str, Iterable[str]]:
        return [
            "a registered pipeline operation",
            "a path to a pipeline file",
            "the pipeline configuration as a Python dict",
        ]

    def validate_and_transform(self, source: Any) -> Union[Pipeline, None]:

        if isinstance(source, Pipeline):
            return source
        elif isinstance(source, (PipelineConfig, PipelineStructure, Mapping, str)):
            pipeline = Pipeline.create_pipeline(kiara=self._kiara, pipeline=source)
            return pipeline
        else:
            return None


class PipelineRendererHtml(BaseJinjaRenderer[Type[Pipeline], RenderInputsSchema]):
    """Renders a pipeline structure as static html page."""

    _renderer_name = "pipeline_html"

    # _render_profiles: Mapping[str, Mapping[str, Any]] = {"html": {}}

    def retrieve_supported_render_sources(cls) -> str:
        return "pipeline"

    def retrieve_supported_render_targets(self) -> Union[Set[str], str]:
        return "html"

    def retrieve_source_transformers(self) -> Iterable[SourceTransformer]:
        return [PipelineTransformer(kiara=self._kiara)]

    def retrieve_jinja_env(self) -> JinjaEnv:

        jinja_env = JinjaEnv(template_base="kiara")
        return jinja_env

    def get_template(self, render_config: RenderInputsSchema) -> Template:

        return self.get_jinja_env().get_template("pipeline/static_page/page.html.j2")

    def assemble_render_inputs(
        self, instance: Any, render_config: RenderInputsSchema
    ) -> Mapping[str, Any]:

        inputs = render_config.model_dump()
        inputs["pipeline"] = instance
        return inputs


class PipelineRendererMarkdown(BaseJinjaRenderer[Type[Pipeline], RenderInputsSchema]):
    """Renders a pipeline structure as static html page."""

    _renderer_name = "pipeline_markdown"

    # _render_profiles: Mapping[str, Mapping[str, Any]] = {"html": {}}

    def retrieve_supported_render_sources(cls) -> str:
        return "pipeline"

    def retrieve_supported_render_targets(self) -> Union[Set[str], str]:
        return "markdown"

    def retrieve_source_transformers(self) -> Iterable[SourceTransformer]:
        return [PipelineTransformer(kiara=self._kiara)]

    def retrieve_jinja_env(self) -> JinjaEnv:

        jinja_env = JinjaEnv(template_base="kiara")
        return jinja_env

    def get_template(self, render_config: RenderInputsSchema) -> Template:

        return self.get_jinja_env().get_template("pipeline/markdown/pipeline.md.j2")

    def assemble_render_inputs(
        self, instance: Any, render_config: RenderInputsSchema
    ) -> Mapping[str, Any]:

        inputs = render_config.model_dump()
        inputs["pipeline"] = instance
        return inputs


class PipelineRendererPngConfig(KiaraRendererConfig):

    graph_type: Literal["execution", "data-flow", "data-flow-simple", "stages"] = Field(
        description="The type of graph to render."
    )


class PipelineInputsSchema(RenderInputsSchema):

    stages_extraction_type: str = Field(
        description="The type of stages extraction to use.",
        default=KIARA_DEFAULT_STAGES_EXTRACTION_TYPE,
    )


class PipelineRendererPng(
    KiaraRenderer[Pipeline, PipelineInputsSchema, bytes, PipelineRendererPngConfig]
):

    _renderer_name = "pipeline_png"
    _renderer_config_cls = PipelineRendererPngConfig  # type: ignore
    _inputs_schema = PipelineInputsSchema

    _renderer_profiles: Mapping[str, Mapping[str, Any]] = {
        "execution-graph-image": {"graph_type": "execution"},
        "data-flow-graph-image": {"graph_type": "data-flow"},
        "data-flow-simple-graph-image": {"graph_type": "data-flow-simple"},
        "stages-graph-image": {"graph_type": "stages"},
    }

    def retrieve_doc(self) -> Union[str, None]:

        graph_type = self.renderer_config.graph_type

        if graph_type == "execution":
            graph = "the execution graph"
        elif graph_type == "data-flow":
            graph = "the data flow graph"
        elif graph_type == "data-flow-simple":
            graph = "a simplified version of the data flow graph"
        elif graph_type == "stages":
            graph = "the stages graph"
        else:
            raise Exception(f"Invalid graph type: {graph_type}")

        return f"Render {graph} of the given pipeline as a image (in png format)."

    def retrieve_source_transformers(self) -> Iterable[SourceTransformer]:
        return [PipelineTransformer(kiara=self._kiara)]

    def get_renderer_alias(self) -> str:
        return f"{self.renderer_config.graph_type}_png"

    def retrieve_supported_render_sources(self) -> str:
        return "pipeline"

    def retrieve_supported_render_targets(self) -> Union[Iterable[str], str]:
        return f"{self.renderer_config.graph_type}_png"

    def _render(self, instance: Pipeline, render_config: PipelineInputsSchema) -> bytes:

        graph_type = self.renderer_config.graph_type

        if graph_type == "execution":
            graph = instance.structure.execution_graph
        elif graph_type == "data-flow":
            graph = instance.structure.data_flow_graph
        elif graph_type == "data-flow-simple":
            graph = instance.structure.data_flow_graph_simple
        elif graph_type == "stages":
            graph = instance.structure.get_stages_graph(
                stages_extraction_type=render_config.stages_extraction_type
            )
        else:
            raise Exception(f"Invalid graph type: {graph_type}")

        image = create_image(graph)
        return image


class PipelineInfoRenderer(BaseJinjaRenderer[Pipeline, RenderInputsSchema]):
    """Renders a basic text file containing pipeline details from a pipeline.

    This is mostly used for debugging and as a base template to show how this is done.
    """

    _renderer_name = "pipeline_info"

    def retrieve_supported_render_sources(self) -> str:
        return "pipeline"

    def retrieve_supported_render_targets(cls) -> Union[Iterable[str], str]:
        return "pipeline_info"

    def retrieve_source_transformers(self) -> Iterable[SourceTransformer]:
        return [PipelineTransformer(kiara=self._kiara)]

    def retrieve_jinja_env(self) -> JinjaEnv:

        jinja_env = JinjaEnv(template_base="kiara")
        return jinja_env

    def get_template(self, render_config: RenderInputsSchema) -> Template:

        return self.get_jinja_env().get_template("pipeline/pipeline_info.md.j2")

    def assemble_render_inputs(
        self, instance: Any, render_config: RenderInputsSchema
    ) -> Mapping[str, Any]:

        inputs: MutableMapping[str, Any] = render_config.model_dump()
        inputs["pipeline"] = instance
        return inputs


class PythonScriptRenderInputSchema(RenderInputsSchema):
    inputs: Dict[str, Any] = Field(
        description="The pipeline inputs.", default_factory=dict
    )


class PipelinePythonScriptRenderer(
    BaseJinjaRenderer[Pipeline, PythonScriptRenderInputSchema]
):
    """Renders a simple executable python script from a pipeline.

        If the pipeline inputs have required inputs, you can either specify those in in the render config, or you have to edit the rendered Python in the places indicted with `<TODO_SET_INPUT>` before execution.

    ## Examples

    ### Terminal

    Example invoication from the command line (using [this](https://github.com/DHARPA-Project/kiara_plugin.tabular/blob/develop/examples/pipelines/tables_from_csv_files.yaml) pipeline):

    ```
    kiara render --source-type pipeline --target-type python_script item tables_from_csv_files.yaml inputs='{"path": "/home/markus/projects/kiara/kiara_plugin.tabular/examples/data/journals"}' > tables_from_csv_files.py

    python tables_from_csv_files.py
    ```

    ### Python API

    Example usage from the Python API:

    ``` python
    from kiara.api import KiaraAPI

    kiara = KiaraAPI.instance()

    pipeline = "logic.xor"  # any valid pipeline operation (or reference to one)
    pipeline_inputs = {
        "a": True,
        "b": False,
    }
    rendered = kiara.render(pipeline, source_type="pipeline", target_type="python_script", render_config={"inputs": pipeline_inputs})
    print("# Rendered python script for pipeline 'logic.xor':")
    print(rendered)
    ```

    """

    _renderer_name = "pipeline_to_python_script"
    _inputs_schema = PythonScriptRenderInputSchema

    def retrieve_supported_render_sources(self) -> str:
        return "pipeline"

    def retrieve_supported_render_targets(cls) -> Union[Iterable[str], str]:
        return "python_script"

    def retrieve_source_transformers(self) -> Iterable[SourceTransformer]:
        return [PipelineTransformer(kiara=self._kiara)]

    def retrieve_jinja_env(self) -> JinjaEnv:

        jinja_env = JinjaEnv(template_base="kiara")
        return jinja_env

    def get_template(self, render_config: PythonScriptRenderInputSchema) -> Template:

        return self.get_jinja_env().get_template("pipeline/python_script.py.j2")

    def assemble_render_inputs(
        self, instance: Any, render_config: PythonScriptRenderInputSchema
    ) -> Mapping[str, Any]:

        from kiara.utils.rendering import create_pipeline_render_inputs

        pipeline: Pipeline = instance
        pipeline_user_inputs: Mapping[str, Any] = render_config.inputs
        result = create_pipeline_render_inputs(pipeline, pipeline_user_inputs)
        return result


# kiara\kiara\src\kiara\renderers\included_renderers\value.py
# -*- coding: utf-8 -*-
import uuid
from typing import TYPE_CHECKING, Any, Iterable, Mapping, Type, Union

from pydantic import Field

from kiara.models.rendering import RenderValueResult
from kiara.models.values.value import Value
from kiara.operations.included_core_operations.render_value import (
    RenderValueOperationType,
)
from kiara.renderers import (
    KiaraRenderer,
    KiaraRendererConfig,
    RenderInputsSchema,
    SourceTransformer,
)

if TYPE_CHECKING:
    from kiara.context import Kiara


class ValueTransformer(SourceTransformer):
    def __init__(self, kiara: "Kiara", target_type: str):

        self._kiara: Kiara = kiara
        self._target_type: str = target_type
        super().__init__()

    def retrieve_supported_python_classes(self) -> Iterable[Type]:

        return [Value, uuid.UUID, str]

    def validate_and_transform(self, source: Any) -> Union[Value, None]:

        return self._kiara.data_registry.get_value(source)

    def retrieve_supported_inputs_descs(self) -> Union[str, Iterable[str]]:
        op_type: RenderValueOperationType = self._kiara.operation_registry.get_operation_type("render_value")  # type: ignore
        ops = op_type.get_render_operations_for_target_type(
            target_type=self._target_type
        )

        return [
            f"a value of type '{x}'"
            for x in sorted(ops.keys())
            if x not in ["any", "none"]
        ]


class ValueRenderInputsSchema(RenderInputsSchema):
    render_config: Mapping[str, Any] = Field(
        description="The render config data.", default_factory=dict
    )
    include_metadata: bool = Field(
        description="Whether to include metadata.", default=False
    )
    include_data: bool = Field(
        description="Whether to include data (only applies when 'include_metadata' is set to 'True').",
        default=True,
    )


class ValueRendererConfig(KiaraRendererConfig):
    target_type: str = Field(description="The target type to render the value as.")


class ValueRenderer(
    KiaraRenderer[
        Value, ValueRenderInputsSchema, RenderValueResult, ValueRendererConfig
    ]
):
    _renderer_name = "value_renderer"
    _inputs_schema = ValueRenderInputsSchema
    _renderer_config_cls = ValueRendererConfig

    # _render_profiles: Mapping[str, Mapping[str, Any]] = {
    #     "terminal_renderable": {"target_type": "terminal_renderable"},
    #     "string": {"target_type": "string"},
    # }

    def get_renderer_alias(self) -> str:
        return f"value_to_{self.renderer_config.target_type}"

    def retrieve_supported_render_sources(self) -> str:
        return "value"

    def retrieve_supported_render_targets(self) -> Union[Iterable[str], str]:
        return f"value:{self.renderer_config.target_type}"

    def retrieve_source_transformers(self) -> Iterable[SourceTransformer]:
        return [
            ValueTransformer(
                kiara=self._kiara, target_type=self.renderer_config.target_type
            )
        ]

    def retrieve_doc(self) -> Union[str, None]:

        return f"Render a value (of a supported type) to a value of type '{self.renderer_config.target_type}'."

    def _render(self, instance: Value, render_config: ValueRenderInputsSchema) -> Any:

        target_type = self.renderer_config.target_type
        op_type: RenderValueOperationType = (
            self._kiara.operation_registry.get_operation_type("render_value")
        )  # type: ignore
        render_op = op_type.get_render_operation(
            source_type=instance.data_type_name, target_type=target_type
        )
        if render_op is None:
            raise Exception(
                f"Can't find render operation for source type '{instance.data_type_name}' to '{target_type}'."
            )

        result: Any = render_op.run(
            self._kiara,
            inputs={"value": instance, "render_config": render_config.render_config},
        )
        rendered: RenderValueResult = result["render_value_result"].data  # type: ignore

        if not render_config.include_metadata:
            result = rendered.rendered
        else:
            if not render_config.include_data:
                rendered.rendered = None
            result = rendered

        return result


# kiara\kiara\src\kiara\renderers\included_renderers\__init__.py


# kiara\kiara\src\kiara\renderers\included_renderers\api\base_api.py
# -*- coding: utf-8 -*-
import abc
import re
from pathlib import Path
from typing import (
    TYPE_CHECKING,
    Any,
    Dict,
    Iterable,
    Mapping,
    Set,
    Union,
)

from pydantic.fields import Field

from kiara.interfaces.python_api.base_api import BaseAPI
from kiara.interfaces.python_api.proxy import ApiEndpoints
from kiara.models.rendering import RenderValueResult
from kiara.renderers import (
    KiaraRenderer,
    KiaraRendererConfig,
    RenderInputsSchema,
)
from kiara.utils.cli import terminal_print
from kiara.utils.introspection import (
    create_signature_string,
    extract_arg_names,
    extract_proxy_arg_str,
)
from kiara.utils.output import (
    create_table_from_base_model_v1_cls,
)

if TYPE_CHECKING:
    from kiara.context import Kiara


class BaseApiRenderInputsSchema(RenderInputsSchema):

    pass


class BaseApiRendererConfig(KiaraRendererConfig):

    tags: Union[None, str, Iterable[str]] = Field(
        description="The tag to filter the api endpoints by (if any tag matches, the endpoint will be included.",
        default="kiara_api",
    )
    filter: Union[str, Iterable[str], None] = Field(
        description="One or a list of filter tokens -- if provided -- all of which must match for the api endpoing to be in the render result.",
        default=None,
    )


class BaseApiRenderer(
    KiaraRenderer[
        BaseAPI, BaseApiRenderInputsSchema, RenderValueResult, BaseApiRendererConfig
    ]
):
    _inputs_schema = BaseApiRenderInputsSchema
    _renderer_config_cls = BaseApiRendererConfig

    def __init__(
        self,
        kiara: "Kiara",
        renderer_config: Union[None, Mapping[str, Any], KiaraRendererConfig] = None,
    ):

        super().__init__(kiara=kiara, renderer_config=renderer_config)

        filters = self.renderer_config.filter
        tags = self.renderer_config.tags

        self._api_endpoints: ApiEndpoints = ApiEndpoints(
            api_cls=BaseAPI, filters=filters, include_tags=tags
        )

    @property
    def api_endpoints(self) -> ApiEndpoints:
        return self._api_endpoints

    def get_renderer_alias(self) -> str:
        return f"api_to_{self.get_target_type()}"

    def retrieve_supported_render_sources(self) -> str:
        return "base_api"

    def retrieve_doc(self) -> Union[str, None]:

        return f"Render the kiara base API to: '{self.get_target_type()}'."

    @abc.abstractmethod
    def get_target_type(self) -> str:
        pass


class BaseApiDocRenderer(BaseApiRenderer):

    _renderer_name = "base_api_doc_renderer"

    def get_target_type(self) -> str:
        return "html"

    def retrieve_supported_render_targets(self) -> Union[Iterable[str], str]:
        return "html"

    def _render(
        self, instance: BaseAPI, render_config: BaseApiRenderInputsSchema
    ) -> Any:

        # details = self.api_endpoints.get_api_endpoint("get_value")
        details = self.api_endpoints.get_api_endpoint("retrieve_aliases_info")

        # for k, v in details.arg_schema.items():
        #     print(k)
        #     print(type(v))

        terminal_print(create_table_from_base_model_v1_cls(details.arg_model))

        return "xxx"


class BaseApiDocTextRenderer(BaseApiRenderer):

    _renderer_name = "base_api_doc_markdown_renderer"

    def get_target_type(self) -> str:
        return "markdown"

    def retrieve_supported_render_targets(self) -> Union[Iterable[str], str]:
        return "markdown"

    def _render(
        self, instance: BaseAPI, render_config: BaseApiRenderInputsSchema
    ) -> Any:

        template = self._kiara.render_registry.get_template(
            "kiara_api/api_doc.md.j2", "kiara"
        )

        result = ""
        for ep in self.api_endpoints.api_endpint_names:
            doc = self.api_endpoints.get_api_endpoint(ep).doc
            rendered = template.render(endpoint_name=ep, doc=doc)
            result += f"{rendered}\n"

        # details = self.api_endpoints.get_api_endpoint("get_value")
        # dbg(details.validated_func.__dict__)

        # for k, v in details.arg_schema.items():
        #     print(k)
        #     print(type(v))

        # inputs = {
        #     "value": "tm.date_array"
        # }
        # result = details.execute(instance, **inputs)
        # print(result)

        return result


class BaseApiRenderKiaraApiInputsSchema(BaseApiRenderInputsSchema):

    template_file: str = Field(
        description="The file that should contain the rendered code."
    )
    target_file: Union[str, None] = Field(
        description="The file to write the rendered code to.", default=None
    )


class BaseToKiaraApiRenderer(BaseApiRenderer):

    _renderer_name = "base_api_kiara_api_renderer"
    _inputs_schema = BaseApiRenderKiaraApiInputsSchema  # type: ignore
    _renderer_config_cls = BaseApiRendererConfig

    def __init__(
        self,
        kiara: "Kiara",
        renderer_config: Union[None, Mapping[str, Any], KiaraRendererConfig] = None,
    ):

        self._api_endpoints: ApiEndpoints = ApiEndpoints(api_cls=BaseAPI)
        super().__init__(kiara=kiara, renderer_config=renderer_config)

    def get_target_type(self) -> str:
        return "kiara_api"

    def retrieve_supported_render_targets(self) -> Union[Iterable[str], str]:
        return "kiara_api"

    def _render(
        self, instance: BaseAPI, render_config: BaseApiRenderInputsSchema
    ) -> Any:

        assert isinstance(render_config, BaseApiRenderKiaraApiInputsSchema)

        template_file = Path(render_config.template_file)

        if not template_file.is_file():
            raise ValueError(f"File '{template_file}' does not exist.")

        BEGIN_IMPORTS_MARKER = "# BEGIN AUTO-GENERATED-IMPORTS"
        END_IMPORTS_MARKER = "# END AUTO-GENERATED-IMPORTS"
        BEGIN_MARKER = "# BEGIN IMPORTED-ENDPOINTS"
        END_MARKER = "# END IMPORTED-ENDPOINTS"

        template_file_content = template_file.read_text()
        if BEGIN_IMPORTS_MARKER not in template_file_content:
            raise ValueError(
                f"File '{template_file}' does not contain BEGIN_IMPORTS_MARKER '{BEGIN_IMPORTS_MARKER}'."
            )
        if END_IMPORTS_MARKER not in template_file_content:
            raise ValueError(
                f"File '{template_file}' does not contain END_IMPORTS_MARKER '{END_IMPORTS_MARKER}'."
            )
        if BEGIN_MARKER not in template_file_content:
            raise ValueError(
                f"File '{template_file}' does not contain BEGIN_MARKER '{BEGIN_MARKER}'."
            )
        if END_MARKER not in template_file_content:
            raise ValueError(
                f"File '{template_file}' does not contain END_MARKER '{END_MARKER}'."
            )

        endpoint_code_template = self._kiara.render_registry.get_template(
            "kiara_api/kiara_api_endpoint.py.j2", "kiara"
        )

        # tag = render_config.tag
        # endpoints = find_base_api_endpoints(BaseAPI, label=tag)

        endpoint_data = []
        imports: Dict[str, Set[str]] = {}
        imports.setdefault("typing", set()).add("Dict")
        imports.setdefault("typing", set()).add("Mapping")
        imports.setdefault("typing", set()).add("ClassVar")

        for endpoint_name in self.api_endpoints.api_endpint_names:
            endpoint_instance = self.api_endpoints.get_api_endpoint(endpoint_name)

            doc = endpoint_instance.raw_doc

            sig_args = extract_arg_names(endpoint_instance.func)
            sig_args.remove("self")

            arg_names_str = extract_proxy_arg_str(endpoint_instance.func)

            sig_string, return_type = create_signature_string(
                endpoint_instance.func, imports=imports
            )
            regex_str = ""
            if "\\" in doc:
                regex_str = "r"
            endpoint_data.append(
                {
                    "endpoint_name": endpoint_name,
                    "doc": doc.strip(),
                    "signature_str": sig_string,
                    "arg_names_str": arg_names_str,
                    "result_type": return_type,
                    "regex_str": regex_str,
                }
            )

        # remove abc modules
        imports.pop("collections.abc", None)

        result = ""
        for endpoint_item in endpoint_data:

            rendered = endpoint_code_template.render(**endpoint_item)
            result += f"{rendered}\n"

        result = f"{BEGIN_MARKER}\n{result}    {END_MARKER}"

        pattern = rf"{BEGIN_MARKER}.*?{END_MARKER}"
        new_content = re.sub(pattern, result, template_file_content, flags=re.DOTALL)

        TYPE_CHECKING_FILTER = ["typing", "builtins", "collections", "uuid", "pathlib"]

        imports.setdefault("typing", set()).add("TYPE_CHECKING")

        imports_str = ""
        for module, items in imports.items():
            first_token = module.split(".")[0]
            if first_token in TYPE_CHECKING_FILTER:
                items_str = ", ".join(sorted(items))
                imports_str += f"from {module} import {items_str}\n"

        match = False
        imports_str += "if TYPE_CHECKING:\n"
        for module, items in imports.items():
            first_token = module.split(".")[0]
            if first_token not in TYPE_CHECKING_FILTER:
                match = True
                items_str = ", ".join(sorted(items))
                imports_str += f"    from {module} import {items_str}\n"

        if not match:
            imports_str += "    pass\n"

        imports_pattern = rf"{BEGIN_IMPORTS_MARKER}\n.*?{END_IMPORTS_MARKER}"
        new_content = re.sub(
            imports_pattern,
            f"{BEGIN_IMPORTS_MARKER}\n{imports_str}\n{END_IMPORTS_MARKER}",
            new_content,
            flags=re.DOTALL,
        )

        try_formatting = False
        try:
            import black

            try_formatting = True
        except ImportError:
            pass

        if try_formatting:
            try:
                new_content = black.format_str(new_content, mode=black.FileMode())
            except Exception as e:
                raise ValueError(f"Failed to format code: {e}")

        if render_config.target_file:
            target_file = Path(render_config.target_file)
            target_file.parent.mkdir(parents=True, exist_ok=True)
            target_file.write_text(new_content)
            terminal_print()
            terminal_print(f"Rendered api to file '{target_file}'.")
        else:
            return new_content


# kiara\kiara\src\kiara\renderers\included_renderers\api\kiara_api.py
# -*- coding: utf-8 -*-
import abc
from typing import (
    TYPE_CHECKING,
    Any,
    Iterable,
    Mapping,
    Union,
)

from pydantic.fields import Field

from kiara.api import KiaraAPI
from kiara.interfaces.python_api.proxy import ApiEndpoints
from kiara.models.rendering import RenderValueResult
from kiara.renderers import (
    KiaraRenderer,
    KiaraRendererConfig,
    RenderInputsSchema,
)
from kiara.utils.cli import terminal_print
from kiara.utils.output import (
    create_table_from_base_model_v1_cls,
)

if TYPE_CHECKING:
    from kiara.context import Kiara


class ApiRenderInputsSchema(RenderInputsSchema):

    pass


class ApiRendererConfig(KiaraRendererConfig):

    filter: Union[str, Iterable[str]] = Field(
        description="One or a list of filter tokens -- if provided -- all of which must match for the api endpoing to be in the render result.",
        default_factory=list,
    )
    # target_type: str = Field(description="The target type to render the api as.")


class ApiRenderer(
    KiaraRenderer[KiaraAPI, ApiRenderInputsSchema, RenderValueResult, ApiRendererConfig]
):
    _inputs_schema = ApiRenderInputsSchema
    _renderer_config_cls = ApiRendererConfig

    def __init__(
        self,
        kiara: "Kiara",
        renderer_config: Union[None, Mapping[str, Any], KiaraRendererConfig] = None,
    ):

        super().__init__(kiara=kiara, renderer_config=renderer_config)

        filters: Union[None, str, Iterable[str]] = self.renderer_config.filter
        if not filters:
            filters = None
        elif isinstance(filters, str):
            filters = [filters]

        self._api_endpoints: ApiEndpoints = ApiEndpoints(
            api_cls=KiaraAPI, filters=filters
        )

    @property
    def api_endpoints(self) -> ApiEndpoints:
        return self._api_endpoints

    def get_renderer_alias(self) -> str:
        return f"api_to_{self.get_target_type()}"

    def retrieve_supported_render_sources(self) -> str:
        return "kiara_api"

    def retrieve_doc(self) -> Union[str, None]:

        return f"Render the kiara API endpoints to: '{self.get_target_type()}'."

    @abc.abstractmethod
    def get_target_type(self) -> str:
        pass


class KiaraApiDocRenderer(ApiRenderer):

    _renderer_name = "kiara_api_doc_renderer"

    def get_target_type(self) -> str:
        return "html"

    def retrieve_supported_render_targets(self) -> Union[Iterable[str], str]:
        return "html"

    def _render(self, instance: KiaraAPI, render_config: ApiRenderInputsSchema) -> Any:

        # details = self.api_endpoints.get_api_endpoint("get_value")
        details = self.api_endpoints.get_api_endpoint("retrieve_aliases_info")

        # for k, v in details.arg_schema.items():
        #     print(k)
        #     print(type(v))

        terminal_print(create_table_from_base_model_v1_cls(details.arg_model))

        return "xxx"


class KiaraApiDocTextRenderer(ApiRenderer):

    _renderer_name = "kiara_api_doc_markdown_renderer"

    def get_target_type(self) -> str:
        return "markdown"

    def retrieve_supported_render_targets(self) -> Union[Iterable[str], str]:
        return "markdown"

    def _render(self, instance: KiaraAPI, render_config: ApiRenderInputsSchema) -> Any:

        template = self._kiara.render_registry.get_template(
            "kiara_api/api_doc.md.j2", "kiara"
        )

        result = ""
        for ep in self.api_endpoints.api_endpint_names:
            doc = self.api_endpoints.get_api_endpoint(ep).doc
            rendered = template.render(endpoint_name=ep, doc=doc)
            result += f"{rendered}\n"

        # details = self.api_endpoints.get_api_endpoint("get_value")
        # dbg(details.validated_func.__dict__)

        # for k, v in details.arg_schema.items():
        #     print(k)
        #     print(type(v))

        # inputs = {
        #     "value": "tm.date_array"
        # }
        # result = details.execute(instance, **inputs)
        # print(result)

        return result


# kiara\kiara\src\kiara\renderers\included_renderers\api\__init__.py


# kiara\kiara\src\kiara\resources\templates\render\models\generic_model_info.html
{{ instance | render_model | safe }}


# kiara\kiara\src\kiara\resources\templates\render\models\info.operation.html
{% import 'kiara/render/models/macros.html' as model_macros %}

<table>
    <tr>
        <td>Documentation</td>
        <td>{{ instance.documentation.full_doc | markdown | safe }}</td>
    </tr>
    <tr>
        <td>Inputs</td>
        <td>{{ model_macros.field_infos(field_infos=instance.input_fields) }}</td>
    </tr>
    <tr>
        <td>Outputs</td>
        <td>{{ model_macros.field_infos(field_infos=instance.output_fields) }}</td>
    </tr>
</table>


# kiara\kiara\src\kiara\resources\templates\render\models\info.value.html
<table>
    <thead>
        <tr>
            <th colspan="2">Metadata for value: <i>{{ instance.value_id }}</i></th>
        </tr>
    </thead>
    <tbody>
        {% for k, v in instance.create_info_data().items() %}
        <tr>
            <td>{{ k }}</td>
            <td>{{ v }}</td>
        </tr>
        {% endfor %}
    </tbody>
</table>


# kiara\kiara\src\kiara\resources\templates\render\models\macros.html
{% macro values_schema(values_schema) %}
<table>
    <thead>
    <tr>
        <th>field name</th>
        <th>type</th>
        <th>description</th>
        <th>required</th>
        <th>default</th>
    </tr>
    </thead>
    <tbody>
    <tr>
{% for field_name, schema in values_schema.items() %}
<td>{{ field_name }}</td>
<td>{{ schema.type }}</td>
<td>{{ schema.doc.description }}</td>
<td>{{ schema.is_required() | render_bool }}</td>
<td>{{ schema.default | render_default }}</td>
    </tr>
{% endfor %}
    </tbody>
</table>
{% endmacro %}


{% macro field_infos(field_infos) %}
<table>
    <thead>
    <tr>
        <th>field name</th>
        <th>type</th>
        <th>description</th>
        <th>required</th>
        <th>default</th>
    </tr>
    </thead>
    <tbody>
    <tr>
{% for field_name, field_info in field_infos.items() %}
<td>{{ field_name }}</td>
<td>{{ field_info.field_schema.type }}</td>
<td>{{ field_info.field_schema.doc.description }}</td>
<td>{{ field_info.value_required| render_bool }}</td>
<td>{{ field_info.field_schema.default | render_default }}</td>
    </tr>
{% endfor %}
    </tbody>
</table>
{% endmacro %}


# kiara\kiara\src\kiara\resources\templates\render\models\metadata.authors.html
<ul>
{% for author in instance.authors %}
    <li>{{ author.name }}{% if author.email %} ( <a href="mailto:{{ author.email }}">{{ author.email }}</a> ){% endif %}</li>
{% endfor %}
</ul>


# kiara\kiara\src\kiara\resources\templates\render\models\metadata.context.html
<table xmlns="http://www.w3.org/1999/html">
    <tbody>
    <tr>
    <td>tags</td>
    <td>{{ instance.tags | join(',') }}</td>
    </tr>
    <tr>
        <td>labels</td>
        <td>
            <ul>
        {% for k, v in instance.labels.items() %}
        <li>{{ k }}: {{ v }}</li>
        {% endfor %}
            </ul>
        </td>
    </tr>
    <tr>
        <td>references</td>
        <td>
        <ul>
            {% for k, v in instance.references.items() %}
            <li><a href="{{ v.url }}">{{ k }}</a>{% if v.desc %}: {{ v.desc }}{% endif %}</li>
            {% endfor %}
        </ul>
        </td>
    </tr>
    </tbody>
</table>


# kiara\kiara\src\kiara\resources\templates\render\models\metadata.documentation.html
{{ instance.full_doc | markdown | safe }}


# kiara\kiara\src\kiara\resources\templates\render\models\model_data.html
<table>
    <thead>
        <tr>
            <th colspan="2">{{ instance.model_type_id }}: <i>{{ instance.instance_id }}</i></th>
        </tr>
    </thead>
    <tbody>
        {% for k, v in instance.create_info_data().items() %}
        <tr>
            <td>{{ k }}</td>
            <td>{{ v }}</td>
        </tr>
        {% endfor %}
    </tbody>
</table>


# kiara\kiara\src\kiara\resources\tui\pager_app.css
/*Screen {*/
/*    align: center middle;*/
/*}*/

/*DataPreview {*/
/*    height: 100%;*/
/*    !*height: 100%;*!*/
/*    !*padding: 1 2;*!*/
/*    background: $panel;*/
/*    color: $text;*/
/*    !*overflow: scroll;*!*/
/*    !*border: $secondary tall;*!*/
/*    !*content-align: center middle;*!*/
/*}*/

DataViewPane {
    /*overflow: scroll;*/
    /*background: blue;*/
    /*border: $secondary tall;*/
}
DataViewControl {
    dock: bottom;
    /*border: $secondary tall;*/
    padding: 1 0;
    /*background: red;*/
    /*height: 5;*/
}


# kiara\kiara\src\kiara\utils\archives.py
# -*- coding: utf-8 -*-
from functools import lru_cache
from typing import TYPE_CHECKING, Dict, Union

if TYPE_CHECKING:
    from kiara.context import Kiara
    from kiara.interfaces.python_api.models.info import TypeInfo
    from kiara.models.archives import ArchiveTypeClassesInfo


@lru_cache(maxsize=None)
def find_archive_types(
    alias: Union[str, None] = None, only_for_package: Union[str, None] = None
) -> "ArchiveTypeClassesInfo":

    from kiara.models.archives import ArchiveTypeClassesInfo
    from kiara.utils.class_loading import find_all_archive_types

    archive_types = find_all_archive_types()

    kiara: Kiara = None  # type: ignore
    group: ArchiveTypeClassesInfo = ArchiveTypeClassesInfo.create_from_type_items(  # type: ignore
        kiara=kiara, group_title=alias, **archive_types
    )

    if only_for_package:
        temp: Dict[str, TypeInfo] = {}
        for key, info in group.item_infos.items():
            if info.context.labels.get("package") == only_for_package:
                temp[key] = info  # type: ignore

        group = ArchiveTypeClassesInfo(
            group_id=group.group_id, group_title=group.group_alias, item_infos=temp  # type: ignore
        )

    return group


# kiara\kiara\src\kiara\utils\class_loading.py
# -*- coding: utf-8 -*-
import functools

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)
import importlib
import inspect
import logging
import os
import sys
from pkgutil import iter_modules
from types import ModuleType
from typing import (
    TYPE_CHECKING,
    Any,
    Callable,
    Dict,
    Iterable,
    List,
    Mapping,
    Tuple,
    Type,
    TypeVar,
    Union,
)

import structlog

from kiara.utils import (
    _get_all_subclasses,
    camel_case_to_snake_case,
    is_debug,
    is_develop,
    log_exception,
    log_message,
)

if TYPE_CHECKING:
    from click import Command

    from kiara.data_types import DataType
    from kiara.models import KiaraModel
    from kiara.modules import KiaraModule
    from kiara.operations import OperationType
    from kiara.registries import KiaraArchive
    from kiara.renderers import KiaraRenderer


logger = structlog.getLogger()

KiaraEntryPointItem = Union[Type, Tuple, Callable]
KiaraEntryPointIterable = Iterable[KiaraEntryPointItem]

SUBCLASS_TYPE = TypeVar("SUBCLASS_TYPE")


def _default_id_func(cls: Type) -> str:
    """Utility method to auto-generate a more or less nice looking id_or_alias for a class."""
    name = camel_case_to_snake_case(cls.__name__)
    path = cls.__module__

    if path.startswith("kiara_modules."):
        tokens = path.split(".")
        if len(tokens) == 2:
            path = tokens[1]
        else:
            path = ".".join(tokens[2:])

    if path:
        full_name = f"{path}.{name}"
    else:
        full_name = name
    return full_name


def _cls_name_id_func(cls: Type) -> str:
    """Utility method to auto-generate a more or less nice looking id_or_alias for a class."""
    name: str = camel_case_to_snake_case(cls.__name__)
    return name


def find_subclasses_under(
    base_class: Type[SUBCLASS_TYPE],
    python_module: Union[str, ModuleType],
) -> List[Type[SUBCLASS_TYPE]]:
    """
    Find all (non-abstract) subclasses of a base class that live under a module (recursively).

    Arguments:
    ---------
        base_class: the parent class
        python_module: the Python module to search

    Returns:
    -------
        a list of all subclasses
    """
    # if hasattr(sys, "frozen"):
    #     raise NotImplementedError("Pyinstaller bundling not supported yet.")

    try:
        if isinstance(python_module, str):
            python_module = importlib.import_module(python_module)

        _import_modules_recursively(python_module)
    except Exception as e:
        log_exception(e)
        log_message("ignore.python_module", module=str(python_module), reason=str(e))
        return []

    subclasses: Iterable[Type[SUBCLASS_TYPE]] = _get_all_subclasses(base_class)

    result = []
    for sc in subclasses:

        if not sc.__module__.startswith(python_module.__name__):
            continue

        result.append(sc)

    return result


def _process_subclass(
    sub_class: Type,
    base_class: Type,
    type_id_key: Union[str, None],
    type_id_func: Union[Callable, None],
    type_id_no_attach: bool,
    attach_python_metadata: Union[bool, str] = False,
    ignore_abstract_classes: bool = True,
    ignore_modules_with_null_module_name: bool = True,
) -> Union[str, None]:
    """
    Process subclasses of a base class that live under a module (recursively).

    Arguments:
    ---------
        base_class: the parent class
        python_module: the Python module to search
        ignore_abstract_classes: whether to include abstract classes in the result
        type_id_key: if provided, the found classes will have their id attached as an attribute, using the value of this as the name. if an attribute of this name already exists, it will be used as id without further processing
        type_id_func: a function to take the found class as input, and returns a string representing the id of the class. By default, the module path + "." + class name (snake-case) is used (minus the string 'kiara_modules.<project_name>'', if it exists at the beginning
        type_id_no_attach: in case you want to use the type_id_key to set the id, but don't want it attached to classes that don't have it, set this to true. In most cases, you won't need this option
        attach_python_metadata: whether to attach a [PythonClass][kiara.models.python_class.PythonClass] metadata model to the class. By default, '_python_class' is used as attribute name if this argument is 'True', If this argument is a string, that will be used as name instead.
        ignore_modules_with_null_module_name: ignore modules that have their '_module_type_name' attribute set to 'None', this is mostly useful to filter out base classes

    Returns:
    -------
        the type id
    """
    is_abstract = inspect.isabstract(sub_class)
    if ignore_abstract_classes and is_abstract:

        if sub_class.__dict__.get("_is_abstract", False):
            return None

        if is_develop():
            from kiara.modules import KiaraModule

            if base_class == KiaraModule and is_develop():
                missing = []
                abs_meth = sub_class.__abstractmethods__
                if "create_inputs_schema" in abs_meth:
                    missing.append("create_inputs_schema")
                if "create_outputs_schema" in abs_meth:
                    missing.append("create_outputs_schema")
                if not hasattr(sub_class, "process"):
                    missing.append("process")

                if missing:
                    name = f"{sub_class.__module__}.{sub_class.__name__}"
                    title = "Invalid kiara module"
                    if hasattr(sub_class, "_module_type_name"):
                        name = f"**{name}** ( *{sub_class._module_type_name}* )"  # type: ignore
                        title = f"{title} '[i]{sub_class._module_type_name}[/i]'"  # type: ignore
                    msg = f"Invalid kiara module: {name}\n\nMissing method(s):"
                    for m in missing:
                        msg = f"{msg}\n- *{m}*"

                    from rich.markdown import Markdown

                    from kiara.utils.develop import log_dev_message

                    log_dev_message(msg=Markdown(msg), title=title)

        log_message(
            "ignore.subclass",
            sub_class=f"{sub_class.__module__}.{sub_class.__name__}",
            base_class=f"{base_class.__module__}.{base_class.__name__}",
            reason="subclass is abstract",
        )
        return None

    if type_id_func is None:
        type_id_func = _default_id_func

    if type_id_key:

        if hasattr(sub_class, type_id_key):
            type_id: Union[str, None] = getattr(sub_class, type_id_key)
            if type_id is None and ignore_modules_with_null_module_name:
                log_message(
                    "ignore.subclass",
                    sub_class=f"{sub_class.__module__}.{sub_class.__name__}",
                    base_class=f"{base_class.__module__}.{base_class.__name__}",
                    reason=f"'{ type_id_key }' subclass is set to 'None'",
                )
                return None
            if not type_id and not is_abstract:
                raise Exception(
                    f"Class attribute '{type_id_key}' is 'None' for class '{sub_class.__name__}', this is not allowed."
                )
            elif not type_id:
                type_id = type_id_func(sub_class)
        else:
            type_id = type_id_func(sub_class)
            if not type_id_no_attach:
                setattr(sub_class, type_id_key, type_id)
    else:
        type_id = type_id_func(sub_class)

    if attach_python_metadata:
        from kiara.models.python_class import PythonClass

        pm_key = "_python_class"
        if isinstance(attach_python_metadata, str):
            pm_key = attach_python_metadata
        pc = PythonClass.from_class(sub_class)
        setattr(sub_class, pm_key, pc)

    return type_id


def load_all_subclasses_for_entry_point(
    entry_point_name: str,
    base_class: Type[SUBCLASS_TYPE],
    ignore_abstract_classes: bool = True,
    type_id_key: Union[str, None] = None,
    type_id_func: Union[Callable, None] = None,
    type_id_no_attach: bool = False,
    attach_python_metadata: Union[bool, str] = False,
) -> Dict[str, Type[SUBCLASS_TYPE]]:
    """
    Find all subclasses of a base class via package entry points.

    Arguments:
    ---------
        entry_point_name: the entry point name to query entries for
        base_class: the base class to look for
        ignore_abstract_classes: whether to include abstract classes in the result
        type_id_key: if provided, the found classes will have their id attached as an attribute, using the value of this as the name. if an attribute of this name already exists, it will be used as id without further processing
        type_id_func: a function to take the found class as input, and returns a string representing the id of the class. By default, the module path + "." + class name (snake-case) is used (minus the string 'kiara_modules.<project_name>'', if it exists at the beginning
        type_id_no_attach: in case you want to use the type_id_key to set the id, but don't want it attached to classes that don't have it, set this to true. In most cases, you won't need this option
        attach_python_metadata: whether to attach a [PythonClass][kiara.models.python_class.PythonClass] metadata model to the class. By default, '_python_class' is used as attribute name if this argument is 'True', If this argument is a string, that will be used as name instead.
    """
    log2 = logging.getLogger("stevedore")
    out_hdlr = logging.StreamHandler(sys.stdout)
    out_hdlr.setFormatter(
        logging.Formatter(
            f"{entry_point_name} plugin search message/error -> %(message)s"
        )
    )
    out_hdlr.setLevel(logging.INFO)
    log2.addHandler(out_hdlr)
    if is_debug():
        log2.setLevel(logging.DEBUG)
    else:
        out_hdlr.setLevel(logging.INFO)
        log2.setLevel(logging.INFO)

    log_message("events.loading.entry_points", entry_point_name=entry_point_name)

    from stevedore import ExtensionManager

    mgr = ExtensionManager(
        namespace=entry_point_name,
        invoke_on_load=False,
        propagate_map_exceptions=True,
    )

    result_entrypoints: Dict[str, Type[SUBCLASS_TYPE]] = {}
    result_dynamic: Dict[str, Type[SUBCLASS_TYPE]] = {}

    for plugin in mgr:
        name = plugin.name

        if isinstance(plugin.plugin, type):
            # this means an actual (sub-)class was provided in the entrypoint

            cls = plugin.plugin
            if not issubclass(cls, base_class):
                log_message(
                    "ignore.entrypoint",
                    entry_point=name,
                    base_class=base_class,
                    sub_class=plugin.plugin,
                    reason=f"Entry point reference not a subclass of '{base_class}'.",
                )
                continue

            _process_subclass(
                sub_class=cls,
                base_class=base_class,
                type_id_key=type_id_key,
                type_id_func=type_id_func,
                type_id_no_attach=type_id_no_attach,
                attach_python_metadata=attach_python_metadata,
                ignore_abstract_classes=ignore_abstract_classes,
            )

            result_entrypoints[name] = cls
        elif (
            isinstance(plugin.plugin, tuple)
            and len(plugin.plugin) >= 1
            and callable(plugin.plugin[0])
        ) or callable(plugin.plugin):
            try:
                if callable(plugin.plugin):
                    func = plugin.plugin
                    args = []
                else:
                    func = plugin.plugin[0]
                    args = plugin.plugin[1:]
                classes = func(*args)
            except Exception as e:
                log_exception(e)
                raise Exception(f"Error trying to load plugin '{plugin.plugin}': {e}")

            for sub_class in classes:
                type_id = _process_subclass(
                    sub_class=sub_class,
                    base_class=base_class,
                    type_id_key=type_id_key,
                    type_id_func=type_id_func,
                    type_id_no_attach=type_id_no_attach,
                    attach_python_metadata=attach_python_metadata,
                    ignore_abstract_classes=ignore_abstract_classes,
                )

                if type_id is None:
                    continue

                if type_id in result_dynamic.keys():
                    raise Exception(
                        f"Duplicate type id '{type_id}' for type {entry_point_name}: {result_dynamic[type_id]} -- {sub_class}"
                    )
                result_dynamic[type_id] = sub_class

        else:
            raise Exception(
                f"Can't load subclasses for entry point {entry_point_name} and base class {base_class}: invalid plugin type {type(plugin.plugin)}"
            )

    for k, v in result_dynamic.items():
        if k in result_entrypoints.keys():
            msg = f"Duplicate item name '{k}' for type {entry_point_name}: {v} -- {result_entrypoints[k]}."
            try:
                if type_id_key not in v.__dict__.keys():
                    msg = f"{msg} Most likely the name is picked up from a subclass, try to add a '{type_id_key}' class attribute to your implementing class, with the name you want to give your type as value."
            except Exception:
                pass

            raise Exception(msg)
        result_entrypoints[k] = v

    return result_entrypoints


def find_all_kiara_modules() -> Dict[str, Type["KiaraModule"]]:
    """
    Find all [KiaraModule][kiara.module.KiaraModule] subclasses via package entry points.

    Todo:
    ----
    """
    from kiara.modules import KiaraModule

    modules = load_all_subclasses_for_entry_point(
        entry_point_name="kiara.modules",
        base_class=KiaraModule,  # type: ignore
        type_id_key="_module_type_name",
        attach_python_metadata=True,
    )

    result = {}
    # need to test this, since I couldn't add an abstract method to the KiaraModule class itself (mypy complained because it is potentially overloaded)
    for k, cls in modules.items():

        if not hasattr(cls, "process"):
            if is_develop():
                from rich.markdown import Markdown

                msg = f"Invalid kiara module: **{cls.__module__}.{cls.__name__}**\n\nMissing method(s):\n- *process*"
                from kiara.utils.develop import log_dev_message

                log_dev_message(msg=Markdown(msg))

            # TODO: check signature of process method
            log_message(
                "ignore.subclass",
                sub_class=cls,
                base_class=KiaraModule,
                reason="'process' method is missing",
            )
            continue

        result[k] = cls
    return result


def find_all_kiara_model_classes() -> Dict[str, Type["KiaraModel"]]:
    """
    Find all [KiaraModule][kiara.module.KiaraModule] subclasses via package entry points.

    Todo:
    ----
    """
    from kiara.models import KiaraModel

    return load_all_subclasses_for_entry_point(
        entry_point_name="kiara.model_classes",
        base_class=KiaraModel,  # type: ignore
        type_id_key="_kiara_model_id",
        type_id_func=_cls_name_id_func,
        attach_python_metadata=False,
    )


# def find_all_value_metadata_models() -> Dict[str, Type["ValueMetadata"]]:
#     """Find all [KiaraModule][kiara.module.KiaraModule] subclasses via package entry points.
#
#     TODO
#     """
#
#     from kiara.models.values.value_metadata import ValueMetadata
#
#     return load_all_subclasses_for_entry_point(
#         entry_point_name="kiara.metadata_models",
#         base_class=ValueMetadata,  # type: ignore
#         type_id_key="_metadata_key",
#         type_id_func=_cls_name_id_func,
#         attach_python_metadata=False,
#     )


@functools.lru_cache(maxsize=1)
def find_all_archive_types() -> Dict[str, Type["KiaraArchive"]]:
    """Find all [KiaraArchive][kiara.registries.KiaraArchive] subclasses via package entry points."""
    from kiara.registries import KiaraArchive

    return load_all_subclasses_for_entry_point(
        entry_point_name="kiara.archive_type",
        base_class=KiaraArchive,  # type: ignore
        type_id_key="_archive_type_name",
        type_id_func=_cls_name_id_func,
        attach_python_metadata=False,
    )


def find_all_data_types() -> Dict[str, Type["DataType"]]:
    """
    Find all [KiaraModule][kiara.module.KiaraModule] subclasses via package entry points.

    Todo:
    ----
    """
    from kiara.data_types import DataType

    all_data_types = load_all_subclasses_for_entry_point(
        entry_point_name="kiara.data_types",
        base_class=DataType,  # type: ignore
        type_id_key="_data_type_name",
        type_id_func=_cls_name_id_func,
    )

    invalid = [x for x in all_data_types.keys() if "." in x]
    if invalid:
        raise Exception(
            f"Invalid value type name(s), type names can't contain '.': {', '.join(invalid)}"
        )

    return all_data_types


def find_all_operation_types() -> Dict[str, Type["OperationType"]]:

    from kiara.operations import OperationType

    result = load_all_subclasses_for_entry_point(
        entry_point_name="kiara.operation_types",
        base_class=OperationType,  # type: ignore
        type_id_key="_operation_type_name",
    )
    return result


def find_kiara_modules_under(
    module: Union[str, ModuleType],
) -> List[Type["KiaraModule"]]:

    from kiara.modules import KiaraModule

    return find_subclasses_under(
        base_class=KiaraModule,  # type: ignore
        python_module=module,
    )


def find_kiara_model_classes_under(
    module: Union[str, ModuleType]
) -> List[Type["KiaraModel"]]:

    from kiara.models import KiaraModel

    result = find_subclasses_under(
        base_class=KiaraModel,  # type: ignore
        python_module=module,
    )

    return result


# def find_value_metadata_models_under(
#     module: Union[str, ModuleType]
# ) -> List[Type["ValueMetadata"]]:
#
#     from kiara.models.values.value_metadata import ValueMetadata
#
#     result = find_subclasses_under(
#         base_class=ValueMetadata,  # type: ignore
#         python_module=module,
#     )
#
#     return result


def find_data_types_under(module: Union[str, ModuleType]) -> List[Type["DataType"]]:

    from kiara.data_types import DataType

    return find_subclasses_under(
        base_class=DataType,  # type: ignore
        python_module=module,
    )


def find_operations_under(
    module: Union[str, ModuleType]
) -> List[Type["OperationType"]]:

    from kiara.operations import OperationType

    return find_subclasses_under(
        base_class=OperationType,  # type: ignore
        python_module=module,
    )


def find_pipeline_base_path_for_module(
    module: Union[str, ModuleType]
) -> Union[str, None]:

    # if hasattr(sys, "frozen"):
    #     raise NotImplementedError("Pyinstaller bundling not supported yet.")

    if isinstance(module, str):
        module = importlib.import_module(module)

    module_file = module.__file__
    assert module_file is not None
    path = os.path.dirname(module_file)

    if not os.path.exists(path):
        log_message("ignore.pipeline_folder", path=path, reason="folder does not exist")
        return None

    return path


def find_all_kiara_pipeline_paths(
    skip_errors: bool = False,
) -> Dict[str, Union[Dict[str, Any], None]]:

    import logging

    log2 = logging.getLogger("stevedore")
    out_hdlr = logging.StreamHandler(sys.stdout)
    out_hdlr.setFormatter(
        logging.Formatter("kiara pipeline search plugin error -> %(message)s")
    )
    out_hdlr.setLevel(logging.INFO)
    log2.addHandler(out_hdlr)
    log2.setLevel(logging.INFO)

    log_message("events.loading.pipelines")

    from stevedore import ExtensionManager

    mgr = ExtensionManager(
        namespace="kiara.pipelines", invoke_on_load=False, propagate_map_exceptions=True
    )

    paths: Dict[str, Union[Dict[str, Any], None]] = {}
    # TODO: make sure we load 'core' first?
    for plugin in mgr:

        name = plugin.name
        if (
            isinstance(plugin.plugin, tuple)
            and len(plugin.plugin) >= 1
            and callable(plugin.plugin[0])
        ) or callable(plugin.plugin):
            try:
                if callable(plugin.plugin):
                    func = plugin.plugin
                    args = []
                else:
                    func = plugin.plugin[0]
                    args = plugin.plugin[1:]

                f_args = []
                metadata: Union[Dict[str, Any], None] = None
                if len(args) >= 1:
                    f_args.append(args[0])
                if len(args) >= 2:
                    metadata = args[1]
                    assert isinstance(metadata, Mapping)
                if len(args) > 3:
                    logger.debug(
                        "ignore.pipeline_lookup_arguments",
                        reason="more than 2 arguments provided",
                        surplus_args=args[2:],
                        path=f_args[0],
                    )

                result = func(f_args[0])
                if not result:
                    continue
                if isinstance(result, str):
                    paths[result] = metadata
                else:
                    for path in paths:
                        assert path not in paths.keys()
                        paths[path] = metadata

            except Exception as e:
                log_exception(e)
                if skip_errors:
                    log_message(
                        "ignore.pipline_entrypoint", entrypoint_name=name, reason=str(e)
                    )
                    continue
                raise Exception(f"Error trying to load plugin '{plugin.plugin}': {e}")
        else:
            if skip_errors:
                log_message(
                    "ignore.pipline_entrypoint",
                    entrypoint_name=name,
                    reason=f"invalid plugin type '{type(plugin.plugin)}'",
                )
                continue
            msg = f"Can't load pipelines for entrypoint '{name}': invalid plugin type '{type(plugin.plugin)}'"
            raise Exception(msg)

    return paths


def find_all_cli_subcommands() -> Iterable["Command"]:

    entry_point_name = "kiara.cli_subcommands"
    log2 = logging.getLogger("stevedore")
    out_hdlr = logging.StreamHandler(sys.stdout)
    out_hdlr.setFormatter(
        logging.Formatter(
            f"{entry_point_name} plugin search message/error -> %(message)s"
        )
    )
    out_hdlr.setLevel(logging.INFO)
    log2.addHandler(out_hdlr)
    if is_debug():
        log2.setLevel(logging.DEBUG)
    else:
        out_hdlr.setLevel(logging.INFO)
        log2.setLevel(logging.INFO)

    log_message("events.loading.entry_points", entry_point_name=entry_point_name)
    from stevedore import ExtensionManager

    mgr = ExtensionManager(
        namespace=entry_point_name,
        invoke_on_load=False,
        propagate_map_exceptions=True,
    )

    return [plugin.plugin for plugin in mgr]


def find_all_kiara_renderers() -> Dict[str, Type["KiaraRenderer"]]:
    """
    Find all [KiaraComponent][kiara_plugin.streamilt.components.KiaraComponent] subclasses via package entry points.

    Todo:
    ----
    """
    from kiara.renderers import KiaraRenderer

    components = load_all_subclasses_for_entry_point(
        entry_point_name="kiara.renderers",
        base_class=KiaraRenderer,  # type: ignore
        type_id_key="_renderer_name",
        type_id_func=_cls_name_id_func,
        attach_python_metadata=True,
    )

    return components


def find_kiara_renderers_under(
    module: Union[str, ModuleType],
) -> List[Type["KiaraRenderer"]]:

    from kiara.renderers import KiaraRenderer

    return find_subclasses_under(
        base_class=KiaraRenderer,  # type: ignore
        python_module=module,
    )


# def _find_pipeline_folders_using_callable(
#     func: Union[Callable, Tuple]
# ) -> Tuple[Optional[str], str]:
#
#     if not callable(func):
#         assert len(func) >= 2
#         args = func[1]
#         assert len(args) == 1
#         module_path: Optional[str] = args[0]
#     else:
#         module_path = None
#     path = _callable_wrapper(func=func)  # type: ignore
#     assert isinstance(path, str)
#     return (module_path, path)


# def _find_kiara_modules_using_callable(
#     func: typing.Union[typing.Callable, typing.Tuple]
# ) -> typing.Mapping[str, typing.Type[KiaraModule]]:
#
#     # TODO: typecheck?
#     return _callable_wrapper(func=func)  # type: ignore
def _import_modules_recursively(module: "ModuleType"):

    if not hasattr(module, "__path__"):
        return

    for submodule in iter_modules(module.__path__):  # type: ignore

        try:
            submodule_mod = importlib.import_module(
                f"{module.__name__}.{submodule.name}"
            )
            if hasattr(submodule_mod, "__path__"):
                _import_modules_recursively(submodule_mod)
        except Exception as e:
            logger.error(
                "ignore.python_module",
                module=f"{module.__name__}.{submodule.name}",
                reason=str(e),
                base_module=str(module),
            )


# kiara\kiara\src\kiara\utils\concurrency.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import threading


class ThreadSaveCounter(object):
    """A thread-safe counter, can be used in kiara modules to update completion percentage."""

    def __init__(self):

        self._current = 0
        self._lock = threading.Lock()

    @property
    def current(self):
        return self._current

    def current_percent(self, total: int) -> int:

        return int((self.current / total) * 100)

    def increment(self):

        with self._lock:
            self._current += 1
            return self._current

    def decrement(self):

        with self._lock:
            self._current -= 1
            return self._current


# kiara\kiara\src\kiara\utils\config.py
# -*- coding: utf-8 -*-
from pathlib import Path
from typing import TYPE_CHECKING, Union

from kiara.defaults import KIARA_CONFIG_FILE_NAME, KIARA_MAIN_CONFIG_FILE
from kiara.exceptions import KiaraException

if TYPE_CHECKING:
    from kiara.context import KiaraConfig


def assemble_kiara_config(
    config_file: Union[str, None] = None, create_config_file: bool = False
) -> "KiaraConfig":
    """Assemble a KiaraConfig object from a config file path or create a new one.



    Arguments:
        config_file: The path to a Kiara config file or a folder containing one named 'kiara.config'.
        create_config_file: If True, create a new config file if it does not exist.

    """

    exists = False
    if config_file:
        config_path = Path(config_file)
        if config_path.exists():
            if config_path.is_file():
                config_file_path = config_path
                exists = True
            else:
                config_file_path = config_path / KIARA_CONFIG_FILE_NAME
                if config_file_path.exists():
                    exists = True
        else:
            config_path.parent.mkdir(parents=True, exist_ok=True)
            config_file_path = config_path

    else:
        config_file_path = Path(KIARA_MAIN_CONFIG_FILE)
        if not config_file_path.exists():
            exists = False
        else:
            exists = True

    from kiara.context import KiaraConfig

    if not exists:
        kiara_config = KiaraConfig()

        if config_file:
            if not create_config_file:
                raise KiaraException(
                    f"specified config file does not exist: {config_file}."
                )
        else:
            if create_config_file:
                kiara_config.save(config_file_path)
    else:
        kiara_config = KiaraConfig.load_from_file(config_file_path)

    return kiara_config


# kiara\kiara\src\kiara\utils\data.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import uuid
from typing import TYPE_CHECKING, Any, Union

import orjson
import structlog
from ruamel.yaml import YAML

if TYPE_CHECKING:
    from kiara.context import Kiara
    from kiara.models.module.operation import Operation
    from kiara.operations.included_core_operations.pretty_print import (
        PrettyPrintOperationType,
    )


logger = structlog.getLogger()
yaml = YAML(typ="safe")


def pretty_print_data(
    kiara: "Kiara",
    value_id: uuid.UUID,
    target_type="terminal_renderable",
    **render_config: Any,
) -> Any:

    value = kiara.data_registry.get_value(value=value_id)

    op_type: PrettyPrintOperationType = kiara.operation_registry.get_operation_type("pretty_print")  # type: ignore

    data_type = value.data_type_name
    if data_type not in kiara.data_type_names:
        data_type = "any"

    try:
        op: Union[Operation, None] = op_type.get_operation_for_render_combination(
            source_type=data_type, target_type=target_type
        )
    except Exception as e:

        logger.debug(
            "error.pretty_print",
            source_type=data_type,
            target_type=target_type,
            error=e,
        )

        op = None
        if target_type == "terminal_renderable":
            try:
                op = op_type.get_operation_for_render_combination(
                    source_type="any", target_type="string"
                )
            except Exception:
                pass

    if op is None:
        raise Exception(
            f"Can't find operation to render '{value.value_schema.type}' as '{target_type}."
        )

    result = op.run(
        kiara=kiara, inputs={"value": value, "render_config": render_config}
    )
    rendered = result.get_value_data("rendered_value")
    return rendered


def get_data_from_string(
    string_data: str, content_type: Union[str, None] = None
) -> Any:

    if content_type:
        assert content_type in ["json", "yaml"]

    if content_type == "json":
        data = orjson.loads(string_data.encode())
    elif content_type == "yaml":
        data = yaml.load(string_data)
    else:
        try:
            data = orjson.loads(string_data.encode())
        except Exception:
            try:
                data = yaml.load(string_data)
            except Exception:
                raise ValueError(
                    "Invalid data format, only 'json' or 'yaml' are supported currently."
                )

    return data


# kiara\kiara\src\kiara\utils\dates.py
# -*- coding: utf-8 -*-
import time
from datetime import datetime

import humanfriendly
import pytz

from kiara.utils import log_message


def get_current_time_incl_timezone() -> datetime:

    current_tz_name = time.tzname[0]
    try:
        current_tz = pytz.timezone(current_tz_name)
    except pytz.exceptions.UnknownTimeZoneError:
        log_message(
            "error.unknown.timezone",
            tz_name=current_tz_name,
            solution="using utc instead",
        )
        current_tz = pytz.utc

    return datetime.now(tz=current_tz)


def get_earliest_time_incl_timezone() -> datetime:
    return datetime(1970, 1, 1, tzinfo=pytz.utc)


def to_human_readable_date_string(datetime: datetime) -> str:

    now = get_current_time_incl_timezone()
    time_gone = (now - datetime).total_seconds()

    relative_time_str: str = humanfriendly.format_timespan(time_gone, max_units=1)
    return relative_time_str


# kiara\kiara\src\kiara\utils\db.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import os
from pathlib import Path
from typing import TYPE_CHECKING, Any, Dict

import orjson

from kiara import is_debug
from kiara.utils import log_message

if TYPE_CHECKING:
    from sqlalchemy.engine import Engine


def get_kiara_db_url(base_path: str):

    abs_path = os.path.abspath(os.path.expanduser(base_path))
    db_url = f"sqlite+pysqlite:///{abs_path}/kiara.db"
    return db_url


# def orm_json_serialize(obj: Any) -> str:
#
#     if hasattr(obj, "json"):
#         return obj.json()
#
#     if isinstance(obj, str):
#         return obj
#     elif isinstance(obj, Mapping):
#         return orjson_dumps(obj, default=None)
#     else:
#         raise Exception(f"Unsupported type for json serialization: {type(obj)}")


def orm_json_deserialize(obj: str) -> Any:
    return orjson.loads(obj)


def create_archive_engine(
    db_path: Path, force_read_only: bool, use_wal_mode: bool
) -> "Engine":

    from sqlalchemy import create_engine, text

    # if use_wal_mode:
    #     # TODO: not sure this does anything
    #     connect_args = {"check_same_thread": False, "isolation_level": "IMMEDIATE"}
    #     execution_options = {"sqlite_wal_mode": True}
    # else:

    connect_args: Dict[str, Any] = {}
    execution_options: Dict[str, Any] = {}

    # TODO: enable this for read-only mode?
    # def _pragma_on_connect(dbapi_con, con_record):
    #     dbapi_con.execute("PRAGMA query_only = ON")

    db_url = f"sqlite+pysqlite:///{db_path.as_posix()}"
    if force_read_only:
        db_url = db_url + "?mode=ro&uri=true"

    db_engine = create_engine(
        db_url,
        future=True,
        connect_args=connect_args,
        execution_options=execution_options,
    )

    if use_wal_mode:
        with db_engine.connect() as conn:
            conn.execute(text("PRAGMA journal_mode=wal;"))

    if is_debug():
        with db_engine.connect() as conn:
            wal_mode = conn.execute(text("PRAGMA journal_mode;")).fetchone()
            log_message(
                "detect.sqlite.journal_mode", result={wal_mode[0]}, db_url=db_url
            )

    return db_engine


def delete_archive_db(db_path: Path):

    db_path.unlink(missing_ok=True)
    shm_file = db_path.parent / f"{db_path.name}-shm"
    shm_file.unlink(missing_ok=True)
    wal_file = db_path.parent / f"{db_path.name}-wal"
    wal_file.unlink(missing_ok=True)


# def ensure_current_environments_persisted(
#     engine: Engine,
# ) -> Mapping[str, EnvironmentOrm]:
#
#     from kiara.kiara import EnvironmentRegistry
#
#     envs = {}
#     with engine.create_session() as session:
#         for (
#             env_name,
#             env,
#         ) in EnvironmentRegistry.instance().current_environments.items():
#
#             md = (
#                 session.query(EnvironmentOrm)
#                 .filter_by(metadata_hash=env.model_data_hash)
#                 .first()
#             )
#             if not md:
#
#                 md_schema = (
#                     session.query(MetadataSchemaOrm)
#                     .filter_by(metadata_schema_hash=env.get_schema_hash())
#                     .first()
#                 )
#                 if not md_schema:
#                     md_schema = MetadataSchemaOrm(
#                         metadata_schema_hash=env.get_schema_hash(),
#                         metadata_type=env.get_category_alias(),
#                         metadata_schema=env.schema_json(),
#                     )
#                     session.add(md_schema)
#                     session.commit()
#
#                 md = EnvironmentOrm(
#                     metadata_hash=env.model_data_hash,
#                     metadata_schema_id=md_schema.id,
#                     metadata_payload=env,
#                 )
#                 session.add(md)
#                 session.commit()
#
#             envs[env_name] = md
#
#     return envs


# kiara\kiara\src\kiara\utils\debug.py
# -*- coding: utf-8 -*-
import logging
import uuid
from typing import TYPE_CHECKING, Any, List, Mapping

from rich import box
from rich.console import RenderableType
from rich.table import Table

from kiara.models.module.jobs import ActiveJob, JobConfig
from kiara.models.module.manifest import Manifest
from kiara.models.values.value import Value
from kiara.utils import get_dev_config
from kiara.utils.cli import terminal_print
from kiara.utils.develop import DetailLevel
from kiara.utils.modules import module_config_is_empty

if TYPE_CHECKING:
    from kiara.context import Kiara
    from kiara.modules import KiaraModule

DEFAULT_VALUE_MAP_RENDER_CONFIG = {
    "ignore_fields": [
        "kiara_id",
        "data_type_class",
        "destiny_backlinks",
        "environments",
        "property_links",
    ],
}


def create_module_preparation_table(
    kiara: "Kiara",
    job_config: JobConfig,
    job_id: uuid.UUID,
    module: "KiaraModule",
    **render_config: Any,
) -> Table:

    dev_config = get_dev_config()
    table = Table(show_header=False, box=box.SIMPLE)
    table.add_column("key", style="i")
    table.add_column("value")

    table.add_row("job_id", str(job_id))

    module_details = dev_config.log.pre_run.module_info
    if module_details not in [DetailLevel.NONE.value, DetailLevel.NONE]:
        pipeline_name = job_config.module_config.get("pipeline_name", None)
        if module_details in [DetailLevel.MINIMAL.value, DetailLevel.MINIMAL]:
            table.add_row("module", job_config.module_type)
            if pipeline_name:
                table.add_row("pipeline name", pipeline_name)
            doc = module.operation.doc
            table.add_row(
                "module desc",
                doc.description,
                # kiara.context_info.module_types.item_infos[
                #     job_config.module_type
                # ].documentation.description,
            )
        elif module_details in [DetailLevel.FULL.value, DetailLevel.FULL]:
            table.add_row("module", job_config.module_type)
            if pipeline_name:
                table.add_row("pipeline name", pipeline_name)
            doc = module.operation.doc
            table.add_row(
                "module doc",
                doc.full_doc,
                # kiara.context_info.module_types.item_infos[
                #     job_config.module_type
                # ].documentation.full_doc,
            )
            if module_config_is_empty(job_config.module_config):
                table.add_row("module_config", "-- no config --")
            else:
                module = kiara.module_registry.create_module(manifest=job_config)
                table.add_row("module_config", module.config)

    inputs_details = dev_config.log.pre_run.inputs_info
    if inputs_details not in [DetailLevel.NONE.value, DetailLevel.NONE]:
        if inputs_details in [DetailLevel.MINIMAL, DetailLevel.MINIMAL.value]:
            render_config["show_type"] = False
            value_map_rend = create_value_map_renderable(
                value_map=job_config.inputs, **render_config
            )
            table.add_row("inputs", value_map_rend)
        elif inputs_details in [DetailLevel.FULL, DetailLevel.FULL.value]:
            value_map = kiara.data_registry.load_values(values=job_config.inputs)
            table.add_row("inputs", value_map.create_renderable(**render_config))

    return table


def create_post_run_table(
    kiara: "Kiara",
    job: ActiveJob,
    module: "KiaraModule",
    job_config: JobConfig,
    **render_config: Any,
) -> Table:

    dev_config = get_dev_config()
    table = Table(show_header=False, box=box.SIMPLE)
    table.add_column("key", style="i")
    table.add_column("value")

    table.add_row("job_id", str(job.job_id))
    table.add_row("status", job.status.value)
    if job.error:
        table.add_row("error", job.error)
    if job.job_log.log:
        start_time = job.job_log.log[0].timestamp
        last_time = start_time
        log_table = Table(show_header=False, box=box.SIMPLE_HEAD)
        log_table.add_column("log", style="i")
        log_table.add_column("level")
        log_table.add_column("timestamp")
        for log in job.job_log.log:
            log_time = log.timestamp
            if log_time == start_time:
                time_str = str(log_time)
            else:
                time_str = f"+ {log_time - last_time}"
            log_level = logging.getLevelName(log.log_level).lower()
            log_table.add_row(log.msg, log_level, time_str)
            last_time = log_time
        table.add_row("duration", f"{last_time - start_time}")
        table.add_row("logs", log_table)
    module_details = dev_config.log.post_run.module_info
    if module_details not in [DetailLevel.NONE.value, DetailLevel.NONE]:
        if module_details in [DetailLevel.MINIMAL.value, DetailLevel.MINIMAL]:
            table.add_row("module", module.module_type_name)
            table.add_row(
                "module desc",
                kiara.context_info.module_types.item_infos[
                    module.module_type_name
                ].documentation.description,
            )
        elif module_details in [DetailLevel.FULL.value, DetailLevel.FULL]:
            table.add_row("module", module.module_type_name)
            table.add_row(
                "module doc",
                kiara.context_info.module_types.item_infos[
                    module.module_type_name
                ].documentation.full_doc,
            )
            if module_config_is_empty(module.config.model_dump()):
                table.add_row("module_config", "-- no config --")
            else:
                table.add_row("module_config", module.config)

    if job_config.pipeline_metadata is not None:
        pm_table = Table(show_header=False, box=box.SIMPLE)
        pm_table.add_column("key")
        pm_table.add_column("value")
        pm_table.add_row("pipeline_id", str(job_config.pipeline_metadata.pipeline_id))
        pm_table.add_row("step_id", job_config.pipeline_metadata.step_id)
        table.add_row("pipeline_step_metadata", pm_table)
    else:
        table.add_row("pipeline_step_metadata", "-- not a pipeline step --")

    inputs_details = dev_config.log.post_run.inputs_info
    if inputs_details not in [DetailLevel.NONE.value, DetailLevel.NONE]:
        if inputs_details in [DetailLevel.MINIMAL, DetailLevel.MINIMAL.value]:
            render_config["show_type"] = False
            value_map_rend: RenderableType = create_value_map_renderable(
                value_map=job_config.inputs, **render_config
            )
            table.add_row("inputs", value_map_rend)
        elif inputs_details in [DetailLevel.FULL, DetailLevel.FULL.value]:
            value_map = kiara.data_registry.load_values(values=job_config.inputs)
            table.add_row("inputs", value_map.create_renderable(**render_config))

    outputs_details = dev_config.log.post_run.outputs_info
    if outputs_details not in [DetailLevel.NONE.value, DetailLevel.NONE]:
        if outputs_details in [DetailLevel.MINIMAL, DetailLevel.MINIMAL.value]:
            render_config["show_type"] = False
            if job.results is None:
                value_map_rend = "-- no results --"
            else:
                value_map_rend = create_value_map_renderable(
                    value_map=job.results, **render_config
                )
            table.add_row("outputs", value_map_rend)
        elif outputs_details in [DetailLevel.FULL, DetailLevel.FULL.value]:
            if job.results is None:
                value_map_rend = "-- no results --"
            else:
                value_map = kiara.data_registry.load_values(values=job.results)
                value_map_rend = value_map.create_renderable(**render_config)
            table.add_row("outputs", value_map_rend)

    return table


def terminal_print_manifest(manifest: Manifest):

    terminal_print(manifest.create_renderable())


def create_value_map_renderable(value_map: Mapping[str, Any], **render_config: Any):

    show_type = render_config.get("show_type", True)

    rc = dict(DEFAULT_VALUE_MAP_RENDER_CONFIG)
    rc.update(render_config)

    table = Table(show_header=True, box=box.SIMPLE)
    table.add_column("field name", style="i")
    if show_type:
        table.add_column("type")
    table.add_column("value")

    for k, v in value_map.items():
        row: List[Any] = [k]
        if isinstance(v, Value):
            if show_type:
                row.append("value object")
            row.append(v.create_renderable(**rc))
        elif isinstance(v, uuid.UUID):
            if show_type:
                row.append("value id")
            row.append(str(v))
        else:
            if show_type:
                row.append("raw data")
            row.append(str(v))

        table.add_row(*row)

    return table


# kiara\kiara\src\kiara\utils\dicts.py
# -*- coding: utf-8 -*-
import copy
from typing import Any, Dict, Mapping

import dpath


def merge_dicts(*dicts: Mapping[str, Any]) -> Dict[str, Any]:

    if not dicts:
        return {}

    current: Dict[str, Any] = {}
    for d in dicts:
        dpath.merge(current, dict(copy.deepcopy(d)))

    return current


# kiara\kiara\src\kiara\utils\doc.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import typing
from inspect import cleandoc

from kiara.defaults import DEFAULT_NO_DESC_VALUE
from kiara.utils import first_line


def extract_doc_from_cls(cls: typing.Type, only_first_line: bool = False):

    doc = cls.__doc__
    if not doc:
        doc = DEFAULT_NO_DESC_VALUE
    else:
        doc = cleandoc(doc)

    if only_first_line:
        return first_line(doc)
    else:
        return doc.strip()


def extract_doc_from_func(func: typing.Callable, only_first_line: bool = False):

    doc = func.__doc__
    if not doc:
        doc = DEFAULT_NO_DESC_VALUE
    else:
        doc = cleandoc(doc)

    if only_first_line:
        return first_line(doc)
    else:
        return doc.strip()


# kiara\kiara\src\kiara\utils\downloads.py
# -*- coding: utf-8 -*-
from typing import Any, Mapping, Union

import httpx
from ruamel.yaml import YAML

yaml = YAML(typ="safe")


def get_data_from_url(
    url: str, content_type: Union[str, None] = None
) -> Mapping[str, Any]:

    if content_type:
        assert content_type in ["json", "yaml"]

    r = httpx.get(url, follow_redirects=True)

    if not content_type:
        if url.endswith(".json"):
            content_type = "json"
        elif url.endswith(".yaml") or url.endswith(".yml"):
            content_type = "yaml"

    if content_type == "json":
        result = r.json()
    elif content_type == "yaml":
        result = yaml.load(r.text)
    else:
        try:
            result = r.json()
        except Exception:
            try:
                result = yaml.load(r.text)
            except Exception:
                raise ValueError(f"Can't parse data from url '{url}'")

    if not isinstance(result, Mapping):
        raise ValueError(f"Data from url '{url}' is not a Mapping")
    return result


# kiara\kiara\src\kiara\utils\files.py
# -*- coding: utf-8 -*-
import json
import os
from pathlib import Path
from typing import Any, Union

from ruamel.yaml import YAML

from kiara.exceptions import KiaraException

yaml = YAML(typ="safe")


def get_data_from_file(
    path: Union[str, Path], content_type: Union[str, None] = None
) -> Any:

    if isinstance(path, str):
        path = Path(os.path.expanduser(path))

    if not path.exists():
        raise KiaraException(f"File not found: {path}")

    if not path.is_file():
        raise KiaraException(f"Path is not a file: {path}")

    content = path.read_text()

    if not content_type:
        if path.name.endswith(".json"):
            content_type = "json"
        elif path.name.endswith(".yaml") or path.name.endswith(".yml"):
            content_type = "yaml"

    if content_type:

        if content_type not in ["json", "yaml"]:
            raise KiaraException(
                "Invalid content type, only 'json' or 'yaml' are supported currently."
            )

        if content_type == "json":
            data = json.loads(content)
        else:
            data = yaml.load(content)
    else:
        try:
            data = json.loads(content)
        except Exception:
            try:
                data = yaml.load(content)
            except Exception:
                raise ValueError("Could not determine data format from file extension.")

    return data


def unpack_archive(
    archive_file: str, out_dir: str, autodetect_file_type: bool = False
) -> None:

    if autodetect_file_type:
        raise NotImplementedError("Autodetecting file type is not implemented yet.")
        # import puremagic
        # type_matches = puremagic.magic_file(archive_file)
        #
        # for type_match in type_matches:
        #     print("----")
        #     dbg(type_match._asdict())
        #     if type_match.confidence >= 0.6:

    error = None
    try:
        import shutil

        shutil.unpack_archive(archive_file, out_dir)
    except Exception:
        # try patool, maybe we're lucky
        try:
            import patoolib

            patoolib.extract_archive(archive_file, outdir=out_dir)
        except Exception as e:
            error = e

    if error is not None:
        if not autodetect_file_type:
            unpack_archive(archive_file, out_dir, autodetect_file_type=True)
        else:
            raise KiaraException(msg=f"Could not extract archive: {error}.")


# kiara\kiara\src\kiara\utils\global_metadata.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import importlib
import typing
from functools import lru_cache
from types import ModuleType

from kiara.defaults import KIARA_MODULE_METADATA_ATTRIBUTE


@lru_cache()
def get_metadata_for_python_module_or_class(
    module_or_class: typing.Union[ModuleType, typing.Type]
) -> typing.List[typing.Dict[str, typing.Any]]:

    metadata: typing.List[typing.Dict[str, typing.Any]] = []

    if isinstance(module_or_class, type):
        if hasattr(module_or_class, KIARA_MODULE_METADATA_ATTRIBUTE):
            md = getattr(module_or_class, KIARA_MODULE_METADATA_ATTRIBUTE)
            assert isinstance(md, typing.Mapping)
            metadata.append(md)  # type: ignore
        _module_or_class: typing.Union[str, ModuleType, typing.Type] = (
            module_or_class.__module__
        )
    else:
        _module_or_class = module_or_class

    current_module = _module_or_class
    while current_module:

        if isinstance(current_module, str):
            current_module = importlib.import_module(current_module)

        if hasattr(current_module, KIARA_MODULE_METADATA_ATTRIBUTE):
            md = getattr(current_module, KIARA_MODULE_METADATA_ATTRIBUTE)
            assert isinstance(md, typing.Mapping)
            metadata.append(md)  # type: ignore

        if "." in current_module.__name__:
            current_module = ".".join(current_module.__name__.split(".")[0:-1])
        else:
            current_module = ""

    metadata.reverse()
    return metadata


# kiara\kiara\src\kiara\utils\graphs.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import os
import sys
import typing
from typing import Literal, Union

import networkx as nx

from kiara.defaults import KIARA_DEFAULT_STAGES_EXTRACTION_TYPE
from kiara.utils import log_message
from kiara.utils.cli import terminal_print

if typing.TYPE_CHECKING:
    from IPython.core.display import Image

    from kiara.models.module.pipeline import PipelineConfig, PipelineStructure
    from kiara.models.module.pipeline.pipeline import Pipeline


def print_ascii_graph(
    graph: nx.Graph, restart_interpreter_if_asciinet_installed: bool = False
):

    try:
        from asciinet import graph_to_ascii  # type: ignore
    except:  # noqa
        import pip._internal.cli.main as pip

        cmd = ["-q", "--isolated", "install"]
        cmd.append("asciinet")

        log_message("install.python_package", packages="asciinet")
        exit_code = pip.main(cmd)
        try:
            from asciinet import graph_to_ascii  # type: ignore
        except:  # noqa
            exit_code = 1

        if restart_interpreter_if_asciinet_installed:
            os.execvp(sys.executable, (sys.executable,) + tuple(sys.argv))  # noqa

        if exit_code != 0:
            terminal_print(
                "\nCan't print graph on terminal, package 'asciinet' not available. Please install it into the current virtualenv using:\n\npip install 'git+https://github.com/cosminbasca/asciinet.git#egg=asciinet&subdirectory=pyasciinet'"
            )
            return

    try:
        from asciinet._libutil import check_java  # type: ignore

        check_java("Java ")
    except Exception:
        terminal_print()
        terminal_print(
            "\nJava is currently necessary to print ascii graph. This might change in the future, but to use this functionality please install a JRE."
        )
        return

    print(graph_to_ascii(graph))  # noqa


def create_image(graph: nx.Graph) -> bytes:

    try:
        import pygraphviz as pgv  # noqa  # type: ignore
    except:  # noqa
        raise Exception(
            "pygraphviz not available, please install it manually into the current virtualenv"
        )

    # graph = nx.relabel_nodes(graph, lambda x: hash(x))
    G = nx.nx_agraph.to_agraph(graph)

    G.node_attr["shape"] = "box"
    # G.unflatten().layout(prog="dot")
    G.layout(prog="dot")

    b: bytes = G.draw(format="png")
    return b


def save_image(graph: nx.Graph, path: str):

    with open(path, "wb") as f:
        try:
            graph_b = create_image(graph=graph)
        except Exception as e:
            graph_b = str(e).encode("utf-8")
        f.write(graph_b)


def graph_to_image(
    graph: nx.Graph, return_bytes: bool = False
) -> Union[bytes, "Image"]:

    b = create_image(graph=graph)

    if return_bytes:
        return b
    else:
        try:
            from IPython.core.display import Image

            return Image(b)
        except Exception:
            raise Exception(
                "pygraphviz not available, please install it manually into the current virtualenv."
            )


def pipeline_graph_to_image(
    pipeline: Union["Pipeline", "PipelineConfig", "PipelineStructure"],
    graph_type: Literal[
        "data-flow", "data-flow-simple", "execution", "stages"
    ] = "execution",
    stages_extraction_type: str = KIARA_DEFAULT_STAGES_EXTRACTION_TYPE,
    return_bytes: bool = False,
):

    if hasattr(pipeline, "structure"):
        pipeline = pipeline.structure  # type: ignore

    if graph_type == "data-flow":
        graph = pipeline.data_flow_graph  # type: ignore
    elif graph_type == "data-flow-simple":
        graph = pipeline.data_flow_graph_simple  # type: ignore
    elif graph_type == "execution":
        graph = pipeline.execution_graph  # type: ignore
    elif graph_type == "stages":
        graph = pipeline.get_stages_graph(stages_extraction_type=stages_extraction_type)  # type: ignore
    else:
        raise Exception(
            f"Invalid graph type '{graph_type}': must be one of 'data-flow', 'data-flow-simple', 'execution', 'stages'."
        )

    return graph_to_image(graph=graph, return_bytes=return_bytes)


# kiara\kiara\src\kiara\utils\hashing.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)
import hashlib
from typing import Tuple, Union

import dag_cbor
import mmh3
from dag_cbor import IPLDKind
from multiformats import CID, multihash
from multiformats.multicodec import Multicodec
from multiformats.multihash import Multihash
from multiformats.varint import BytesLike

KIARA_HASH_FUNCTION = mmh3.hash


def compute_cid(
    data: IPLDKind,
    hash_codec: str = "sha2-256",
    encode: str = "base58btc",
) -> Tuple[bytes, CID]:

    encoded = dag_cbor.encode(data)

    hash_func = multihash.get(hash_codec)
    digest = hash_func.digest(encoded)

    cid = CID(encode, 1, codec="dag-cbor", digest=digest)
    return encoded, cid


_, NONE_CID = compute_cid(data=None)


def compute_cid_from_file(
    file: str, codec: Union[str, int, Multicodec] = "raw", hash_codec: str = "sha2-256"
):

    assert hash_codec == "sha2-256"

    hash_func = hashlib.sha256
    file_hash = hash_func()

    CHUNK_SIZE = 65536
    with open(file, "rb") as f:
        fb = f.read(CHUNK_SIZE)
        while len(fb) > 0:
            file_hash.update(fb)
            fb = f.read(CHUNK_SIZE)

    wrapped = multihash.wrap(file_hash.digest(), "sha2-256")
    return create_cid_digest(digest=wrapped, codec=codec)


def create_cid_digest(
    digest: Union[
        str, BytesLike, Tuple[Union[str, int, Multihash], Union[str, BytesLike]]
    ],
    codec: Union[str, int, Multicodec] = "raw",
) -> CID:

    cid = CID("base58btc", 1, codec, digest)
    return cid


# kiara\kiara\src\kiara\utils\html.py
# -*- coding: utf-8 -*-
from typing import TYPE_CHECKING, Any, Iterable, Mapping, Union

from pydantic import BaseModel

from kiara.registries.templates import TemplateRegistry

if TYPE_CHECKING:
    from airium import Airium  # type: ignore


def generate_html(
    item: Any,
    render_config: Union[None, Mapping[str, Any]] = None,
    add_header: bool = False,
    add_type_column: bool = False,
) -> "Airium":
    """Create html representing this models data."""
    from airium import Airium  # type: ignore

    doc = Airium()

    if render_config is None:
        render_config = {}
    else:
        render_config = dict(render_config)

    if isinstance(item, str):
        doc(item)
    elif isinstance(item, BaseModel):

        from kiara.models import KiaraModel

        if isinstance(item, KiaraModel):
            template_registry = TemplateRegistry.instance()
            template = template_registry.get_template_for_model_type(
                model_type=item.model_type_id, template_format="html"
            )

            if template:
                rendered = template.render(instance=item)
                doc(rendered)
                return doc

        exclude_fields = None
        model_cls = item.__class__
        props = model_cls.schema().get("properties", {})

        rows = []
        for field_name, field in model_cls.model_fields.items():

            if exclude_fields and field_name in exclude_fields:
                continue

            row = [field_name]

            p = props.get(field_name, None)
            if add_type_column:
                p_type = None
                if p is not None:
                    p_type = p.get("type", None)
                    # TODO: check 'anyOf' keys

                if p_type is None:
                    p_type = "-- check source --"
                row.append(p_type)

            data = getattr(item, field_name)
            row.append(generate_html(data, render_config=render_config))

            if p is not None:
                desc = p.get("description", "")
            else:
                desc = ""
            row.append(desc)

            rows.append(row)

        with doc.table():
            if add_header:
                with doc.tr():
                    doc.th(_t="field")
                    if add_type_column:
                        doc.th(_t="type")
                    doc.th(_t="data")
                    doc.th(_t="description")

            for row in rows:
                with doc.tr():
                    doc.td(_t=row[0])
                    doc.td(_t=row[1])
                    doc.td(_t=row[2])
                    if add_type_column:
                        doc.td(_t=row[3])

    elif isinstance(item, Mapping):
        with doc.table():
            for k, v in item.items():
                with doc.tr():
                    doc.td(_t=k)
                    value_el = generate_html(v)
                    doc.td(_t=value_el)
    elif isinstance(item, Iterable):

        with doc.ul():
            for i in item:
                with doc.li():
                    value_el = generate_html(i)
                    doc(str(value_el))

    else:
        doc(str(item))

    return doc


# kiara\kiara\src\kiara\utils\introspection.py
# -*- coding: utf-8 -*-
import inspect
import typing
from typing import Any, Callable, Dict


def extract_cls(arg: Any, imports: Dict[str, typing.Set[str]]) -> str:

    if arg in (type(None), None):
        return "None"
    elif isinstance(arg, type):
        name = arg.__name__
        module = arg.__module__
        if module == "typing":
            imports.setdefault(module, set()).add(name)
            return name
        elif module != "builtins":
            imports.setdefault(module, set()).add(name)
            return f'"{name}"'
        else:
            return name
    elif isinstance(arg, typing._UnionGenericAlias):  # type: ignore
        all_args = []
        for a in arg.__args__:
            cls = extract_cls(a, imports=imports)
            all_args.append(cls)

        imports.setdefault("typing", set()).add("Union")
        return f"Union[{', '.join(all_args)}]"
    elif isinstance(arg, typing._LiteralSpecialForm):  # type: ignore
        return "Literal"

    elif isinstance(arg, typing._GenericAlias):  # type: ignore

        origin_cls = extract_cls(arg.__origin__, imports=imports)
        if origin_cls == "Literal":
            all_args_str = ", ".join((f'"{x}"' for x in arg.__args__))
            imports.setdefault("typing", set()).add("Literal")
            return f"Literal[{all_args_str}]"

        all_args = []
        for a in arg.__args__:
            cls = extract_cls(a, imports=imports)
            all_args.append(cls)

        if origin_cls == '"Mapping"':
            imports.setdefault("typing", set()).add("Mapping")
            assert len(all_args) == 2
            return f"Mapping[{all_args[0]}, {all_args[1]}]"
        elif origin_cls == '"Iterable"':
            imports.setdefault("typing", set()).add("Iterable")
            return f"Iterable[{', '.join(all_args)}]"
        elif origin_cls in ('"List"', "list"):
            imports.setdefault("typing", set()).add("List")
            return f"List[{', '.join(all_args)}]"
        elif origin_cls == "dict":
            assert len(all_args) == 2
            imports.setdefault("typing", set()).add("Dict")
            return f"Dict[{all_args[0]}, {all_args[1]}]"
        elif origin_cls == "type":
            imports.setdefault("typing", set()).add("Type")
            result = f"Type[{', '.join(all_args)}]"
            return result
        else:
            raise Exception(f"Unexpected generic alias: {origin_cls}")
    elif isinstance(arg, typing.ForwardRef):
        return f'"{arg.__forward_arg__}"'
    else:
        raise Exception(f"Unexpected type '{type(arg)}' for arg: {arg}")


def create_default_string(default: Any) -> str:

    if default is None:
        return "None"
    elif isinstance(default, bool):
        return str(default)
    elif isinstance(default, str):
        if "\\" in default:
            default = f'r"{default}"'
            return default
        else:
            return f'"{default}"'
    else:
        raise Exception(f"Unexpected default value: {default}")


def parse_signature_args(func: Callable, imports: Dict[str, typing.Set[str]]) -> str:
    sig = inspect.signature(func)

    all_tokens = []
    param: inspect.Parameter
    for field_name, param in sig.parameters.items():
        if field_name == "self":
            all_tokens.append("self")
        else:
            arg_str = extract_cls(arg=param.annotation, imports=imports)

            if param.kind == inspect.Parameter.VAR_POSITIONAL:
                sig_token = f"*{field_name}: {arg_str}"
            elif param.kind == inspect.Parameter.VAR_KEYWORD:
                sig_token = f"**{field_name}: {arg_str}"
            else:
                sig_token = f"{field_name}: {arg_str}"

            if param.default != inspect.Parameter.empty:
                default_str = create_default_string(default=param.default)
                sig_token += f" = {default_str}"

            all_tokens.append(sig_token)

    return ", ".join(all_tokens)


def parse_signature_return(func: Callable, imports: Dict[str, typing.Set[str]]) -> str:

    sig = inspect.signature(func)
    sig_return_type = sig.return_annotation
    if isinstance(sig_return_type, str):
        return f'"{sig_return_type}"'
    elif sig_return_type == inspect.Parameter.empty:
        return "None"
    else:
        sig_return_type_str = extract_cls(arg=sig_return_type, imports=imports)
        return sig_return_type_str


def create_signature_string(
    func: Callable, imports: Dict[str, typing.Set[str]]
) -> typing.Tuple[str, typing.Union[str, None]]:

    params = parse_signature_args(func=func, imports=imports)
    return_type = parse_signature_return(func=func, imports=imports)
    if return_type == "None":
        sig_str = f"def {func.__name__}({params}):"
        _return_type = None
    else:
        sig_str = f"def {func.__name__}({params}) -> {return_type}:"
        _return_type = return_type

    return sig_str, _return_type


def extract_arg_names(func: Callable) -> typing.List[str]:
    sig = inspect.signature(func)
    return list(sig.parameters.keys())


def extract_proxy_arg_str(func: Callable) -> str:

    sig = inspect.signature(func)
    arg_str = ""
    for field_name, param in sig.parameters.items():

        if field_name == "self":
            continue

        if param.kind == inspect.Parameter.VAR_POSITIONAL:
            arg_str += f"*{field_name}, "
        elif param.kind == inspect.Parameter.VAR_KEYWORD:
            arg_str += f"**{field_name}, "
        else:
            arg_str += f"{field_name}={field_name}, "

    if arg_str.endswith(", "):
        arg_str = arg_str[:-2]
    return arg_str


# kiara\kiara\src\kiara\utils\json.py
# -*- coding: utf-8 -*-
import orjson

from kiara.utils import is_debug

DEFAULT_ORJSON_OPTIONS = (
    orjson.OPT_SERIALIZE_NUMPY
    | orjson.OPT_SERIALIZE_DATACLASS
    | orjson.OPT_NON_STR_KEYS
)
DEFAULT_ORJSON_DUMP_ARGS = {"option": DEFAULT_ORJSON_OPTIONS}


def orjson_dumps(v, *, default=None, **args) -> str:
    # orjson.dumps returns bytes, to match standard json.dumps we need to decode

    if not args:
        args = DEFAULT_ORJSON_DUMP_ARGS

    try:
        return orjson.dumps(v, default=default, **args).decode()
    except Exception as e:
        if is_debug():
            from kiara.utils.cli import terminal_print

            terminal_print(f"Error dumping json data: {e}")
            from kiara import dbg

            dbg(v)

        raise e


# kiara\kiara\src\kiara\utils\metadata.py
# -*- coding: utf-8 -*-
from functools import lru_cache

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)
from typing import TYPE_CHECKING, Dict, Type, Union

from kiara.models.values.value_metadata import ValueMetadata
from kiara.registries.models import ModelRegistry

if TYPE_CHECKING:
    from kiara.context import Kiara
    from kiara.interfaces.python_api.models.info import (
        MetadataTypeClassesInfo,
    )


@lru_cache()
def find_metadata_models(
    alias: Union[str, None] = None, only_for_package: Union[str, None] = None
) -> "MetadataTypeClassesInfo":

    from kiara.interfaces.python_api.models.info import MetadataTypeClassesInfo

    model_registry = ModelRegistry.instance()
    _group = model_registry.get_models_of_type(ValueMetadata)  # type: ignore

    classes: Dict[str, Type[ValueMetadata]] = {}
    for model_id, info in _group.item_infos.items():
        model_cls = info.python_class.get_class()  # type: ignore
        classes[model_id] = model_cls

    group: MetadataTypeClassesInfo = MetadataTypeClassesInfo.create_from_type_items(group_title=alias, kiara=None, **classes)  # type: ignore

    if only_for_package:
        temp = {}
        for key, _info in group.item_infos.items():
            if _info.context.labels.get("package") == only_for_package:
                temp[key] = _info

        group = MetadataTypeClassesInfo(
            group_id=group.instance_id, group_title=group.group_alias, item_infos=temp  # type: ignore
        )

    return group


def get_metadata_model_for_data_type(
    kiara: "Kiara", data_type: str
) -> "MetadataTypeClassesInfo":
    """
    Return all available metadata extract operations for the provided type (and it's parent types).

    Arguments:
    ---------
        data_type: the value type

    Returns:
    -------
        a mapping with the metadata type as key, and the operation as value
    """

    from kiara.interfaces.python_api.models.info import MetadataTypeClassesInfo

    # TODO: add models for parent types?
    # lineage = set(kiara.type_registry.get_type_lineage(data_type_name=data_type))

    model_registry = ModelRegistry.instance()
    all_metadata_models = model_registry.get_models_of_type(ValueMetadata)

    matching_types = {}

    for name, model_info in all_metadata_models.item_infos.items():

        metadata_cls: Type[ValueMetadata] = model_info.python_class.get_class()
        supported = metadata_cls.retrieve_supported_data_types()
        if data_type in supported:
            matching_types[name] = metadata_cls

    result: MetadataTypeClassesInfo = MetadataTypeClassesInfo.create_from_type_items(
        kiara=kiara,
        group_title=f"Metadata models for type '{data_type}'",
        **matching_types,
    )

    return result


# kiara\kiara\src\kiara\utils\models.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)
from typing import TYPE_CHECKING, Any, Iterable, Mapping, Type, Union

import networkx as nx
from pydantic import RootModel
from pydantic.main import BaseModel
from rich.panel import Panel
from rich.tree import Tree

from kiara.defaults import KIARA_DEFAULT_ROOT_NODE_ID, PYDANTIC_USE_CONSTRUCT
from kiara.utils import log_message

if TYPE_CHECKING:
    from kiara.models import KiaraModel


def create_pydantic_model(
    model_cls: Type[BaseModel],
    _use_pydantic_construct: bool = PYDANTIC_USE_CONSTRUCT,
    **field_values: Any,
):

    if _use_pydantic_construct:
        raise NotImplementedError()
        return model_cls(**field_values)
    else:
        return model_cls(**field_values)


def retrieve_data_subcomponent_keys(data: Any) -> Iterable[str]:

    if isinstance(data, RootModel):
        if isinstance(data.root, Mapping):
            result = set()
            for k, v in data.root.items():
                if isinstance(v, BaseModel):
                    result.add(k.split(".")[0])
            return result
        else:
            return []
    elif isinstance(data, BaseModel):
        matches = sorted(data.model_fields.keys())
        return matches
    else:
        log_message(
            f"No subcomponents retrieval supported for data of type: {type(data)}"
        )
        return []


def get_subcomponent_from_model(data: "KiaraModel", path: str) -> "KiaraModel":
    """Return subcomponents of a model under a specified path."""
    if "." in path:
        first_token, rest = path.split(".", maxsplit=1)
        sc = data.get_subcomponent(first_token)
        return sc.get_subcomponent(rest)

    # TODO: pydantic refactor
    if isinstance(data, RootModel):
        if isinstance(data.root, Mapping):  # type: ignore
            if path in data.root.keys():  # type: ignore
                return data.root[path]  # type: ignore
            else:
                matches = {}
                for k in data.root.keys():  # type: ignore
                    if k.startswith(f"{path}."):
                        rest = k[len(path) + 1 :]
                        matches[rest] = data.root[k]  # type: ignore

                if not matches:
                    raise KeyError(f"No child models under '{path}'.")
                else:
                    raise NotImplementedError()
                    # subcomponent_group = KiaraModelGroup.create_from_child_models(**matches)
                    # return subcomponent_group

        else:
            raise NotImplementedError("Only mapping root models are supported for now.")
    else:
        if path in data.model_fields.keys():
            result: KiaraModel = getattr(data, path)
            # TODO: test isinstance?
            return result
        else:
            raise KeyError(
                f"No subcomponent for key '{path}' in model: {data.instance_id}."
            )


def assemble_subcomponent_graph(data: "KiaraModel") -> Union[nx.DiGraph, None]:

    from kiara.models import KiaraModel

    graph = nx.DiGraph()

    def assemble_graph(info_model: KiaraModel, current_node_id, level: int = 0):
        graph.add_node(current_node_id, obj=info_model, level=level)
        scn = info_model.subcomponent_keys
        if not scn:
            return
        for child_path in scn:
            child_obj = info_model.get_subcomponent(child_path)
            new_node_id = f"{current_node_id}.{child_path}"
            graph.add_edge(current_node_id, new_node_id)
            if isinstance(child_obj, KiaraModel):
                assemble_graph(child_obj, new_node_id, level + 1)

    assemble_graph(data, KIARA_DEFAULT_ROOT_NODE_ID)
    return graph


def create_subcomponent_tree_renderable(
    data: "KiaraModel", show_data: bool = False
) -> Tree:

    from kiara.models import KiaraModel
    from kiara.utils.output import extract_renderable

    def extract_type_string(obj: Any) -> str:

        if isinstance(obj, KiaraModel):
            return f"model: {obj.model_type_id}"
        elif isinstance(obj, Mapping):
            return "dict"
        else:
            return type(obj).__name__

    def assemble_tree(node: Tree, model: Any, level: int):

        if isinstance(model, Mapping) and model:
            for k, v in model.items():
                child_tree = node.add(f"[b i]{k}[/b i] ({extract_type_string(v)})")
                assemble_tree(node=child_tree, model=v, level=level + 1)
            return

        if not isinstance(model, KiaraModel):
            if show_data:
                renderable = extract_renderable(model)
                panel = Panel(
                    renderable, title="[i]data[/i]", title_align="left", expand=False
                )
                node.add(panel)
            return

        scn = model.subcomponent_keys
        if not scn:
            return
        for child_path in scn:
            child_obj = model.get_subcomponent(child_path)
            child_tree = node.add(
                f"[b i]{child_path}[/b i] ({extract_type_string(child_obj)})"
            )
            assemble_tree(node=child_tree, model=child_obj, level=level + 1)

    tree = Tree(f"[b]{data.model_type_id}[/b]: [b]{data.instance_id}[/b]")
    assemble_tree(node=tree, model=data, level=0)

    return tree


# kiara\kiara\src\kiara\utils\modules.py
# -*- coding: utf-8 -*-
from typing import Any, Mapping


def module_config_is_empty(config: Mapping[str, Any]):

    c = dict(config)
    d = c.pop("defaults", None)
    if d:
        return False
    constants = c.pop("constants", None)
    if constants:
        return False

    return False if c else True


# kiara\kiara\src\kiara\utils\operations.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)
import os
from typing import TYPE_CHECKING, Any, Dict, List, Mapping, Union

from rich.console import Group, RenderableType
from rich.markdown import Markdown
from ruamel.yaml import YAML

from kiara.exceptions import (
    InvalidValuesException,
    NoSuchExecutionTargetException,
    NoSuchOperationException,
)
from kiara.interfaces.python_api.models.info import OperationGroupInfo, OperationInfo
from kiara.models.module.jobs import ExecutionContext
from kiara.models.module.manifest import Manifest
from kiara.models.module.operation import Operation
from kiara.models.module.pipeline import PipelineConfig
from kiara.utils.files import get_data_from_file
from kiara.utils.output import (
    create_table_from_field_schemas,
    create_value_map_status_renderable,
)

if TYPE_CHECKING:
    from kiara.api import ValueMap
    from kiara.context import Kiara

yaml = YAML(typ="safe")


def filter_operations(
    kiara: "Kiara", pkg_name: Union[str, None] = None, **operations: "Operation"
) -> OperationGroupInfo:

    result: Dict[str, OperationInfo] = {}

    # op_infos = kiara.operation_registry.get_context_metadata(only_for_package=pkg_name)
    modules = kiara.module_registry.get_context_metadata(only_for_package=pkg_name)

    for op_id, op in operations.items():

        if op.module.module_type_name != "pipeline":
            if op.module.module_type_name in modules.item_infos.keys():
                result[op_id] = OperationInfo.create_from_operation(
                    kiara=kiara, operation=op
                )
                continue
        else:
            package: Union[str, None] = op.metadata.get("labels", {}).get(
                "package", None
            )
            if not pkg_name or (package and package == pkg_name):
                result[op_id] = OperationInfo.create_from_operation(
                    kiara=kiara, operation=op
                )

        # opt_types = kiara.operation_registry.find_all_operation_types(op_id)
        # match = False
        # for ot in opt_types:
        #     if ot in op_infos.keys():
        #         match = True
        #         break
        #
        # if match:
        #     result[op_id] = OperationInfo.create_from_operation(
        #         kiara=kiara, operation=op
        #     )

    return OperationGroupInfo(item_infos=result)  # type: ignore


def create_operation(
    module_or_operation: str,
    operation_config: Union[None, Mapping[str, Any]] = None,
    kiara: Union[None, "Kiara"] = None,
) -> Operation:

    operation: Union[Operation, None]

    if kiara is None:
        from kiara.context import Kiara

        kiara = Kiara.instance()

    operation = None

    if module_or_operation in kiara.operation_registry.operation_ids:

        operation = kiara.operation_registry.get_operation(module_or_operation)
        if operation_config:
            if module_or_operation in kiara.module_type_names:
                manifest = Manifest(
                    module_type=module_or_operation, module_config=operation_config
                )
                module = kiara.module_registry.create_module(manifest=manifest)
                operation = Operation.create_from_module(module)
            else:
                raise Exception(
                    f"Specified run target '{module_or_operation}' is an operation, additional module configuration is not allowed."
                )

    elif (
        module_or_operation != "pipeline"
        and module_or_operation in kiara.module_type_names
    ):

        if operation_config is None:
            operation_config = {}
        manifest = Manifest(
            module_type=module_or_operation, module_config=operation_config
        )
        module = kiara.module_registry.create_module(manifest=manifest)
        operation = Operation.create_from_module(module)

    elif os.path.isfile(module_or_operation):
        _data = get_data_from_file(module_or_operation)
        pipeline_name = _data.pop("pipeline_name", None)
        if pipeline_name is None:
            pipeline_name = os.path.basename(module_or_operation)

        # self._defaults = data.pop("inputs", {})

        execution_context = ExecutionContext(
            pipeline_dir=os.path.abspath(os.path.dirname(module_or_operation))
        )
        pipeline_config = PipelineConfig.from_config(
            pipeline_name=pipeline_name,
            data=_data,
            kiara=kiara,
            execution_context=execution_context,
        )

        manifest = kiara.create_manifest(
            "pipeline", config=pipeline_config.model_dump()
        )
        module = kiara.module_registry.create_module(manifest=manifest)

        operation = Operation.create_from_module(module, doc=pipeline_config.doc)

    else:
        if module_or_operation == "pipeline":
            data: Union[None, Mapping[str, Any]] = operation_config
        else:
            try:
                import json

                data = json.loads(module_or_operation)
            except Exception:
                try:
                    data = yaml.load(module_or_operation)
                except Exception:
                    data = None

            if data and not isinstance(data, Mapping):
                raise Exception(
                    f"Could not parse module or operation: {module_or_operation}"
                )

        if data:
            d = dict(data)
            pipeline_name = d.pop("pipeline_name", None)
            if pipeline_name is not None:

                execution_context = ExecutionContext(
                    pipeline_dir=os.path.abspath(os.path.dirname(module_or_operation))
                )
                pipeline_config = PipelineConfig.from_config(
                    pipeline_name=pipeline_name,
                    data=d,
                    kiara=kiara,
                    execution_context=execution_context,
                )

                manifest = kiara.create_manifest(
                    "pipeline", config=pipeline_config.model_dump()
                )
                module = kiara.module_registry.create_module(manifest=manifest)

                operation = Operation.create_from_module(
                    module, doc=pipeline_config.doc
                )
            else:
                raise Exception("Invalid pipeline config, missing 'pipeline_name' key.")

        if operation is None:

            if module_or_operation == "pipeline":
                msg = "Can't assemble pipeline."
            else:
                msg = f"Can't assemble operation, invalid operation/module name: {module_or_operation}. Must be registered module or operation name, or file."
            raise NoSuchOperationException(
                msg=msg,
                operation_id=module_or_operation,
                available_operations=sorted(kiara.operation_registry.operation_ids),
            )

    if operation is None:

        merged = set(kiara.module_type_names)
        merged.update(kiara.operation_registry.operation_ids)
        raise NoSuchExecutionTargetException(
            selected_target=module_or_operation,
            msg=f"Invalid run target name '{module_or_operation}'. Must be a path to a pipeline file, or one of the available modules/operations.",
            available_targets=sorted(merged),
        )
    return operation


def create_operation_status_renderable(
    operation: Operation, inputs: Union["ValueMap", None], render_config: Any
) -> RenderableType:

    show_operation_name = render_config.get("show_operation_name", True)
    show_operation_doc = render_config.get("show_operation_doc", True)
    show_only_description = render_config.get("show_only_description", True)
    show_inputs = render_config.get("show_inputs", False)
    show_outputs_schema = render_config.get("show_outputs_schema", False)
    show_headers = render_config.get("show_headers", True)

    items: List[Any] = []

    if show_operation_name:
        items.append(f"Operation: [bold]{operation.operation_id}[/bold]")
    if show_operation_doc and operation.doc.is_set:
        items.append("")
        if show_only_description:
            items.append(Markdown(operation.doc.description))
        else:
            items.append(Markdown(operation.doc.full_doc))

    if show_inputs:
        assert inputs is not None
        if show_headers:
            items.append("\nInputs:")
        _inputs: Union[None, RenderableType] = None
        try:
            _inputs = create_value_map_status_renderable(
                inputs, render_config=render_config
            )
        except InvalidValuesException as ive:
            _inputs = ive.create_renderable(**render_config)
        except Exception as e:
            _inputs = f"[red bold]{e}[/red bold]"
        finally:
            assert _inputs is not None
            items.append(_inputs)
    if show_outputs_schema:
        if show_headers:
            items.append("\nOutputs:")
        outputs_schema = create_table_from_field_schemas(
            _add_default=False,
            _add_required=False,
            _show_header=True,
            _constants=None,
            fields=operation.outputs_schema,
        )
        items.append(outputs_schema)

    return Group(*items)


# kiara\kiara\src\kiara\utils\output.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import json
import uuid
from abc import ABC, abstractmethod
from enum import Enum
from typing import (
    TYPE_CHECKING,
    Any,
    Dict,
    Iterable,
    Iterator,
    List,
    Mapping,
    Set,
    Type,
    Union,
)

import orjson
import structlog
from pydantic import BaseModel, Field, model_validator
from pydantic.v1.main import BaseModel as BaseModel1
from pydantic_core import PydanticUndefined
from rich import box
from rich.console import ConsoleRenderable, Group, RenderableType, RichCast
from rich.markdown import Markdown
from rich.table import Table as RichTable
from rich.tree import Tree

from kiara.defaults import SpecialValue
from kiara.exceptions import KiaraException
from kiara.models.values.value import ORPHAN, Value, ValueMap
from kiara.utils.json import orjson_dumps

if TYPE_CHECKING:
    from pyarrow import Table as ArrowTable

    from kiara.context import Kiara
    from kiara.models.events.pipeline import PipelineState
    from kiara.models.module.pipeline import PipelineStructure
    from kiara.models.values.value_schema import ValueSchema


log = structlog.getLogger()


class RenderConfig(BaseModel):

    render_format: str = Field(description="The output format.", default="terminal")


class OutputDetails(BaseModel):
    @classmethod
    def from_data(cls, data: Any):

        if isinstance(data, str):
            if "=" in data:
                data = [data]
            else:
                data = [f"format={data}"]

        if isinstance(data, Iterable):
            from kiara.utils.cli import dict_from_cli_args

            data = list(data)
            if len(data) == 1 and isinstance(data[0], str) and "=" not in data[0]:
                data = [f"format={data[0]}"]
            output_details_dict = dict_from_cli_args(*data)
        else:
            raise TypeError(
                f"Can't parse output detail config: invalid input type '{type(data)}'."
            )

        output_details = OutputDetails(**output_details_dict)
        return output_details

    format: str = Field(description="The output format.")
    target: str = Field(description="The output target.")
    config: Dict[str, Any] = Field(
        description="Output configuration.", default_factory=dict
    )

    @model_validator(mode="before")
    @classmethod
    def _set_defaults(cls, values) -> Dict[str, Any]:

        target: str = values.pop("target", "terminal")
        format: str = values.pop("format", None)
        if format is None:
            if target == "terminal":
                format = "terminal"
            else:
                if target == "file":
                    format = "json"
                else:
                    ext = target.split(".")[-1]
                    if ext in ["yaml", "json"]:
                        format = ext
                    else:
                        format = "json"
        result = {"format": format, "target": target, "config": dict(values)}

        return result


class TabularWrap(ABC):
    def __init__(self) -> None:
        self._num_rows: Union[int, None] = None
        self._column_names: Union[Iterable[str], None] = None
        self._force_single_line: bool = True

    @property
    def num_rows(self) -> int:
        if self._num_rows is None:
            self._num_rows = self.retrieve_number_of_rows()
        return self._num_rows

    @property
    def column_names(self) -> Iterable[str]:
        if self._column_names is None:
            self._column_names = self.retrieve_column_names()
        return self._column_names

    @abstractmethod
    def retrieve_column_names(self) -> Iterable[str]:
        pass

    @abstractmethod
    def retrieve_number_of_rows(self) -> int:
        pass

    @abstractmethod
    def slice(self, offset: int = 0, length: Union[int, None] = None) -> "TabularWrap":
        pass

    @abstractmethod
    def to_pydict(self) -> Mapping:
        pass

    def as_string(
        self,
        rows_head: Union[int, None] = None,
        rows_tail: Union[int, None] = None,
        max_row_height: Union[int, None] = None,
        max_cell_length: Union[int, None] = None,
    ):

        table_str = ""
        for cn in self.column_names:
            table_str = f"{table_str}{cn}\t"
        table_str = f"{table_str}\n"

        for data in self.prepare_table_data(
            return_column_names=False,
            rows_head=rows_head,
            rows_tail=rows_tail,
            max_row_height=max_row_height,
            max_cell_length=max_cell_length,
        ):
            for cell in data:
                table_str = f"{table_str}{cell}\t"
            table_str = f"{table_str}\n"

        return table_str

    def as_html(
        self,
        rows_head: Union[int, None] = None,
        rows_tail: Union[int, None] = None,
        max_row_height: Union[int, None] = None,
        max_cell_length: Union[int, None] = None,
    ) -> str:

        table_str = "<table><tr>"
        for cn in self.column_names:
            table_str = f"{table_str}<th>{cn}</th>"
        table_str = f"{table_str}</tr>"

        for data in self.prepare_table_data(
            return_column_names=False,
            rows_head=rows_head,
            rows_tail=rows_tail,
            max_row_height=max_row_height,
            max_cell_length=max_cell_length,
        ):
            table_str = f"{table_str}<tr>"
            for cell in data:
                table_str = f"{table_str}<td>{cell}</td>"
            table_str = f"{table_str}</tr>"
        table_str = f"{table_str}</table>"
        return table_str

    def as_terminal_renderable(
        self,
        rows_head: Union[int, None] = None,
        rows_tail: Union[int, None] = None,
        max_row_height: Union[int, None] = None,
        max_cell_length: Union[int, None] = None,
        show_table_header: bool = True,
    ) -> RichTable:

        rich_table = RichTable(show_header=show_table_header, box=box.SIMPLE)
        if max_row_height == 1:
            overflow = "ignore"
        else:
            overflow = "ellipsis"

        for cn in self.column_names:
            rich_table.add_column(cn, overflow=overflow)  # type: ignore

        data = self.prepare_table_data(
            return_column_names=False,
            rows_head=rows_head,
            rows_tail=rows_tail,
            max_row_height=max_row_height,
            max_cell_length=max_cell_length,
        )

        for row in data:
            rich_table.add_row(*row)

        return rich_table

    def prepare_table_data(
        self,
        return_column_names: bool = False,
        rows_head: Union[int, None] = None,
        rows_tail: Union[int, None] = None,
        max_row_height: Union[int, None] = None,
        max_cell_length: Union[int, None] = None,
    ) -> Iterator[Iterable[Any]]:

        if return_column_names:
            yield self.column_names

        num_split_rows = 2

        if rows_head is not None:

            if rows_head < 0:
                rows_head = 0

            if rows_head > self.num_rows:
                rows_head = self.num_rows
                rows_tail = None
                num_split_rows = 0

            if rows_tail is not None:
                if rows_head + rows_tail >= self.num_rows:  # type: ignore
                    rows_head = self.num_rows
                    rows_tail = None
                    num_split_rows = 0
        else:
            num_split_rows = 0

        if rows_head is not None:
            head = self.slice(0, rows_head)
            num_rows = rows_head
        else:
            head = self
            num_rows = self.num_rows

        table_dict = head.to_pydict()
        for i in range(0, num_rows):  # noqa
            row = []
            for cn in self.column_names:
                cell = table_dict[cn][i]
                cell_str = str(cell)
                if max_row_height and max_row_height > 0 and "\n" in cell_str:
                    lines = cell_str.split("\n")
                    if len(lines) > max_row_height:
                        if max_row_height == 1:
                            lines = lines[0:1]
                        else:
                            half = int(max_row_height / 2)
                            lines = lines[0:half] + [".."] + lines[-half:]
                    cell_str = "\n".join(lines)

                if max_cell_length and max_cell_length > 0:
                    lines = []
                    for line in cell_str.split("\n"):
                        if len(line) > max_cell_length:
                            line = line[0:max_cell_length] + " ..."

                        lines.append(line)
                    cell_str = "\n".join(lines)

                row.append(cell_str)

            yield row

        if num_split_rows:
            for i in range(0, num_split_rows):  # noqa
                row = []
                for _ in self.column_names:
                    row.append("...")
                yield row

        if rows_head:
            if rows_tail is not None:
                if rows_tail < 0:
                    rows_tail = 0

                tail = self.slice(self.num_rows - rows_tail)
                table_dict = tail.to_pydict()
                for i in range(0, num_rows):  # noqa

                    row = []
                    for cn in self.column_names:

                        cell = table_dict[cn][i]
                        cell_str = str(cell)

                        if max_row_height and max_row_height > 0 and "\n" in cell_str:
                            lines = cell_str.split("\n")
                            if len(lines) > max_row_height:
                                if max_row_height == 1:
                                    lines = lines[0:1]
                                else:
                                    half = int(len(lines) / 2)
                                    lines = lines[0:half] + [".."] + lines[-half:]
                            cell_str = "\n".join(lines)

                        if max_cell_length and max_cell_length > 0:
                            lines = []
                            for line in cell_str.split("\n"):

                                if len(line) > max_cell_length:
                                    line = line[0:(max_cell_length)] + " ..."
                                lines.append(line)
                            cell_str = "\n".join(lines)

                        row.append(cell_str)

                    yield row


class ArrowTabularWrap(TabularWrap):
    def __init__(self, table: "ArrowTable"):
        self._table: "ArrowTable" = table
        super().__init__()

    def retrieve_column_names(self) -> List[str]:
        return self._table.column_names  # type: ignore

    def retrieve_number_of_rows(self) -> int:
        return self._table.num_rows  # type: ignore

    def slice(self, offset: int = 0, length: Union[int, None] = None):
        return self._table.slice(offset=offset, length=length)

    def to_pydict(self) -> Mapping[str, Any]:
        return self._table.to_pydict()  # type: ignore


class DictTabularWrap(TabularWrap):
    def __init__(self, data: Mapping[str, List[Any]]):

        self._data: Mapping[str, List[Any]] = data
        # TODO: assert all rows are equal length
        super().__init__()

    def retrieve_number_of_rows(self) -> int:
        key = next(iter(self._data.keys()))
        return len(self._data[key])

    def retrieve_column_names(self) -> Iterable[str]:
        return self._data.keys()

    def to_pydict(self) -> Mapping[str, List[Any]]:
        return self._data

    def slice(self, offset: int = 0, length: Union[int, None] = None) -> "TabularWrap":

        result = {}
        start = None
        end = None
        for cn in self._data.keys():
            if start is None:
                if offset > len(self._data):
                    return DictTabularWrap({cn: [] for cn in self._data.keys()})
                start = offset
                if not length:
                    end = len(self._data)
                else:
                    end = start + length
                    if end > len(self._data):
                        end = len(self._data)
            result[cn] = self._data[cn][start:end]
        return DictTabularWrap(result)


def create_table_from_base_model_cls(model_cls: Type[BaseModel]):

    table = RichTable(box=box.SIMPLE, show_lines=True)
    table.add_column("Field")
    table.add_column("Type")
    table.add_column("Description")
    table.add_column("Required")
    table.add_column("Default")

    props = model_cls.model_json_schema().get("properties", {})

    for field_name, field in sorted(model_cls.model_fields.items()):
        row = [field_name]
        p = props.get(field_name, None)
        p_type = None
        desc = ""
        if p is not None:
            p_type = p.get("type", None)
            # TODO: check 'anyOf' keys
            desc = p.get("description", "")

        if p_type is None:
            p_type = "-- check source --"
        row.append(p_type)

        row.append(desc)
        row.append("yes" if field.is_required() else "no")
        default = field.default
        if callable(default):
            default = default()

        if default in [None, PydanticUndefined]:
            default = ""
        else:
            try:
                default = json.dumps(default, indent=2)
            except Exception:
                default = str(default)
        row.append(default)
        table.add_row(*row)

    return table


def create_table_from_base_model_v1_cls(model_cls: Type[BaseModel1]):

    table = RichTable(box=box.SIMPLE, show_lines=True)
    table.add_column("Field")
    table.add_column("Type")
    table.add_column("Description")
    table.add_column("Required")
    table.add_column("Default")

    props = model_cls.schema().get("properties", {})

    for field_name, field in sorted(model_cls.__fields__.items()):
        row = [field_name]
        p = props.get(field_name, None)
        p_type = None
        desc = ""
        if p is not None:
            p_type = p.get("type", None)
            # TODO: check 'anyOf' keys
            desc = p.get("description", "")

        if p_type is None:
            p_type = "-- check source --"
        row.append(p_type)

        row.append(desc)
        row.append("yes" if field.required else "no")
        default = field.default
        if callable(default):
            default = default()

        if default is None:
            default = ""
        else:
            try:
                default = json.dumps(default, indent=2)
            except Exception:
                default = str(default)
        row.append(default)
        table.add_row(*row)

    return table


def create_dict_from_field_schemas(
    fields: Mapping[str, "ValueSchema"],
    _add_default: bool = True,
    _add_required: bool = True,
    _show_header: bool = False,
    _constants: Union[Mapping[str, Any], None] = None,
    _doc_to_string: bool = True,
) -> Mapping[str, List[Any]]:

    table: Dict[str, List[Any]] = {}
    table["field_name"] = []
    table["data_type"] = []
    table["description"] = []

    if _add_required:
        table["required"] = []
    if _add_default:
        table["default"] = []

    for field_name, schema in fields.items():

        table["field_name"].append(field_name)
        table["data_type"].append(schema.type)
        if _doc_to_string:
            table["description"].append(schema.doc.full_doc)
        else:
            table["description"].append(schema.doc)

        if _add_required:
            req = schema.is_required()
            table["required"].append(req)

        if _add_default:
            if _constants and field_name in _constants.keys():
                d = f"{_constants[field_name]} (constant)"
            else:
                if schema.default in [
                    None,
                    SpecialValue.NO_VALUE,
                    SpecialValue.NOT_SET,
                ]:
                    d = "-- no default --"
                else:
                    d = str(schema.default)
            table["default"].append(d)

    return table


def create_table_from_field_schemas(
    fields: Mapping[str, "ValueSchema"],
    _add_default: bool = True,
    _add_required: bool = True,
    _show_header: bool = False,
    _constants: Union[Mapping[str, Any], None] = None,
) -> RichTable:

    table = RichTable(box=box.SIMPLE, show_header=_show_header)
    table.add_column("field name", style="i", overflow="fold")
    table.add_column("type")
    table.add_column("description")

    if _add_required:
        table.add_column("Required")
    if _add_default:
        if _constants:
            table.add_column("Default / Constant")
        else:
            table.add_column("Default")
    for field_name, schema in fields.items():

        row: List[RenderableType] = [field_name, schema.type, schema.doc]

        if _add_required:
            req = schema.is_required()
            if not req:
                req_str = "no"
            else:
                if schema.default in [
                    None,
                    SpecialValue.NO_VALUE,
                    SpecialValue.NOT_SET,
                ]:
                    req_str = "[b]yes[b]"
                else:
                    req_str = "no"
            row.append(req_str)

        if _add_default:
            if _constants and field_name in _constants.keys():
                d = f"[b]{_constants[field_name]}[/b] (constant)"
            else:
                if schema.default in [
                    None,
                    SpecialValue.NO_VALUE,
                    SpecialValue.NOT_SET,
                ]:
                    d = "-- no default --"
                else:
                    d = str(schema.default)
            row.append(d)

        table.add_row(*row)

    return table


def create_value_map_status_renderable(
    inputs: ValueMap,
    render_config: Union[Mapping[str, Any], None] = None,
    fields: Union[None, Iterable[str]] = None,
) -> RichTable:

    if render_config is None:
        render_config = {}

    show_description: bool = render_config.get("show_description", True)
    show_type: bool = render_config.get("show_type", True)
    show_required: bool = render_config.get("show_required", True)
    show_default: bool = render_config.get("show_default", True)
    show_value_ids: bool = render_config.get("show_value_ids", False)
    show_data: bool = render_config.get("show_data", False)
    data_max_no_rows: Union[int, None] = render_config.get("max_no_rows", None)
    if data_max_no_rows is None:
        data_max_no_rows = render_config.get("max_lines", 0)

    table = RichTable(box=box.SIMPLE, show_header=True)
    table.add_column("field name", style="i")
    table.add_column("status", style="b")
    if show_type:
        table.add_column("type")
    if show_description:
        table.add_column("description")

    if show_required:
        table.add_column("required")

    if show_default:
        table.add_column("default")

    if show_value_ids:
        table.add_column("value id", overflow="fold")

    if show_data:
        table.add_column("data")

    invalid = inputs.check_invalid()

    if fields:
        field_order = fields
    else:
        field_order = sorted(inputs.keys())

    for field_name in field_order:

        value = inputs.get(field_name, None)
        if value is None:
            log.debug(
                "ignore.field", field_name=field_name, available_fields=inputs.keys()
            )
            continue

        row: List[RenderableType] = [field_name]

        if field_name in invalid.keys():
            row.append(f"[red]{invalid[field_name]}[/red]")
        else:
            row.append("[green]valid[/green]")

        value_schema = inputs.values_schema[field_name]

        if show_type:
            row.append(value_schema.type)

        if show_description:
            row.append(value_schema.doc.description)

        if show_required:
            req = value_schema.is_required()
            if not req:
                req_str = "no"
            else:
                if value_schema.default in [
                    None,
                    SpecialValue.NO_VALUE,
                    SpecialValue.NOT_SET,
                ]:
                    req_str = "[b]yes[b]"
                else:
                    req_str = "no"
            row.append(req_str)

        if show_default:
            default = value_schema.default
            if callable(default):
                default_val = default()
            else:
                default_val = default

            if default_val in [None, SpecialValue.NOT_SET, SpecialValue.NO_VALUE]:
                default_str = ""
            else:
                default_str = str(default_val)

            row.append(default_str)

        if show_value_ids:
            row.append(str(inputs.get_value_obj(field_name=field_name).value_id))

        if show_data:
            render_config = dict(render_config)
            render_config["max_no_rows"] = data_max_no_rows
            data = value._data_registry.pretty_print_data(
                value_id=value.value_id,
                target_type="terminal_renderable",
                **render_config,
            )
            row.append(data)

        table.add_row(*row)

    return table


def create_table_from_model_object(
    model: BaseModel,
    render_config: Union[Mapping[str, Any], None] = None,
    exclude_fields: Union[Set[str], None] = None,
) -> RichTable:

    model_cls = model.__class__

    show_header: bool = True
    show_type_column: bool = True
    show_value_column: bool = True
    show_desc: bool = True

    if render_config:
        show_header = render_config.get("show_header", True)
        show_type_column = render_config.get("show_type_column", True)
        show_value_column = render_config.get("show_value_column", True)
        show_desc = render_config.get("show_description", True)

    table = RichTable(box=box.SIMPLE, show_lines=True, show_header=show_header)
    table.add_column("Field")
    if show_type_column:
        table.add_column("Type")
    if show_value_column:
        table.add_column("Value")
    if show_desc:
        table.add_column("Description")

    props = model_cls.schema().get("properties", {})

    for field_name, field in sorted(model_cls.model_fields.items()):
        if exclude_fields and field_name in exclude_fields:
            continue
        row: List[RenderableType] = [field_name]

        p = props.get(field_name, None)
        p_type = None
        desc = ""
        if p is not None:
            p_type = p.get("type", None)
            # TODO: check 'anyOf' keys
            desc = p.get("description", "")

        if show_type_column:
            if p_type is None:
                p_type = "-- check source --"
            row.append(p_type)

        data = getattr(model, field_name)
        if show_value_column:
            row.append(extract_renderable(data, render_config=render_config))
        if show_desc:
            row.append(desc)
        table.add_row(*row)

    return table


def extract_renderable(
    item: Any, render_config: Union[Mapping[str, Any], None] = None
) -> RenderableType:
    """Try to automatically find and extract or create an object that is renderable by the 'rich' library."""
    if render_config is None:
        render_config = {}
    else:
        render_config = dict(render_config)

    inline_models_as_json = render_config.setdefault("inline_models_as_json", True)

    if hasattr(item, "create_renderable"):
        return item.create_renderable(**render_config)  # type: ignore
    elif isinstance(item, (ConsoleRenderable, RichCast, str)):
        return item
    elif isinstance(item, BaseModel) and not inline_models_as_json:
        return create_table_from_model_object(item)
    elif isinstance(item, BaseModel):
        return item.model_dump_json(indent=2)
    elif isinstance(item, Mapping) and not inline_models_as_json:
        table = RichTable(show_header=False, box=box.SIMPLE)
        table.add_column("Key", style="i")
        table.add_column("Value")
        for k, v in item.items():
            table.add_row(k, extract_renderable(v, render_config=render_config))
        return table
    elif isinstance(item, Mapping):
        result = {}
        for k, v in item.items():
            if isinstance(v, BaseModel):
                v = v.model_dump()
            result[k] = v
        return orjson_dumps(
            result, option=orjson.OPT_INDENT_2 | orjson.OPT_NON_STR_KEYS
        )
    elif isinstance(item, Iterable):
        _all = []
        for i in item:
            _all.append(extract_renderable(i))
        rg = Group(*_all)
        return rg
    elif isinstance(item, Enum):
        return str(item.value)
    elif isinstance(item, Exception):
        msg = str(item)
        details = KiaraException.get_root_details(item)
        if details and details != msg:
            return Group(msg, "", Markdown(details))
        else:
            return msg
    else:
        return str(item)


def create_renderable_from_values(
    values: Mapping[str, "Value"], config: Union[Mapping[str, Any], None] = None
) -> RenderableType:
    """Create a renderable for this module configuration."""
    if config is None:
        config = {}

    render_format = config.get("render_format", "terminal")
    if render_format not in ["terminal"]:
        raise Exception(f"Invalid render format: {render_format}")

    show_pedigree = config.get("show_pedigree", False)
    show_data = config.get("show_data", False)
    show_hash = config.get("show_hash", True)
    show_size = config.get("show_size", True)
    # show_load_config = config.get("show_load_config", False)
    value_title = config.get("value_title", "value")

    table = RichTable(show_lines=True, box=box.MINIMAL_DOUBLE_HEAD)
    table.add_column(value_title, "i")
    table.add_column("data_type")
    if show_size:
        table.add_column("size")
    if show_hash:
        table.add_column("hash")
    if show_pedigree:
        table.add_column("pedigree")
    if show_data:
        table.add_column("data")

    for id, value in sorted(values.items(), key=lambda item: item[1].value_schema.type):
        row: List[RenderableType] = [id, value.value_schema.type]
        if show_size:
            row.append(str(value.value_size))
        if show_hash:
            row.append(str(value.value_hash))
        if show_pedigree:
            if value.pedigree == ORPHAN:
                pedigree = "-- n/a --"
            else:
                pedigree = value.pedigree.model_dump_json(indent=2)
            row.append(pedigree)
        if show_data:
            data = value._data_registry.pretty_print_data(
                value_id=value.value_id, target_type="terminal_renderable", **config
            )
            row.append(data)
        # if show_load_config:
        #     load_config = value.retrieve_load_config()
        #     if load_config is None:
        #         load_config_str: RenderableType = "-- not stored (yet) --"
        #     else:
        #         load_config_str = load_config.create_renderable()
        #     row.append(load_config_str)
        table.add_row(*row)

    return table


def create_pipeline_steps_tree(
    pipeline_structure: "PipelineStructure", pipeline_details: "PipelineState"
) -> Tree:

    from kiara.models.module.pipeline import StepStatus

    steps = Tree("steps")

    for idx, stage in enumerate(pipeline_structure.processing_stages, start=1):
        stage_node = steps.add(f"stage: [i]{idx}[/i]")
        for step_id in sorted(stage):
            step_node = stage_node.add(f"step: [i]{step_id}[/i]")
            step_details = pipeline_details.step_states[step_id]
            status = step_details.status
            if status is StepStatus.INPUTS_READY:
                step_node.add("status: [yellow]inputs ready[/yellow]")
            elif status is StepStatus.RESULTS_READY:
                step_node.add("status: [green]results ready[/green]")
            else:
                invalid_node = step_node.add("status: [red]inputs invalid[/red]")
                invalid = step_details.invalid_details
                for k, v in invalid.items():
                    invalid_node.add(f"[i]{k}[/i]: {v}")

    return steps


def create_recursive_table_from_model_object(
    model: BaseModel,
    render_config: Union[Mapping[str, Any], None] = None,
):

    if render_config is None:
        render_config = {}

    show_lines = render_config.get("show_lines", True)
    show_header = render_config.get("show_header", True)
    show_description = render_config.get("show_description", False)
    model_cls = model.__class__

    table = RichTable(box=box.SIMPLE, show_lines=show_lines, show_header=show_header)
    table.add_column("Field")
    table.add_column("Value")

    props = model_cls.schema().get("properties", {})

    for field_name in sorted(model_cls.model_fields.keys()):

        data = getattr(model, field_name)
        p = props.get(field_name, None)
        p_type = None
        desc = None
        if p is not None:
            p_type = p.get("type", None)
            # TODO: check 'anyOf' keys
            desc = p.get("description", None)

        if p_type is not None:
            p_type = f"[i]{p_type}[/i]"

        if not isinstance(data, BaseModel):
            data_renderable = extract_renderable(data, render_config=render_config)
            sub_model = None
        else:
            updated_render_config = dict(render_config)
            updated_render_config["show_header"] = False
            sub_model = create_recursive_table_from_model_object(
                data, render_config=updated_render_config
            )

            data_renderable = None

        group = []

        if data_renderable:
            group.append(data_renderable)
            group.append("")
        if desc and show_description:
            group.append(f"[i]{desc}[/i]")

        if sub_model:
            group.append(sub_model)

        if p_type:
            field_name = f"[b i]{field_name}[/b i] ([i]{p_type}[/i])"
        else:
            field_name = f"[b i]{field_name}[/b i]"
        table.add_row(field_name, Group(*group))

    return table


def create_table_from_data_and_schema(
    data: Mapping[str, Any], schema: Union[None, Dict[str, Any]] = None
):

    table = RichTable(box=box.SIMPLE, show_header=False)
    table.add_column("key", style="i")
    table.add_column("value")

    for key, value in data.items():
        value_renderable = extract_renderable(value)
        table.add_row(key, value_renderable)

    return table


def create_renderable_from_value_id_map(
    kiara: "Kiara",
    values: Mapping[str, uuid.UUID],
    config: Union[Mapping[str, Any], None] = None,
) -> RenderableType:
    """Create a renderable for a map of value ids."""

    return create_value_map_status_renderable(
        kiara.data_registry.load_values(values), render_config=config
    )
    # return create_renderable_from_values(kiara.data_registry.load_values(values), config=config)


# kiara\kiara\src\kiara\utils\pipelines.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import os
import typing
from pathlib import Path
from typing import Any, Dict, Iterable, List, Mapping, Union

import structlog

from kiara.defaults import (
    DEFAULT_EXCLUDE_DIRS,
    MODULE_TYPE_NAME_KEY,
    VALID_PIPELINE_FILE_EXTENSIONS,
)
from kiara.exceptions import InvalidOperationException, NoSuchOperationException
from kiara.utils import log_exception
from kiara.utils.files import get_data_from_file

if typing.TYPE_CHECKING:
    from kiara.context import Kiara
    from kiara.models.module.pipeline import PipelineConfig
    from kiara.models.module.pipeline.value_refs import StepValueAddress
    from kiara.modules.included_core_modules.pipeline import PipelineModule

logger = structlog.get_logger()


def create_step_value_address(
    value_address_config: Union[str, Mapping[str, Any]],
    default_field_name: str,
) -> "StepValueAddress":

    from kiara.models.module.pipeline.value_refs import StepValueAddress

    if isinstance(value_address_config, StepValueAddress):
        return value_address_config

    sub_value: Union[Mapping[str, Any], None] = None

    if isinstance(value_address_config, str):

        tokens = value_address_config.split(".")
        if len(tokens) == 1:
            step_id = value_address_config
            output_name = default_field_name
        elif len(tokens) == 2:
            step_id = tokens[0]
            output_name = tokens[1]
        elif len(tokens) == 3:
            step_id = tokens[0]
            output_name = tokens[1]
            sub_value = {"config": tokens[2]}
        else:
            raise NotImplementedError()

    elif isinstance(value_address_config, Mapping):

        step_id = value_address_config["step_id"]
        output_name = value_address_config["value_name"]
        sub_value = value_address_config.get("sub_value", None)
    else:
        raise TypeError(
            f"Invalid type for creating step value address: {type(value_address_config)}"
        )

    if sub_value is not None and not isinstance(sub_value, Mapping):
        raise ValueError(
            f"Invalid type '{type(sub_value)}' for sub_value (step_id: {step_id}, value name: {output_name}): {sub_value}"
        )

    input_link = StepValueAddress(
        step_id=step_id, value_name=output_name, sub_value=sub_value
    )
    return input_link


def ensure_step_value_addresses(
    link: Union[str, Mapping, Iterable], default_field_name: str
) -> List["StepValueAddress"]:

    if isinstance(link, (str, Mapping)):
        input_links: List[StepValueAddress] = [
            create_step_value_address(
                value_address_config=link, default_field_name=default_field_name
            )
        ]

    elif isinstance(link, Iterable):
        input_links = []
        for o in link:
            il = create_step_value_address(
                value_address_config=o, default_field_name=default_field_name
            )
            input_links.append(il)
    else:
        raise TypeError(f"Can't parse input map, invalid type for output: {link}")

    return input_links


def get_pipeline_details_from_path(
    path: Union[str, Path],
    module_type_name: Union[str, None] = None,
    base_module: Union[str, None] = None,
) -> Dict[str, Any]:
    """
    Load a pipeline description, save it's content, and determine it the pipeline base name.

    Arguments:
    ---------
        path: the path to the pipeline file
        module_type_name: if specifies, overwrites any auto-detected or assigned pipeline name
        base_module: overrides the base module the assembled pipeline module will be located in the python hierarchy

    """
    if isinstance(path, str):
        path = Path(os.path.expanduser(path))

    if not path.is_file():
        raise Exception(
            f"Can't add pipeline description '{path.as_posix()}': not a file"
        )

    data = get_data_from_file(path)

    if not data:
        raise Exception(
            f"Can't register pipeline file '{path.as_posix()}': no content."
        )

    if module_type_name:
        data[MODULE_TYPE_NAME_KEY] = module_type_name

    if not isinstance(data, Mapping):
        raise Exception("Not a dictionary type.")

    result = {"data": data, "source": path.as_posix(), "source_type": "file"}
    if base_module:
        result["base_module"] = base_module
    return result


def check_doc_sidecar(path: Union[Path, str], data: Dict[str, Any]) -> Dict[str, Any]:

    if isinstance(path, str):
        path = Path(os.path.expanduser(path))

    _doc = data["data"].get("documentation", None)
    if _doc is None:
        _doc_path = Path(path.as_posix() + ".md")
        if _doc_path.is_file():
            doc = _doc_path.read_text()
            if doc:
                data["data"]["documentation"] = doc

    return data


def get_pipeline_config(
    pipeline: str, kiara: typing.Union["Kiara", None] = None
) -> "PipelineConfig":
    """
    Extract a pipeline config from the item specified.

    The lookup of the 'pipeline' reference happens in this order (first match returns the result):
    - check whether there is an operation with that name that is a pipeline
    - check whether the provided string is a path to an existing file
    - check whether the provided string starts with 'workflow:' and matches a workflow alias (or id), in which case it returns the pipeline config for the workflows current state

    Arguments:
        pipeline: a reference to the desired pipeline
        kiara: the kiara context

    Returns:
        a pipeline config object
    """
    if kiara is None:
        from kiara.context import Kiara

        kiara = Kiara.instance()

    pc: Union["PipelineConfig", None] = None
    error: Union[Exception, None] = None
    try:
        _operation = kiara.operation_registry.get_operation(pipeline)

        pipeline_module: PipelineModule = _operation.module  # type: ignore
        if pipeline_module.is_pipeline():
            pc = pipeline_module.config
    except NoSuchOperationException as nsoe:
        error = nsoe
    except InvalidOperationException as ioe:
        error = ioe

    if pc is None:
        if os.path.isfile(pipeline):
            from kiara.models.module.pipeline import PipelineConfig

            pc = PipelineConfig.from_file(pipeline, kiara=kiara)

    if pc is None and pipeline.startswith("workflow:"):
        try:
            workflow = pipeline[9:]
            if "@" in workflow:
                raise NotImplementedError()

            wfm = kiara.workflow_registry.get_workflow_metadata(workflow=workflow)
            if wfm.current_state:
                state = kiara.workflow_registry.get_workflow_state(
                    workflow_state_id=wfm.current_state, workflow=wfm.workflow_id
                )
                pc = state.pipeline_config
        except Exception as e:
            log_exception(e)

    if pc is None:
        if error:
            raise error
        else:
            raise Exception(f"Could not resolve pipeline reference '{pipeline}'.")

    return pc


def find_pipeline_data_in_paths(
    pipeline_paths: Dict[str, Union[Dict[str, Any], None]]
) -> Mapping[str, Mapping[str, Any]]:
    """
    Find pipeline data in the provided paths.

    The 'pipeline_paths' argument has a local path as key, and a mapping as value that contains optional metadata about the context for all the pipelines that are found under the path.

    Arguments:
    ---------
        pipeline_paths: a mapping of pipeline names to paths
    """
    all_pipelines = []

    for _path in pipeline_paths.keys():
        path = Path(_path)
        if not path.exists():
            logger.warning(
                "ignore.pipeline_path", path=path, reason="path does not exist"
            )
            continue

        elif path.is_dir():

            for root, dirnames, filenames in os.walk(path, topdown=True):

                dirnames[:] = [
                    d
                    for d in dirnames
                    if d not in DEFAULT_EXCLUDE_DIRS and not d.startswith(".")
                ]

                for filename in [
                    f
                    for f in filenames
                    if os.path.isfile(os.path.join(root, f))
                    and any(f.endswith(ext) for ext in VALID_PIPELINE_FILE_EXTENSIONS)
                ]:

                    full_path = os.path.join(root, filename)
                    try:

                        data = get_pipeline_details_from_path(path=full_path)
                        data = check_doc_sidecar(full_path, data)
                        existing_metadata = data.pop("metadata", {})
                        _md = pipeline_paths[_path]
                        if _md is None:
                            md = {}
                        else:
                            md = dict(_md)
                        md.update(existing_metadata)
                        data["metadata"] = md

                        all_pipelines.append(data)

                    except Exception as e:
                        log_exception(e)
                        logger.warning(
                            "ignore.pipeline_file", path=full_path, reason=str(e)
                        )

        elif path.is_file():
            data = get_pipeline_details_from_path(path=path)
            data = check_doc_sidecar(path, data)
            existing_metadata = data.pop("metadata", {})
            _md = pipeline_paths[_path]
            if _md is None:
                md = {}
            else:
                md = dict(_md)
            md.update(existing_metadata)
            data["metadata"] = md
            all_pipelines.append(data)

    pipelines = {}
    for pipeline in all_pipelines:
        name = pipeline["data"].get("pipeline_name", None)
        if name is None:
            source = pipeline["source"]
            name = os.path.basename(source)
            if "." in name:
                name, _ = name.rsplit(".", maxsplit=1)
            pipeline["data"]["pipeline_name"] = name
        pipelines[name] = pipeline

    return pipelines


def extract_data_to_hash_from_pipeline_config(
    pipeline_config: Mapping[str, Any]
) -> Mapping[str, Any]:

    if "steps" not in pipeline_config:
        return pipeline_config

    step_configs = []
    # TODO: make sure order is unique so hashes are stable?
    for step in pipeline_config["steps"]:
        step_module_config = extract_data_to_hash_from_pipeline_config(
            step["module_config"]
        )
        data = {
            "module_type": step["module_type"],
            "module_config": step_module_config,
            "step_id": step["step_id"],
        }
        step_configs.append(data)

    result = {
        "steps": step_configs,
        "constants": pipeline_config.get("constants", {}),
        "defaults": pipeline_config.get("defaults", {}),
        "input_aliases": pipeline_config.get("input_aliases", {}),
        "output_aliases": pipeline_config.get("output_aliases", {}),
    }
    return result


# kiara\kiara\src\kiara\utils\reflection.py
# -*- coding: utf-8 -*-
import inspect
from typing import Any, Callable, Dict


def extract_signature_metadata(func: Callable) -> Dict[str, Any]:

    signature = inspect.signature(func)
    result: Dict[str, Any] = {}
    for param_name, param in signature.parameters.items():
        result.setdefault("parameters", {})[param_name] = {
            "type": param.annotation,
        }
        default = None if param.default == inspect._empty else param.default
        result.setdefault("parameters", {})[param_name]["default"] = default
        result.setdefault("parameters", {})[param_name]["required"] = (
            param.default == inspect._empty
        )

    result["return_type"] = signature.return_annotation

    return result


# kiara\kiara\src\kiara\utils\rendering.py
# -*- coding: utf-8 -*-
from typing import TYPE_CHECKING, Any, Mapping, MutableMapping

from kiara.exceptions import KiaraException

if TYPE_CHECKING:
    from kiara.models.module.pipeline import Pipeline


def create_pipeline_render_inputs(
    pipeline: "Pipeline", pipeline_inputs_user: Mapping[str, Any]
) -> Mapping[str, Any]:
    from kiara.defaults import SpecialValue

    invalid = []
    for field_name in pipeline_inputs_user.keys():
        if field_name not in pipeline.pipeline_inputs_schema.keys():
            invalid.append(field_name)

    if invalid:
        msg = "Valid pipeline inputs:\n"
        for field_name, field in pipeline.pipeline_inputs_schema.items():
            msg = f"{msg}  - *{field_name}*: {field.doc.description}\n"
        raise KiaraException(
            msg=f"Invalid pipeline inputs: {', '.join(invalid)}.", details=msg
        )

    pipeline_inputs = {}
    for field_name, schema in pipeline.pipeline_inputs_schema.items():
        if field_name in pipeline_inputs_user.keys():
            value = pipeline_inputs_user[field_name]
        elif schema.default not in [SpecialValue.NOT_SET]:
            if callable(schema.default):
                value = schema.default()
            else:
                value = schema.default
        elif not schema.is_required():
            value = None
        else:
            value = "<TODO_SET_INPUT>"

        if isinstance(value, str):
            value = f'"{value}"'
        pipeline_inputs[field_name] = value

    inputs: MutableMapping[str, Any] = {}
    inputs["pipeline"] = pipeline
    inputs["pipeline_inputs"] = pipeline_inputs
    return inputs


# kiara\kiara\src\kiara\utils\stores.py
# -*- coding: utf-8 -*-
from typing import TYPE_CHECKING, Any, Dict, Iterable, List, Mapping, Type, Union

from kiara.defaults import ARCHIVE_NAME_MARKER

if TYPE_CHECKING:
    from kiara.registries import KiaraArchive


def create_new_archive(
    archive_name: str,
    store_base_path: str,
    store_type: str,
    allow_write_access: bool = False,
    set_archive_name_metadata: bool = True,
    **kwargs: Any,
) -> "KiaraArchive":
    """Create a new archive instance of the specified type.

    Arguments:
        archive_name: Name of the archive.
        store_base_path: Base path for the archive.
        store_type: Type of the archive.
        allow_write_access: Whether write access should be allowed.
        set_archive_name_metadata: Whether to set the archive name as metadata within the archive.
        **kwargs: Additional arguments to pass to the archive config constructor.
    """

    from kiara.utils.class_loading import find_all_archive_types

    archive_types = find_all_archive_types()

    archive_cls: Union[Type[KiaraArchive], None] = archive_types.get(store_type, None)
    if archive_cls is None:
        raise Exception(
            f"Can't create context: no archive type '{store_type}' available. Available types: {', '.join(archive_types.keys())}"
        )

    # config = archive_cls.create_store_config_instance(config=store_config)
    config = archive_cls.create_new_store_config(store_base_path, **kwargs)
    # TODO: make sure this constructor always exists?

    force_read_only = not allow_write_access

    archive_instance = archive_cls(archive_name=archive_name, archive_config=config, force_read_only=force_read_only)  # type: ignore

    if not force_read_only and set_archive_name_metadata:
        archive_instance.set_archive_metadata_value(ARCHIVE_NAME_MARKER, archive_name)

    return archive_instance


def check_external_archive(
    archive: Union[str, "KiaraArchive", Iterable[Union["KiaraArchive", str]]],
    allow_write_access: bool = False,
    archive_name: Union[str, None] = None,
) -> Mapping[str, "KiaraArchive"]:

    from kiara.context.config import KiaraArchiveReference
    from kiara.registries import KiaraArchive

    if isinstance(archive, (KiaraArchive, str)):
        _archives: List[Union[str, KiaraArchive]] = [archive]
    else:
        _archives = list(archive)

    archive_instances: Dict[str, KiaraArchive] = {}

    for _archive in _archives:

        if isinstance(_archive, KiaraArchive):
            for archive_type in _archive.supported_item_types():
                if archive_type in archive_instances.keys():
                    raise Exception(
                        "Multiple archives of the same type are not supported."
                    )
                if archive_name and _archive.archive_name != archive_name:
                    raise Exception(
                        f"Archive alias '{_archive.archive_name}' does not match expected alias '{archive_name}'"
                    )
                archive_instances[archive_type] = _archive
            continue

        loaded = KiaraArchiveReference.load_existing_archive(
            archive_uri=_archive,
            allow_write_access=allow_write_access,
            archive_name=archive_name,
        )

        for _archive_inst in loaded.archives:
            for archive_type in _archive_inst.supported_item_types():
                if archive_type in archive_instances.keys():
                    raise Exception(
                        "Multiple archives of the same type are not supported."
                    )
                archive_instances[archive_type] = _archive_inst

    return archive_instances


# kiara\kiara\src\kiara\utils\string_vars.py
# -*- coding: utf-8 -*-
#  Copyright (c) 2019, Markus Binsteiner
#
# Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)
# as well as
# Parity Public License, version 7.0.0 (see https://paritylicense.com/)

import typing
from typing import Any, Mapping, Sequence, Set, Union

import regex as re
import structlog
from regex.regex import Pattern

log = structlog.getLogger()


def create_var_regex(
    delimiter_start: Union[str, None] = None, delimiter_end: Union[str, None] = None
) -> Pattern:

    if delimiter_start is None:
        delimiter_start = "\\$\\{"

    # TODO: make this smarter
    if delimiter_end is None:
        delimiter_end = "\\}"

    regex = re.compile(delimiter_start + "\\s*(.+?)\\s*" + delimiter_end)
    return regex


def find_var_names_in_obj(
    template_obj: Any,
    delimiter: Union[Pattern, str, None] = None,
    delimiter_end: Union[str, None] = None,
) -> Set[str]:

    if isinstance(delimiter, Pattern):
        regex = delimiter
    else:
        regex = create_var_regex(delimiter_start=delimiter, delimiter_end=delimiter_end)

    var_names = find_regex_matches_in_obj(template_obj, regex=regex)

    return var_names


def replace_var_names_in_obj(
    template_obj: Any,
    repl_dict: typing.Mapping[str, Any],
    delimiter: Union[Pattern, str, None] = None,
    delimiter_end: Union[str, None] = None,
    ignore_missing_keys: bool = False,
) -> Any:

    if isinstance(delimiter, Pattern):
        regex = delimiter
    else:
        regex = create_var_regex(delimiter_start=delimiter, delimiter_end=delimiter_end)

    if not template_obj:
        return template_obj

    if isinstance(template_obj, Mapping):
        result: Any = {}
        for k, v in template_obj.items():
            key = replace_var_names_in_obj(
                template_obj=k,
                repl_dict=repl_dict,
                delimiter=regex,
                ignore_missing_keys=ignore_missing_keys,
            )
            value = replace_var_names_in_obj(
                template_obj=v,
                repl_dict=repl_dict,
                delimiter=regex,
                ignore_missing_keys=ignore_missing_keys,
            )
            result[key] = value
    elif isinstance(template_obj, str):
        result = replace_var_names_in_string(
            template_obj,
            repl_dict=repl_dict,
            regex=regex,
            ignore_missing_keys=ignore_missing_keys,
        )
    elif isinstance(template_obj, Sequence):
        result = []
        for item in template_obj:
            r = replace_var_names_in_obj(
                item,
                repl_dict=repl_dict,
                delimiter=regex,
                ignore_missing_keys=ignore_missing_keys,
            )
            result.append(r)
    else:
        result = template_obj

    return result


def replace_var_names_in_string(
    template_string: str,
    repl_dict: typing.Mapping[str, Any],
    regex: Pattern,
    ignore_missing_keys: bool = False,
) -> str:
    def sub(match):

        key = match.groups()[0]

        if key not in repl_dict.keys():
            if not ignore_missing_keys:
                raise Exception(
                    f"Can't insert variable '{key}'. Key not in provided input values, available keys: {', '.join(repl_dict.keys())}"
                )
            else:
                return match[0]
        else:
            result = repl_dict[key]
            return result

    result: str = regex.sub(sub, template_string)
    return result


def find_regex_matches_in_obj(
    source_obj: Any, regex: Pattern, current: Union[Set[str], None] = None
) -> Set[str]:

    if current is None:
        current = set()

    if not source_obj:
        return current

    if isinstance(source_obj, Mapping):
        for k, v in source_obj.items():
            find_regex_matches_in_obj(k, regex=regex, current=current)
            find_regex_matches_in_obj(v, regex=regex, current=current)
    elif isinstance(source_obj, str):

        matches = regex.findall(source_obj)
        current.update(matches)

    elif isinstance(source_obj, Sequence):

        for item in source_obj:
            find_regex_matches_in_obj(item, regex=regex, current=current)

    return current


# kiara\kiara\src\kiara\utils\values.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import copy
import uuid
from typing import TYPE_CHECKING, Any, Dict, Mapping, Union

from kiara.defaults import (
    DEFAULT_NO_DESC_VALUE,
    INVALID_VALUE_NAMES,
    NONE_VALUE_ID,
    SpecialValue,
)
from kiara.models.values.value_schema import ValueSchema
from kiara.utils import check_valid_field_names

if TYPE_CHECKING:
    from kiara.context import Kiara
    from kiara.interfaces.python_api.base_api import BaseAPI
    from kiara.models.values.value import ValueMapReadOnly
    from kiara.registries.data import ValueLink


def construct_valuemap(
    kiara_api: "BaseAPI",
    values: Mapping[str, Union[uuid.UUID, None, str, "ValueLink"]],
) -> "ValueMapReadOnly":

    value_items = {}
    schemas = {}
    for field_name, value_id in values.items():
        if value_id is None:
            value_id = NONE_VALUE_ID

        value = kiara_api.get_value(value=value_id)  # type: ignore
        value_items[field_name] = value
        schemas[field_name] = value.value_schema

    from kiara.models.values.value import ValueMapReadOnly

    return ValueMapReadOnly(value_items=value_items, values_schema=schemas)


def create_schema_dict(
    schema_config: Mapping[str, Union[ValueSchema, Mapping[str, Any]]],
) -> Mapping[str, ValueSchema]:

    invalid = check_valid_field_names(*schema_config.keys())
    if invalid:
        raise Exception(
            f"Can't assemble schema because it contains invalid input field name(s) '{', '.join(invalid)}'. Change the input schema to not contain any of the reserved keywords: {', '.join(INVALID_VALUE_NAMES)}"
        )

    result = {}
    for k, v in schema_config.items():

        if isinstance(v, ValueSchema):
            result[k] = v
        elif isinstance(v, Mapping):
            _v = dict(v)
            if "doc" not in _v.keys():
                _v["doc"] = DEFAULT_NO_DESC_VALUE
            schema = ValueSchema(**_v)

            result[k] = schema
        else:
            if v is None:
                msg = "None"
            else:
                msg = v.__class__
            raise Exception(
                f"Invalid return type '{msg}' for field '{k}' when trying to create schema."
            )

    return result


def overlay_constants_and_defaults(
    schemas: Mapping[str, ValueSchema],
    defaults: Mapping[str, Any],
    constants: Mapping[str, Any],
):

    for k, v in schemas.items():

        default_value = defaults.get(k, None)
        constant_value = constants.get(k, None)

        # value_to_test = None
        if default_value is not None and constant_value is not None:
            raise Exception(
                f"Module configuration error. Value '{k}' set in both 'constants' and 'defaults', this is not allowed."
            )

        # TODO: perform validation for constants/defaults

        if default_value is not None:
            schemas[k].default = default_value

        if constant_value is not None:
            schemas[k].default = constant_value
            schemas[k].is_constant = True

    input_schemas = {}
    constants = {}
    for k, v in schemas.items():
        if v.is_constant:
            constants[k] = v
        else:
            input_schemas[k] = v

    return input_schemas, constants


def augment_values(
    values: Mapping[str, Any],
    schemas: Mapping[str, ValueSchema],
    constants: Union[Mapping[str, ValueSchema], None] = None,
) -> Dict[str, Any]:

    # TODO: check if extra fields were provided

    if constants:
        for k, v in constants.items():
            if k in values.keys():
                raise Exception(f"Invalid input: value provided for constant '{k}'")

    values_new = {}

    if constants:
        for field_name, schema in constants.items():
            v = schema.default
            assert v not in [None, SpecialValue.NO_VALUE, SpecialValue.NOT_SET]
            if callable(v):
                values_new[field_name] = v()
            else:
                values_new[field_name] = copy.deepcopy(v)

    for field_name, schema in schemas.items():

        if field_name in values_new.keys():
            raise Exception(
                f"Duplicate field '{field_name}', this is most likely a bug."
            )

        val = values.get(field_name, None)
        use_default = False
        if val is None:
            use_default = True
        elif hasattr(val, "is_set"):
            if not val.is_set:  # type: ignore
                use_default = True
        if use_default:
            if schema.default != SpecialValue.NOT_SET:
                if callable(schema.default):
                    values_new[field_name] = schema.default()
                else:
                    values_new[field_name] = copy.deepcopy(schema.default)
            else:
                values_new[field_name] = SpecialValue.NOT_SET
        else:
            value = values[field_name]
            assert value is not None

            values_new[field_name] = value

    return values_new


def extract_raw_values(kiara: "Kiara", **value_ids: uuid.UUID) -> Dict[str, Any]:

    result = {}
    for field_name, value_id in value_ids.items():
        result[field_name] = extract_raw_value(kiara=kiara, value_id=value_id)
    return result


# def extract_raw_value(kiara: "Kiara", value_id: uuid.UUID):
#     value = kiara.data_registry.get_value(value_id=value_id)
#     if value.pedigree != ORPHAN:
#         return f"value:{value_id}"
#     else:
#         return value.data


def extract_raw_value(kiara: "Kiara", value_id: uuid.UUID):
    value = kiara.data_registry.get_value(value=value_id)

    # TODO: check without import
    from kiara.models.values.value import ORPHAN

    if value.pedigree != ORPHAN:
        # TODO: find alias?
        return f'"value:{value_id}"'
    else:
        if value.value_schema.type == "string":
            return f'"{value.data}"'
        elif value.value_schema.type == "list":
            return value.data.list_data
        else:
            return value.data


# kiara\kiara\src\kiara\utils\windows.py
# -*- coding: utf-8 -*-
import os
import platform
import shutil
import tempfile
from functools import lru_cache
from pathlib import Path

is_windows = any(platform.win32_ver())


def fix_windows_longpath(path: Path) -> Path:
    if not is_windows:
        return path

    normalized = os.fspath(path.resolve())
    if not normalized.startswith("\\\\?\\"):
        normalized = "\\\\?\\" + normalized
    return Path(normalized)


def fix_windows_symlink(source: Path, target: Path) -> None:

    if not is_windows:
        target.symlink_to(source)
        return
    else:
        try:
            target.symlink_to(source)
        except OSError:
            import traceback

            raise Exception(
                "Operating system does not support symbolic links.",
                "link",
                (source, target),
                traceback.format_exc(),
            )


@lru_cache
def check_symlink_works() -> bool:

    dirname = tempfile.mkdtemp()

    source = Path(dirname) / "source"
    target = Path(dirname) / "target"

    source.touch()
    try:
        target.symlink_to(source)
        return True
    except OSError:
        return False
    finally:
        shutil.rmtree(dirname, ignore_errors=True)


# kiara\kiara\src\kiara\utils\yaml.py
# -*- coding: utf-8 -*-
from io import StringIO

from ruamel.yaml import YAML


class StringYAML(YAML):
    def dump(self, data, stream=None, **kw):
        inefficient = False
        if stream is None:
            inefficient = True
            stream = StringIO()
        YAML.dump(self, data, stream, **kw)
        if inefficient:
            return stream.getvalue()


# kiara\kiara\src\kiara\utils\__init__.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import inspect
import os
import re
from typing import TYPE_CHECKING, Dict, Iterable, List, Type, TypeVar

import structlog

from kiara.defaults import INVALID_VALUE_NAMES

if TYPE_CHECKING:
    from kiara.utils.develop import KiaraDevSettings

logger = structlog.get_logger()

CAMEL_TO_SNAKE_REGEX = re.compile(r"(?<!^)(?=[A-Z])")

WORD_REGEX_PATTERN = re.compile("[^A-Za-z]+")


def is_debug() -> bool:

    debug = os.environ.get("DEBUG", "")
    if debug.lower() == "true":
        return True
    else:
        return False


def is_develop() -> bool:

    develop = os.environ.get("DEVELOP", "")
    if not develop:
        develop = os.environ.get("DEV", "")

    if develop and develop.lower() != "false":
        return True

    return False


def get_dev_config() -> "KiaraDevSettings":

    from kiara.utils.develop import KIARA_DEV_SETTINGS

    return KIARA_DEV_SETTINGS


def is_jupyter() -> bool:

    try:
        get_ipython  # type: ignore
    except NameError:
        return False
    ipython = get_ipython()  # type: ignore  # noqa
    shell = ipython.__class__.__name__
    if shell == "TerminalInteractiveShell":
        return False
    elif "google.colab" in str(ipython.__class__) or shell == "ZMQInteractiveShell":
        return True
    else:
        return False


def log_exception(exc: Exception):

    if is_debug():
        logger.error(exc)

    if is_develop():
        from kiara.utils.develop import DetailLevel

        config = get_dev_config()
        if config.log.exc in [DetailLevel.NONE, "none"]:
            return

        show_locals = config.log.exc in [DetailLevel.FULL, "full"]

        from kiara.interfaces import get_console
        from kiara.utils.develop import log_dev_message

        # exc_info = sys.exc_info()
        exc_info = (type(exc), exc, exc.__traceback__)

        # if not exc_info or not exc_info[0]:
        #     # TODO: create exc_info from exception?
        #     if not is_debug():
        #         logger.error(exc)
        # else:
        console = get_console()
        from rich.traceback import Traceback

        log_dev_message(
            Traceback.from_exception(
                exc_info[0], exc_info[1], traceback=exc_info[2], show_locals=show_locals, width=console.width - 4  # type: ignore
            ),
            title="Exception details",
        )


def log_message(msg: str, **data):

    if is_debug():
        logger.debug(msg, **data)
    # else:
    #     logger.debug(msg, **data)


_AUTO_MODULE_ID: Dict[str, int] = {}


def get_auto_workflow_alias(module_type: str, use_incremental_ids: bool = False) -> str:
    """
    Return an id for a workflow obj of a provided module class.

    If 'use_incremental_ids' is set to True, a unique id is returned.

    Args:
    ----
        module_type (str): the name of the module type
        use_incremental_ids (bool): whether to return a unique (incremental) id

    Returns:
    -------
        str: a module id
    """
    if not use_incremental_ids:
        return module_type

    nr = _AUTO_MODULE_ID.setdefault(module_type, 0)
    _AUTO_MODULE_ID[module_type] = nr + 1

    return f"{module_type}_{nr}"


def camel_case_to_snake_case(camel_text: str, repl: str = "_") -> str:
    return CAMEL_TO_SNAKE_REGEX.sub(repl, camel_text).lower()


def to_camel_case(text: str) -> str:

    words = WORD_REGEX_PATTERN.split(text)
    return "".join(w.title() for i, w in enumerate(words))


SUBCLASS_TYPE = TypeVar("SUBCLASS_TYPE")


def _get_all_subclasses(
    cls: Type[SUBCLASS_TYPE], ignore_abstract: bool = False
) -> Iterable[Type[SUBCLASS_TYPE]]:

    result = []
    for subclass in cls.__subclasses__():
        if ignore_abstract and inspect.isabstract(subclass):
            continue
        result.append(subclass)
        result.extend(_get_all_subclasses(subclass))

    return result


def check_valid_field_names(*field_names) -> List[str]:
    """
    Check whether the provided field names are all valid.

    Returns:
    -------
        an iterable of strings with invalid field names
    """
    return [x for x in field_names if x in INVALID_VALUE_NAMES or x.startswith("_")]


def find_free_id(
    stem: str,
    current_ids: Iterable[str],
    sep="_",
) -> str:
    """
    Find a free var (or other name) based on a stem string, based on a list of provided existing names.

    Args:
    ----
        stem (str): the base string to use
        current_ids (Iterable[str]): currently existing names
        method (str): the method to create new names (allowed: 'count' -- for now)
        method_args (dict): prototing_config for the creation method

    Returns:
    -------
        str: a free name
    """
    start_count = 1
    if stem not in current_ids:
        return stem

    i = start_count

    # new_name = None
    while True:
        new_name = f"{stem}{sep}{i}"
        if new_name in current_ids:
            i = i + 1
            continue
        break
    return new_name


def first_line(text: str):

    if "\n" in text:
        return text.split("\n")[0].strip()
    else:
        return text


# kiara\kiara\src\kiara\utils\cli\exceptions.py
# -*- coding: utf-8 -*-
from functools import partial, wraps

from kiara.utils import is_debug, is_develop
from kiara.utils.cli import terminal_print


def handle_exception(
    func=None,
):

    if not func:
        return partial(handle_exception)

    @wraps(func)
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:

            if is_debug() or is_develop():
                import traceback

                traceback.print_exc()
            terminal_print("")
            terminal_print(e)

    return wrapper


# kiara\kiara\src\kiara\utils\cli\rich_click.py
# -*- coding: utf-8 -*-
import inspect
import textwrap
from typing import ClassVar, List, Union

import click
from rich import box
from rich.align import Align
from rich.console import Group, RenderableType
from rich.highlighter import RegexHighlighter
from rich.markdown import Markdown
from rich.padding import Padding
from rich.panel import Panel
from rich.table import Table
from rich.text import Text

# Support rich <= 10.6.0
from rich_click.rich_click import (
    ALIGN_COMMANDS_PANEL,
    ALIGN_OPTIONS_PANEL,
    ARGUMENTS_PANEL_TITLE,
    COMMAND_GROUPS,
    COMMANDS_PANEL_TITLE,
    FOOTER_TEXT,
    GROUP_ARGUMENTS_OPTIONS,
    HEADER_TEXT,
    MAX_WIDTH,
    OPTION_GROUPS,
    OPTIONS_PANEL_TITLE,
    RANGE_STRING,
    REQUIRED_SHORT_STRING,
    SHOW_ARGUMENTS,
    SHOW_METAVARS_COLUMN,
    STYLE_COMMANDS_PANEL_BORDER,
    STYLE_COMMANDS_TABLE_BORDER_STYLE,
    STYLE_COMMANDS_TABLE_BOX,
    STYLE_COMMANDS_TABLE_LEADING,
    STYLE_COMMANDS_TABLE_PAD_EDGE,
    STYLE_COMMANDS_TABLE_PADDING,
    STYLE_COMMANDS_TABLE_ROW_STYLES,
    STYLE_COMMANDS_TABLE_SHOW_LINES,
    STYLE_FOOTER_TEXT,
    STYLE_HEADER_TEXT,
    STYLE_METAVAR,
    STYLE_OPTIONS_PANEL_BORDER,
    STYLE_OPTIONS_TABLE_BORDER_STYLE,
    STYLE_OPTIONS_TABLE_BOX,
    STYLE_OPTIONS_TABLE_LEADING,
    STYLE_OPTIONS_TABLE_PAD_EDGE,
    STYLE_OPTIONS_TABLE_PADDING,
    STYLE_OPTIONS_TABLE_ROW_STYLES,
    STYLE_OPTIONS_TABLE_SHOW_LINES,
    STYLE_REQUIRED_SHORT,
    USE_CLICK_SHORT_HELP,
    _get_option_help,
    _make_command_help,
    _make_rich_rext,
    highlighter,
)

from kiara.api import ValueMap
from kiara.interfaces.python_api.base_api import BaseAPI
from kiara.models.module.operation import Operation

# from kiara.interfaces.python_api.operation import KiaraOperation
from kiara.operations.included_core_operations.filter import FilterOperationType
from kiara.utils.cli import terminal_print
from kiara.utils.operations import create_operation_status_renderable

# MIT License
# Copyright (c) 2022 Phil Ewels
# adapted from: https://github.com/ewels/rich-click


def rich_format_filter_operation_help(
    api: BaseAPI,
    obj: Union[click.Command, click.Group],
    ctx: click.Context,
    cmd_help: str,
    value: Union[None, str] = None,
) -> None:
    """Print nicely formatted help text using rich."""
    renderables: List[RenderableType] = []
    # Header text if we have it
    if HEADER_TEXT:
        renderables.append(
            Padding(_make_rich_rext(HEADER_TEXT, STYLE_HEADER_TEXT), (1, 1, 0, 1))
        )

    # Print usage

    _cmd = cmd_help
    renderables.append(Padding(_cmd, 1))
    d = inspect.getdoc(obj)
    if d is None:
        d = ""
    d = textwrap.dedent(d)
    renderables.append(
        Padding(
            Align(d, width=MAX_WIDTH, pad=False),  # type: ignore
            (0, 1, 1, 1),
        )
    )

    v = None
    if value:
        filter_op_type: FilterOperationType = api.get_operation_type("filter")  # type: ignore
        v = api.get_value(value)
        ops = filter_op_type.find_filter_operations_for_data_type(v.data_type_name)

        from kiara.interfaces.python_api.models.info import OperationGroupInfo

        ops_info = OperationGroupInfo.create_from_operations(
            kiara=api.context, group_title=f"{v.data_type_name} filters", **ops
        )
        p = Panel(
            ops_info,
            title=f"Available filter operations for type [i]'{v.data_type_name}'[/i]",
            title_align="left",
        )
        renderables.append(p)

    # Epilogue if we have it
    if obj.epilog:
        # Remove single linebreaks, replace double with single
        lines = obj.epilog.split("\n\n")
        epilogue = "\n".join([x.replace("\n", " ").strip() for x in lines])
        renderables.append(
            Padding(Align(highlighter(epilogue), width=MAX_WIDTH, pad=False), 1)
        )

    # Footer text if we have it
    if FOOTER_TEXT:
        renderables.append(
            Padding(_make_rich_rext(FOOTER_TEXT, STYLE_FOOTER_TEXT), (1, 1, 0, 1))
        )

    terminal_print(Group(*renderables))


def rich_format_operation_help(
    obj: Union[click.Command, click.Group],
    ctx: click.Context,
    operation: Operation,
    op_inputs: ValueMap,
    cmd_help: str,
) -> None:
    """
    Print nicely formatted help text using rich.

    Based on original code from rich-cli, by @willmcgugan.
    https://github.com/Textualize/rich-cli/blob/8a2767c7a340715fc6fbf4930ace717b9b2fc5e5/src/rich_cli/__main__.py#L162-L236

    Replacement for the click function format_help().
    Takes a command or group and builds the help text output.

    Args:
    ----
        obj (click.Command or click.Group): Command or group to build help text for
        ctx (click.Context): Click Context object
        table: a rich table, including all the inputs of the current operation
    """

    renderables: List[RenderableType] = []
    # Header text if we have it
    if HEADER_TEXT:
        renderables.append(
            Padding(_make_rich_rext(HEADER_TEXT, STYLE_HEADER_TEXT), (1, 1, 0, 1))
        )

    # Print usage

    _cmd = cmd_help
    renderables.append(Padding(_cmd, 1))
    # renderables.append(obj.get_usage(ctx))
    # renderables.append(Panel(Padding(highlighter(obj.get_usage(ctx)), 1), style=STYLE_USAGE_COMMAND, box=box.MINIMAL))

    # Print command / group help if we have some
    desc = operation.doc.full_doc
    renderables.append(
        Padding(
            Align(Markdown(desc), width=MAX_WIDTH, pad=False),
            (0, 1, 1, 1),
        )
    )

    # if obj.help:
    #
    #     # Print with a max width and some padding
    #     renderables.append(
    #         Padding(
    #             Align(_get_help_text(obj), width=MAX_WIDTH, pad=False),
    #             (0, 1, 1, 1),
    #         )
    #     )

    # Look through OPTION_GROUPS for this command
    # stick anything unmatched into a default group at the end
    option_groups = OPTION_GROUPS.get(ctx.command_path, []).copy()
    option_groups.append({"options": []})
    argument_group_options = []

    for param in obj.get_params(ctx):

        # Skip positional arguments - they don't have opts or helptext and are covered in usage
        # See https://click.palletsprojects.com/en/8.0.x/documentation/#documenting-arguments
        if type(param) is click.core.Argument and not SHOW_ARGUMENTS:
            continue

        # Skip if option is hidden
        if getattr(param, "hidden", False):
            continue

        # Already mentioned in a config option group
        for option_group in option_groups:
            if any(opt in option_group.get("options", []) for opt in param.opts):
                break

        # No break, no mention - add to the default group
        else:
            if type(param) is click.core.Argument and not GROUP_ARGUMENTS_OPTIONS:
                argument_group_options.append(param.opts[0])
            else:
                list_of_option_groups: List = option_groups[-1]["options"]  # type: ignore
                list_of_option_groups.append(param.opts[0])

    # If we're not grouping arguments and we got some, prepend before default options
    if len(argument_group_options) > 0:
        extra_option_group = {
            "name": ARGUMENTS_PANEL_TITLE,
            "options": argument_group_options,
        }
        option_groups.insert(len(option_groups) - 1, extra_option_group)  # type: ignore

    # Print each option group panel
    for option_group in option_groups:

        options_rows = []
        for opt in option_group.get("options", []):

            # Get the param
            for param in obj.get_params(ctx):
                if any([opt in param.opts]):
                    break
            # Skip if option is not listed in this group
            else:
                continue

            # Short and long form
            opt_long_strs = []
            opt_short_strs = []
            for idx, opt in enumerate(param.opts):
                opt_str = opt
                try:
                    opt_str += "/" + param.secondary_opts[idx]
                except IndexError:
                    pass
                if "--" in opt:
                    opt_long_strs.append(opt_str)
                else:
                    opt_short_strs.append(opt_str)

            # Column for a metavar, if we have one
            metavar = Text(style=STYLE_METAVAR, overflow="fold")
            metavar_str = param.make_metavar()

            # Do it ourselves if this is a positional argument
            if type(param) is click.core.Argument and metavar_str == param.name.upper():  # type: ignore
                metavar_str = param.type.name.upper()

            # Skip booleans and choices (handled above)
            if metavar_str != "BOOLEAN":
                metavar.append(metavar_str)

            # Range - from
            # https://github.com/pallets/click/blob/c63c70dabd3f86ca68678b4f00951f78f52d0270/src/click/core.py#L2698-L2706
            try:
                # skip count with default range type
                if isinstance(param.type, click.types._NumberRangeBase) and not (
                    param.count and param.type.min == 0 and param.type.max is None  # type: ignore
                ):
                    range_str = param.type._describe_range()
                    if range_str:
                        metavar.append(RANGE_STRING.format(range_str))
            except AttributeError:
                # click.types._NumberRangeBase is only in Click 8x onwards
                pass

            # Required asterisk
            required: RenderableType = ""
            if param.required:
                required = Text(REQUIRED_SHORT_STRING, style=STYLE_REQUIRED_SHORT)

            # Highlighter to make [ | ] and <> dim
            class MetavarHighlighter(RegexHighlighter):
                highlights: ClassVar = [  # type: ignore
                    r"^(?P<metavar_sep>(\[|<))",
                    r"(?P<metavar_sep>\|)",
                    r"(?P<metavar_sep>(\]|>)$)",
                ]

            metavar_highlighter = MetavarHighlighter()
            rows = [
                required,
                highlighter(highlighter(",".join(opt_long_strs))),
                highlighter(highlighter(",".join(opt_short_strs))),
                metavar_highlighter(metavar),
                _get_option_help(param, ctx),  # type: ignore
            ]

            # Remove metavar if specified in config
            if not SHOW_METAVARS_COLUMN:
                rows.pop(3)

            options_rows.append(rows)

        if len(options_rows) > 0:
            t_styles = {
                "show_lines": STYLE_OPTIONS_TABLE_SHOW_LINES,
                "leading": STYLE_OPTIONS_TABLE_LEADING,
                "box": STYLE_OPTIONS_TABLE_BOX,
                "border_style": STYLE_OPTIONS_TABLE_BORDER_STYLE,
                "row_styles": STYLE_OPTIONS_TABLE_ROW_STYLES,
                "pad_edge": STYLE_OPTIONS_TABLE_PAD_EDGE,
                "padding": STYLE_OPTIONS_TABLE_PADDING,
            }
            t_styles.update(option_group.get("table_styles", {}))  # type: ignore
            box_style = getattr(box, t_styles.pop("box"), None)  # type: ignore

            options_table = Table(
                highlight=True,
                show_header=False,
                expand=True,
                box=box_style,
                **t_styles,  # type: ignore
            )
            # Strip the required column if none are required
            if all(not x[0] for x in options_rows):
                options_rows = [x[1:] for x in options_rows]
            for row in options_rows:
                options_table.add_row(*row)
            renderables.append(
                Panel(
                    options_table,
                    border_style=STYLE_OPTIONS_PANEL_BORDER,  # type: ignore
                    title=option_group.get("name", OPTIONS_PANEL_TITLE),  # type: ignore
                    title_align=ALIGN_OPTIONS_PANEL,  # type: ignore
                    width=MAX_WIDTH,  # type: ignore
                )
            )

    #
    # Groups only:
    # List click command groups
    #
    if hasattr(obj, "list_commands"):
        # Look through COMMAND_GROUPS for this command
        # stick anything unmatched into a default group at the end
        cmd_groups = COMMAND_GROUPS.get(ctx.command_path, []).copy()
        cmd_groups.append({"commands": []})
        for command in obj.list_commands(ctx):  # type: ignore
            for cmd_group in cmd_groups:
                if command in cmd_group.get("commands", []):
                    break
            else:
                commands: List = cmd_groups[-1]["commands"]  # type: ignore
                commands.append(command)

        # Print each command group panel
        for cmd_group in cmd_groups:
            t_styles = {
                "show_lines": STYLE_COMMANDS_TABLE_SHOW_LINES,
                "leading": STYLE_COMMANDS_TABLE_LEADING,
                "box": STYLE_COMMANDS_TABLE_BOX,
                "border_style": STYLE_COMMANDS_TABLE_BORDER_STYLE,
                "row_styles": STYLE_COMMANDS_TABLE_ROW_STYLES,
                "pad_edge": STYLE_COMMANDS_TABLE_PAD_EDGE,
                "padding": STYLE_COMMANDS_TABLE_PADDING,
            }
            t_styles.update(cmd_group.get("table_styles", {}))  # type: ignore
            box_style = getattr(box, t_styles.pop("box"), None)  # type: ignore

            commands_table = Table(
                highlight=False,
                show_header=False,
                expand=True,
                box=box_style,  # type: ignore
                **t_styles,  # type: ignore
            )
            # Define formatting in first column, as commands don't match highlighter regex
            commands_table.add_column(style="bold cyan", no_wrap=True)
            for command in cmd_group.get("commands", []):
                # Skip if command does not exist
                if command not in obj.list_commands(ctx):  # type: ignore
                    continue
                cmd = obj.get_command(ctx, command)  # type: ignore
                assert cmd is not None
                if cmd.hidden:
                    continue
                # Use the truncated short text as with vanilla text if requested
                if USE_CLICK_SHORT_HELP:
                    helptext = cmd.get_short_help_str()
                else:
                    # Use short_help function argument if used, or the full help
                    helptext = cmd.short_help or cmd.help or ""
                commands_table.add_row(command, _make_command_help(helptext))
            if commands_table.row_count > 0:
                renderables.append(
                    Panel(
                        commands_table,
                        border_style=STYLE_COMMANDS_PANEL_BORDER,  # type: ignore
                        title=cmd_group.get("name", COMMANDS_PANEL_TITLE),  # type: ignore
                        title_align=ALIGN_COMMANDS_PANEL,  # type: ignore
                        width=MAX_WIDTH,  # type: ignore
                    )
                )

    inputs_table = create_operation_status_renderable(
        operation=operation,
        inputs=op_inputs,
        render_config={
            "show_operation_name": False,
            "show_inputs": True,
            "show_outputs_schema": False,
            "show_headers": False,
            "show_operation_doc": False,
        },
    )
    # inputs_table = operation.create_renderable(
    #     show_operation_name=False,
    #     show_operation_doc=False,
    #     show_inputs=True,
    #     show_outputs_schema=False,
    #     show_headers=False,
    # )

    inputs_panel = Panel(
        inputs_table,
        title="Inputs",
        border_style=STYLE_COMMANDS_PANEL_BORDER,  # type: ignore
        title_align=ALIGN_COMMANDS_PANEL,  # type: ignore
        width=MAX_WIDTH,  # type: ignore
    )
    renderables.append(inputs_panel)

    # Epilogue if we have it
    if obj.epilog:
        # Remove single linebreaks, replace double with single
        lines = obj.epilog.split("\n\n")
        epilogue = "\n".join([x.replace("\n", " ").strip() for x in lines])
        renderables.append(
            Padding(Align(highlighter(epilogue), width=MAX_WIDTH, pad=False), 1)
        )

    # Footer text if we have it
    if FOOTER_TEXT:
        renderables.append(
            Padding(_make_rich_rext(FOOTER_TEXT, STYLE_FOOTER_TEXT), (1, 1, 0, 1))
        )

    group = Group(*renderables)
    terminal_print(group)


# kiara\kiara\src\kiara\utils\cli\run.py
# -*- coding: utf-8 -*-
import sys
import uuid
from pathlib import Path
from typing import Any, Dict, Iterable, List, Mapping, Union

from click import Context as ClickContext
from pydantic import ValidationError
from rich.console import Group, RenderableType
from rich.markdown import Markdown
from rich.rule import Rule

from kiara.exceptions import (
    FailedJobException,
    InvalidCommandLineInvocation,
    KiaraException,
    NoSuchExecutionTargetException,
)
from kiara.interfaces.python_api.base_api import BaseAPI
from kiara.interfaces.python_api.utils import create_save_config
from kiara.models.module.operation import Operation
from kiara.models.values.value import ValueMap

# from kiara.interfaces.python_api.operation import KiaraOperation
from kiara.utils import log_exception
from kiara.utils.cli import dict_from_cli_args, terminal_print, terminal_print_model
from kiara.utils.cli.rich_click import rich_format_operation_help
from kiara.utils.operations import create_operation_status_renderable
from kiara.utils.output import create_table_from_base_model_cls


def _validate_save_option(save: Iterable[str]) -> bool:

    if save:
        for a in save:
            if "=" in a:
                tokens = a.split("=")
                if len(tokens) != 2:
                    raise InvalidCommandLineInvocation(
                        msg=f"Invalid alias format, can only contain a single '=': {a}",
                        error_code=1,
                    )

        return True
    else:
        return False


def validate_operation_in_terminal(
    api: BaseAPI,
    module_or_operation: Union[str, Path, Mapping[str, Any]],
    allow_external=False,
) -> Operation:

    # kiara_op = KiaraOperation(
    #     kiara=kiara,
    #     operation_name=module_or_operation,
    #     operation_config=module_config,
    # )
    try:
        operation: Operation = api.get_operation(operation=module_or_operation)
        # validate that operation config is valid, ignoring inputs for now
        # kiara_op.operation
    except NoSuchExecutionTargetException as nset:

        terminal_print()
        terminal_print(nset)
        terminal_print()
        terminal_print("Existing operations:")
        terminal_print()
        for n in nset.avaliable_targets:
            terminal_print(f"  - [i]{n}[/i]")
        raise InvalidCommandLineInvocation("No such target.", parent=nset, error_code=1)
    except ValidationError as ve:

        renderables: List[RenderableType] = [""]
        renderables.append("Invalid module configuration:")
        renderables.append("")
        for error in ve.errors():
            loc = ", ".join(error["loc"])  # type: ignore
            renderables.append(f"  [b]{loc}[/b]: [red]{error['msg']}[/red]")

        try:
            if isinstance(module_or_operation, str):
                m = api.context.module_registry.get_module_class(module_or_operation)
                schema = create_table_from_base_model_cls(m._config_cls)
                renderables.append("")
                renderables.append(f"Module configuration schema for '[b i]{m._module_type_name}[/b i]':")  # type: ignore
                renderables.append("")
                renderables.append(schema)
        except Exception:
            pass

        msg = Group(*renderables)
        terminal_print()
        terminal_print(msg, in_panel="[b red]Module configuration error[/b red]")
        raise InvalidCommandLineInvocation(
            "Invalid module config.", parent=ve, error_code=1
        )

    except Exception as e:
        log_exception(e)
        terminal_print()
        terminal_print(
            f"Error when trying to validate the operation [i]'{module_or_operation}'[/i]:\n"
        )
        terminal_print(f"    [red]{e}[/red]")
        root_cause = KiaraException.get_root_details(e)
        if root_cause:
            terminal_print()
            terminal_print(Markdown(root_cause))
        raise InvalidCommandLineInvocation(
            "Can't validate operation.", parent=e, error_code=1
        )

    return operation


def calculate_aliases(
    operation: Operation,
    alias_tokens: Iterable[str],
    extra_aliases: Union[Mapping[str, str], None] = None,
) -> Mapping[str, List[str]]:

    if not alias_tokens:
        aliases: Dict[str, List[str]] = {}
        full_aliases: List[str] = []
    else:
        aliases = {}
        full_aliases = []
        for a in alias_tokens:
            if "=" not in a:
                full_aliases.append(a)
            else:
                tokens = a.split("=")
                if len(tokens) != 2:
                    terminal_print()
                    terminal_print(
                        f"Invalid alias format, can only contain a single '=': {a}"
                    )
                    sys.exit(1)

                aliases.setdefault(tokens[0], []).append(tokens[1])

    # =========================================================================
    # check save user input
    final_aliases: Dict[str, List[str]] = {}
    if alias_tokens:
        op_output_names = operation.outputs_schema.keys()
        invalid_fields = []
        for field_name, alias in aliases.items():
            if field_name not in op_output_names:
                invalid_fields.append(field_name)
            else:
                final_aliases[field_name] = alias

        for _alias in full_aliases:
            for field_name in op_output_names:
                final_aliases.setdefault(field_name, []).append(
                    f"{_alias}.{field_name}"
                )

        if invalid_fields:
            terminal_print()
            terminal_print(
                f"Can't run workflow, invalid field name(s) when specifying aliases: {', '.join(invalid_fields)}. Valid field names: {', '.join(op_output_names)}"
            )
            sys.exit(1)

    if extra_aliases:
        op_output_names = operation.outputs_schema.keys()
        invalid_fields = []
        for field_name, _alias in extra_aliases.items():
            if field_name not in op_output_names:
                invalid_fields.append(field_name)
            elif _alias not in final_aliases:
                final_aliases.setdefault(field_name, []).append(_alias)

        if invalid_fields:
            terminal_print()
            terminal_print(
                f"Can't run workflow, invalid field name(s) in extra save aliases: {', '.join(invalid_fields)}. Valid field names: {', '.join(op_output_names)}"
            )
            sys.exit(1)

    return final_aliases


def set_and_validate_inputs(
    api: BaseAPI,
    operation: Operation,
    inputs: Iterable[str],
    explain: bool,
    print_help: bool,
    click_context: ClickContext,
    cmd_help: str,
    base_inputs: Union[None, Mapping[str, Any]] = None,
) -> Union[ValueMap, None]:

    # =========================================================================
    # prepare inputs
    list_keys = []
    for (
        name,
        value_schema,
    ) in operation.operation_details.inputs_schema.items():
        if value_schema.type in ["list", "kiara_model_list"]:
            list_keys.append(name)

    try:
        inputs_dict: Dict[str, Any] = dict_from_cli_args(*inputs, list_keys=list_keys)
        if base_inputs:
            for k, v in base_inputs.items():
                if k not in inputs_dict.keys():
                    inputs_dict[k] = v

        value_map = api.assemble_value_map(
            values=inputs_dict,
            values_schema=operation.inputs_schema,
            register_data=True,
            reuse_existing_data=False,
        )
    except Exception as e:
        log_exception(e)
        terminal_print()
        rg = Group(
            "",
            f"Can't run operation: {e}",
            "",
            Rule(),
            "",
            create_operation_status_renderable(
                operation=operation,
                inputs=None,
                render_config={
                    "show_operation_name": True,
                    "show_inputs": False,
                    "show_outputs_schema": True,
                },
            ),
        )
        terminal_print(rg, in_panel=f"Run info: [b]{operation.operation_id}[/b]")
        sys.exit(1)

    if value_map.check_invalid():
        terminal_print()
        rg = Group(
            "",
            "Can't run operation: invalid or insufficient input(s)",
            "",
            Rule(),
            "",
            create_operation_status_renderable(
                operation=operation,
                inputs=value_map,
                render_config={
                    "show_operation_name": True,
                    "show_inputs": True,
                    "show_outputs_schema": True,
                },
            ),
        )
        terminal_print(rg, in_panel=f"Run info: [b]{operation.operation_id}[/b]")
        raise InvalidCommandLineInvocation(
            msg="Invalid or insufficient input(s)", error_code=1
        )

    if print_help:
        rich_format_operation_help(
            obj=click_context.command,
            ctx=click_context,
            operation=operation,
            op_inputs=value_map,
            cmd_help=cmd_help,
        )
        return None

    if explain:
        terminal_print()
        rg = Group(
            "",
            create_operation_status_renderable(
                operation=operation,
                inputs=value_map,
                render_config={
                    "show_operation_name": True,
                    "show_inputs": True,
                    "show_outputs_schema": True,
                },
            ),
        )
        terminal_print(rg, in_panel=f"Operation info: [b]{operation.operation_id}[/b]")
        sys.exit(0)

    if value_map.check_invalid():
        terminal_print()
        rg = Group(
            "",
            "Can't run operation: invalid or insufficient input(s)",
            "",
            Rule(),
            "",
            create_operation_status_renderable(
                operation=operation,
                inputs=value_map,
                render_config={
                    "show_operation_name": True,
                    "show_inputs": True,
                    "show_outputs_schema": True,
                },
            ),
        )
        terminal_print(rg, in_panel=f"Run info: [b]{operation.operation_id}[/b]")
        sys.exit(1)

    if print_help:
        rich_format_operation_help(
            obj=click_context.command,
            ctx=click_context,
            operation=operation,
            op_inputs=value_map,
            cmd_help=cmd_help,
        )
        sys.exit(0)

    return value_map


def execute_job(
    api: BaseAPI,
    operation: Operation,
    inputs: ValueMap,
    silent: bool,
    save_results: bool,
    aliases: Union[None, Mapping[str, List[str]]],
    properties: bool = False,
    comment: Union[str, None] = None,
) -> uuid.UUID:
    """Execute the job."""

    job_metadata = {}
    if comment is not None:
        job_metadata["comment"] = comment

    try:
        job_id = api.queue_job(
            operation=operation, inputs=inputs, operation_config=None, **job_metadata
        )
    except Exception as e:
        log_exception(e)
        terminal_print()
        terminal_print(e)
        sys.exit(1)

    try:
        outputs = api.get_job_result(job_id=job_id)
    except FailedJobException as fje:
        terminal_print()
        error = fje.msg
        details: Union[str, None] = KiaraException.get_root_details(fje)
        if details:
            error = f"{error}\n\n{details}"
        _error = Markdown(error)
        terminal_print(_error, in_panel="Processing error")

        sys.exit(1)
    except Exception as e:
        terminal_print()
        terminal_print(e)
        sys.exit(1)

    if not silent:
        if len(outputs) > 1:
            title = "[b]Results[/b]"
        else:
            title = "[b]Result[/b]"

        # for field_name, value in outputs.items():
        #     results.append("")
        #     results.append(f"* [b i]{field_name}[/b i]")
        #     results.append(kiara_obj.data_registry.render_data(value.value_id))

        terminal_print(
            outputs, in_panel=title, empty_line_before=True, show_data_type=True
        )

    if properties:
        render_config = {
            "show_pedigree": False,
            "show_serialized": False,
            "show_data_preview": False,
            "show_properties": True,
            "show_destinies": False,
            "show_destiny_backlinks": False,
            "show_lineage": False,
            "show_environment_hashes": False,
            "show_environment_data": False,
        }

        title = "Result details"
        format = "terminal"

        from kiara.interfaces.python_api.models.info import ValueInfo

        v_infos = (
            ValueInfo.create_from_instance(kiara=api.context, instance=v)
            for v in outputs.values()
        )

        terminal_print_model(*v_infos, format=format, in_panel=title, **render_config)

    # for k, v in outputs.items():
    #     rendered = kiara_obj.data_registry.render_data(v)
    #     rich_print(rendered)

    if save_results:
        try:

            alias_map = create_save_config(
                field_names=outputs.field_names, aliases=aliases
            )

            saved_results = api.store_values(outputs, alias_map=alias_map)

            error = False
            for field, v in saved_results.root.items():
                if v.error:
                    error = True
                    terminal_print()
                    terminal_print(
                        f"[red]Error saving result for field '{field}'[/red]: {v.error}"
                    )
            if error:
                sys.exit(1)

            # api.context.job_registry.store_job_record(job_id=job_id)

            if len(saved_results) == 1:
                title = "[b]Stored result value[/b]"
            else:
                title = "[b]Stored result values[/b]"
            terminal_print(saved_results, in_panel=title, empty_line_before=True)
        except Exception as e:
            log_exception(e)
            terminal_print(f"[red]Error saving results[/red]: {e}")
            sys.exit(1)

    return job_id


# kiara\kiara\src\kiara\utils\cli\__init__.py
# -*- coding: utf-8 -*-
import json
import os
from enum import Enum
from typing import TYPE_CHECKING, Any, Callable, Dict, Iterable, Mapping, TypeVar, Union

import rich_click as click
from click import Command, Context, Option, Parameter, option
from rich import box
from rich.box import Box
from rich.console import ConsoleRenderable, Group, RichCast
from rich.panel import Panel
from rich.rule import Rule
from rich.syntax import Syntax

from kiara.defaults import KIARA_MAIN_CONFIG_FILE, kiara_app_dirs

# ======================================================================================================================
# click helper methods
from kiara.utils import logger

if TYPE_CHECKING:
    from pydantic import BaseModel

F = TypeVar("F", bound=Callable[..., Any])
FC = TypeVar("FC", bound=Union[Callable[..., Any], Command])


def _param_memo(f: Union[Callable[..., Any], Command], param: Parameter) -> None:
    if isinstance(f, Command):
        f.params.append(param)
    else:
        if not hasattr(f, "__click_params__"):
            f.__click_params__ = []  # type: ignore

        f.__click_params__.append(param)  # type: ignore


HORIZONTALS_NO_TO_AND_BOTTOM: Box = Box(
    """\
    
    
 ── 
    
 ── 
 ── 
    
    
"""
)


# ======================================================================================================================
def terminal_print(
    msg: Union[Any, None] = None,
    in_panel: Union[str, None] = None,
    rich_config: Union[Mapping[str, Any], None] = None,
    empty_line_before: bool = False,
    **config: Any,
) -> None:

    from kiara.interfaces import get_console
    from kiara.utils.output import extract_renderable

    if msg is None:
        msg = ""
    console = get_console()

    msg = extract_renderable(msg, render_config=config)
    # if hasattr(msg, "create_renderable"):
    #     msg = msg.create_renderable(**config)  # type: ignore

    if in_panel is not None:
        msg = Panel(msg, title_align="left", title=in_panel)

    if empty_line_before:
        console.print()
    if rich_config:
        console.print(msg, **rich_config)
    else:
        console.print(msg)


def is_rich_renderable(item: Any):
    return isinstance(item, (ConsoleRenderable, RichCast, str))


class OutputFormat(Enum):
    @classmethod
    def as_dict(cls):
        return {i.name: i.value for i in cls}

    @classmethod
    def keys_as_list(cls):
        return cls._member_names_

    @classmethod
    def values_as_list(cls):
        return [i.value for i in cls]

    TERMINAL = "terminal"
    HTML = "html"
    JSON = "json"
    JSON_INCL_SCHEMA = "json-incl-schema"
    JSON_SCHEMA = "json-schema"


def output_format_option(*param_decls: str) -> Callable[[FC], FC]:
    """
    Attaches an option to the command.  All positional arguments are
    passed as parameter declarations to :class:`Option`; all keyword
    arguments are forwarded unchanged (except ``cls``).
    This is equivalent to creating an :class:`Option` instance manually
    and attaching it to the :attr:`Command.params` list.

    :param cls: the option class to instantiate.  This defaults to
                :class:`Option`.
    """
    if not param_decls:
        param_decls = ("--format", "-f")

    attrs = {
        "help": "The output format. Defaults to 'terminal'.",
        "type": click.Choice(OutputFormat.values_as_list()),
    }

    def decorator(f: FC) -> FC:
        # Issue 926, copy attrs, so pre-defined options can re-use the same cls=
        option_attrs = attrs.copy()
        OptionClass = option_attrs.pop("cls", None) or Option
        _param_memo(f, OptionClass(param_decls, **option_attrs))  # type: ignore
        return f

    return decorator


def render_json_str(model: "BaseModel"):

    # import orjson

    # TODO: pydantic refactor
    json_str = model.model_dump_json(indent=2)

    # try:
    #     json_str = model.json(option=orjson.OPT_INDENT_2 | orjson.OPT_NON_STR_KEYS)
    # except TypeError:
    #     json_str = model.model_dump_json(indent=2)

    return json_str


def render_json_schema_str(model: "BaseModel"):

    #
    # try:
    #     json_str = model.schema_json(option=orjson.OPT_INDENT_2)
    # except TypeError:
    #     json_str = model.schema_json(indent=2)

    import orjson

    from kiara.utils.json import orjson_dumps

    schema = model.model_json_schema()
    json_str = orjson_dumps(schema, option=orjson.OPT_INDENT_2)

    return json_str


def terminal_print_model(
    *models: "BaseModel",
    format: Union[None, OutputFormat, str] = None,
    empty_line_before: Union[bool, None] = None,
    in_panel: Union[str, None] = None,
    **render_config: Any,
):

    import orjson

    from kiara.utils.json import orjson_dumps
    from kiara.utils.output import extract_renderable

    if format is None:
        format = OutputFormat.TERMINAL

    if isinstance(format, str):
        format = OutputFormat(format)

    if empty_line_before is None:
        if format == OutputFormat.TERMINAL:
            empty_line_before = True
        else:
            empty_line_before = False

    if format == OutputFormat.TERMINAL:
        if len(models) == 1:
            terminal_print(
                models[0],
                in_panel=in_panel,
                empty_line_before=empty_line_before,
                **render_config,
            )
        else:
            rg = []
            if not models:
                return
            for model in models[0:-1]:
                renderable = extract_renderable(model, render_config)
                rg.append(renderable)
                rg.append(Rule(style="b"))
            last = extract_renderable(models[-1], render_config)
            rg.append(last)
            group = Group(*rg)
            terminal_print(group, in_panel=in_panel, **render_config)
    elif format == OutputFormat.JSON:
        if len(models) == 1:
            json_str = render_json_str(models[0])
            syntax = Syntax(json_str, "json", background_color="default")
            terminal_print(
                syntax,
                empty_line_before=empty_line_before,
                rich_config={"soft_wrap": True},
            )
        else:
            json_strs = []
            for model in models:
                json_str = render_json_str(model)
                json_strs.append(json_str)

            json_str_full = "[" + ",\n".join(json_strs) + "]"
            syntax = Syntax(json_str_full, "json", background_color="default")
            terminal_print(
                syntax,
                empty_line_before=empty_line_before,
                rich_config={"soft_wrap": True},
            )

    elif format == OutputFormat.JSON_SCHEMA:
        if len(models) == 1:
            _schema = models[0].model_json_schema()
            schema_str = orjson_dumps(_schema, option=orjson.OPT_INDENT_2)
            syntax = Syntax(
                schema_str,
                "json",
                background_color="default",
            )
            terminal_print(
                syntax,
                empty_line_before=empty_line_before,
                rich_config={"soft_wrap": True},
            )
        else:
            json_strs = []
            for model in models:
                json_strs.append(render_json_schema_str(model))
            json_str_full = "[" + ",\n".join(json_strs) + "]"
            syntax = Syntax(json_str_full, "json", background_color="default")
            terminal_print(
                syntax,
                empty_line_before=empty_line_before,
                rich_config={"soft_wrap": True},
            )
    elif format == OutputFormat.JSON_INCL_SCHEMA:
        if len(models) == 1:
            data = models[0].model_dump()
            schema = models[0].model_json_schema()
            all = {"data": data, "schema": schema}
            json_str = orjson_dumps(all, option=orjson.OPT_INDENT_2)
            syntax = Syntax(json_str, "json", background_color="default")
            terminal_print(
                syntax,
                empty_line_before=empty_line_before,
                rich_config={"soft_wrap": True},
            )
        else:
            all_data = []
            for model in models:
                data = model.model_dump()
                schema = model.model_json_schema()
                all_data.append({"data": data, "schema": schema})
            json_str = orjson_dumps(all_data, option=orjson.OPT_INDENT_2)
            # print(json_str)
            syntax = Syntax(json_str, "json", background_color="default")
            terminal_print(
                syntax,
                empty_line_before=empty_line_before,
                rich_config={"soft_wrap": True},
            )

    elif format == OutputFormat.HTML:

        all_html = ""
        for model in models:
            if hasattr(model, "create_html"):
                html = model.create_html()  # type: ignore
                all_html = f"{all_html}\n{html}"
            else:
                raise NotImplementedError()

        syntax = Syntax(all_html, "html", background_color="default")
        terminal_print(
            syntax, empty_line_before=empty_line_before, rich_config={"soft_wrap": True}
        )


def dict_from_cli_args(
    *args: str, list_keys: Union[Iterable[str], None] = None
) -> Dict[str, Any]:

    if not args:
        return {}

    config: Dict[str, Any] = {}
    for arg in args:
        if "=" in arg:
            key, value = arg.split("=", maxsplit=1)
            if value.startswith("alias:"):
                if key in config.keys():
                    raise Exception(f"alias key already set: {key} : {value}")
                config[key] = value
                continue

            try:
                _v = json.loads(value)
            except Exception:
                _v = value
            part_config = {key: _v}
        elif os.path.isfile(os.path.realpath(os.path.expanduser(arg))):
            path = os.path.realpath(os.path.expanduser(arg))
            from kiara.utils.files import get_data_from_file

            part_config = get_data_from_file(path)
            assert isinstance(part_config, Mapping)
        else:
            try:
                part_config = json.loads(arg)
                assert isinstance(part_config, Mapping)
            except Exception:
                raise Exception(f"Could not parse argument into data: {arg}")

        if list_keys is None:
            list_keys = []

        for k, v in part_config.items():
            if k in list_keys:
                config.setdefault(k, []).append(v)
            else:
                if k in config.keys():
                    logger.warning("duplicate.key", old_value=k, new_value=v)
                config[k] = v

    return config


def kiara_version_option(
    **kwargs: Any,
) -> Callable[[FC], FC]:
    """Add a ``--version`` option which immediately prints the version
    number of kiara and all installed plugins.
    """

    def callback(ctx: Context, param: Parameter, value: bool) -> None:

        from rich.table import Table

        if not value or ctx.resilient_parsing:
            return

        from kiara.models.runtime_environment.python import (
            PythonRuntimeEnvironment,
        )
        from kiara.registries.environment import EnvironmentRegistry

        registry = EnvironmentRegistry.instance()
        python_env: PythonRuntimeEnvironment = registry.environments["python"]  # type: ignore

        kiara_version = None
        plugins = {}
        for pkg in python_env.packages:
            if pkg.name == "kiara":
                kiara_version = pkg.version
            elif pkg.name.startswith("kiara"):
                plugins[pkg.name] = pkg.version

        table = Table(show_header=False, box=box.SIMPLE)
        table.add_column("package")
        table.add_column("version", style="i")

        table.add_row("kiara", kiara_version)
        table.add_row("", "")
        for name in sorted(plugins.keys()):
            table.add_row(name, plugins[name])

        table.add_row("", "")
        table.add_row("python", python_env.python_version)
        terminal_print()
        terminal_print(table, in_panel="Version information")

        ctx.exit()

    param_decls = ("--version", "-v")

    kwargs.setdefault("is_flag", True)
    kwargs.setdefault("expose_value", False)
    kwargs.setdefault("is_eager", True)
    kwargs.setdefault(
        "help", ("Show the version of kiara and installed plugins, then exit.")
    )
    kwargs["callback"] = callback
    result: Callable[[FC], FC] = option(*param_decls, **kwargs)
    return result


def kiara_runtime_info_option(
    **kwargs: Any,
) -> Callable[[FC], FC]:
    """Add a ``--runtime-info`` option which immediately prints information about the current application environment."""

    def callback(ctx: Context, param: Parameter, value: bool) -> None:

        from rich.table import Table

        if not value or ctx.resilient_parsing:
            return

        table = Table(show_header=False, box=box.SIMPLE)
        table.add_column("key")
        table.add_column("value", style="i")
        config_file = KIARA_MAIN_CONFIG_FILE
        table.add_row("config file", config_file)
        table.add_row("", "")
        data_path = kiara_app_dirs.user_data_dir
        table.add_row("data path", data_path)
        table.add_row("", "")
        cache_path = kiara_app_dirs.user_cache_dir
        table.add_row("cache path", cache_path)
        table.add_row("", "")

        terminal_print()
        terminal_print(table, in_panel="Application runtime information")

        ctx.exit()

    param_decls = ("--runtime-info", "-ri")

    kwargs.setdefault("is_flag", True)
    kwargs.setdefault("expose_value", False)
    kwargs.setdefault("is_eager", True)
    kwargs.setdefault("help", ("Show current environment information, then exit."))
    kwargs["callback"] = callback
    result: Callable[[FC], FC] = option(*param_decls, **kwargs)
    return result


# kiara\kiara\src\kiara\utils\develop\__init__.py
# -*- coding: utf-8 -*-
from enum import Enum
from typing import Any, ClassVar, Dict, Tuple, Type, Union

from pydantic import BaseModel, ConfigDict, Field
from pydantic_settings import (
    BaseSettings,
    PydanticBaseSettingsSource,
    SettingsConfigDict,
)
from rich.console import Group, RenderableType
from rich.panel import Panel

from kiara.utils import is_develop


def log_dev_message(msg: RenderableType, title: Union[str, None] = None):

    if not is_develop():
        return

    if not title:
        title = "Develop-mode message"
    panel = Panel(Group("", msg), title=f"[yellow]{title}[/yellow]", title_align="left")

    from kiara.utils.cli import terminal_print

    terminal_print(panel)


# def dev_config_file_settings_source(settings: BaseSettings) -> Dict[str, Any]:
#     """
#     A simple settings source that loads variables from a JSON file
#     at the project's root.
#
#     Here we happen to choose to use the `env_file_encoding` from Config
#     when reading `config.json`
#     """
#     if os.path.exists(KIARA_DEV_CONFIG_FILE):
#         dev_config: Dict[str, Any] = get_data_from_file(KIARA_DEV_CONFIG_FILE)
#     else:
#         dev_config = {}
#     return dev_config


# def profile_settings_source(settings: BaseSettings) -> Dict[str, Any]:
#
#     profile_name = os.environ.get("DEVELOP", None)
#     if not profile_name:
#         profile_name = os.environ.get("develop", None)
#     if not profile_name:
#         profile_name = os.environ.get("DEV", None)
#     if not profile_name:
#         profile_name = os.environ.get("dev", None)
#     if not profile_name:
#         profile_name = os.environ.get("DEV_PROFILE", None)
#     if not profile_name:
#         profile_name = os.environ.get("dev_profile", None)
#
#     result: Dict[str, Any] = {}
#     if not profile_name:
#         return result
#
#     profile_name = profile_name.lower()
#
#     for model in KiaraDevSettings.model_fields.values():
#         raise NotImplementedError("TODO: fix this after pydantic v2 refactoring")
#         if not issubclass(model.type_, BaseModel):
#             continue
#
#         profiles = getattr(model.type_, "PROFILES", None)
#         if not profiles:
#             continue
#
#         p = profiles.get(profile_name, None)
#         if not p:
#             continue
#         result[model.name] = p
#
#     return result


class DetailLevel(Enum):

    NONE = "none"
    MINIMAL = "minimal"
    FULL = "full"


class PreRunMsgDetails(BaseModel):
    model_config = ConfigDict(
        extra="forbid", validate_assignment=True, use_enum_values=True
    )

    pipeline_steps: bool = Field(
        description="Whether to also display information for modules that are run as part of a pipeline.",
        default=False,
    )
    module_info: DetailLevel = Field(
        description="Whether to display details about the module to be run.",
        default=DetailLevel.MINIMAL,
    )
    internal_modules: bool = Field(
        description="Whether to also print details about runs of internal modules.",
        default=False,
    )
    inputs_info: DetailLevel = Field(
        description="Whether to display details about the run inputs.",
        default=DetailLevel.MINIMAL,
    )


class PostRunMsgDetails(BaseModel):
    model_config = ConfigDict(
        extra="forbid", validate_assignment=True, use_enum_values=True
    )

    pipeline_steps: bool = Field(
        description="Whether to also display information for modules that are run as part of a pipeline",
        default=False,
    )
    module_info: DetailLevel = Field(
        description="Whether to display details about the module that was run.",
        default=DetailLevel.NONE,
    )
    internal_modules: bool = Field(
        description="Whether to also print details about runs of internal module.",
        default=False,
    )
    inputs_info: DetailLevel = Field(
        description="Whether to display details about the run inputs.",
        default=DetailLevel.NONE,
    )
    outputs_info: DetailLevel = Field(
        description="Whether to display details about the run outputs.",
        default=DetailLevel.MINIMAL,
    )


class KiaraDevLogSettings(BaseModel):

    PROFILES: ClassVar[Dict[str, Any]] = {
        "full": {
            "log_pre_run": True,
            "pre_run": {
                "pipeline_steps": True,
                "module_info": "full",
                "inputs_info": "full",
            },
            "log_post_run": True,
            "post_run": {
                "pipeline_steps": True,
                "module_info": "minimal",
                "inputs_info": "minimal",
                "outputs_info": "full",
            },
        },
        "internal": {
            "pre_run": {"internal_modules": True},
            "post_run": {"internal_modules": True},
        },
    }
    model_config = ConfigDict(
        extra="forbid", validate_assignment=True, use_enum_values=True
    )

    exc: DetailLevel = Field(
        description="How detailed to print exceptions", default=DetailLevel.MINIMAL
    )
    log_pre_run: bool = Field(
        description="Print details about a module and its inputs before running it.",
        default=True,
    )
    pre_run: PreRunMsgDetails = Field(
        description="Fine-grained settings about what to display in the pre-run message.",
        default_factory=PreRunMsgDetails,
    )
    log_post_run: bool = Field(
        description="Print details about the results of a module run.", default=True
    )
    post_run: PostRunMsgDetails = Field(
        description="Fine-grained settings aobut what to display in the post-run message.",
        default_factory=PostRunMsgDetails,
    )


class KiaraDevSettings(BaseSettings):
    # TODO[pydantic]: We couldn't refactor this class, please create the `model_config` manually.
    # Check https://docs.pydantic.dev/dev-v2/migration/#changes-to-config for more information.

    model_config = SettingsConfigDict(
        extra="forbid",
        validate_assignment=True,
        use_enum_values=True,
        env_prefix="dev_",
        env_nested_delimiter="__",
    )

    # class Config:
    #
    #     extra = Extra.forbid
    #     validate_assignment = True
    #     env_prefix = "dev_"
    #     use_enum_values = True
    #     env_nested_delimiter = "__"
    #
    #     @classmethod
    #     def customise_sources(
    #         cls,
    #         init_settings,
    #         env_settings,
    #         file_secret_settings,
    #     ):
    #         return (
    #             init_settings,
    #             # profile_settings_source,
    #             dev_config_file_settings_source,
    #             env_settings,
    #         )

    @classmethod
    def settings_customise_sources(
        cls,
        settings_cls: Type[BaseSettings],
        init_settings: PydanticBaseSettingsSource,
        env_settings: PydanticBaseSettingsSource,
        dotenv_settings: PydanticBaseSettingsSource,
        file_secret_settings: PydanticBaseSettingsSource,
    ) -> Tuple[PydanticBaseSettingsSource, ...]:
        return (
            init_settings,
            env_settings,
        )

    log: KiaraDevLogSettings = Field(
        description="Settings about what messages to print in 'develop' mode, and what details to include.",
        default_factory=KiaraDevLogSettings,
    )
    job_cache: bool = Field(
        description="Whether to always disable the job cache (ignores the runtime_job_cache setting in the kiara configuration).",
        default=True,
    )

    def create_renderable(self, **render_config: Any):
        from kiara.utils.output import create_recursive_table_from_model_object

        return create_recursive_table_from_model_object(
            self, render_config=render_config
        )


KIARA_DEV_SETTINGS = KiaraDevSettings()


# kiara\kiara\src\kiara\utils\hashfs\__init__.py
# -*- coding: utf-8 -*-

"""
HashFS is a content-addressable file management system. What does that mean?
Simply, that HashFS manages a directory where files are saved based on the
file's hash.

Typical use cases for this kind of system are ones where:

- Files are written once and never change (e.g. image storage).
- It's desirable to have no duplicate files (e.g. user uploads).
- File metadata is stored elsewhere (e.g. in a database).

Adapted from: https://github.com/dgilland/hashfs

License
=======

The MIT License (MIT)

Copyright (c) 2015, Derrick Gilland

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
"""

import glob
import hashlib
import io
import os
import shutil
from collections import namedtuple
from contextlib import closing
from os import walk
from pathlib import Path
from tempfile import NamedTemporaryFile
from typing import Any, BinaryIO, List, Union


def to_bytes(text: Union[str, bytes]):
    if not isinstance(text, bytes):
        text = bytes(text, "utf8")
    return text


def compact(items: List[Any]) -> List[Any]:
    """Return only truthy elements of `items`."""
    return [item for item in items if item]


def issubdir(subpath: str, path: str):
    """Return whether `subpath` is a sub-directory of `path`."""
    # Append os.sep so that paths like /usr/var2/log doesn't match /usr/var.
    path = os.path.realpath(path) + os.sep
    subpath = os.path.realpath(subpath)
    return subpath.startswith(path)


def shard(digest, depth, width):
    # This creates a list of `depth` number of tokens with width
    # `width` from the first part of the id plus the remainder.
    return compact(
        [digest[i * width : width * (i + 1)] for i in range(depth)]
        + [digest[depth * width :]]
    )


class HashFS(object):
    """
    Content addressable file manager.

    Attributes:
    ----------
        root (str): Directory path used as root of storage space.
        depth (int, optional): Depth of subfolders when saving a
            file.
        width (int, optional): Width of each subfolder to create when saving a
            file.
        algorithm (str): Hash algorithm to use when computing file hash.
            Algorithm should be available in ``hashlib`` module. Defaults to
            ``'sha256'``.
        fmode (int, optional): File mode permission to set when adding files to
            directory. Defaults to ``0o664`` which allows owner/group to
            read/write and everyone else to read.
        dmode (int, optional): Directory mode permission to set for
            subdirectories. Defaults to ``0o755`` which allows owner/group to
            read/write and everyone else to read and everyone to execute.
    """

    def __init__(
        self,
        root: str,
        depth: int = 4,
        width: int = 1,
        algorithm: str = "sha256",
        fmode=0o664,
        dmode=0o755,
    ):
        self.root: str = os.path.realpath(root)
        self.depth: int = depth
        self.width: int = width
        self.algorithm: str = algorithm
        self.fmode = fmode
        self.dmode = dmode

    def put(self, file: BinaryIO) -> "HashAddress":
        """
        Store contents of `file` on disk using its content hash for the
        address.

        Args:
        ----
            file (mixed): Readable object or path to file.

        Returns:
        -------
            HashAddress: File's hash address.
        """
        stream = Stream(file)

        with closing(stream):
            id = self.computehash(stream)
            filepath, is_duplicate = self._copy(stream, id)

        return HashAddress(id, self.relpath(filepath), filepath, is_duplicate)

    def put_with_precomputed_hash(
        self, file: Union[str, Path, BinaryIO], hash_id: str
    ) -> "HashAddress":

        stream = Stream(file)
        with closing(stream):
            filepath, is_duplicate = self._copy(stream=stream, id=hash_id)

        return HashAddress(hash_id, self.relpath(filepath), filepath, is_duplicate)

    def _copy(self, stream: "Stream", id: str):
        """
        Copy the contents of `stream` onto disk with an optional file
        extension appended. The copy process uses a temporary file to store the
        initial contents and then moves that file to it's final location.
        """
        filepath = self.idpath(id)

        if not os.path.isfile(filepath):
            # Only move file if it doesn't already exist.
            is_duplicate = False
            fname = self._mktempfile(stream)
            self.makepath(os.path.dirname(filepath))
            shutil.move(fname, filepath)
        else:
            is_duplicate = True

        return (filepath, is_duplicate)

    def _mktempfile(self, stream):
        """
        Create a named temporary file from a :class:`Stream` object and
        return its filename.
        """
        tmp = NamedTemporaryFile(delete=False)

        if self.fmode is not None:
            oldmask = os.umask(0)

            try:
                os.chmod(tmp.name, self.fmode)
            finally:
                os.umask(oldmask)

        for data in stream:
            tmp.write(to_bytes(data))

        tmp.close()

        return tmp.name

    def get(self, file) -> Union[None, "HashAddress"]:
        """
        Return :class:`HashAdress` from given id or path. If `file` does not
        refer to a valid file, then ``None`` is returned.

        Args:
        ----
            file (str): Address ID or path of file.

        Returns:
        -------
            HashAddress: File's hash address.
        """
        realpath = self.realpath(file)

        if realpath is None:
            return None
        else:
            return HashAddress(self.unshard(realpath), self.relpath(realpath), realpath)

    def open(self, file, mode="rb"):
        """
        Return open buffer object from given id or path.

        Args:
        ----
            file (str): Address ID or path of file.
            mode (str, optional): Mode to open file in. Defaults to ``'rb'``.

        Returns:
        -------
            Buffer: An ``io`` buffer dependent on the `mode`.

        Raises:
        ------
            IOError: If file doesn't exist.
        """
        realpath = self.realpath(file)
        if realpath is None:
            raise IOError("Could not locate file: {0}".format(file))

        return io.open(realpath, mode)

    def delete(self, file):
        """
        Delete file using id or path. Remove any empty directories after
        deleting. No exception is raised if file doesn't exist.

        Args:
        ----
            file (str): Address ID or path of file.
        """
        realpath = self.realpath(file)
        if realpath is None:
            return

        try:
            os.remove(realpath)
        except OSError:  # pragma: no cover
            pass
        else:
            self.remove_empty(os.path.dirname(realpath))

    def remove_empty(self, subpath):
        """
        Successively remove all empty folders starting with `subpath` and
        proceeding "up" through directory tree until reaching the :attr:`root`
        folder.
        """
        # Don't attempt to remove any folders if subpath is not a
        # subdirectory of the root directory.
        if not self.haspath(subpath):
            return

        while subpath != self.root:
            if len(os.listdir(subpath)) > 0 or os.path.islink(subpath):
                break
            os.rmdir(subpath)
            subpath = os.path.dirname(subpath)

    def files(self):
        """
        Return generator that yields all files in the :attr:`root`
        directory.
        """
        for folder, subfolders, files in walk(self.root):
            for file in files:
                yield os.path.abspath(os.path.join(folder, file))

    def folders(self):
        """
        Return generator that yields all folders in the :attr:`root`
        directory that contain files.
        """
        for folder, subfolders, files in walk(self.root):
            if files:
                yield folder

    def count(self):
        """Return count of the number of files in the :attr:`root` directory."""
        count = 0
        for _ in self:
            count += 1
        return count

    def size(self):
        """
        Return the total size in bytes of all files in the :attr:`root`
        directory.
        """
        total = 0

        for path in self.files():
            total += os.path.getsize(path)

        return total

    def exists(self, file):
        """Check whether a given file id or path exists on disk."""
        return bool(self.realpath(file))

    def haspath(self, path):
        """
        Return whether `path` is a subdirectory of the :attr:`root`
        directory.
        """
        return issubdir(path, self.root)

    def makepath(self, path):
        """Physically create the folder path on disk."""
        try:
            os.makedirs(path, self.dmode)
        except FileExistsError:
            assert os.path.isdir(path), "expected {} to be a directory".format(path)

    def relpath(self, path):
        """Return `path` relative to the :attr:`root` directory."""
        return os.path.relpath(path, self.root)

    def realpath(self, file):
        """
        Attempt to determine the real path of a file id or path through
        successive checking of candidate paths. If the real path is stored with
        an extension, the path is considered a match if the basename matches
        the expected file path of the id.
        """
        # Check for absoluate path.
        if os.path.isfile(file):
            return file

        # Check for relative path.
        relpath = os.path.join(self.root, file)
        if os.path.isfile(relpath):
            return relpath

        # Check for sharded path.
        filepath = self.idpath(file)
        if os.path.isfile(filepath):
            return filepath

        # Check for sharded path with any extension.
        paths = glob.glob("{0}.*".format(filepath))
        if paths:
            return paths[0]

        # Could not determine a match.
        return None

    def idpath(self, id):
        """
        Build the file path for a given hash id. Optionally, append a
        file extension.
        """
        paths = self.shard(id)

        return os.path.join(self.root, *paths)

    def computehash(self, stream) -> str:
        """Compute hash of file using :attr:`algorithm`."""
        hashobj = hashlib.new(self.algorithm)
        for data in stream:
            hashobj.update(to_bytes(data))
        return hashobj.hexdigest()

    def shard(self, id):
        """Shard content ID into subfolders."""
        return shard(id, self.depth, self.width)

    def unshard(self, path):
        """Unshard path to determine hash value."""
        if not self.haspath(path):
            raise ValueError(
                "Cannot unshard path. The path {0!r} is not "
                "a subdirectory of the root directory {1!r}".format(path, self.root)
            )

        return os.path.splitext(self.relpath(path))[0].replace(os.sep, "")

    def repair(self):
        """
        Repair any file locations whose content address doesn't match it's
        file path.
        """
        repaired = []
        corrupted = tuple(self.corrupted())
        oldmask = os.umask(0)

        try:
            for path, address in corrupted:
                if os.path.isfile(address.abspath):
                    # File already exists so just delete corrupted path.
                    os.remove(path)
                else:
                    # File doesn't exists so move it.
                    self.makepath(os.path.dirname(address.abspath))
                    shutil.move(path, address.abspath)

                os.chmod(address.abspath, self.fmode)
                repaired.append((path, address))
        finally:
            os.umask(oldmask)

        return repaired

    def corrupted(self):
        """
        Return generator that yields corrupted files as ``(path, address)``
        where ``path`` is the path of the corrupted file and ``address`` is
        the :class:`HashAddress` of the expected location.
        """
        for path in self.files():
            stream = Stream(path)

            with closing(stream):
                id = self.computehash(stream)

            expected_path = self.idpath(id)

            if expected_path != path:
                yield (
                    path,
                    HashAddress(id, self.relpath(expected_path), expected_path),
                )

    def __contains__(self, file):
        """
        Return whether a given file id or path is contained in the
        :attr:`root` directory.
        """
        return self.exists(file)

    def __iter__(self):
        """Iterate over all files in the :attr:`root` directory."""
        return self.files()

    def __len__(self):
        """Return count of the number of files in the :attr:`root` directory."""
        return self.count()


class HashAddress(
    namedtuple("HashAddress", ["id", "relpath", "abspath", "is_duplicate"])
):
    """
    File address containing file's path on disk and it's content hash ID.

    Attributes:
    ----------
        id (str): Hash ID (hexdigest) of file contents.
        relpath (str): Relative path location to :attr:`HashFS.root`.
        abspath (str): Absoluate path location of file on disk.
        is_duplicate (boolean, optional): Whether the hash address created was
            a duplicate of a previously existing file. Can only be ``True``
            after a put operation. Defaults to ``False``.
    """

    def __new__(cls, id, relpath, abspath, is_duplicate=False):
        return super(HashAddress, cls).__new__(cls, id, relpath, abspath, is_duplicate)  # type: ignore


class Stream(object):
    """
    Common interface for file-like objects.

    The input `obj` can be a file-like object or a path to a file. If `obj` is
    a path to a file, then it will be opened until :meth:`close` is called.
    If `obj` is a file-like object, then it's original position will be
    restored when :meth:`close` is called instead of closing the object
    automatically. Closing of the stream is deferred to whatever process passed
    the stream in.

    Successive readings of the stream is supported without having to manually
    set it's position back to ``0``.
    """

    def __init__(self, obj: Union[BinaryIO, str, Path]):
        if hasattr(obj, "read"):
            pos = obj.tell()  # type: ignore
        elif os.path.isfile(obj):  # type: ignore
            obj = io.open(obj, "rb")  # type: ignore
            pos = None
        else:
            raise ValueError("Object must be a valid file path or a readable object")

        try:
            file_stat = os.stat(obj.name)  # type: ignore
            buffer_size = file_stat.st_blksize
        except Exception:
            buffer_size = 8192

        self._obj: BinaryIO = obj  # type: ignore
        self._pos = pos
        self._buffer_size = buffer_size

    def __iter__(self):
        """
        Read underlying IO object and yield results. Return object to
        original position if we didn't open it originally.
        """
        self._obj.seek(0)

        while True:
            data = self._obj.read(self._buffer_size)

            if not data:
                break

            yield data

        if self._pos is not None:
            self._obj.seek(self._pos)

    def close(self):
        """
        Close underlying IO object if we opened it, else return it to
        original position.
        """
        if self._pos is None:
            self._obj.close()
        else:
            self._obj.seek(self._pos)


# kiara\kiara\src\kiara\utils\testing\__init__.py
# -*- coding: utf-8 -*-
import types
from inspect import getmembers, isfunction
from pathlib import Path
from typing import Any, Callable, Dict, List, Mapping, Union

from kiara.defaults import INIT_EXAMPLE_NAME
from kiara.interfaces.python_api.models.job import JobDesc
from kiara.utils import log_exception, log_message
from kiara.utils.files import get_data_from_file


def get_init_job(jobs_folder: Path) -> Union[None, JobDesc]:

    init_test_yaml = jobs_folder / f"{INIT_EXAMPLE_NAME}.yaml"
    if init_test_yaml.is_file():
        return JobDesc.create_from_file(init_test_yaml)

    init_test_yaml = jobs_folder / f"{INIT_EXAMPLE_NAME}.yml"
    if init_test_yaml.is_file():
        return JobDesc.create_from_file(init_test_yaml)

    init_test_json = jobs_folder / f"{INIT_EXAMPLE_NAME}.json"
    if init_test_json.is_file():
        return JobDesc.create_from_file(init_test_json)

    return None


def list_job_descs(jobs_folder: Union[Path, List[Path]]):

    if isinstance(jobs_folder, Path):
        jobs_folders = [jobs_folder]
    else:
        jobs_folders = jobs_folder

    for _jobs_folder in jobs_folders:
        init_job = get_init_job(_jobs_folder)
        if init_job is not None:
            yield init_job

    job_names = set()
    for _jobs_folder in jobs_folders:

        files = (
            list(_jobs_folder.glob("*.yaml"))
            + list(_jobs_folder.glob("*.yml"))
            + list(_jobs_folder.glob("*.json"))
        )
        for f in sorted(files):
            if f.name in [
                f"{INIT_EXAMPLE_NAME}.yaml",
                f"{INIT_EXAMPLE_NAME}.yml",
                "{INIT_EXAMPLE_NAME}.json",
            ]:
                continue
            try:
                job_desc = JobDesc.create_from_file(f)
                if job_desc.job_alias in job_names:
                    log_message(
                        f"Duplicate job alias: {job_desc.job_alias}. Skipping..."
                    )
                    continue
                job_names.add(job_desc.job_alias)
                yield job_desc
            except Exception as e:
                log_exception(e)


def get_tests_for_job(
    job_alias: str, job_tests_folder: Path
) -> Union[None, Mapping[str, Union[Any, Callable]]]:
    """Get tests for a job.

    In case the tests are Python code, this will use 'exec' to execute it, which is
    usually discouraged. However, this is only used for testing, and it makes it easier
    to create those tests, which is why it is used here.
    """

    module_base_name = f"kiara_tests.job_tests.{job_alias}"

    test_folder = job_tests_folder / job_alias
    tests = {}
    if test_folder.is_dir():
        for f in test_folder.glob("*.py"):
            test_name = f.stem
            code = f.read_text()
            module_name = f"{module_base_name}.{test_name}"
            module = types.ModuleType(module_name)
            exec(code, module.__dict__)  # noqa

            for func in getmembers(module, isfunction):
                tests[func[0]] = func[1]

    test_checks = test_folder / "outputs.json"
    if not test_checks.is_file():
        test_checks = test_folder / "outputs.yaml"

    if test_checks.is_file():
        test_data: Dict[str, Any] = get_data_from_file(test_checks)
        if test_data is None:
            test_data = {}
    else:
        test_data = {}

    for k, v in test_data.items():
        if k in tests.keys():
            raise Exception(f"Duplicate test name: {k}")
        tests[k] = v

    return tests


# kiara\kiara\src\kiara\zmq\client.py
# -*- coding: utf-8 -*-
import sys
from typing import Any, Union

from kiara.interfaces import get_console


class KiaraZmqClient(object):
    def __init__(self, host: Union[None, str] = None, port: Union[None, int] = None):

        import zmq

        from kiara.zmq.messages import KiaraApiMsgBuilder

        if host is None:
            host = "localhost"
        elif host in ["0.0.0.0", "*"]:  # noqa
            host = "localhost"

        if port is None:
            port = 8080

        self._host: str = host
        self._port: int = port
        self._context = zmq.Context()
        self._msg_builder = KiaraApiMsgBuilder()
        self._socket = self._context.socket(zmq.REQ)
        self._socket.connect(f"tcp://{host}:%s" % self._port)

    def close(self):
        self._context.destroy()

    def request_cli(self, args: Any) -> Any:

        width = get_console().width
        args = {"console_width": width, "sub-command": args, "executable": sys.argv[0]}

        msg = self._msg_builder.encode_msg(endpoint_name="cli", args=args)

        self._socket.send_multipart(msg)
        response = self._socket.recv_multipart()
        response_msg = self._msg_builder.decode_msg(response)

        print(response_msg.args["stdout"])  # noqa
        stderr = response_msg.args["stderr"]
        if stderr:
            print(stderr, file=sys.stderr)  # noqa

    def request(self, endpoint_name: str, args: Any = None) -> Any:

        if endpoint_name == "cli":
            self.request_cli(args=args)
            return

        msg = self._msg_builder.encode_msg(endpoint_name=endpoint_name, args=args)

        self._socket.send_multipart(msg)
        response = self._socket.recv_multipart()
        response_msg = self._msg_builder.decode_msg(response)

        return response_msg.args


# kiara\kiara\src\kiara\zmq\__init__.py
# -*- coding: utf-8 -*-
import os
import sys
import typing
from typing import Dict, List, Union

import orjson
from pydantic import BaseModel, Field

from kiara.defaults import KIARA_MAIN_CONTEXT_DATA_PATH, KIARA_MAIN_CONTEXT_LOCKS_PATH
from kiara.interfaces import BaseAPIWrap

if typing.TYPE_CHECKING:
    pass


class KiaraZmqServiceDetails(BaseModel):

    context_name: str = Field(description="The name of the kiara context.")
    process_id: Union[None, int] = Field(
        None, description="The process id of the kiara service."
    )
    stdout: str = Field(description="The stdout handle.")
    stderr: str = Field(description="The stderr handle.")
    host: str = Field(description="The host the service is running on.")
    port: int = Field(description="The port the service is running on.")
    newly_started: Union[bool, None] = Field(
        description="If the service was newly started, or already running.",
        default=None,
    )


def get_default_stdout_zmq_service_log_path(context_name: str):
    return os.path.join(
        KIARA_MAIN_CONTEXT_DATA_PATH, context_name, "logs", "zmq_stdout.log"
    )


def get_default_stderr_zmq_service_log_path(context_name: str):
    return os.path.join(
        KIARA_MAIN_CONTEXT_DATA_PATH, context_name, "logs", "zmq_stderr.log"
    )


def zmq_context_registered(context_name: str) -> bool:

    zmq_base = os.path.join(
        KIARA_MAIN_CONTEXT_LOCKS_PATH, "zmq", f"{context_name}.json"
    )
    service_info_file = os.path.join(zmq_base, f"{context_name}.zmq")

    return os.path.exists(service_info_file)


def list_registered_contexts() -> List[str]:

    zmq_base = os.path.join(KIARA_MAIN_CONTEXT_LOCKS_PATH, "zmq")
    if not os.path.exists(zmq_base):
        return []

    return [x.replace(".zmq", "") for x in os.listdir(zmq_base) if x.endswith(".zmq")]


def get_context_details(context_name: str) -> Union[Dict, None]:

    zmq_base = os.path.join(KIARA_MAIN_CONTEXT_LOCKS_PATH, "zmq")
    service_info_file = os.path.join(zmq_base, f"{context_name}.zmq")

    if not os.path.exists(service_info_file):
        return None

    with open(service_info_file, "r") as f:
        result: Dict = orjson.loads(f.read())
        return result


def start_zmq_service(
    api_wrap: BaseAPIWrap,
    host: Union[str, None],
    port: Union[int, None] = None,
    stdout: Union[str, None] = None,
    stderr: Union[str, None] = None,
    timeout: Union[None, int] = None,
    monitor: bool = False,
) -> Union[None, KiaraZmqServiceDetails]:

    from kiara.exceptions import KiaraException

    if monitor:
        from kiara.zmq.service import KiaraZmqAPI

        zmq_api = KiaraZmqAPI(
            api_wrap=api_wrap,
            host=host,
            port=port,
            listen_timout_in_ms=timeout,
            stdout=stdout,
            stderr=stderr,
        )
        try:
            thread = zmq_api.start()
        except Exception as e:
            raise KiaraException(msg="Error starting zmq service.", parent=e)

        try:
            thread.join()
        except KeyboardInterrupt:
            zmq_api.stop()
            raise KiaraException(msg="User requested service stop...")
        return None

    else:

        import subprocess

        from kiara.zmq.client import KiaraZmqClient

        context_details = get_context_details(context_name=api_wrap.kiara_context_name)
        _newly_started = True

        if context_details is not None:
            _newly_started = False
            _process_id: int = context_details["process_id"]
            _stdout: str = context_details["stdout"]
            _stderr: str = context_details["stderr"]
            _host: str = context_details["host"]
            _port: int = context_details["port"]
            # TODO: check if stdout/stderr differ

        else:
            if host is None:
                _host = "127.0.0.1"
            else:
                _host = host
            if port:
                _port = port
            else:
                if host in [None, "*", "localhost"]:
                    host_ip = "127.0.0.1"
                else:
                    host_ip = _host
                import socketserver

                with socketserver.TCPServer((host_ip, 0), None) as s:  # type: ignore
                    _port = s.server_address[1]

            if stdout is None:
                _stdout = get_default_stdout_zmq_service_log_path(
                    context_name=api_wrap.kiara_context_name
                )
            else:
                _stdout = stdout

            if stderr is None:
                _stderr = get_default_stderr_zmq_service_log_path(
                    context_name=api_wrap.kiara_context_name
                )
            else:
                _stderr = stderr

            cli = [
                sys.argv[0],
                "-c",
                api_wrap.kiara_context_name,
                "context",
                "service",
                "start",
                "--monitor",
                "--host",
                _host,
                "--port",
                str(_port),
                "--stdout",
                _stdout,
                "--stderr",
                _stderr,
                "--timeout",
                str(timeout),
            ]
            p = subprocess.Popen(cli)
            _process_id = p.pid

        zmq_client = KiaraZmqClient(host=_host, port=_port)
        response = zmq_client.request("ping")
        assert response == "pong"

        return KiaraZmqServiceDetails(
            context_name=api_wrap.kiara_context_name,
            process_id=_process_id,
            stdout=_stdout,
            stderr=_stderr,
            newly_started=_newly_started,
            host=_host,
            port=_port,
        )


def ensure_zmq_service(
    api_wrap: BaseAPIWrap,
    host: str,
    port: int,
    stdout: str,
    stderr: str,
    timeout: int = 0,
    monitor: bool = False,
) -> Union[None, KiaraZmqServiceDetails]:

    return start_zmq_service(
        api_wrap=api_wrap,
        host=host,
        port=port,
        stdout=stdout,
        stderr=stderr,
        timeout=timeout,
        monitor=monitor,
    )


# kiara\kiara\src\kiara\zmq\messages\__init__.py
# -*- coding: utf-8 -*-
from collections import namedtuple
from typing import Any, List

import orjson

from kiara.utils.json import DEFAULT_ORJSON_OPTIONS

ReqMsg = namedtuple("ReqMsg", ["version", "endpoint", "args"])


class KiaraApiMsgBuilder(object):
    def __init__(self):
        self._version_nr_mayor = 0
        self._version_nr_minor = 0
        self._version = int.to_bytes(
            self._version_nr_mayor, length=1, byteorder="big"
        ) + int.to_bytes(self._version_nr_minor, length=1, byteorder="big")

    def encode_msg(self, endpoint_name: str, args: Any) -> List[bytes]:

        try:
            if args:
                if hasattr(args, "model_dump_json"):
                    _args = args.model_dump_json()
                elif hasattr(args, "json"):
                    _args = args.json()
                else:
                    _args = orjson.dumps(args, option=DEFAULT_ORJSON_OPTIONS)
                return [self._version, endpoint_name.encode(), _args]
            else:
                return [self._version, endpoint_name.encode()]
        except Exception as e:
            return [
                self._version,
                endpoint_name.encode(),
                orjson.dumps({"error": str(e)}),
            ]

    def decode_msg(self, msg: List[bytes]) -> ReqMsg:

        version, endpoint = msg[0], msg[1]
        if len(msg) == 3:
            args = orjson.loads(msg[2])
        else:
            args = {}

        return ReqMsg(version, endpoint.decode(), args)


# kiara\kiara\src\kiara\zmq\service\__init__.py
# -*- coding: utf-8 -*-
import atexit
import os
from threading import Thread
from typing import Any, Mapping, Union

import orjson
import zmq

from kiara.defaults import KIARA_MAIN_CONTEXT_LOCKS_PATH
from kiara.exceptions import KiaraException
from kiara.interfaces import BaseAPIWrap, get_console, get_proxy_console
from kiara.interfaces.cli.proxy_cli import proxy_cli
from kiara.interfaces.python_api.base_api import BaseAPI
from kiara.interfaces.python_api.proxy import ApiEndpoints
from kiara.zmq import (
    KiaraZmqServiceDetails,
    get_default_stderr_zmq_service_log_path,
    get_default_stdout_zmq_service_log_path,
)
from kiara.zmq.messages import KiaraApiMsgBuilder

DEFAULT_LISTEN_HOST = "*"
DEFAULT_PORT = 8000


class KiaraZmqAPI(object):
    def __init__(
        self,
        api_wrap: BaseAPIWrap,
        stdout: Union[str, None] = None,
        stderr: Union[str, None] = None,
        host: Union[str, None] = None,
        port: Union[int, None] = None,
        listen_timout_in_ms: Union[int, None] = None,
    ):

        if listen_timout_in_ms is None:
            listen_timout_in_ms = 0

        if host in [None, "*", "localhost"]:
            host_ip = "127.0.0.1"
        else:
            host_ip = host  # type: ignore

        if not port:
            import socketserver

            with socketserver.TCPServer((host_ip, 0), None) as s:  # type: ignore
                port = s.server_address[1]

        self._api_wrap: BaseAPIWrap = api_wrap
        self._api_wrap.exit_process = False

        self._listen_host: str = host_ip
        self._port: int = int(port)
        self._service_thread = None
        self._msg_builder = KiaraApiMsgBuilder()
        self._api_endpoints: ApiEndpoints = ApiEndpoints(api_cls=BaseAPI)

        self._initial_timeout = listen_timout_in_ms
        self._allow_timeout_change = False

        if stdout is None:
            stdout = get_default_stdout_zmq_service_log_path(
                context_name=api_wrap.kiara_context_name
            )

        if stderr is None:
            stderr = get_default_stderr_zmq_service_log_path(
                context_name=api_wrap.kiara_context_name
            )

        if isinstance(stdout, str):
            os.makedirs(os.path.dirname(stdout), exist_ok=True)
            self._stdout = open(stdout, "w")
        else:
            self._stdout = stdout

        if isinstance(stderr, str):
            os.makedirs(os.path.dirname(stderr), exist_ok=True)
            self._stderr = open(stderr, "w")
        else:
            self._stderr = stderr

        # reserving host and port, cross-process
        zmq_base = os.path.join(KIARA_MAIN_CONTEXT_LOCKS_PATH, "zmq")
        service_info_file = os.path.join(
            zmq_base, f"{self._api_wrap.kiara_context_name}.zmq"
        )

        if os.path.exists(service_info_file):
            raise KiaraException(
                f"Zmq service port for context '{self._api_wrap.kiara_context_name}' already reserved: {service_info_file}"
            )

        os.makedirs(os.path.dirname(service_info_file), exist_ok=True)

        details = KiaraZmqServiceDetails(
            context_name=self._api_wrap.kiara_context_name,
            process_id=os.getpid(),
            stdout=stdout,
            stderr=stderr,
            newly_started=None,
            host=host_ip,
            port=port,
        )

        with open(service_info_file, "wb") as f:
            f.write(orjson.dumps(details.model_dump()))

        def delete_info_file():
            os.unlink(service_info_file)

        atexit.register(delete_info_file)

    def service_loop(self):

        try:

            api = self._api_wrap.base_api

            timeout = self._initial_timeout

            context = zmq.Context()
            context_rep_socket = context.socket(zmq.REP)
            context_rep_socket.bind(f"tcp://{self._listen_host}:{self._port}")

            poller = zmq.Poller()
            poller.register(context_rep_socket, zmq.POLLIN)

            stop = False
            while not stop:

                if timeout:
                    socks = dict(poller.poll(timeout))
                else:
                    socks = dict(poller.poll())

                if not socks:
                    print(
                        "Socket timed out, shutting down service...", file=self._stdout
                    )
                    stop = True

                if (
                    context_rep_socket in socks
                    and socks[context_rep_socket] == zmq.POLLIN
                ):

                    #  Wait for next request from client
                    msg = context_rep_socket.recv_multipart()
                    print("Received request: ", msg, file=self._stdout)
                    decoded = self._msg_builder.decode_msg(msg)

                    if decoded.endpoint == "ping":
                        result = "pong"
                    elif decoded.endpoint in ["shutdown", "stop"]:
                        print("Shutting down...", file=self._stdout)
                        result = "ok"
                        stop = True
                    elif decoded.endpoint == "service_status":
                        context_config = (
                            self._api_wrap.base_api.context.context_config.model_dump()
                        )
                        runtime_config = (
                            self._api_wrap.base_api.context.runtime_config.model_dump()
                        )

                        result = {
                            "state": "running",
                            "timeout": timeout,
                            "context_config": context_config,
                            "runtime_config": runtime_config,
                        }
                    elif decoded.endpoint == "cli":
                        result = self.call_cli(api=api, **decoded.args)
                    elif decoded.endpoint == "control":
                        raise NotImplementedError()
                    else:
                        result = self.call_endpoint(
                            api=api, endpoint=decoded.endpoint, **decoded.args
                        )

                    resp_msg = self._msg_builder.encode_msg(decoded.endpoint, result)
                    context_rep_socket.send_multipart(resp_msg)

        except Exception as e:
            import traceback

            traceback.print_exc()
            print(f"ERROR IN ZMQ SERVICE: {e}", file=self._stderr)
            print("Stopping...", file=self._stderr)

    def call_cli(self, api: BaseAPI, **kwargs) -> Mapping[str, str]:

        console = get_console()
        old_width = console.width

        console_width = kwargs.get("console_width", old_width)
        color_system = kwargs.get("color_system", None)

        sub_command = kwargs.get("sub-command")

        console.width = console_width
        stdout = ""
        stderr = ""
        try:
            with get_proxy_console(
                width=console_width,
                color_system=color_system,
                restore_default_console=False,
            ) as proxy_console:
                with proxy_console.capture() as capture:

                    try:
                        proxy_cli.main(
                            args=sub_command,
                            prog_name="kiara",
                            obj=self._api_wrap,
                            standalone_mode=False,
                        )
                    except Exception as e:
                        stderr = str(e)

                if not stderr:
                    stdout = capture.get()
        except Exception as oe:
            stderr = str(oe)

        return {"stdout": stdout, "stderr": stderr}

    def call_endpoint(self, api: BaseAPI, endpoint: str, **kwargs) -> Any:

        try:
            endpoint_proxy = self._api_endpoints.get_api_endpoint(
                endpoint_name=endpoint
            )
        except Exception as e:
            msg = str(e)
            return {"error": msg}

        result = endpoint_proxy.execute(instance=api, **kwargs)
        return result

    def start(self):

        if self._service_thread is not None:
            raise Exception("Service already running")

        self._service_thread = Thread(target=self.service_loop)
        self._service_thread.start()

        return self._service_thread

    def stop(self):

        if self._service_thread is None:
            raise Exception("Service not running")

        if self._listen_host in ["0.0.0.0", "*"]:  # noqa
            c_host = "localhost"
        else:
            c_host = self._listen_host

        from kiara.zmq.client import KiaraZmqClient

        zmq_client = KiaraZmqClient(host=c_host, port=self._port)
        zmq_client.request(endpoint_name="stop", args={})

        self._service_thread.join()
        self._service_thread = None


# kiara\kiara\src\mkdocstrings_handlers\kiara\__init__.py
# -*- coding: utf-8 -*-
#  Copyright (c) 2022, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

from kiara.doc.mkdocstrings.handler import get_handler

__all__ = ["get_handler"]


# kiara\kiara\tests\conftest.py
#!/usr/bin/env python
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

"""
    Dummy conftest.py for kiara.

    If you don't know what this is for, just leave it empty.
    Read more about conftest.py under:
    https://pytest.org/latest/plugins.html
"""
import os
import tempfile
import uuid
from pathlib import Path

import pytest

from kiara.context import Kiara
from kiara.context.config import KiaraConfig
from kiara.interfaces.python_api.base_api import BaseAPI
from kiara.interfaces.python_api.batch import BatchOperation

from .utils import INVALID_PIPELINES_FOLDER, MODULE_CONFIGS_FOLDER, PIPELINES_FOLDER

ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))

DATA_FOLDER = os.path.join(ROOT_DIR, "examples", "data")
TEST_RESOURCES_FOLDER = os.path.join(ROOT_DIR, "tests", "resources")


def create_temp_dir():
    session_id = str(uuid.uuid4())
    TEMP_DIR = Path(os.path.join(tempfile.gettempdir(), "kiara_tests"))

    instance_path = os.path.join(
        TEMP_DIR.resolve().absolute(), f"instance_{session_id}"
    )
    return instance_path


@pytest.fixture
def pipeline_paths():

    result = {}
    for root, dirnames, filenames in os.walk(PIPELINES_FOLDER, topdown=True):

        for f in filenames:
            full = os.path.join(root, f)
            if os.path.isfile(full) and f.endswith(".json"):
                result[os.path.splitext(f)[0]] = full

    return result


@pytest.fixture
def invalid_pipeline_paths():

    result = {}
    for root, dirnames, filenames in os.walk(INVALID_PIPELINES_FOLDER, topdown=True):

        for f in filenames:
            full = os.path.join(root, f)
            if os.path.isfile(full) and f.endswith(".json"):
                result[os.path.splitext(f)[0]] = full

    return result


@pytest.fixture
def module_config_paths():

    result = {}
    for root, dirnames, filenames in os.walk(MODULE_CONFIGS_FOLDER, topdown=True):

        for f in filenames:
            full = os.path.join(root, f)
            if os.path.isfile(full) and f.endswith(".json"):
                result[os.path.splitext(f)[0]] = full

    return result


@pytest.fixture
def kiara() -> Kiara:

    instance_path = create_temp_dir()
    kc = KiaraConfig.create_in_folder(instance_path)
    kc.runtime_config.runtime_profile = "default"

    kiara = kc.create_context()
    return kiara


@pytest.fixture
def api() -> BaseAPI:

    instance_path = create_temp_dir()
    kc = KiaraConfig.create_in_folder(instance_path)
    kc.runtime_config.runtime_profile = "default"
    api = BaseAPI(kc)
    return api


@pytest.fixture(scope="function")
def presseeded_data_store_minimal() -> Kiara:

    instance_path = create_temp_dir()

    pipeline_file = os.path.join(PIPELINES_FOLDER, "table_import.json")

    kc = KiaraConfig.create_in_folder(instance_path)
    kc.runtime_config.runtime_profile = "default"

    kiara = kc.create_context()

    batch_op = BatchOperation.from_file(pipeline_file, kiara=kiara)

    inputs = {
        "import_file__path": os.path.join(
            ROOT_DIR, "examples", "data", "journals", "JournalNodes1902.csv"
        ),
        "create_table_from_files__first_row_is_header": True,
    }

    batch_op.run(inputs=inputs, save="preseed_minimal")

    return kiara


@pytest.fixture(scope="function")
def preseeded_data_store() -> Kiara:

    instance_path = create_temp_dir()
    kc = KiaraConfig.create_in_folder(instance_path)
    kc.runtime_config.runtime_profile = "default"

    kiara = kc.create_context()

    pipeline = os.path.join(PIPELINES_FOLDER, "test_preseed_1.yaml")
    batch_op = BatchOperation.from_file(pipeline, kiara=kiara)

    inputs = {
        "edges_file_path": os.path.join(DATA_FOLDER, "journals/JournalEdges1902.csv"),
        "nodes_file_path": os.path.join(DATA_FOLDER, "journals/JournalNodes1902.csv"),
        "journals_folder_path": os.path.join(DATA_FOLDER, "journals"),
        "text_corpus_folder_path": os.path.join(DATA_FOLDER, "text_corpus"),
        "city_column_name": "City",
        "label_column_name": "Label",
    }

    batch_op.run(inputs=inputs, save="preseed")

    print(f"kiara data store: {kiara.data_registry.get_archive()}")  # noqa

    return kiara


# kiara\kiara\tests\test_kiara_context.py
# -*- coding: utf-8 -*-


#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)


# def test_multiple_kiara_instances():
#
#     kiara = Kiara()
#     kiara_2 = Kiara()
#
#     assert kiara.id == kiara_2.id
#     assert (
#         kiara.operation_registry.operation_ids
#         == kiara_2.operation_registry.operation_ids
#     )
#     assert (
#         kiara.module_registry.module_types.keys()
#         == kiara_2.module_registry.module_types.keys()
#     )


# def test_multiple_kiara_instances_threaded():
#     def create_kiara_context(ops: typing.List[str]):
#         kiara = Kiara()
#         ops.extend(kiara.operation_registry.operation_ids)
#
#     ops_1 = []
#     ops_2 = []
#
#     thread_1 = threading.Thread(target=create_kiara_context, args=(ops_1,))
#     thread_1.start()
#
#     thread_2 = threading.Thread(target=create_kiara_context, args=(ops_2,))
#     thread_2.start()
#
#     thread_1.join()
#     thread_2.join()
#
#     assert ops_1
#     assert ops_1 == ops_2


# kiara\kiara\tests\test_module_instances.py
# -*- coding: utf-8 -*-
import pytest

from kiara.api import Kiara
from kiara.exceptions import (
    InvalidManifestException,
    InvalidValuesException,
    KiaraException,
)
from kiara.models.module.manifest import Manifest
from kiara_plugin.core_types.modules.boolean import AndModule

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)


def test_module_instance_creation(kiara: Kiara):

    l_and = kiara.module_registry.create_module("logic.and")
    assert l_and.config.model_dump() == {"constants": {}, "defaults": {}, "delay": 0}

    with pytest.raises(InvalidManifestException):
        kiara.module_registry.create_module("logic.xor")

    l_and = kiara.module_registry.create_module("logic.and")
    assert "delay" in l_and.config.model_dump().keys()

    with pytest.raises(InvalidManifestException) as e_info:
        manifest = Manifest(module_type="logic.and", module_config={"xxx": "fff"})
        kiara.module_registry.create_module(manifest)

    msg = KiaraException.get_root_details(e_info.value)
    assert "xxx" in msg
    assert "Extra inputs" in msg


def test_module_instance_run(kiara: Kiara):

    l_and = kiara.module_registry.create_module("logic.and")

    result = l_and.run(kiara, a=True, b=True)
    assert result.get_all_value_data() == {"y": True}

    with pytest.raises(InvalidValuesException) as e_info:
        l_and.run(kiara=kiara)

    assert "Invalid inputs for module" in str(e_info.value)

    with pytest.raises(InvalidValuesException) as e_info:
        l_and.run(kiara=kiara, x=True)

    assert "Invalid inputs for module" in str(e_info.value)


def test_module_instance_direct(kiara: Kiara):

    and_module = AndModule()
    result = and_module.run(kiara=kiara, a=True, b=True)
    assert result.get_all_value_data() == {"y": True}


# kiara\kiara\tests\test_module_types.py
# -*- coding: utf-8 -*-
from kiara.api import Kiara

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)


def test_module_types_exist(kiara: Kiara):

    assert "logic.and" in kiara.module_registry.get_module_type_names()
    assert "logic.xor" not in kiara.module_registry.get_module_type_names()


def test_module_type_metadata(kiara: Kiara):

    l_and = kiara.module_registry.get_module_class("logic.and")
    assert not l_and.is_pipeline()

    pipeline = kiara.module_registry.get_module_class("pipeline")
    assert pipeline.is_pipeline()

    assert hasattr(l_and, "_module_type_name")
    assert hasattr(pipeline, "_module_type_name")


# kiara\kiara\tests\test_operations.py
# -*- coding: utf-8 -*-
from kiara.api import Kiara

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)


def test_operation_type_list(kiara: Kiara):

    assert "pretty_print" in kiara.operation_registry.operation_types.keys()

    op_xor = kiara.operation_registry.get_operation("logic.xor")
    assert op_xor.module.is_pipeline()
    assert op_xor.operation_id == "logic.xor"


# kiara\kiara\tests\test_pipeline_creation.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)


# def test_workflow_desc_files(pipeline_paths):
#
#     for path in pipeline_paths.values():
#         c = PipelineConfig.parse_file(path)
#         assert isinstance(c, PipelineConfig)
#         assert len(c.steps) > 0
#         assert c.steps[0].step_id
#         assert c.steps[0].module_type

# def test_workflow_obj_attributes(pipeline_configs: typing.Mapping[str, PipelineConfig]):
#
#     logic_1 = pipeline_configs["logic_1"]
#
#     assert len(logic_1.steps) == 1
#     assert len(logic_1.input_aliases) == 0
#     assert len(logic_1.output_aliases) == 0
#
#     logic_2 = pipeline_configs["logic_2"]
#
#     assert len(logic_2.steps) == 2
#     assert len(logic_2.input_aliases) == 3
#     assert len(logic_2.output_aliases) == 1
#
#     logic_3 = pipeline_configs["logic_3"]
#
#     assert len(logic_3.steps) == 3
#     assert len(logic_3.input_aliases) == 0
#     assert len(logic_3.output_aliases) == 0
#
#
# def test_workflow_obj_creation(pipeline_configs: typing.Mapping[str, PipelineConfig]):
#
#     logic_1 = pipeline_configs["logic_1"]
#     c = PipelineModule(id="logic_1", module_config=logic_1)
#     assert isinstance(c, PipelineModule)
#
#     assert c.full_id == "logic_1"
#     assert len(c.structure.steps) == 1
#     assert "and_1" in c.structure.to_details().steps.keys()
#     assert isinstance(c.structure.to_details().steps["and_1"].step, PipelineStep)
#
#
# def test_pipeline_structure_creation(kiara: Kiara):
#
#     pipeline_file = os.path.join(PIPELINES_FOLDER, "logic", "logic_3.json")
#
#     config = PipelineConfig.create_pipeline_config(pipeline_file)
#
#     structure = PipelineStructure(config=config, kiara=kiara)
#
#     for idx, step in enumerate(structure.steps):
#         assert isinstance(step, PipelineStep)
#
#     assert idx == 2
#
#     Pipeline(structure=structure)
#
#
# def test_invalid_pipeline_creation(kiara: Kiara):
#
#     pipeline_file = os.path.join(INVALID_PIPELINES_FOLDER, "logic_4.json")
#     config = PipelineConfig.create_pipeline_config(pipeline_file)
#
#     structure = PipelineStructure(config=config, kiara=kiara)
#
#     for idx, step in enumerate(structure.steps):
#         assert isinstance(step, PipelineStep)
#     assert idx == 2
#
#     with pytest.raises(Exception) as exc_info:
#         Pipeline(structure=structure)
#     assert "a1" in str(exc_info.value)


# kiara\kiara\tests\test_rendering.py
# -*- coding: utf-8 -*-
from kiara.interfaces.python_api.base_api import BaseAPI

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)


def test_render_python_script(api: BaseAPI):

    render_config = {"inputs": {"a": True, "b": False}}

    rendered = api.render(
        "logic.xor",
        source_type="pipeline",
        target_type="python_script",
        render_config=render_config,
    )

    compile(rendered, "<string>", "exec")

    local_vars = {}
    exec(rendered, {}, local_vars)  # noqa
    assert local_vars["pipeline_result_y"].data is True


# kiara\kiara\tests\utils.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import os

KIARA_TEST_RESOURCES = os.path.join(os.path.dirname(__file__), "resources")
PIPELINES_FOLDER = os.path.join(KIARA_TEST_RESOURCES, "pipelines")
INVALID_PIPELINES_FOLDER = os.path.join(KIARA_TEST_RESOURCES, "invalid_pipelines")
MODULE_CONFIGS_FOLDER = os.path.join(KIARA_TEST_RESOURCES, "module_configs")


def get_workflow_config_path(workflow_name: str):
    return os.path.join(PIPELINES_FOLDER, workflow_name)


def get_module_config_path(module_config_name: str):
    return os.path.join(MODULE_CONFIGS_FOLDER, module_config_name)


# kiara\kiara\tests\__init__.py


# kiara\kiara\tests\resources\archives\nand_true.0.10.kiarchive.json
{"type_name":"nand_true.0.10.kiarchive","documentation":{"description":"-- n/a --","doc":null},"authors":{"authors":[]},"context":{"references":{},"tags":[],"labels":{}},"data_archive_info":{"type_name":"1df47c50-058f-4f7a-8cdc-4ccab0ab1da9","documentation":{"description":"-- n/a --","doc":null},"authors":{"authors":[]},"context":{"references":{},"tags":[],"labels":{}},"archive_id":"1df47c50-058f-4f7a-8cdc-4ccab0ab1da9","archive_alias":"nand_true.0.10","archive_type_info":{"type_name":"sqlite_data_archive","documentation":{"description":"-- n/a --","doc":null},"authors":{"authors":[{"name":"Markus Binsteiner","email":"markus@frkl.io"}]},"context":{"references":{"source_repo":{"url":"https://github.com/DHARPA-Project/kiara","desc":"The kiara project git repository."},"documentation":{"url":"https://dharpa.org/kiara_documentation/","desc":"The url for kiara documentation."}},"tags":[],"labels":{"package":"kiara"}},"python_class":{"python_class_name":"SqliteDataArchive","python_module_name":"kiara.registries.data.data_store.sqlite_store","full_name":"kiara.registries.data.data_store.sqlite_store.SqliteDataArchive"},"is_writable":false,"supported_item_types":["data"]},"config":{"sqlite_db_path":"/home/markus/projects/kiara/kiara/tests/resources/archives/nand_true.0.10.kiarchive","use_wal_mode":false},"details":{"no_values":6,"value_ids":["2ed7942b-3719-4102-b42f-134a42969945","468dd2b4-0e0c-46f5-9def-05aa57396f46","4c929a5b-c91a-449e-9d5f-f21124298ea7","5668b4ae-a867-4238-bb34-be79115ff36e","60d0a0b3-1030-40c4-8c51-247fea623d83","7b7cd80a-8acd-4543-aa18-7e27ee124a70"],"dynamic_archive":false,"size":126976},"metadata":{"archive_id":"1df47c50-058f-4f7a-8cdc-4ccab0ab1da9","archive_name":"nand_true.0.10"}},"alias_archive_info":{"type_name":"1df47c50-058f-4f7a-8cdc-4ccab0ab1da9","documentation":{"description":"-- n/a --","doc":null},"authors":{"authors":[]},"context":{"references":{},"tags":[],"labels":{}},"archive_id":"1df47c50-058f-4f7a-8cdc-4ccab0ab1da9","archive_alias":"nand_true.0.10","archive_type_info":{"type_name":"sqlite_alias_archive","documentation":{"description":"-- n/a --","doc":null},"authors":{"authors":[{"name":"Markus Binsteiner","email":"markus@frkl.io"}]},"context":{"references":{"source_repo":{"url":"https://github.com/DHARPA-Project/kiara","desc":"The kiara project git repository."},"documentation":{"url":"https://dharpa.org/kiara_documentation/","desc":"The url for kiara documentation."}},"tags":[],"labels":{"package":"kiara"}},"python_class":{"python_class_name":"SqliteAliasArchive","python_module_name":"kiara.registries.aliases.sqlite_store","full_name":"kiara.registries.aliases.sqlite_store.SqliteAliasArchive"},"is_writable":false,"supported_item_types":["alias"]},"config":{"sqlite_db_path":"/home/markus/projects/kiara/kiara/tests/resources/archives/nand_true.0.10.kiarchive","use_wal_mode":false},"details":{"no_aliases":1,"aliases":["y"],"dynamic_archive":false},"metadata":{"archive_id":"1df47c50-058f-4f7a-8cdc-4ccab0ab1da9","archive_name":"nand_true.0.10"}},"metadata_archive_info":{"type_name":"1df47c50-058f-4f7a-8cdc-4ccab0ab1da9","documentation":{"description":"-- n/a --","doc":null},"authors":{"authors":[]},"context":{"references":{},"tags":[],"labels":{}},"archive_id":"1df47c50-058f-4f7a-8cdc-4ccab0ab1da9","archive_alias":"nand_true.0.10","archive_type_info":{"type_name":"sqlite_metadata_archive","documentation":{"description":"-- n/a --","doc":null},"authors":{"authors":[{"name":"Markus Binsteiner","email":"markus@frkl.io"}]},"context":{"references":{"source_repo":{"url":"https://github.com/DHARPA-Project/kiara","desc":"The kiara project git repository."},"documentation":{"url":"https://dharpa.org/kiara_documentation/","desc":"The url for kiara documentation."}},"tags":[],"labels":{"package":"kiara"}},"python_class":{"python_class_name":"SqliteMetadataArchive","python_module_name":"kiara.registries.metadata.metadata_store.sqlite_store","full_name":"kiara.registries.metadata.metadata_store.sqlite_store.SqliteMetadataArchive"},"is_writable":false,"supported_item_types":["metadata"]},"config":{"sqlite_db_path":"/home/markus/projects/kiara/kiara/tests/resources/archives/nand_true.0.10.kiarchive","use_wal_mode":false},"details":{"no_metadata_items":4,"no_references":1,"dynamic_archive":false},"metadata":{"archive_id":"1df47c50-058f-4f7a-8cdc-4ccab0ab1da9","archive_name":"nand_true.0.10"}},"job_archive_info":{"type_name":"1df47c50-058f-4f7a-8cdc-4ccab0ab1da9","documentation":{"description":"-- n/a --","doc":null},"authors":{"authors":[]},"context":{"references":{},"tags":[],"labels":{}},"archive_id":"1df47c50-058f-4f7a-8cdc-4ccab0ab1da9","archive_alias":"nand_true.0.10","archive_type_info":{"type_name":"sqlite_job_archive","documentation":{"description":"-- n/a --","doc":null},"authors":{"authors":[{"name":"Markus Binsteiner","email":"markus@frkl.io"}]},"context":{"references":{"source_repo":{"url":"https://github.com/DHARPA-Project/kiara","desc":"The kiara project git repository."},"documentation":{"url":"https://dharpa.org/kiara_documentation/","desc":"The url for kiara documentation."}},"tags":[],"labels":{"package":"kiara"}},"python_class":{"python_class_name":"SqliteJobArchive","python_module_name":"kiara.registries.jobs.job_store.sqlite_store","full_name":"kiara.registries.jobs.job_store.sqlite_store.SqliteJobArchive"},"is_writable":false,"supported_item_types":["job_record"]},"config":{"sqlite_db_path":"/home/markus/projects/kiara/kiara/tests/resources/archives/nand_true.0.10.kiarchive","use_wal_mode":false},"details":{"no_job_records":4,"dynamic_archive":false},"metadata":{"archive_id":"1df47c50-058f-4f7a-8cdc-4ccab0ab1da9","archive_name":"nand_true.0.10"}}}

# kiara\kiara\tests\resources\archives\archive_create_jobs\nand_true.yaml
operation: logic.and
inputs:
  a: true
  b: true


# kiara\kiara\tests\resources\invalid_pipelines\logic_4.json
{
  "steps": [
    {
      "module_type": "logic.and",
      "module_config": {
        "constants": {
          "a1": false
        }
      },
      "step_id": "and_1_1"
    },
    {
      "module_type": "logic.and",
      "step_id": "and_1_2"
    },
    {
      "module_type": "logic.and",
      "step_id": "and_2",
      "input_links": {
        "a": "and_1_1.y",
        "b": "and_1_2.y"
      }
    }
  ]
}


# kiara\kiara\tests\resources\module_configs\and.json
{
  "module_type": "logic.and"
}


# kiara\kiara\tests\resources\module_configs\and_wrapped.json
{
  "module_type": "pipeline",
  "module_config": {
    "steps": [
      {
        "module_type": "logic.and",
        "step_id": "and_1"
      }
    ],
    "doc": "Simple example pipeline, wrapping a single 'and' module."
  }
}


# kiara\kiara\tests\resources\module_configs\table_load.json
{
  "module_type": "table.load"
}


# kiara\kiara\tests\resources\pipelines\table_import.json
{
  "pipeline_name": "table_import",
  "steps": [
    {
      "module_type": "import.local.file",
      "step_id": "import_file"
    },
    {
      "module_type": "create.table.from.file",
      "step_id": "create_table_from_files",
      "input_links": {
        "file": "import_file.file"
      }
    }]
}


# kiara\kiara\tests\resources\pipelines\test_preseed_1.yaml
pipeline_name: test_preseed
doc: Onboarding of a few tables from csv files to run unit tests against.
steps:
  - module_type: import.local.file
    step_id: import_edges_file
  - module_type: import.local.file
    step_id: import_nodes_file
  - module_type: import.local.file_bundle
    step_id: import_journal_csvs
  - module_type: import.local.file_bundle
    step_id: import_text_corpus
  - module_type: create.table.from.file
    step_id: create_edges_table
    input_links:
      _file: import_edges_file.file
  - module_type: create.table.from.file
    step_id: create_nodes_table
    input_links:
      file: import_nodes_file.file
  - module_type: table.pick.column
    step_id: create_city_array
    input_links:
      table: create_nodes_table.table
  - module_type: table.pick.column
    step_id: create_label_array
    input_links:
      table: create_nodes_table.table

input_aliases:
  import_edges_file.path: edges_file_path
  import_nodes_file.path: nodes_file_path
  import_journal_csvs.path: journals_folder_path
  import_text_corpus.path: text_corpus_folder_path
  create_city_array.column_name: city_column_name
  create_label_array.column_name: label_column_name

output_aliases:
  import_edges_file.file: journal_edges_file
  import_nodes_file.file: journal_nodes_file
  import_journal_csvs.file_bundle: journals_file_bundle
  import_text_corpus.file_bundle: text_corpus_file_bundle
  create_edges_table.table: journal_edges_table
  create_nodes_table.table: journal_nodes_table
  create_city_array.array: cities_array
  create_label_array.array: labels_array

inputs:
  edges_file_path: examples/data/journals/JournalEdges1902.csv
  nodes_file_path: examples/data/journals/JournalNodes1902.csv
  city_column_name: City
  journals_folder_path: examples/data/journals
  label_column_name: Label
  text_corpus_folder_path: examples/data/text_corpus


# kiara\kiara\tests\resources\pipelines\dummy\dummy_1.json
{
  "steps": [
    {
      "module_type": "dev.dummy",
      "step_id": "and_1",
      "module_config": {
        "input_schema": {
          "a": {
            "type": "integer"
          }
        },
        "output_schema": {
          "x": {
            "type": "integer"
          },
          "y": {
            "type": "string"
          },
          "z": {
            "type": "boolean"
          }
        }
      }
    }
  ]
}


# kiara\kiara\tests\resources\pipelines\dummy\dummy_1_delay.json
{
  "steps": [
    {
      "module_type": "dev.dummy",
      "step_id": "and_1",
      "module_config": {
        "input_schema": {
          "a": {
            "type": "integer"
          }
        },
        "output_schema": {
          "x": {
            "type": "integer"
          },
          "y": {
            "type": "string"
          },
          "z": {
            "type": "boolean"
          }
        },
        "delay": 2,
        "outputs": {
          "x": 2
        }
      }
    }
  ]
}


# kiara\kiara\tests\resources\pipelines\logic\logic_1.json
{
  "steps": [
    {
      "module_type": "logic.and",
      "step_id": "and_1"
    }
  ],
  "doc": "Simple example pipeline, wrapping a single 'and' module."
}


# kiara\kiara\tests\resources\pipelines\logic\logic_2.json
{
  "steps": [
    {
      "module_type": "logic.and",
      "step_id": "and_1"
    },
    {
      "module_type": "logic.and",
      "step_id": "and_2",
      "input_links": {
        "a": "and_1.y"
      }
    }
  ],
  "input_aliases": {
    "and_1__a": "a",
    "and_1__b": "b",
    "and_2__b": "c"
  },
  "output_aliases": {
    "and_2__y": "y"
  },
  "doc": "Returns 'true' only if all 3 inputs are also 'true'."
}


# kiara\kiara\tests\resources\pipelines\logic\logic_3.json
{
  "steps": [
    {
      "module_type": "logic.and",
      "step_id": "and_1_1"
    },
    {
      "module_type": "logic.and",
      "step_id": "and_1_2"
    },
    {
      "module_type": "logic.and",
      "step_id": "and_2",
      "input_links": {
        "a": "and_1_1.y",
        "b": "and_1_2.y"
      }
    }
  ],
  "doc": "Returns 'true' only if all 4 inputs are 'true'."
}


# kiara\kiara\tests\resources\pipelines\logic\logic_4.json
{
  "steps": [
    {
      "module_type": "logic.and",
      "module_config": {
        "constants": {
          "a": false
        }
      },
      "step_id": "and_1_1"
    },
    {
      "module_type": "logic.and",
      "step_id": "and_1_2"
    },
    {
      "module_type": "logic.and",
      "step_id": "and_2",
      "input_links": {
        "a": "and_1_1.y",
        "b": "and_1_2.y"
      }
    }
  ]
}


# kiara\kiara\tests\test_api\test_data_types.py
# -*- coding: utf-8 -*-
from kiara.interfaces.python_api.base_api import BaseAPI

#  Copyright (c) 2023, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)


def test_data_types(api: BaseAPI):
    # test 'boolean' and 'table' are in the available data type names

    assert "boolean" in api.list_data_type_names()
    assert "table" in api.list_data_type_names()


def test_internal_data_types(api: BaseAPI):

    assert not api.is_internal_data_type("table")
    assert not api.is_internal_data_type("boolean")

    assert api.is_internal_data_type("render_value_result")


def test_data_type_info(api: BaseAPI):

    infos = api.retrieve_data_types_info(filter="table")
    assert len(infos.item_infos) == 2

    info = api.retrieve_data_type_info("table")
    infos = iter(infos.item_infos.values())
    try:
        assert info == next(infos)
    except Exception:
        assert info == next(infos)

    assert info.type_name in ["table", "tables"]


# kiara\kiara\tests\test_api\test_misc.py
# -*- coding: utf-8 -*-
from kiara.interfaces.python_api.base_api import BaseAPI

#  Copyright (c) 2023, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)


def test_api_instance(api: BaseAPI):

    assert api.context.id


def test_api_doc(api: BaseAPI):

    assert "doc" in api.doc.keys()
    assert "Get the documentation" in api.doc["doc"]


def test_runtime_config(api: BaseAPI):

    rtc = api.get_runtime_config()
    assert "job_cache" in rtc.model_dump().keys()


def test_context_names(api: BaseAPI):

    # this is specific to the test setup api context, usually there are names in there, at least 'default'
    assert not api.list_context_names()


# kiara\kiara\tests\test_api\test_module_types.py
# -*- coding: utf-8 -*-
from kiara.interfaces.python_api.base_api import BaseAPI

#  Copyright (c) 2023, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)


def test_module_types(api: BaseAPI):
    # test 'boolean' and 'table' are in the available data type names

    assert "logic.and" in api.list_module_type_names()
    assert "query.database" in api.list_module_type_names()


def test_module_type_info(api: BaseAPI):

    infos = api.retrieve_module_types_info(filter="query.databas")
    assert len(infos.item_infos) == 1

    info = api.retrieve_module_type_info("query.database")
    assert info == next(iter(infos.item_infos.values()))

    assert info.type_name == "query.database"


# kiara\kiara\tests\test_api\test_operations.py
# -*- coding: utf-8 -*-
from kiara.interfaces.python_api.base_api import BaseAPI

#  Copyright (c) 2023, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)


def test_operation_list(api: BaseAPI):

    op_list = api.list_operation_ids()

    assert "logic.and" in op_list
    assert "logic.xor" in op_list
    assert "query.database" in op_list
    assert "render.database.as.string" not in op_list

    op_list = api.list_operation_ids(include_internal=True)
    assert "render.database.as.string" in op_list

    op_list = api.list_operation_ids("query.database")
    assert "query.database" in op_list


def test_get_operation(api: BaseAPI):

    op = api.get_operation("query.database")
    assert "Execute a sql query against a (sqlite) database." in op.doc.full_doc

    op = api.get_operation("logic.and")
    result = op.run(kiara=api.context, inputs={"a": True, "b": True})

    assert result.get_value_data("y") is True

    op = api.get_operation("logic.nand")
    result = op.run(kiara=api.context, inputs={"a": True, "b": True})

    assert result.get_value_data("y") is False


# kiara\kiara\tests\test_api\__init__.py


# kiara\kiara\tests\test_archives\test_archive_export.py
# -*- coding: utf-8 -*-
import datetime
import os
import sqlite3
import sys
import tempfile
import uuid
from pathlib import Path
from typing import List, Union

import pytest

from kiara.defaults import (
    ALL_REQUIRED_TABLES,
    TABLE_NAME_DATA_PEDIGREE,
    TABLE_NAME_DATA_METADATA,
    TABLE_NAME_ARCHIVE_METADATA,
    TABLE_NAME_DATA_CHUNKS,
    TABLE_NAME_DATA_DESTINIES,
    TABLE_NAME_DATA_SERIALIZATION_METADATA,
    TABLE_NAME_ALIASES,
)
from kiara.interfaces.python_api.base_api import BaseAPI
from kiara.models.values.value import ValueMapReadOnly, Value


# flake8: noqa


def run_sql_query(sql: str, archive_file: Union[str, Path]):
    con = sqlite3.connect(archive_file)
    cursor = con.cursor()
    cursor.execute(sql)
    result = cursor.fetchall()
    con.close()
    return result


def check_archive_contains_table_names(
    archive_file: Union[str, Path], required_tables: List[str]
):

    con = sqlite3.connect(archive_file)

    cursor = con.cursor()
    sql = "SELECT name FROM sqlite_master WHERE type='table';"
    cursor.execute(sql)

    tables = {t[0] for t in cursor.fetchall()}
    con.close()

    if not set(required_tables).issubset(tables):
        raise Exception(
            f"Archive file does not contain all required tables: {required_tables} not in {tables}"
        )


def check_table_is_empty(archive_file: Union[str, Path], table_name: str):

    con = sqlite3.connect(archive_file)

    cursor = con.cursor()
    sql = f'SELECT COUNT(*) FROM "{table_name}";'
    cursor.execute(sql)

    count = cursor.fetchone()[0]
    con.close()

    if count > 0:
        raise Exception(f"Table {table_name} is not empty")


def check_tables_are_empty(archive_file: Union[str, Path], *table_names: str):

    for table_name in table_names:
        check_table_is_empty(archive_file, table_name)


def check_table_is_not_empty(archive_file: Union[str, Path], table_name: str):

    con = sqlite3.connect(archive_file)

    cursor = con.cursor()
    sql = f'SELECT COUNT(*) FROM "{table_name}";'
    cursor.execute(sql)

    count = cursor.fetchone()[0]
    con.close()

    if count == 0:
        raise Exception(f"Table {table_name} is empty")


def check_tables_are_not_empty(archive_file: Union[str, Path], *table_names: str):

    for table_name in table_names:
        check_table_is_not_empty(archive_file, table_name)


# TODO: fix for windows
@pytest.mark.skipif(
    sys.platform == "win32",
    reason="Does not run on Windows for some reason, need to investigate",
)
def test_archive_export_values_no_alias(api: BaseAPI):

    result: ValueMapReadOnly = api.run_job(
        operation="logic.and", inputs={"a": True, "b": True}
    )

    with tempfile.TemporaryDirectory(suffix="no_alias") as temp_dir:

        temp_file_path = Path(temp_dir) / "export_test_no_alias.kiarchive"
        temp_file_path = temp_file_path.resolve()

        store_result = api.export_values(
            temp_file_path, result, alias_map=False, export_related_metadata=False
        )

        if not temp_file_path.is_file():
            raise Exception(f"Export file {temp_file_path.as_posix()} was not created")

        assert temp_file_path.stat().st_size > 0

        assert len(store_result) == 1
        assert "y" in store_result.keys()

        required_tables = ALL_REQUIRED_TABLES

        check_archive_contains_table_names(temp_file_path, required_tables)

        check_table_is_empty(temp_file_path, TABLE_NAME_ALIASES)
        check_tables_are_not_empty(
            temp_file_path,
            TABLE_NAME_DATA_PEDIGREE,
            TABLE_NAME_DATA_METADATA,
            TABLE_NAME_ARCHIVE_METADATA,
            TABLE_NAME_DATA_CHUNKS,
            TABLE_NAME_DATA_DESTINIES,
            TABLE_NAME_DATA_SERIALIZATION_METADATA,
        )


# TODO: fix for windows
@pytest.mark.skipif(
    sys.platform == "win32",
    reason="Does not run on Windows for some reason, need to investigate",
)
def test_archive_export_values_alias(api: BaseAPI):

    result: ValueMapReadOnly = api.run_job(
        operation="logic.and", inputs={"a": True, "b": True}
    )

    with tempfile.TemporaryDirectory(suffix="alias") as temp_dir:

        temp_file_path = Path(temp_dir) / "export_test_alias.kiarchive"
        temp_file_path = temp_file_path.resolve()

        store_result = api.export_values(
            temp_file_path, result, alias_map=True, export_related_metadata=False
        )

        if not temp_file_path.is_file():
            raise Exception(f"Export file {temp_file_path.name} was not created")

        assert temp_file_path.stat().st_size > 0

        assert len(store_result) == 1
        assert "y" in store_result.keys()

        required_tables = ALL_REQUIRED_TABLES
        check_archive_contains_table_names(temp_file_path, required_tables)

        check_tables_are_not_empty(
            temp_file_path,
            TABLE_NAME_DATA_PEDIGREE,
            TABLE_NAME_DATA_METADATA,
            TABLE_NAME_ARCHIVE_METADATA,
            TABLE_NAME_DATA_CHUNKS,
            TABLE_NAME_DATA_DESTINIES,
            TABLE_NAME_DATA_SERIALIZATION_METADATA,
            TABLE_NAME_ALIASES,
        )

        result = run_sql_query(f'SELECT * FROM "{TABLE_NAME_ALIASES}";', temp_file_path)
        assert len(result) == 1

        assert result[0][0] == "y"
        assert uuid.UUID(result[0][1])


# TODO: fix for windows
@pytest.mark.skipif(
    sys.platform == "win32",
    reason="Does not run on Windows for some reason, need to investigate",
)
def test_archive_export_values_alias_multipe_values(api: BaseAPI):

    result_1: Value = api.run_job(operation="logic.and", inputs={"a": True, "b": True})[
        "y"
    ]
    result_2: Value = api.run_job(
        operation="logic.nand", inputs={"a": True, "b": True}
    )["y"]

    results = {
        "result_1": result_1,
        "result_2": result_2,
    }

    with tempfile.TemporaryDirectory(suffix="multiple") as temp_dir:

        temp_file_path = Path(temp_dir) / "export_test_alias_multiple_values.kiarchive"
        temp_file_path = temp_file_path.resolve()

        store_result = api.export_values(
            temp_file_path, results, alias_map=True, export_related_metadata=False
        )

        if not temp_file_path.is_file():
            raise Exception(f"Export file {temp_file_path.name} was not created")

        assert temp_file_path.stat().st_size > 0

        assert len(store_result) == 2
        assert "result_1" in store_result.keys()
        assert "result_2" in store_result.keys()

        required_tables = ALL_REQUIRED_TABLES
        check_archive_contains_table_names(temp_file_path, required_tables)

        check_tables_are_not_empty(
            temp_file_path,
            TABLE_NAME_DATA_PEDIGREE,
            TABLE_NAME_DATA_METADATA,
            TABLE_NAME_ARCHIVE_METADATA,
            TABLE_NAME_DATA_CHUNKS,
            TABLE_NAME_DATA_DESTINIES,
            TABLE_NAME_DATA_SERIALIZATION_METADATA,
            TABLE_NAME_ALIASES,
        )

        result = run_sql_query(f'SELECT * FROM "{TABLE_NAME_ALIASES}";', temp_file_path)

        print(result)
        assert len(result) == 2
        assert len(result[0]) == 3
        assert result[0][0] in ["result_1", "result_2"]
        assert uuid.UUID(result[0][1])
        datetime.datetime.fromisoformat(result[0][2])

        assert len(result[1]) == 3
        assert result[1][0] in ["result_1", "result_2"]
        assert uuid.UUID(result[1][1])
        datetime.datetime.fromisoformat(result[1][2])


# kiara\kiara\tests\test_archives\test_archive_import.py
# -*- coding: utf-8 -*-
import os
import uuid
from pathlib import Path

from kiara.interfaces.python_api.base_api import BaseAPI

ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
TEST_RESOURCES_FOLDER = os.path.join(ROOT_DIR, "tests", "resources")

VALUE_ID = "4c929a5b-c91a-449e-9d5f-f21124298ea7"


def test_archive_import_values_no_alias(api: BaseAPI):

    resources_folder = Path(TEST_RESOURCES_FOLDER)

    archive_file = resources_folder / "archives" / "nand_true.0.10.kiarchive"

    assert not api.list_all_value_ids()

    result = api.import_archive(archive_file, no_aliases=True)

    assert not result.errors

    assert len(result) == 6
    assert VALUE_ID in result.keys()

    assert uuid.UUID(VALUE_ID) in api.list_all_value_ids()

    assert ["nand_true.0.10#y"] == api.list_alias_names()


def test_archive_import_values_with_alias(api: BaseAPI):

    resources_folder = Path(TEST_RESOURCES_FOLDER)

    archive_file = resources_folder / "archives" / "nand_true.0.10.kiarchive"

    assert not api.list_all_value_ids()

    result = api.import_archive(archive_file, no_aliases=False)

    assert not result.errors

    assert len(result) == 6
    assert VALUE_ID in result.keys()

    assert uuid.UUID(VALUE_ID) in api.list_all_value_ids()

    assert {"y", "nand_true.0.10#y"} == set(api.list_alias_names())


# kiara\kiara\tests\test_cli\test_context_subcommands.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)
from click.testing import CliRunner

from kiara.interfaces.cli import cli


def test_context_subcommand():

    runner = CliRunner()
    result = runner.invoke(cli, "context")
    assert result.exit_code == 0
    assert "context related sub-commands" in result.stdout


# def test_delete_context_subcommand():
#
#     runner = CliRunner()
#     result = runner.invoke(cli, "module list")
#
#     assert result.exit_code == 0
#     assert "logic.or" in result.stdout
#     assert "logic.xor" not in result.stdout


# kiara\kiara\tests\test_cli\test_data_subcommands.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)
from click.testing import CliRunner

from kiara.context import Kiara
from kiara.interfaces.cli import cli


def test_data_subcommand():

    runner = CliRunner()
    result = runner.invoke(cli, "data")
    assert result.exit_code == 0
    assert "Print the metadata" in result.stdout


def _run_command(kiara_ctx: Kiara, cmd):

    config_path = kiara_ctx.context_config._context_config_path

    cmd = f"--context '{config_path}' {cmd}"
    print(f"Running command:\n\nkiara {cmd}")  # noqa

    runner = CliRunner()
    result = runner.invoke(cli, cmd)
    return result


def test_data_list_subcommand(presseeded_data_store_minimal: Kiara):

    command = "data list --all-values --format json"
    result = _run_command(kiara_ctx=presseeded_data_store_minimal, cmd=command)

    assert result.exit_code == 0
    assert "preseed_minimal.create_table_from_files__table" in result.stdout


def test_data_load_subcommand(presseeded_data_store_minimal: Kiara):

    cmd = "data load alias:preseed_minimal.import_file__file"
    result = _run_command(kiara_ctx=presseeded_data_store_minimal, cmd=cmd)

    assert "Psychiatrische en neurologische bladen" in result.stdout
    assert "City" in result.stdout


def test_data_explain_subcommand(presseeded_data_store_minimal: Kiara):

    cmd = "data explain alias:preseed_minimal.create_table_from_files__table -p"

    result = _run_command(kiara_ctx=presseeded_data_store_minimal, cmd=cmd)

    assert "Latitude" in result.stdout
    assert "type_name" in result.stdout


def test_data_explain_subcommand_2(preseeded_data_store: Kiara):

    cmd = "data explain alias:preseed.journal_nodes_table"

    result = _run_command(kiara_ctx=preseeded_data_store, cmd=cmd)

    assert result.exit_code == 0

    assert "table" in result.stdout


def test_data_load_subcommand_3(preseeded_data_store: Kiara):

    cmd = "data load alias:preseed.journal_nodes_table"
    result = _run_command(kiara_ctx=preseeded_data_store, cmd=cmd)

    assert result.exit_code == 0
    assert "Id" in result.stdout
    assert "City" in result.stdout


# kiara\kiara\tests\test_cli\test_metadata_subcommands.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)


# def test_metadata_subcommand():
#
#     runner = CliRunner()
#     result = runner.invoke(cli, "metadata")
#     assert result.exit_code == 0
#     assert "Print details for a specific" in result.stdout
#
#
# def test_metadata_list_subcommand():
#
#     runner = CliRunner()
#     result = runner.invoke(cli, "metadata list")
#
#     assert result.exit_code == 0
#     assert "python_class" in result.stdout
#
#
# def test_metadata_explain_subcommand():
#
#     runner = CliRunner()
#     result = runner.invoke(cli, "metadata explain python_class")
#
#     assert result.exit_code == 0
#     assert "Python class" in result.stdout
#     assert "Markus Binsteiner" in result.stdout


# kiara\kiara\tests\test_cli\test_misc_commands.py
# -*- coding: utf-8 -*-
from click.testing import CliRunner

from kiara.interfaces.cli import cli

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)


def test_non_arg_cli():

    runner = CliRunner()
    result = runner.invoke(cli)
    result.exit_code == 0
    assert "Module-related sub-commands." in result.stdout
    assert "Operation-related sub-commands" in result.stdout

    result_2 = runner.invoke(cli, "--help")
    assert result.stdout == result_2.stdout


# def test_explain_subcommand():
#
#     runner = CliRunner()
#
#     result = runner.invoke(cli, "explain table.query.sql")
#     assert "table.query.sql" in result.stdout
#
#     result = runner.invoke(cli, "explain table")
#     assert "Load a column" in result.stdout
#     assert "Generate a data profile" in result.stdout


# kiara\kiara\tests\test_cli\test_module_subcommands.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)
from click.testing import CliRunner

from kiara.interfaces.cli import cli


def test_module_subcommand():

    runner = CliRunner()
    result = runner.invoke(cli, "module")
    assert result.exit_code == 0
    assert "Module-related sub-commands" in result.stdout


def test_logic_list_subcommand():

    runner = CliRunner()
    result = runner.invoke(cli, "module list")

    assert result.exit_code == 0
    assert "logic.or" in result.stdout
    assert "logic.xor" not in result.stdout


def test_logic_list_filter_subcommand():

    runner = CliRunner()
    result = runner.invoke(cli, "module list logic")

    assert result.exit_code == 0
    assert "logic.or" in result.stdout
    assert "logic.xor" not in result.stdout


def test_module_explain_subcommand():

    runner = CliRunner()
    result = runner.invoke(cli, "module explain logic.or")

    assert result.exit_code == 0
    assert "Returns 'True' if one of the inputs is 'True'" in result.stdout


def test_module_explain_instance_subcommand():

    runner = CliRunner()

    result = runner.invoke(cli, "module explain-instance logic.and")
    assert result.exit_code == 0
    assert "Inputs" in result.stdout


# kiara\kiara\tests\test_cli\test_operation_subcommands.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)
from click.testing import CliRunner

from kiara.interfaces.cli import cli


def test_operation_list_subcommand():

    runner = CliRunner()

    result = runner.invoke(cli, "operation list")

    assert "logic.nand" in result.stdout


def test_operation_list_by_group_subcommand():

    runner = CliRunner()

    result = runner.invoke(cli, "operation list --by-type")

    assert "logic.nand" in result.stdout


def test_operation_explain_subcommand():

    os_env_vars = {"CONSOLE_WIDTH": "400"}
    runner = CliRunner(env=os_env_vars)

    result = runner.invoke(cli, "operation explain logic.nand")

    assert "'False'" in result.stdout
    # assert "relation_name" in result.stdout
    # assert "Module metadata" in result.stdout


# kiara\kiara\tests\test_cli\test_pipeline_subcommands.py
# -*- coding: utf-8 -*-
import os

from click.testing import CliRunner

from kiara.interfaces.cli import cli

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)


def test_pipeline_subcommand():

    runner = CliRunner()
    result = runner.invoke(cli, "pipeline")
    assert result.exit_code == 0
    assert "Pipeline-related sub-commands" in result.stdout


def test_pipeline_explain_subcommand():

    runner = CliRunner()
    result = runner.invoke(cli, "pipeline explain logic.xor")

    assert result.exit_code == 0
    assert (
        "Returns 'True' if exactly one of it's two inputs is 'True'." in result.stdout
    )
    assert "structure" in result.stdout
    assert "outputs" in result.stdout

    pipeline_file = os.path.join(
        os.path.dirname(__file__),
        "..",
        "resources",
        "pipelines",
        "logic",
        "logic_3.json",
    )
    abs_path = os.path.abspath(pipeline_file)
    result = runner.invoke(cli, f"pipeline explain '{abs_path}'")

    assert result.exit_code == 0
    assert "structure" in result.stdout
    assert "and_1_1__y" in result.stdout


# kiara\kiara\tests\test_cli\test_run_subcommand.py
# -*- coding: utf-8 -*-
import os
import sys

import pytest
from click.testing import CliRunner

from kiara.interfaces.cli import cli

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))

DATA_FOLDER = os.path.join(ROOT_DIR, "examples", "data")
TEST_RESOURCES_FOLDER = os.path.join(ROOT_DIR, "tests", "resources")
KIARA_CONFIG_FILE = os.path.join(TEST_RESOURCES_FOLDER, "kiara.config")


def test_run_without_module():

    runner = CliRunner()
    result = runner.invoke(cli, "run")
    assert result.exit_code == 2
    assert "Missing argument" in result.stdout


def test_run_without_args():

    runner = CliRunner()
    result = runner.invoke(cli, "run logic.and")
    assert result.exit_code == 1
    assert "invalid or insufficient input" in result.stdout
    assert "not set" in result.stdout


def test_run_with_missing_arg():

    runner = CliRunner()
    result = runner.invoke(cli, "run logic.and a=true")
    assert result.exit_code == 1
    assert "not set" in result.stdout
    assert "valid" in result.stdout
    assert "invalid or insufficient input" in result.stdout


@pytest.mark.skipif(
    sys.platform == "win32",
    reason="Config path does not run on Windows for some reason, need to investigate",
)
def test_run_with_valid_inputs():

    runner = CliRunner()
    result = runner.invoke(
        cli,
        f'-cnf {KIARA_CONFIG_FILE} run logic.and a=true b=true --comment "A comment."',
    )
    assert result.exit_code == 0
    assert "True" in result.stdout


@pytest.mark.skipif(
    sys.platform == "win32",
    reason="Config path does not run on Windows for some reason, need to investigate",
)
def test_run_with_save():

    runner = CliRunner(env={"KIARA_CONTEXT": "_unit_tests_run"})
    runner.invoke(cli, "context delete -f")
    result = runner.invoke(
        cli,
        f'-cnf {KIARA_CONFIG_FILE} run logic.and a=true b=true --save test_save --comment "A comment."',
    )
    assert result.exit_code == 0
    assert "True" in result.stdout

    result_data = runner.invoke(cli, "data list")
    assert "test_save.y" in result_data.stdout


@pytest.mark.skipif(
    sys.platform == "win32",
    reason="Config path does not run on Windows for some reason, need to investigate",
)
def test_run_with_missing_comment():

    runner = CliRunner()
    result = runner.invoke(cli, f"-cnf {KIARA_CONFIG_FILE} run logic.and a=true b=true")
    assert result.exit_code == 1
    assert "No job metadata provided." in result.stdout


# kiara\kiara\tests\test_cli\__init__.py
# -*- coding: utf-8 -*-
#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)


# kiara\kiara\tests\test_included_data_types\test_string.py
# -*- coding: utf-8 -*-
import pytest

from kiara.interfaces.python_api.base_api import BaseAPI


def test_pure_string(api: BaseAPI):

    value = api.register_data("test_string", "string")
    assert value.data == "test_string"


def test_string_with_config(api: BaseAPI):
    config = {
        "type": "string",
        "type_config": {"allowed_strings": ["x", "y", "z", "test_string"]},
    }
    value = api.register_data("test_string", data_type=config)
    assert value.data == "test_string"


def test_invalid_string(api: BaseAPI):

    with pytest.raises(ValueError):
        config = {"type": "string", "type_config": {"allowed_strings": ["x", "y", "z"]}}
        api.register_data("test_string", data_type=config)


# kiara\kiara\tests\test_modules\test_operations.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)
# import os
#
# from kiara.context import Kiara
# from kiara.interfaces.python_api.operation import KiaraOperation
# from kiara.models.filesystem import FileBundle, FileModel
#
# ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
# DATA_FOLDER = os.path.join(ROOT_DIR, "examples", "data")
#
#
# def test_import_file_operation(kiara: Kiara):
#
#     op = KiaraOperation(kiara=kiara, operation_name="import.file")
#
#     nodes_file = os.path.join(DATA_FOLDER, "journals", "JournalNodes1902.csv")
#
#     results = op.run(path=nodes_file)
#
#     file_value: FileModel = results.get_value_data("file")
#     assert isinstance(file_value, FileModel)
#     assert file_value.file_name == "JournalNodes1902.csv"
#
#
# def test_import_file_bundle(kiara: Kiara):
#
#     op = KiaraOperation(kiara=kiara, operation_name="import.file_bundle")
#
#     journals_folder = os.path.join(DATA_FOLDER, "journals")
#
#     results = op.run(path=journals_folder)
#
#     bundle_value: FileBundle = results.get_value_data("file_bundle")
#     assert isinstance(bundle_value, FileBundle)
#     assert "JournalNodes1902.csv" in bundle_value.included_files.keys()


# kiara\kiara\tests\test_modules\test_simple_module_exec.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import pytest

from kiara.context import Kiara
from kiara.exceptions import InvalidValuesException


def test_module_processing(kiara: Kiara):

    and_mod = kiara.create_manifest("logic.and")

    # boolean_schema = ValueSchema(type="boolean")
    inputs = {"a": None, "b": True}
    with pytest.raises(InvalidValuesException) as e:
        kiara.process(manifest=and_mod, inputs=inputs)

    assert "a" in e.value.invalid_inputs.keys()
    assert "not set" in e.value.invalid_inputs["a"]

    inputs = {"a": True, "b": True}
    outputs = kiara.process(manifest=and_mod, inputs=inputs)

    assert outputs.get_value_data("y") is True

    inputs = {"a": False, "b": True}
    outputs = kiara.process(manifest=and_mod, inputs=inputs)
    assert outputs.get_value_data("y") is False


# kiara\kiara\tests\test_modules\__init__.py
# -*- coding: utf-8 -*-
#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)


# kiara\kiara\tests\test_operation_types\test_extract_metadata.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)


from kiara.context import Kiara
from kiara.models.values.value_metadata import ValueMetadata
from kiara.operations.included_core_operations.metadata import (
    ExtractMetadataOperationType,
)


def test_extract_metadata_all_available_data(presseeded_data_store_minimal: Kiara):

    op_type: ExtractMetadataOperationType = (  # type: ignore
        presseeded_data_store_minimal.operation_registry.operation_types[
            "extract_metadata"
        ]
    )

    for (
        value_id
    ) in presseeded_data_store_minimal.data_registry.retrieve_all_available_value_ids():

        value = presseeded_data_store_minimal.data_registry.get_value(value_id)
        ops = op_type.get_operations_for_data_type(value.value_schema.type)

        for op in ops.values():
            inputs = {"value": value}
            result = op.run(kiara=presseeded_data_store_minimal, inputs=inputs)

            md = result.get_value_data("value_metadata")
            assert isinstance(md, ValueMetadata)
            # TODO: validate schema
            # schema = json.loads(result["metadata_item_schema"].get_value_data())
            # item = result["metadata_item"].get_value_data()
            #
            # validate(instance=item, schema=schema)


# kiara\kiara\tests\test_operation_types\test_persist_value.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import os

# from kiara.context import Kiara
# from kiara.interfaces.python_api.operation import KiaraOperation
# from kiara.models.values.value import Value

ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
DATA_FOLDER = os.path.join(ROOT_DIR, "examples", "data")


# def test_save_file_data(kiara: Kiara):
#
#     op = KiaraOperation(kiara=kiara, operation_name="import.file")
#
#     nodes_file = os.path.join(DATA_FOLDER, "journals", "JournalNodes1902.csv")
#
#     results = op.run(path=nodes_file)
#
#     file_value: Value = results.get_value_obj("file")
#
#     persisted_value = kiara.data_registry.store_value(value=file_value)
#
#     assert persisted_value is not None
#
#     loaded = kiara.data_registry.get_value(file_value.value_id)
#     assert loaded.value_id == file_value.value_id
#     assert loaded.data == file_value.data
#
#
# def test_save_file_bundle(kiara: Kiara):
#
#     op = KiaraOperation(kiara=kiara, operation_name="import.file_bundle")
#
#     journals_folder = os.path.join(DATA_FOLDER, "journals")
#
#     results = op.run(path=journals_folder)
#
#     bundle_value: Value = results.get_value_obj("file_bundle")
#
#     persisted_value = kiara.data_registry.store_value(value=bundle_value)
#
#     assert persisted_value is not None
#
#     loaded = kiara.data_registry.get_value(bundle_value.value_id)
#     assert loaded.value_id == bundle_value.value_id
#     assert loaded.data == bundle_value.data


# kiara\kiara\tests\test_operation_types\test_render_value.py
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)
from click import Group

from kiara.context import Kiara
from kiara.utils.cli import terminal_print


def test_render_string(kiara: Kiara):
    value = kiara.data_registry.register_data("xxx", schema="string")
    pp_result = kiara.data_registry.pretty_print_data(value_id=value.value_id)

    assert pp_result == "xxx"


def test_render_integer(kiara: Kiara):
    value = kiara.data_registry.register_data(100, schema="integer")
    pp_result = kiara.data_registry.pretty_print_data(value_id=value.value_id)
    assert pp_result == "100"


def test_render_float(kiara: Kiara):
    value = kiara.data_registry.register_data(100.01, schema="float")
    pp_result = kiara.data_registry.pretty_print_data(value_id=value.value_id)
    assert pp_result == "100.01"


def test_render_available_data(preseeded_data_store: Kiara):

    for (
        value_id
    ) in preseeded_data_store.data_registry.retrieve_all_available_value_ids():
        value = preseeded_data_store.data_registry.get_value(value_id)
        pp_result = preseeded_data_store.data_registry.pretty_print_data(
            value_id=value.value_id
        )
        terminal_print(Group(pp_result))


# kiara\kiara\tests\test_operation_types\__init__.py
# -*- coding: utf-8 -*-
#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)


# kiara\kiara\tests\test_pipelines\test_pipelines.py
# -*- coding: utf-8 -*-
#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)


# def test_pipeline_default_controller_invalid_inputs(kiara: Kiara):
#
#     pipeline = kiara.create_pipeline("logic.nand")
#
#     with pytest.raises(Exception) as e:
#         pipeline.inputs.set_values(logic_nand__a1=True, logic_nand__b=True)
#
#     assert "logic_nand__a1" in str(e.value)
#     assert "logic_nand__b" in str(e.value)
#
#
# def test_pipeline_default_controller_invalid_outputs(kiara: Kiara):
#
#     pipeline = kiara.create_pipeline("logic.nand")
#
#     pipeline.inputs.set_values(logic_nand__a=True, logic_nand__b=True)
#
#     with pytest.raises(KeyError) as e:
#         pipeline.outputs.get_value_data("logic_nand__y1")
#
#     assert "logic_nand__y1" in str(e.value)
#     assert "logic_nand__y" in str(e.value)
#
#
# # TODO: more complex pipelines
#
#
# def test_pipeline_default_controller_synchronous_processing(kiara: Kiara):
#
#     processor = ModuleProcessor.from_config(
#         config={"module_processor_type": "synchronous"}
#     )
#     controller = BatchController(processor=processor)
#     pipeline = kiara.create_pipeline("logic.nand", controller=controller)
#
#     pipeline.inputs.set_values(logic_nand__a=True, logic_nand__b=True)
#
#     result = pipeline.outputs.get_all_value_data()
#     assert result == {"logic_nand__y": False}
#
#
# def test_pipeline_default_controller_threaded_processing(kiara: Kiara):
#
#     processor = ModuleProcessor.from_config(
#         config={"module_processor_type": "multi-threaded"}
#     )
#     controller = BatchController(processor=processor)
#     pipeline = kiara.create_pipeline("logic.nand", controller=controller)
#
#     pipeline.inputs.set_values(logic_nand__a=True, logic_nand__b=True)
#
#     result = pipeline.outputs.get_all_value_data()
#     assert result == {"logic_nand__y": False}


# kiara\kiara\tests\test_pipelines\test_pipeline_configs.py
# -*- coding: utf-8 -*-
from kiara.interfaces.python_api.base_api import BaseAPI


def test_pipeline_default_config_simple(api: BaseAPI):

    pipeline_config = """
pipeline_name: test_pipeline
steps:
  - step_id: step_1
    module_type: logic.and
  - step_id: step_2
    module_type: logic.and
     """

    op = api.get_operation(pipeline_config)
    assert op is not None
    inputs_schema = op.inputs_schema
    outputs_schema = op.outputs_schema
    assert len(inputs_schema) == 4
    assert len(outputs_schema) == 2

    assert inputs_schema["step_1__a"].type == "boolean"
    assert outputs_schema["step_1__y"].type == "boolean"


def test_pipeline_config_aliases(api: BaseAPI):

    pipeline_config = """
pipeline_name: test_pipeline
steps:
  - step_id: step_1
    module_type: logic.and
  - step_id: step_2
    module_type: logic.and
input_aliases:
  step_1.a: a
  step_1.b: b
  step_2.a: c
  step_2.b: d
     """

    op = api.get_operation(pipeline_config)
    assert op is not None
    inputs_schema = op.inputs_schema
    outputs_schema = op.outputs_schema
    assert len(inputs_schema) == 4
    assert len(outputs_schema) == 2

    assert inputs_schema["a"].type == "boolean"
    assert inputs_schema["b"].type == "boolean"
    assert inputs_schema["c"].type == "boolean"
    assert inputs_schema["d"].type == "boolean"


def test_pipeline_config_aliases_2(api: BaseAPI):

    pipeline_config = """
pipeline_name: test_pipeline
steps:
  - step_id: step_1
    module_type: logic.and
  - step_id: step_2
    module_type: logic.and
input_aliases:
  step_1.a: a
  step_1.b: b
  step_2.a: a
  step_2.b: b
     """

    op = api.get_operation(pipeline_config)
    assert op is not None
    inputs_schema = op.inputs_schema
    outputs_schema = op.outputs_schema
    assert len(inputs_schema) == 2
    assert len(outputs_schema) == 2

    assert inputs_schema["a"].type == "boolean"
    assert inputs_schema["b"].type == "boolean"


def test_pipeline_module_config(api: BaseAPI):

    pipeline_config = """
pipeline_name: test_pipeline
steps:
  - step_id: step_1
    module_type: logic.and
    module_config:
      delay: 0.1
  - step_id: step_2
    module_type: logic.and
    module_config:
      delay: 0.2
input_aliases:
  step_1.a: a
  step_1.b: b
  step_2.a: a
  step_2.b: b
     """

    api.get_operation(pipeline_config)


# kiara\kiara\tests\test_pipelines\__init__.py


# kiara\kiara\tests\test_values\test_values.py
# -*- coding: utf-8 -*-

from kiara.api import Kiara, ValueSchema
from kiara.defaults import SpecialValue

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)


def test_values_create(kiara: Kiara):

    value_schema = ValueSchema(type="string")
    v = kiara.data_registry.register_data(data=None, schema=value_schema)
    assert not v.is_set


def test_registry_values(kiara: Kiara):

    value_schema_1 = ValueSchema(type="string", optional=True)

    reg = kiara.data_registry

    v = reg.register_data(data=None, schema=value_schema_1)

    v = reg.register_data(data=None, schema=value_schema_1)
    assert v.data is None

    v = reg.register_data(data=SpecialValue.NO_VALUE, schema=value_schema_1)
    assert v.data is None

    v = reg.register_data(data=SpecialValue.NOT_SET, schema=value_schema_1)
    assert v.data is None


# kiara\kiara\tests\test_values\__init__.py
# -*- coding: utf-8 -*-
#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)


# kiara\kiara_plugin.network_analysis\.cruft.json
{
  "template": "https://github.com/DHARPA-Project/kiara_plugin.develop.git",
  "commit": "76feaa6d14f299c8a9068f8acb9329370e02aa60",
  "context": {
    "cookiecutter": {
      "full_name": "Markus Binsteiner",
      "email": "markus@frkl.io",
      "project_name": "network_analysis",
      "project_slug": "network_analysis",
      "project_short_description": "kiara modules and datatypes for network analysis.",
      "github_user": "DHARPA-Project",
      "anaconda_user": "dharpa",
      "_template": "https://github.com/DHARPA-Project/kiara_plugin.develop.git"
    }
  },
  "directory": null,
  "checkout": null
}


# kiara\kiara_plugin.network_analysis\.git_archival.txt
ref-names: $Format:%D$


# kiara\kiara_plugin.network_analysis\.pre-commit-config.yaml
default_language_version:
    python: python3

repos:

- repo: https://github.com/alessandrojcm/commitlint-pre-commit-hook
  rev: 'v9.3.0'
  hooks:
    - id: commitlint
      stages: [commit-msg]
      additional_dependencies: ['@commitlint/config-conventional']

- repo: https://github.com/psf/black
  rev: 22.12.0
  hooks:
    - id: black

- repo: https://github.com/pre-commit/mirrors-mypy
  rev: 'v1.6.1'  # Use the sha / tag you want to point at
  hooks:
  - id: mypy
    files: "^src/"
    pass_filenames: true
    args: ["--config-file", "pyproject.toml", "--ignore-missing-imports"]
    additional_dependencies: [pydantic>=2.0.0, rich>=10.0.0, ruamel.yaml, anyio>=3.0.0, pyzmq>=22.0.3, bidict, sqlalchemy-stubs, types-python-slugify, types-setuptools, types-python-dateutil, dag_cbor, multiformats, textual, regex, types-pytz, types-orjson]

- repo: https://github.com/charliermarsh/ruff-pre-commit
  # Ruff version.
  rev: 'v0.1.4'
  hooks:
    - id: ruff

- repo: https://github.com/Kludex/no-optional
  rev: 0.4.0
  hooks:
    - id: no_optional

- repo: https://github.com/pre-commit/pre-commit-hooks
  rev: 'v4.3.0'
  hooks:
  - id: trailing-whitespace
    exclude: 'setup.cfg'
  - id: check-added-large-files
  - id: check-ast
  - id: check-json
  - id: check-merge-conflict
  - id: check-xml
  - id: check-yaml
    exclude: 'tests/\*'
  - id: debug-statements
  - id: end-of-file-fixer
    exclude: '.*.json'
  - id: requirements-txt-fixer
  - id: fix-encoding-pragma
  - id: mixed-line-ending
    args: ['--fix=no']
  #- id: no-commit-to-branch
  #  args: [--branch, master]


# kiara\kiara_plugin.network_analysis\AUTHORS.md
# Contributors

* Markus Binsteiner <markus@frkl.io>


# kiara\kiara_plugin.network_analysis\CHANGELOG.md
=========
Changelog
=========


## Version 0.5.2 (Upcoming)

- rename 'attribute_map_strategies' input of 'network_data.redefine_edges' operation to 'columns'
- use default transformation 'COUNT' instead of 'SUM/LIST'

## Version 0.5.1

- rename 'network_data.extract_components' to `network_data.calculate_components`


# kiara\kiara_plugin.network_analysis\commitlint.config.js
module.exports = {extends: ['@commitlint/config-conventional']}


# kiara\kiara_plugin.network_analysis\mkdocs.yml
site_name: kiara_plugin.network_analysis
repo_url: https://github.com/DHARPA-Project/kiara_plugin.network_analysis
site_author: Markus Binsteiner
docs_dir: docs
site_dir: build/site

theme:
  name: material
  features:
    - navigation.instant
    - navigation.tracking

extra_css:
  - stylesheets/extra.css

markdown_extensions:
- attr_list
- admonition
- codehilite:
    guess_lang: false
- toc:
    permalink: true
- pymdownx.snippets:
    base_path: docs
- pymdownx.highlight
- pymdownx.superfences

extra:
  version:
    provider: mike

plugins:
- search
- autorefs
- mkdocstrings:
    default_handler: python
    handlers:
      python:
        path: [src]
        options:
          heading_level: 2
          show_category_heading: true
          members_order: source
          show_submodules: false
          docstring_style: google
          show_if_no_docstring: true
          show_signature_annotations: true
          separate_signature: false
          filters:
            - "!^_"  # exlude all members starting with _
            - "^_config_cls"
        import:
        - https://dharpa.org/kiara/latest/objects.inv
        - https://dharpa.org/kiara_plugin.core_types/latest/objects.inv
        - https://dharpa.org/kiara_plugin.tabular/latest/objects.inv
    watch:
      - "src"
    enable_inventory: true

- macros:
   modules:
     - kiara.doc.mkdocs_macros_cli
     - kiara.doc.mkdocs_macros_kiara

- gen-files:
    scripts:
      - scripts/documentation/gen_info_pages.py
      - scripts/documentation/gen_api_doc_pages.py

- literate-nav:
    nav_file: SUMMARY.md
- section-index


# kiara\kiara_plugin.network_analysis\README.md
[![PyPI status](https://img.shields.io/pypi/status/kiara_plugin.network_analysis.svg)](https://pypi.python.org/pypi/kiara_plugin.network_analysis/)
[![PyPI version](https://img.shields.io/pypi/v/kiara_plugin.network_analysis.svg)](https://pypi.python.org/pypi/kiara_plugin.network_analysis/)
[![PyPI pyversions](https://img.shields.io/pypi/pyversions/kiara_plugin.network_analysis.svg)](https://pypi.python.org/pypi/kiara_plugin.network_analysis/)
[![Build Status](https://img.shields.io/endpoint.svg?url=https%3A%2F%2Factions-badge.atrox.dev%2FDHARPA-Project%2Fkiara%2Fbadge%3Fref%3Ddevelop&style=flat)](https://actions-badge.atrox.dev/DHARPA-Project/kiara_plugin.network_analysis/goto?ref=develop)
[![Coverage Status](https://coveralls.io/repos/github/DHARPA-Project/kiara_plugin.network_analysis/badge.svg?branch=develop)](https://coveralls.io/github/DHARPA-Project/kiara_plugin.network_analysis?branch=develop)
[![Code style](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/ambv/black)

# [**kiara**](https://dharpa.org/kiara.documentation) plugin: (network_analysis)

kiara data types and modules for network analysis

 - Documentation: [https://DHARPA-Project.github.io/kiara_plugin.network_analysis](https://DHARPA-Project.github.io/kiara_plugin.network_analysis)
 - Code: [https://github.com/DHARPA-Project/kiara_plugin.network_analysis](https://github.com/DHARPA-Project/kiara_plugin.network_analysis)
 - `kiara`: [https://dharpa.org/kiara.documentation](https://dharpa.org/kiara.documentation)

## Description

TODO

## Development

### Requirements

- Python (version >= 3.8)
- pip, virtualenv
- git
- make (on Linux / Mac OS X -- optional)


### Prepare development environment

If you only want to work on the modules, and not the core *Kiara* codebase, follow the instructions below. Otherwise, please
check the notes on how to setup a *Kiara* development environment under (TODO).

#### Linux & Mac OS X (using make)

For *NIX-like operating system, setting up a development environment is relatively easy:

```console
git clone https://github.com/DHARPA-Project/kiara_plugin.network_analysis.git
cd kiara_plugin.network_analysis
python3 -m venv .venv
source .venv/bin/activate
make init
```

#### Windows (or manual pip install)

It's impossible to lay out all the ways Python can be installed on a machine, and virtual- (or conda-)envs can be created, so I'll assume you know how to do this.
One simple way is to install the [Anaconda (individual edition)](https://docs.anaconda.com/anaconda/install/index.html), then use the Anaconda navigator to create a new environment, install the 'git' package in it (if your system does not already have it), and use the 'Open Terminal' option of that environment to start up a terminal that has that virtual-/conda-environment activated.

Once that is done, `cd` into a directory where you want this project folder to live, and do:

```console
# make sure your virtual env is activated!!!
git clone https://github.com/DHARPA-Project/kiara_plugin.network_analysis.git
cd kiara_plugin.network_analysis
pip install --extra-index-url https://pypi.fury.io/dharpa/ -U -e .[all_dev]
```

#### Try it out

After this is done, you should be able to run the included example module via:

```console
kiara run network_analysis_example text_1="xxx" text_2="yyy"
...
...
```

### Re-activate the development environment

The 'prepare' step from above only has to be done once. After that, to re-enable your virtual environment,
you'll need to navigate to the directory again (wherever that is, in your case), and run the ``source`` command from before again:

```console
cd path/to/kiara_plugin.network_analysis
source .venv/bin/activate  # if it isn't activated already, for example by the Anaconda navigator
kiara --help  # or whatever, point is, kiara should be available for you now,
```

### ``make`` targets (Linux & Mac OS X)

- ``init``: init development project (install project & dev dependencies into virtualenv, as well as pre-commit git hook)
- ``update-dependencies``: update development dependencies (mainly the core ``kiara`` package from git)
- ``flake``: run *flake8* tests
- ``mypy``: run mypy tests
- ``test``: run unit tests
- ``docs``: create static documentation pages (under ``build/site``)
- ``serve-docs``: serve documentation pages (incl. auto-reload) for getting direct feedback when working on documentation
- ``clean``: clean build directories

For details (and other, minor targets), check the ``Makefile``.


### Running tests

``` console
> make test
# or
> make coverage
```


## Copyright & license

This project is MPL v2.0 licensed, for the license text please check the [LICENSE](/LICENSE) file in this repository.


# kiara\kiara_plugin.network_analysis\.github\ISSUE_TEMPLATE\bug_report.md
---
name: Bug report
about: Create a report to help us improve
title: ''
labels: ''
assignees: ''

---

**Describe the bug**
A clear and concise description of what the bug is.

**To Reproduce**
Steps to reproduce the behavior:
1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

**Expected behavior**
A clear and concise description of what you expected to happen.

**Screenshots**
If applicable, add screenshots to help explain your problem.

**Desktop (please complete the following information):**
 - OS: [e.g. iOS]
 - Browser [e.g. chrome, safari]
 - Version [e.g. 22]

**Smartphone (please complete the following information):**
 - Device: [e.g. iPhone6]
 - OS: [e.g. iOS8.1]
 - Browser [e.g. stock browser, safari]
 - Version [e.g. 22]

**Additional context**
Add any other context about the problem here.


# kiara\kiara_plugin.network_analysis\.github\ISSUE_TEMPLATE\suggest-a-module.md
---
name: Suggest a module
about: Suggest a new module for kiara
title: ''
labels: ''
assignees: ''

---

**Module description**
A description of the module, this will be copied into the actual module later on, so make sure you explain the module purpose with kiara users in mind. Use a short one-sentence overview as the first paragraph, then use as many paragraphs as you need to explain the module purpose and what it does.

**Inputs**
List all the inputs this module would need, along with their types, a short description, and whether they are required or should have default values. It can be assumed that a graph will always be an input, so you can skip that.

**Outupts**
List all the outputs this module would produce, along with their types and a short description of what they are.

**Example code**
If you have example code how to do what you are proposing to do, copy and paste it here.


# kiara\kiara_plugin.network_analysis\.github\workflows\build-darwin.yaml
name: "darwin tests for 'kiara_plugin.network_analysis'"
# This workflow is triggered on pushes to the repository.
on: [push]
env:
  DEVELOPER_DIR: /Applications/Xcode_12.4.app/Contents/Developer
  MACOSX_DEPLOYMENT_TARGET: 10.15

jobs:
  test-darwin:
    name: pytest on darwin
    runs-on: macos-11
    strategy:
      matrix:
        python_version: ["3.8", "3.9", "3.10", "3.11"]
    steps:
      - name: "Set up Python ${{ matrix.python_version }}"
        uses: actions/setup-python@v4
        with:
          python-version: "${{ matrix.python_version }}"
      - uses: actions/checkout@v3
      - name: install kiara_plugin.network_analysis
        run: pip install -U --extra-index-url https://pypi.fury.io/dharpa/ .[all,dev_testing]
      - name: display installed kiara and module package versions
        run: pip list | grep kiara
      - name: Test with pytest
        run: make test


# kiara\kiara_plugin.network_analysis\.github\workflows\build-linux.yaml
name: "linux tests and documentation builds for 'kiara_plugin.network_analysis'"
# This workflow is triggered on pushes to the repository.
on: [push]

jobs:

#  commitlint:
#    name: lint commit message
#    runs-on: ubuntu-latest
#    steps:
#      - uses: actions/checkout@v2
#        with:
#          fetch-depth: 0
#      - uses: wagoid/commitlint-github-action@v4

  test-linux:
    name: pytest on linux
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python_version: ["3.8", "3.9", "3.10", "3.11"]
    steps:
      - name: "Set up Python ${{ matrix.python_version }}"
        uses: actions/setup-python@v4
        with:
          python-version: "${{ matrix.python_version }}"
      - uses: actions/checkout@v3
      - name: install kiara_plugin.network_analysis
        run: pip install -U .[all,dev_testing]
      - name: display installed kiara and module package versions
        run: pip list | grep kiara
      - name: Test with pytest
        run: make test

# Uncomment this if you have coveralls.io setup with this repo
#  coverage:
#    name: create and publish test coverage
#    runs-on: ubuntu-latest
#    steps:
#      - name: "Set up Python 3.9"
#        uses: actions/setup-python@v4
#        with:
#          python-version: "3.9"
#      - uses: actions/checkout@v3
#      - name: install kiara
#        run: pip install -U .[all,dev_testing]
#      - name: display installed kiara and module package versions
#        run: pip list | grep kiara
#      - name: Run coverage
#        run: coverage run -m pytest tests
#      - name: coveralls
#        uses: coverallsapp/github-action@v2

# Uncomment this if you want to run mypy
  mypy-linux:
    name: mypy check on linux
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python_version: ["3.8", "3.9", "3.10", "3.11"]
    steps:
      - name: "Set up Python ${{ matrix.python_version }}"
        uses: actions/setup-python@v4
        with:
          python-version: "${{ matrix.python_version }}"
      - uses: actions/checkout@v3
      - name: install kiara_plugin.network_analysis
        run: pip install -U .[all,dev_testing]
      - name: Test with mypy
        run: make mypy

  linting-linux:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Install Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"
      - name: pip cache
        id: pip-cache
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/setup.*') }}
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -U ruff
      # Include `--format=github` to enable automatic inline annotations.
      - name: Run Ruff
        run: ruff --output-format=github src/

  build_python_package:
    name: build python package
    runs-on: ubuntu-latest
    needs:
      - test-linux
      - mypy-linux
      - linting-linux
    steps:
      - name: Set up Python 3.11
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"
      - uses: actions/checkout@v3
        with:
          fetch-depth: 0
      - name: install pip
        run: pip install -U pip setuptools setuptools_scm build
      - name: create packages
        run: python -m build
      - name: upload artifacts
        uses: actions/upload-artifact@v3
        with:
          name: build-dists
          path: dist/

  release_python_package:
    name: publish python package to pypi
    if: github.event_name == 'push' && startsWith(github.ref, 'refs/tags')
    runs-on: ubuntu-latest
    needs:
      - build_python_package
    permissions:
      id-token: write  # IMPORTANT: this permission is mandatory for trusted publishing
    steps:
      - name: Retrieve build distributions
        uses: actions/download-artifact@v3
        with:
          name: build-dists
          path: dist/
      - name: publish to PyPI  # make sure you have pypi trusted publishing configured for this repo
        uses: pypa/gh-action-pypi-publish@release/v1

  build_and_release_conda_package:
    name: conda package build (and upload if release)
    runs-on: ubuntu-latest
    needs:
      - test-linux
      - mypy-linux   # uncomment if this step is enabled
      - linting-linux   # uncomment if this step is enabled
    steps:
      - name: "Set up Python 3.11"
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"
      - name: pip cache
        id: pip-cache
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/setup.*') }}
      - uses: actions/checkout@v3
        with:
          fetch-depth: 0
      - name: install kiara
        run: pip install kiara
      - name: install required plugin packages
        run: pip install git+https://github.com/DHARPA-Project/kiara_plugin.develop.git@develop
      - name: build conda package
        if: ${{ ( github.ref == 'refs/heads/develop') }}
        run: kiara conda build-package --patch-data ci/conda/conda-pkg-patch.yaml .
      - name: extract tag name
        run: echo "RELEASE_VERSION=${GITHUB_REF#refs/*/}" >> $GITHUB_ENV
      - name: build & publish conda package
        if: ${{ startsWith(github.ref, 'refs/tags/') }}
        run: kiara conda build-package --publish --user dharpa --token ${{ secrets.ANACONDA_PUSH_TOKEN }} --patch-data ci/conda/conda-pkg-patch.yaml .

  merge_tag_to_main:
    name: merge current tag to main branch
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && startsWith(github.ref, 'refs/tags')
    needs:
      - release_python_package
      - build_and_release_conda_package
    steps:
    - uses: actions/checkout@v3
      with:
        fetch-depth: 0
    - run: git config --global user.email "markus@frkl.io"
    - run: git config --global user.name "Markus Binsteiner"
    - name: extract tag name
      run: echo "RELEASE_VERSION=${GITHUB_REF#refs/*/}" >> $GITHUB_ENV
    - name: checkout main branch
      run: git checkout main
    - name: merge tag
      run: git merge "${RELEASE_VERSION}"
    - name: push updated main branch
      run: git push https://${{ secrets.GITHUB_TOKEN }}@github.com/DHARPA-Project/kiara_plugin.network_analysis.git


# kiara\kiara_plugin.network_analysis\.github\workflows\build-windows.yaml
name: "windows tests for 'kiara_plugin.network_analysis'"
# This workflow is triggered on pushes to the repository.
on: [push]

jobs:
  test-windows:
    name: pytest on windows
    runs-on: windows-latest
    strategy:
      matrix:
        python_version: ["3.8", "3.9", "3.10", "3.11"]
    steps:
      - name: "Set up Python ${{ matrix.python_version }}"
        uses: actions/setup-python@v4
        with:
          python-version: "${{ matrix.python_version }}"
      - uses: actions/checkout@v3
      - name: install kiara_plugin.network_analysis
        run: pip install -U --extra-index-url https://pypi.fury.io/dharpa/ .[all,dev_testing]
      - name: Test with pytest
        run: make test


# kiara\kiara_plugin.network_analysis\ci\conda\conda-pkg-patch.yaml
channels:
  - conda-forge
  - dharpa

host_requirements:
  - pip
  - python
  - setuptools<=63
  - setuptools_scm

test:
  imports:
    - kiara_plugin.network_analysis
  source_files:
    - tests
    - examples
  commands:
    - kiara module list


# kiara\kiara_plugin.network_analysis\docs\development.md
# Development


## Prepare development environment

### Using conda (recommended)

```
conda create -n network_analysis python=3.9
conda activate network_analysis
conda install -c conda-forge mamba   # this is optional, but makes everything install related much faster, if you don't use it, replace 'mamba' with 'conda' below
mamba install -c conda-forge -c dharpa kiara
mamba install -c conda-forge -c dharpa kiara_plugin.core_types kiara_plugin.tabular   # optional, adjust which plugin packages you depend on, those two are quite common
```

### Using Python venv

Later, alligator.


## Check out the source code

First, fork the [kiara_plugin.network_analysis](https://github.com/DHARPA-Project/kiara_plugin.network_analysis) repository into your personal Github account.

Then, use the resulting url (in my case: https://github.com/makkus/kiara_modules.network_analysis.git) to clone the repository locally:

```
https://github.com/<YOUR_FORKED_GITHUB_ID>/kiara_plugin.network_analysis
```

## Install the kiara plugin package into it

```
cd kiara_plugin.network_analysis
pip install -e '.[all_dev]'
```

Here we use the `-e` option for the `pip install` command. This installs the local folder as a package in development mode into the current environment. Development mode makes it so that if you change any of the files in this folder, the Python environment will pick it up automatically, and whenever you run anything in this environment the latest version of your code/files are used.

We also install a few additional requirements  (the `[all_dev]` part in the command above) that are not strictly necessary for `kiara` itself, or this package, but help with various development-related tasks.

## Install some pre-commit check tooling (optional)

This step is optional, but helps with keeping the code clean and CI from failing. By installing [pre-commit](https://pre-commit.com/) hooks like here,
whenever you do a `git commit` in this repo, a series of checks and cleanup tasks are run, until everything is in a state
that will hopefully make Github Actions not complain when you push your changes.

```
pre-commit install
pre-commit install --hook-type commit-msg
```

In addition to some Python-specific checks and cleanup tasks, this will also check your commit message so it's in line with the suggested format:
https://www.conventionalcommits.org/en/v1.0.0/

## Run kiara

To check if everything works as expected and you can start adding/changing code in this repository, run any `kiara` command:

```
kiara operation list -t network_analysis
```

If everything is set up correctly, the output of this command should contain a few operations that are implemented in this repository.


# kiara\kiara_plugin.network_analysis\docs\index.md
# [**kiara**](https://dharpa.org/kiara.documentation) plugin: network_analysis

This package contains a set of commonly used/useful modules, pipelines, types and metadata schemas for [*Kiara*](https://github.com/DHARPA-project/kiara).

## Description

kiara data types and modules for network analysis

## Package content

{% for item_type, item_group in get_context_info().get_all_info().items() %}

### {{ item_type }}
{% for item, details in item_group.item_infos.items() %}
- [`{{ item }}`][kiara_info.{{ item_type }}.{{ item }}]: {{ details.documentation.description }}
{% endfor %}
{% endfor %}

## Links

 - Documentation: [https://DHARPA-Project.github.io/kiara_plugin.network_analysis](https://DHARPA-Project.github.io/kiara_plugin.network_analysis)
 - Code: [https://github.com/DHARPA-Project/kiara_plugin.network_analysis](https://github.com/DHARPA-Project/kiara_plugin.network_analysis)


# kiara\kiara_plugin.network_analysis\docs\SUMMARY.md
* [Home](index.md)
* [Package contents](info/)
* [Usage](usage.md)
* [Development](development.md)
* [API reference](reference/)


# kiara\kiara_plugin.network_analysis\docs\usage.md
# Usage

## Introduction

## The `network_data` type

If you access the `.data` attribute of a value of the `network_data` type, you will get a Python instance of the class [`NetworkData`](https://github.com/DHARPA-Project/kiara_plugin.network_analysis/blob/develop/src/kiara_plugin/network_analysis/models/__init__.py).

In Python, this would look something like:

```
from kiara.api import KiaraAPI
from kiara_plugin.network_analysis.models import NetworkData

kiara = KiaraAPI.instance()
network_data_value = api.get_value("my_network_data_alias_or_id")

network_data: NetworkData = network_data_value.data
```

or, from within a module `process` method:

```
from kiara.api import ValueMap, Value
from kiara_plugin.network_analysis.models import NetworkData

def process(self, inputs: ValueMap, outputs: ValueMap):

    network_data_obj = inputs.get_value_obj("network_data_input_field_name")
    network_data: NetworkData = network_data_obj.data
```

This is a wrapper class that stores all the data related to the nodes and edges of the network data in two separate tables (inheriting from [`KiaraTables`](https://github.com/DHARPA-Project/kiara_plugin.tabular/blob/develop/src/kiara_plugin/tabular/models/tables.py), which in turn uses [`KiaraTable`](https://github.com/DHARPA-Project/kiara_plugin.tabular/blob/develop/src/kiara_plugin/tabular/models/table.py) to store the actual per-table data).

The only two tables that are available in a `NetworkData` instance are called `nodes` and `edges`. You can access them via the `.nodes` and `.edges` attributes of the `NetworkData` instance. As mentioned above, Both of these attributes are instances of `KiaraTable`, so you can use all the methods of that class to access the data. The most important ones are:

- `.arrow_table`: to get the data as an [Apache Arrow](https://arrow.apache.org/) table
- `.to_pandas_dataframe()`: to get the data as a [pandas](https://pandas.pydata.org/) dataframe -- please try to always use the arrow table, as it is much more efficient and avoides loading the whole data into memory in some cases

As a convention, *kiara* will add columns prefixed with an underscore if the values in it have internal 'meaning', normal/original attributes are stored in columns without that prefix.

Both node and edge tables contain a unique `id` column (`_node_id`, `_edge_id`) that is generated for eacch specific network_data instance. You can not rely on this id being consistent across network_data values (e.g. if you create a filtered `network_data` instance from another one, the same node_id will most likely not refer to the original node row).

### The 'edges' table

The `edges` table contains the data about the edges of the network. The most important columns are:

- `_source`: the source node ids of the edge
- `_target`: the target node ids of the edge

In addition, this table contains a number of pre-processed, static metadata concerning this specific `network_data` instance. You can get information about those using the cli command:

```
kiara data-type explain network_data
```

The `nodes' table contains the data about node attributes of the network. The `_node_id` column contains node ids that reference the `_source`/`_target` columns of the `edges` table.

The table also contains additional pre-processed, static metadata for this specific `network_data` instance, which can be accessed using the same cli command as above.

## `network_data`-specific metadata

Along the pre-processed edge- and node- metadata, a `network_data` value also comes with some more general, pre-processed metadata:

```
kiara data explain -p journals_network

...
...
properties:
    "metadata.network_data": {
      "number_of_nodes": 276,
      "properties_by_graph_type": {
        "directed": {
          "number_of_edges": 321,
          "parallel_edges": 0
        },
        "directed_multi": {
          "number_of_edges": 321,
          "parallel_edges": 0
        },
        "undirected": {
          "number_of_edges": 313,
          "parallel_edges": 0
        },
        "undirected_multi": {
          "number_of_edges": 321,
          "parallel_edges": 8
        }
      },
      "number_of_self_loops": 1
    }
...
...

```

In a *kiara* module you'd access this information like:

```python

def process(self, inputs: ValueMap, outputs: ValueMap):

    network_data_obj: Value = inputs.get_value_obj("network_data_input_field_name")
    network_props = network_data_obj.get_property_data('metadata.network_data')
```

This gives you information about the number of edges (and parallel edges), depending as which graph type you interpret the data itself. For example, the 'undirected' graph type would merge all the edges that have the same source/target and target/source combinations into a single edge, whereas the 'directed' graph type would keep them separate.

In addition, you can also retrieve the more generic table column metadata for the `nodes` and `edges` tables:

```python

table_props = network_data_obj.get_property_data('metadata.tables')
```

This can be useful for non-auto-pre-processed node/edge attributes that where copied over from the original data, or just to get
an idea about the general shape of the data.


## Creating a `NetworkData` instance in a *kiara* module

*kiara* tries to make assembling `network_data` as easy as possible for a module developer (this should only ever happen within the context of a module).

The default way to assemble a `network_data` value is to use the `create_network_data` class method of the [`NetworkData`](https://github.com/DHARPA-Project/kiara_plugin.network_analysis/blob/develop/src/kiara_plugin/network_analysis/models/__init__.py) class:

This method is the most flexible and powerful, which means it also requires some preparation of the data, and the data to be in a specific format. To make this easier, there exists a convenience method to create a `network_data` value from an existing `networkx` graph:

```python
def create_from_networkx_graph(
    cls,
    graph: "nx.Graph",
    label_attr_name: Union[str, None] = None,
    ignore_node_attributes: Union[Iterable[str], None] = None,
    ) -> "NetworkData":
```

In addition, there exists a helper function that lets you create a `network_data` instance from an existing one, in addition to a list of node_ids the new graph should contain (nodes/edges containing ids not in that list will be not included in the new graph)

```python
def from_filtered_nodes(
    cls, network_data: "NetworkData", nodes_list: List[int]
) -> "NetworkData":
```

TODO: more example code?

## Assembling a `network_data` value in a workflow

The central operation that is used to assemble a `network_data` value is called `assemble.network_data`:

```
❯ kiara operation explain assemble.network_data

╭─ Operation: assemble.network_data ───────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│                                                                                                                                                  │
│   Documentation   Create a 'network_data' instance from one or two tables.                                                                       │
│                                                                                                                                                  │
│                   This module needs at least one table as input, providing the edges of the resulting network data set.                          │
│                   If no further table is created, basic node information will be automatically created by using unique values from the edges     │
│                   source and target columns.                                                                                                     │
│                                                                                                                                                  │
│                   If no `source_column_name` (and/or `target_column_name`) is provided, *kiara* will try to auto-detect the most likely of the   │
│                   existing columns to use. If that is not possible, an error will be raised.                                                     │
│                                                                                                                                                  │
│   Inputs                                                                                                                                         │
│                     field name         type     description                                                      Required   Default              │
│                    ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────    │
│                     edges              table    A table that contains the edges data.                            yes        -- no default --     │
│                     source_column      string   The name of the source column name in the edges table.           no         -- no default --     │
│                     target_column      string   The name of the target column name in the edges table.           no         -- no default --     │
│                     edges_column_map   dict     An optional map of original column name to desired.              no         -- no default --     │
│                     nodes              table    A table that contains the nodes data.                            no         -- no default --     │
│                     id_column          string   The name (before any potential column mapping) of the            no         -- no default --     │
│                                                 node-table column that contains the node identifier (used in                                     │
│                                                 the edges table).                                                                                │
│                     label_column       string   The name of a column that contains the node label (before any    no         -- no default --     │
│                                                 potential column name mapping). If not specified, the value of                                   │
│                                                 the id value will be used as label.                                                              │
│                     nodes_column_map   dict     An optional map of original column name to desired.              no         -- no default --     │
│                                                                                                                                                  │
│                                                                                                                                                  │
│   Outputs                                                                                                                                        │
│                     field name     type           description                                                                                    │
│                    ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────    │
│                     network_data   network_data   The network/graph data.                                                                        │
│                                                                                                                                                  │
│                                                                                                                                                  │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯

```

This assumes the user has already imported at least a table containing edge data, which in turn is used in the `edges` input field. Providing a 'nodes' information table is optional.

The second option of creating a `network_data` value is to use the `create.network_data.from.file` operation, which takes a (raw) `file` as input. This file needs to contain network data in one of the supported formats (e.g. 'gml, 'gexf', 'graphml', ... -- use 'explain' on the operation to get the latest list of supported formats).


## Other operations for `network_data` values

The following operations are available for `network_data` values. Use the `operation explain` command to get more information about them.

### `export.network_data.*`

Those operations take an existing `network_data` instance and export it as afile (or files) to the local filesystem, optionally including *kiara* specific metadata.

### `network_data.calculate_components`

Add a `component_id` column to the nodes table indicating which (separate) component it belongs to, for single component networks thie value will be '0' for every node.

### `network_data_filter.component`

Filter a `network_data` instance by extracting a single component.


# kiara\kiara_plugin.network_analysis\docs\stylesheets\extra.css
div.doc-contents:not(.first) {
  padding-left: 25px;
  border-left: .05rem solid var(--md-default-fg-color--lightest);
  margin-bottom: 80px;
}


# kiara\kiara_plugin.network_analysis\examples\data\Readme.md
A folder to place example data that is relevant for this plugin. It can be used subsequently for unit tests, and in documentation generation.


# kiara\kiara_plugin.network_analysis\examples\data\gexf\Readme.md
# A collection of sample gexf files for testing

Most datasets come from here https://github.com/gephi/gephi/wiki/Datasets if not otherwise stated.

### lesmis

#### gexf file version: 1.3
The lesmis gephi file represents the same data as the one in gml format found in
Marc Newmans GML dataset collection: http://www-personal.umich.edu/~mejn/netdata/
It also has some additional graph data, like node positions, node colors (by modularity group), node size etc because the gexf format can also store that kind of data.

This gefx file is included as sample data with the installation of the Gephi software.
It seems to be a converted version of Marc Newmans GML file. The reference is the same as for the GML file:

"Les Miserables: coappearance weighted network of characters in the novel Les Miserables.
D. E. Knuth, The Stanford GraphBase: A Platform for Combinatorial Computing, Addison-Wesley, Reading, MA (1993)."

As the title reveals this data represents a coappearance network. This makes it an **undirected** network.
It is furthermore a **weighted** network, edge weights are named "value" in the gml file.
This is a **one-mode** network because there is only one type of nodes (book characters).
- 77 nodes
- 254 edges (undirected)

### ChineseBuddhism_SNA

#### gexf file version:1.2draft
This gexf file is located in another [repository](https://github.com/DHARPA-Project/kiara.examples/blob/main/examples/data/network_analysis/gexf),
because it is too large (20,4 MB).

It comes from this website:

https://github.com/mbingenheimer/ChineseBuddhism_SNA

The data represents an undirected multigraph with:

- 18130 nodes
- 33976 edges (undirected* modified)

### quakers

#### gexf file version: 1.2draft
This file was generated by working through all the steps in this [PH tutorial](https://programminghistorian.org/en/lessons/exploring-and-analyzing-network-data-with-python) and creating a gexf file with networkX at the end. The original CSV files that constitute the original input files of this workflow can be found in this [repository](https://github.com/DHARPA-Project/kiara_plugin.network_analysis/tree/develop/examples/data/quakers).

"Before there were Facebook friends, there was the Society of Friends, known as the Quakers. Founded in England in the mid-seventeenth century, the Quakers were Protestant Christians who dissented from the official Church of England and promoted broad religious toleration, preferring Christians’ supposed “inner light” and consciences to state-enforced orthodoxy. Quakers’ numbers grew rapidly in the mid- to late-seventeenth century and their members spread through the British Isles, Europe, and the New World colonies—especially Pennsylvania, founded by Quaker leader William Penn and the home of your four authors.

Since scholars have long linked Quakers’ growth and endurance to the effectiveness of their networks, the data used in this tutorial is a list of names and relationships among the earliest seventeenth-century Quakers. This dataset is derived from the Oxford Dictionary of National Biography and from the ongoing work of the Six Degrees of Francis Bacon project, which is reconstructing the social networks of early modern Britain (1500-1700). [...]

Each Quaker node also has a number of associated attributes including historical significance, gender, birth/death dates, and SDFB ID—a unique numerical identifier that allows the user to cross-reference nodes in this dataset with the original Six Degrees of Francis Bacon dataset."

This is a **one-mode** network. It is **undirected** and **unweighted**.

- 119 nodes
- 174 edges (undirected)


# kiara\kiara_plugin.network_analysis\examples\data\gml\Readme.md
# A collection of sample gml files for testing

Most gml files will be from Marc Newmans GML dataset collection: http://www-personal.umich.edu/~mejn/netdata/

### lesmis
"The file lesmis.gml contains the weighted network of coappearances of
characters in Victor Hugo's novel "Les Miserables".  Nodes represent
characters as indicated by the labels and edges connect any pair of
characters that appear in the same chapter of the book.  The values on the
edges are the number of such coappearances.  The data on coappearances were
taken from D. E. Knuth, The Stanford GraphBase: A Platform for
Combinatorial Computing, Addison-Wesley, Reading, MA (1993)."

As explained in the file description this data represents a coappearance network. This makes it an **undirected** network. It is furthermore a **weighted** network, edge weights are named "value" in the gml file. This is a **one-mode** network because there is only one type of nodes (book characters).
- 77 nodes
- 254 edges (undirected)

### karate
"The file karate.gml contains the network of friendships between the 34
members of a karate club at a US university, as described by Wayne Zachary
in 1977.  If you use these data in your work, please cite W. W. Zachary, An
information flow model for conflict and fission in small groups, Journal of
Anthropological Research 33, 452-473 (1977)."

See also: https://en.wikipedia.org/wiki/Zachary%27s_karate_club#cite_note-Data-3

This is a **one-mode** network where nodes represent friends. It is **undirected** and **unweighted**.
- 34 nodes
- 78 edges (undirected)

### celegansneural
*Note: The multigraph tag has been manually added to the original gml file so that it can be parsed by the networkX 3.0 read_gml() code.*

"Neural network of the nematode C. Elegans

Compiled by Duncan Watts and Steven Strogatz from original experimental
data by White et al.

The file celegansneural.gml describes a weighted, directed network
representing the neural network of C. Elegans.  The data were taken from
the web site of Prof. Duncan Watts at Columbia University,
http://cdg.columbia.edu/cdg/datasets.  The nodes in the original data were
not consecutively numbered, so they have been renumbered to be consecutive.
The original node numbers from Watts' data file are retained as the labels
of the nodes.  Edge weights are the weights given by Watts.

These data can be cited as:
J. G. White, E. Southgate, J. N. Thompson, and S. Brenner, "The structure
of the nervous system of the nematode C. Elegans", Phil. Trans. R. Soc.
London 314, 1-340 (1986).

D. J. Watts and S. H. Strogatz, "Collective dynamics of `small-world'
networks", Nature 393, 440-442 (1998)."

A **one-mode** network where nodes represent neurons. It is a **directed** **multigraph** (14 parallel edges) that is **weighted**.
- nodes 297
- edges 2359 (directed, multi)

### adjnoun
"The file adjnoun.gml contains the network of common adjective and noun
adjacencies for the novel "David Copperfield" by Charles Dickens, as
described by M. Newman.  Nodes represent the most commonly occurring
adjectives and nouns in the book.  Node values are 0 for adjectives and 1
for nouns.  Edges connect any pair of words that occur in adjacent position
in the text of the book.

Please cite M. E. J. Newman, Finding community
structure in networks using the eigenvectors of matrices, Preprint
physics/0605087 (2006)."

This can be construed as a **two-mode** network taking the two different node types into account (nouns and adjectives) or as a **one-mode** network of adjacent words in a text. As a two-mode network it is not truly bipartite, because there are also links between the same node type (noun-noun, adjective-adjective). The network is **undirected** and **unweighted**.

- nodes 112 (two types, adjectives and nouns)
- edges 425 (undirected)

### dolphins

"The file dolphins.gml contains an undirected social network of frequent
associations between 62 dolphins in a community living off Doubtful Sound,
New Zealand, as compiled by Lusseau et al. (2003).  Please cite

  D. Lusseau, K. Schneider, O. J. Boisseau, P. Haase, E. Slooten, and
  S. M. Dawson, The bottlenose dolphin community of Doubtful Sound features
  a large proportion of long-lasting associations, Behavioral Ecology and
  Sociobiology 54, 396-405 (2003).

Additional information on the network can be found in

  D. Lusseau, The emergent properties of a dolphin social network,
  Proc. R. Soc. London B (suppl.) 270, S186-S188 (2003).

  D. Lusseau, Evidence for social role in a dolphin social network,
  Preprint q-bio/0607048 (http://arxiv.org/abs/q-bio.PE/0607048)"

This data represents a **one-mode**, **undirected**, **unweighted** network.

- nodes 62
- edges 159 (undirected)

### football

 "The file football.gml contains the network of American football games
between Division IA colleges during regular season Fall 2000, as compiled
by M. Girvan and M. Newman.  The nodes have values that indicate to which
conferences they belong.  The values are as follows:

  0 = Atlantic Coast
  1 = Big East
  2 = Big Ten
  3 = Big Twelve
  4 = Conference USA
  5 = Independents
  6 = Mid-American
  7 = Mountain West
  8 = Pacific Ten
  9 = Southeastern
 10 = Sun Belt
 11 = Western Athletic

If you make use of these data, please cite M. Girvan and M. E. J. Newman,
Community structure in social and biological networks,
Proc. Natl. Acad. Sci. USA 99, 7821-7826 (2002).

Correction: Two edges were erroneously duplicated in this data set, and
have been removed (21 SEP 2014)"

This dataset represents a **one-mode**, **undirected**, **unweighted** graph.

- nodes 115
- edges 613 (undirected)

### polbooks

"Books about US politics
Compiled by Valdis Krebs

Nodes represent books about US politics sold by the online bookseller
Amazon.com.  Edges represent frequent co-purchasing of books by the same
buyers, as indicated by the "customers who bought this book also bought
these other books" feature on Amazon.

Nodes have been given values "l", "n", or "c" to indicate whether they are
"liberal", "neutral", or "conservative".  These alignments were assigned
separately by Mark Newman based on a reading of the descriptions and
reviews of the books posted on Amazon.

These data should be cited as V. Krebs, unpublished,
http://www.orgnet.com/."

This dataset represents a **one-mode** (book nodes), **undirected** (co-purchasing on Amazon), **unweighted** network.

- nodes 105
- edges 441 (undirected)

### cond_mat_2005

 This file is too large for this repository, it can be found [here](https://github.com/DHARPA-Project/kiara.examples/tree/main/examples/data/network_analysis/gml).
#### Note: This dataset will not load with networkX because it has a lot of duplicate labels.

"The file cond-mat-2005.gml contains an updated version of cond-mat.gml, the
collaboration network of scientists posting preprints on the condensed
matter archive at www.arxiv.org.  This version is based on preprints posted
to the archive between January 1, 1995 and March 31, 2005.  The network is
weighted, with weights assigned as described in M. E. J. Newman,
Phys. Rev. E 64, 016132 (2001).

These data can be cited (as an updated version of)

  M. E. J. Newman, The structure of scientific collaboration networks,
  Proc. Natl. Acad. Sci. USA 98, 404-409 (2001)."

This dataset represents a **one-mode** (scientists), **undirected**, **weighted** network.

 - nodes 40421
 - edges 175692 (undirected)


# kiara\kiara_plugin.network_analysis\examples\data\journals\Readme.md
Data created by [Lena Jaskov](https://github.com/yaslena)


# kiara\kiara_plugin.network_analysis\examples\data\JSON\peacetreaties.json
{
    "directed": false,
    "multigraph": false,
    "graph": {},
    "nodes": [
        {
            "bipartite": 1,
            "title": "Waffenstillstand von Marche",
            "dates": "1577 II 12",
            "Treaty_Id": 1,
            "node_type": "treaty",
            "start_date": 1577,
            "end_date": 1786,
            "year": 1577,
            "id": "T1"
        },
        {
            "bipartite": 0,
            "sides": "Generalstaaten",
            "Side_Id": 1,
            "node_type": "country",
            "start_date": 1577,
            "end_date": 1786,
            "id": "C1"
        },
        {
            "bipartite": 0,
            "sides": "Spanien",
            "Side_Id": 2,
            "node_type": "country",
            "start_date": 1577,
            "end_date": 1786,
            "id": "C2"
        },
        {
            "bipartite": 1,
            "title": "Friedensprojekt von Köln",
            "dates": "1579 VII 10/18",
            "Treaty_Id": 2,
            "node_type": "treaty",
            "start_date": 1579,
            "end_date": 1786,
            "year": 1579,
            "id": "T2"
        },
        {
            "bipartite": 1,
            "title": "Neutralitätsvertrag von Oldenburg",
            "dates": "1617 VII 19",
            "Treaty_Id": 3,
            "node_type": "treaty",
            "start_date": 1617,
            "end_date": 1786,
            "year": 1617,
            "id": "T3"
        },
        {
            "bipartite": 0,
            "sides": "Oldenburg",
            "Side_Id": 3,
            "node_type": "country",
            "start_date": 1617,
            "end_date": 1786,
            "id": "C3"
        },
        {
            "bipartite": 1,
            "title": "Vertrag von London",
            "dates": "1619 VI 2",
            "Treaty_Id": 4,
            "node_type": "treaty",
            "start_date": 1619,
            "end_date": 1786,
            "year": 1619,
            "id": "T4"
        },
        {
            "bipartite": 0,
            "sides": "Generalstaaten / Ostindische Kompagnie",
            "Side_Id": 5,
            "node_type": "country",
            "start_date": 1619,
            "end_date": 1786,
            "id": "C5"
        },
        {
            "bipartite": 0,
            "sides": "Großbritannien",
            "Side_Id": 6,
            "node_type": "country",
            "start_date": 1619,
            "end_date": 1786,
            "id": "C6"
        },
        {
            "bipartite": 1,
            "title": "Allianz- und Subsidienvertrag von Den Haag",
            "dates": "1619 XII 31",
            "Treaty_Id": 5,
            "node_type": "treaty",
            "start_date": 1619,
            "end_date": 1786,
            "year": 1619,
            "id": "T5"
        },
        {
            "bipartite": 0,
            "sides": "Venedig",
            "Side_Id": 4,
            "node_type": "country",
            "start_date": 1619,
            "end_date": 1786,
            "id": "C4"
        },
        {
            "bipartite": 1,
            "title": "Allianz von Den Haag",
            "dates": "1621 V 14",
            "Treaty_Id": 6,
            "node_type": "treaty",
            "start_date": 1621,
            "end_date": 1786,
            "year": 1621,
            "id": "T6"
        },
        {
            "bipartite": 0,
            "sides": "Dänemark",
            "Side_Id": 7,
            "node_type": "country",
            "start_date": 1621,
            "end_date": 1786,
            "id": "C7"
        },
        {
            "bipartite": 1,
            "title": "Rezess von Bremen",
            "dates": "1621 IX 30_X 10",
            "Treaty_Id": 7,
            "node_type": "treaty",
            "start_date": 1621,
            "end_date": 1786,
            "year": 1621,
            "id": "T7"
        },
        {
            "bipartite": 1,
            "title": "Allianz von Den Haag",
            "dates": "1622 III 10",
            "Treaty_Id": 8,
            "node_type": "treaty",
            "start_date": 1622,
            "end_date": 1786,
            "year": 1622,
            "id": "T8"
        },
        {
            "bipartite": 0,
            "sides": "Brandenburg - Preußen",
            "Side_Id": 8,
            "node_type": "country",
            "start_date": 1622,
            "end_date": 1786,
            "id": "C8"
        },
        {
            "bipartite": 1,
            "title": "Neutralitätsvertrag von Den Haag",
            "dates": "1623 VIII 5",
            "Treaty_Id": 9,
            "node_type": "treaty",
            "start_date": 1623,
            "end_date": 1786,
            "year": 1623,
            "id": "T9"
        },
        {
            "bipartite": 0,
            "sides": "Ostfriesland",
            "Side_Id": 9,
            "node_type": "country",
            "start_date": 1623,
            "end_date": 1786,
            "id": "C9"
        },
        {
            "bipartite": 1,
            "title": "Allianz- und Subsidienvertrag von Compiègne",
            "dates": "1624 VI 10",
            "Treaty_Id": 10,
            "node_type": "treaty",
            "start_date": 1624,
            "end_date": 1786,
            "year": 1624,
            "id": "T10"
        },
        {
            "bipartite": 0,
            "sides": "Frankreich",
            "Side_Id": 10,
            "node_type": "country",
            "start_date": 1624,
            "end_date": 1786,
            "id": "C10"
        },
        {
            "bipartite": 1,
            "title": "Defensivallianz von London",
            "dates": "1624 VI 15",
            "Treaty_Id": 11,
            "node_type": "treaty",
            "start_date": 1624,
            "end_date": 1786,
            "year": 1624,
            "id": "T11"
        },
        {
            "bipartite": 1,
            "title": "Offensiv- und Defensivvertrag von Southampton",
            "dates": "1625 IX 7_17",
            "Treaty_Id": 12,
            "node_type": "treaty",
            "start_date": 1625,
            "end_date": 1786,
            "year": 1625,
            "id": "T12"
        },
        {
            "bipartite": 1,
            "title": "Allianzvertrag von Den Haag",
            "dates": "1625 XII 9",
            "Treaty_Id": 13,
            "node_type": "treaty",
            "start_date": 1625,
            "end_date": 1786,
            "year": 1625,
            "id": "T13"
        },
        {
            "bipartite": 1,
            "title": "Subsidienvertrag von Paris",
            "dates": "1627 VIII 28",
            "Treaty_Id": 14,
            "node_type": "treaty",
            "start_date": 1627,
            "end_date": 1786,
            "year": 1627,
            "id": "T14"
        },
        {
            "bipartite": 1,
            "title": "Erneuerung der bestehenden Allianz",
            "dates": "1630 VI 17",
            "Treaty_Id": 15,
            "node_type": "treaty",
            "start_date": 1630,
            "end_date": 1786,
            "year": 1630,
            "id": "T15"
        },
        {
            "bipartite": 1,
            "title": "Vertrag von Den Haag",
            "dates": "1632 IV 2",
            "Treaty_Id": 16,
            "node_type": "treaty",
            "start_date": 1632,
            "end_date": 1786,
            "year": 1632,
            "id": "T16"
        },
        {
            "bipartite": 1,
            "title": "Allianz- und Freundschaftsvertrag von Den Haag",
            "dates": "1634 IV 15",
            "Treaty_Id": 17,
            "node_type": "treaty",
            "start_date": 1634,
            "end_date": 1786,
            "year": 1634,
            "id": "T17"
        },
        {
            "bipartite": 1,
            "title": "Offensiv- und Defensivallianz von Paris",
            "dates": "1635 II 8",
            "Treaty_Id": 18,
            "node_type": "treaty",
            "start_date": 1635,
            "end_date": 1786,
            "year": 1635,
            "id": "T18"
        },
        {
            "bipartite": 1,
            "title": "Vertrag von Den Haag",
            "dates": "1636 IX 4",
            "Treaty_Id": 19,
            "node_type": "treaty",
            "start_date": 1636,
            "end_date": 1786,
            "year": 1636,
            "id": "T19"
        },
        {
            "bipartite": 1,
            "title": "Subsidienvertrag von Den Haag",
            "dates": "1636 IX 6",
            "Treaty_Id": 20,
            "node_type": "treaty",
            "start_date": 1636,
            "end_date": 1786,
            "year": 1636,
            "id": "T20"
        },
        {
            "bipartite": 1,
            "title": "Allianz von Paris",
            "dates": "1637 XII 17",
            "Treaty_Id": 21,
            "node_type": "treaty",
            "start_date": 1637,
            "end_date": 1786,
            "year": 1637,
            "id": "T21"
        },
        {
            "bipartite": 1,
            "title": "Subsidienvertrag von Paris",
            "dates": "1639 III 24",
            "Treaty_Id": 22,
            "node_type": "treaty",
            "start_date": 1639,
            "end_date": 1786,
            "year": 1639,
            "id": "T22"
        },
        {
            "bipartite": 1,
            "title": "Allianz von Stockholm",
            "dates": "1640 IX 1",
            "Treaty_Id": 23,
            "node_type": "treaty",
            "start_date": 1640,
            "end_date": 1786,
            "year": 1640,
            "id": "T23"
        },
        {
            "bipartite": 0,
            "sides": "Schweden",
            "Side_Id": 11,
            "node_type": "country",
            "start_date": 1640,
            "end_date": 1786,
            "id": "C11"
        },
        {
            "bipartite": 1,
            "title": "Subsidienvertrag von Paris",
            "dates": "1641 II 14",
            "Treaty_Id": 24,
            "node_type": "treaty",
            "start_date": 1641,
            "end_date": 1786,
            "year": 1641,
            "id": "T24"
        },
        {
            "bipartite": 1,
            "title": "Waffenstillstand und Beistandspakt von Den Haag",
            "dates": "1641 VI 12",
            "Treaty_Id": 25,
            "node_type": "treaty",
            "start_date": 1641,
            "end_date": 1786,
            "year": 1641,
            "id": "T25"
        },
        {
            "bipartite": 0,
            "sides": "Portugal",
            "Side_Id": 12,
            "node_type": "country",
            "start_date": 1641,
            "end_date": 1786,
            "id": "C12"
        },
        {
            "bipartite": 1,
            "title": "Subsidienvertrag von Paris",
            "dates": "1643 III 30",
            "Treaty_Id": 26,
            "node_type": "treaty",
            "start_date": 1643,
            "end_date": 1786,
            "year": 1643,
            "id": "T26"
        },
        {
            "bipartite": 1,
            "title": "Erneuerung des Subsidienvertrags von 1643 III 30",
            "dates": "1643 VIII 30",
            "Treaty_Id": 27,
            "node_type": "treaty",
            "start_date": 1643,
            "end_date": 1786,
            "year": 1643,
            "id": "T27"
        },
        {
            "bipartite": 1,
            "title": "Subsidienvertrag von Den Haag",
            "dates": "1644 II 29",
            "Treaty_Id": 28,
            "node_type": "treaty",
            "start_date": 1644,
            "end_date": 1786,
            "year": 1644,
            "id": "T28"
        },
        {
            "bipartite": 1,
            "title": "Vertrag und Garantie",
            "dates": "1644 III 1",
            "Treaty_Id": 29,
            "node_type": "treaty",
            "start_date": 1644,
            "end_date": 1786,
            "year": 1644,
            "id": "T29"
        },
        {
            "bipartite": 1,
            "title": "Provisorischer Waffenstillstand betr. Insel Ceylon",
            "dates": "1644 XI 14",
            "Treaty_Id": 30,
            "node_type": "treaty",
            "start_date": 1644,
            "end_date": 1786,
            "year": 1644,
            "id": "T30"
        },
        {
            "bipartite": 1,
            "title": "Subsidienvertrag von Den Haag",
            "dates": "1645 III 10",
            "Treaty_Id": 31,
            "node_type": "treaty",
            "start_date": 1645,
            "end_date": 1786,
            "year": 1645,
            "id": "T31"
        },
        {
            "bipartite": 1,
            "title": "Vertrag von Den Haag",
            "dates": "1645 III 27",
            "Treaty_Id": 32,
            "node_type": "treaty",
            "start_date": 1645,
            "end_date": 1786,
            "year": 1645,
            "id": "T32"
        },
        {
            "bipartite": 1,
            "title": "Subsidienvertrag von Den Haag",
            "dates": "1645 IV 20",
            "Treaty_Id": 33,
            "node_type": "treaty",
            "start_date": 1645,
            "end_date": 1786,
            "year": 1645,
            "id": "T33"
        },
        {
            "bipartite": 1,
            "title": "Erneuerung von Den Haag",
            "dates": "1645 VIII 4",
            "Treaty_Id": 34,
            "node_type": "treaty",
            "start_date": 1645,
            "end_date": 1786,
            "year": 1645,
            "id": "T34"
        },
        {
            "bipartite": 0,
            "sides": "Hanse",
            "Side_Id": 13,
            "node_type": "country",
            "start_date": 1645,
            "end_date": 1786,
            "id": "C13"
        },
        {
            "bipartite": 1,
            "title": "Erweiterte Erneuerung der Allianz von Stockholm",
            "dates": "1645 VIII 15",
            "Treaty_Id": 35,
            "node_type": "treaty",
            "start_date": 1645,
            "end_date": 1786,
            "year": 1645,
            "id": "T35"
        },
        {
            "bipartite": 1,
            "title": "Subsidienvertrag von Paris",
            "dates": "1646 IV 6",
            "Treaty_Id": 36,
            "node_type": "treaty",
            "start_date": 1646,
            "end_date": 1786,
            "year": 1646,
            "id": "T36"
        },
        {
            "bipartite": 1,
            "title": "Vertrag zur Prisenrückgabe von Paris ",
            "dates": "1646 IV 18",
            "Treaty_Id": 37,
            "node_type": "treaty",
            "start_date": 1646,
            "end_date": 1786,
            "year": 1646,
            "id": "T37"
        },
        {
            "bipartite": 1,
            "title": "Subsidienvertrag von Den Haag",
            "dates": "1646 V 13",
            "Treaty_Id": 38,
            "node_type": "treaty",
            "start_date": 1646,
            "end_date": 1786,
            "year": 1646,
            "id": "T38"
        },
        {
            "bipartite": 1,
            "title": "Allianz von Den Haag",
            "dates": "1646 X 25",
            "Treaty_Id": 39,
            "node_type": "treaty",
            "start_date": 1646,
            "end_date": 1786,
            "year": 1646,
            "id": "T39"
        },
        {
            "bipartite": 0,
            "sides": "Lübeck",
            "Side_Id": 14,
            "node_type": "country",
            "start_date": 1646,
            "end_date": 1786,
            "id": "C14"
        },
        {
            "bipartite": 1,
            "title": "Provisional-Artikel von Münster",
            "dates": "1647 I 8",
            "Treaty_Id": 40,
            "node_type": "treaty",
            "start_date": 1647,
            "end_date": 1786,
            "year": 1647,
            "id": "T40"
        },
        {
            "bipartite": 1,
            "title": "Garantie von Den Haag der Besitzungen beider Länder nach einem Frieden mit Spanien",
            "dates": "1647 VII 29",
            "Treaty_Id": 41,
            "node_type": "treaty",
            "start_date": 1647,
            "end_date": 1786,
            "year": 1647,
            "id": "T41"
        },
        {
            "bipartite": 1,
            "title": "Friedensvertrag von Münster",
            "dates": "1647 XII 27",
            "Treaty_Id": 42,
            "node_type": "treaty",
            "start_date": 1647,
            "end_date": 1786,
            "year": 1647,
            "id": "T42"
        },
        {
            "bipartite": 1,
            "title": "Friedensvertrag von Münster",
            "dates": "1648 I 30",
            "Treaty_Id": 43,
            "node_type": "treaty",
            "start_date": 1648,
            "end_date": 1786,
            "year": 1648,
            "id": "T43"
        },
        {
            "bipartite": 1,
            "title": "Defensivallianz von Den Haag",
            "dates": "1649 X 9",
            "Treaty_Id": 44,
            "node_type": "treaty",
            "start_date": 1649,
            "end_date": 1786,
            "year": 1649,
            "id": "T44"
        },
        {
            "bipartite": 1,
            "title": "Redemptionsvertrag von Den Haag",
            "dates": "1649 IX 29_X 9",
            "Treaty_Id": 45,
            "node_type": "treaty",
            "start_date": 1649,
            "end_date": 1786,
            "year": 1649,
            "id": "T45"
        },
        {
            "bipartite": 1,
            "title": "Allianz- und Handelsvertrag von Kopenhagen",
            "dates": "1653 II 18",
            "Treaty_Id": 46,
            "node_type": "treaty",
            "start_date": 1653,
            "end_date": 1786,
            "year": 1653,
            "id": "T46"
        },
        {
            "bipartite": 1,
            "title": "Friedensvertrag von Westminster",
            "dates": "1654 IV 5",
            "Treaty_Id": 47,
            "node_type": "treaty",
            "start_date": 1654,
            "end_date": 1786,
            "year": 1654,
            "id": "T47"
        },
        {
            "bipartite": 1,
            "title": "Beitritt zum Frieden von 1655 XI 3",
            "dates": "1655 XI 23",
            "Treaty_Id": 48,
            "node_type": "treaty",
            "start_date": 1655,
            "end_date": 1786,
            "year": 1655,
            "id": "T48"
        },
        {
            "bipartite": 1,
            "title": "Beitritt betr. 1655 XI 3",
            "dates": "1656 IV 20",
            "Treaty_Id": 49,
            "node_type": "treaty",
            "start_date": 1656,
            "end_date": 1786,
            "year": 1656,
            "id": "T49"
        },
        {
            "bipartite": 1,
            "title": "Garantievertrag zur Unterstützung der Stadt Danzig",
            "dates": "1656 VIII 16",
            "Treaty_Id": 50,
            "node_type": "treaty",
            "start_date": 1656,
            "end_date": 1786,
            "year": 1656,
            "id": "T50"
        },
        {
            "bipartite": 1,
            "title": "Erneuerungsvertrag von Elbing",
            "dates": "1656 IX 1_11",
            "Treaty_Id": 51,
            "node_type": "treaty",
            "start_date": 1656,
            "end_date": 1786,
            "year": 1656,
            "id": "T51"
        },
        {
            "bipartite": 1,
            "title": "Erweiterungsvertrag von Kopenhagen",
            "dates": "1657 VI 17_27",
            "Treaty_Id": 52,
            "node_type": "treaty",
            "start_date": 1657,
            "end_date": 1786,
            "year": 1657,
            "id": "T52"
        },
        {
            "bipartite": 1,
            "title": "Allianz von Den Haag für einen Frieden zwischen Schweden und Dänemark",
            "dates": "1659 V 21",
            "Treaty_Id": 53,
            "node_type": "treaty",
            "start_date": 1659,
            "end_date": 1786,
            "year": 1659,
            "id": "T53"
        },
        {
            "bipartite": 1,
            "title": "Konvention von Helsingör",
            "dates": "1659 XI 29_XII 9",
            "Treaty_Id": 54,
            "node_type": "treaty",
            "start_date": 1659,
            "end_date": 1786,
            "year": 1659,
            "id": "T54"
        },
        {
            "bipartite": 1,
            "title": "Konvention von Helsingör (betr. 1656 IX 11)",
            "dates": "1659 XII 9",
            "Treaty_Id": 55,
            "node_type": "treaty",
            "start_date": 1659,
            "end_date": 1786,
            "year": 1659,
            "id": "T55"
        },
        {
            "bipartite": 1,
            "title": "Deklaration von Kopenhagen",
            "dates": "1660 VI 2_12",
            "Treaty_Id": 56,
            "node_type": "treaty",
            "start_date": 1660,
            "end_date": 1786,
            "year": 1660,
            "id": "T56"
        },
        {
            "bipartite": 1,
            "title": "Friedensvertrag von Den Haag",
            "dates": "1661 VIII 6",
            "Treaty_Id": 57,
            "node_type": "treaty",
            "start_date": 1661,
            "end_date": 1786,
            "year": 1661,
            "id": "T57"
        },
        {
            "bipartite": 1,
            "title": "Vertrag von Den Haag",
            "dates": "1661 XII 26",
            "Treaty_Id": 58,
            "node_type": "treaty",
            "start_date": 1661,
            "end_date": 1786,
            "year": 1661,
            "id": "T58"
        },
        {
            "bipartite": 1,
            "title": "Allianz und Handelsvertrag von Paris",
            "dates": "1662 IV 27",
            "Treaty_Id": 59,
            "node_type": "treaty",
            "start_date": 1662,
            "end_date": 1786,
            "year": 1662,
            "id": "T59"
        },
        {
            "bipartite": 1,
            "title": "Friedensvertrag von Whitehall",
            "dates": "1662 IX 14",
            "Treaty_Id": 60,
            "node_type": "treaty",
            "start_date": 1662,
            "end_date": 1786,
            "year": 1662,
            "id": "T60"
        },
        {
            "bipartite": 1,
            "title": "Grenzvertrag von Brüssel",
            "dates": "1664 IX 20",
            "Treaty_Id": 61,
            "node_type": "treaty",
            "start_date": 1664,
            "end_date": 1786,
            "year": 1664,
            "id": "T61"
        },
        {
            "bipartite": 1,
            "title": "Vertrag von Den Haag",
            "dates": "1665 III 16",
            "Treaty_Id": 62,
            "node_type": "treaty",
            "start_date": 1665,
            "end_date": 1786,
            "year": 1665,
            "id": "T62"
        },
        {
            "bipartite": 1,
            "title": "Vertrag von Den Haag",
            "dates": "1665 IX 19",
            "Treaty_Id": 63,
            "node_type": "treaty",
            "start_date": 1665,
            "end_date": 1786,
            "year": 1665,
            "id": "T63"
        },
        {
            "bipartite": 0,
            "sides": "Braunschweig - Lüneburg - Celle",
            "Side_Id": 15,
            "node_type": "country",
            "start_date": 1665,
            "end_date": 1786,
            "id": "C15"
        },
        {
            "bipartite": 1,
            "title": "Bündnisvertrag von Den Haag",
            "dates": "1666 II 11",
            "Treaty_Id": 64,
            "node_type": "treaty",
            "start_date": 1666,
            "end_date": 1786,
            "year": 1666,
            "id": "T64"
        },
        {
            "bipartite": 1,
            "title": "Defensivallianz von Kleve",
            "dates": "1666 II 16",
            "Treaty_Id": 65,
            "node_type": "treaty",
            "start_date": 1666,
            "end_date": 1786,
            "year": 1666,
            "id": "T65"
        },
        {
            "bipartite": 1,
            "title": "Provisorische Artikel zur Allianz von Kleve 1666 II 16",
            "dates": "1666 II 16",
            "Treaty_Id": 66,
            "node_type": "treaty",
            "start_date": 1666,
            "end_date": 1786,
            "year": 1666,
            "id": "T66"
        },
        {
            "bipartite": 1,
            "title": "Friedensvertrag von Kleve",
            "dates": "1666 IV 18",
            "Treaty_Id": 67,
            "node_type": "treaty",
            "start_date": 1666,
            "end_date": 1786,
            "year": 1666,
            "id": "T67"
        },
        {
            "bipartite": 0,
            "sides": "Münster",
            "Side_Id": 17,
            "node_type": "country",
            "start_date": 1666,
            "end_date": 1786,
            "id": "C17"
        },
        {
            "bipartite": 1,
            "title": "Erklärung von Stockholm zur schwedischen Vermittlung im Konflikt zwischen Großbritannien und den Vereinigten Provinzen",
            "dates": "1666 VII 17",
            "Treaty_Id": 68,
            "node_type": "treaty",
            "start_date": 1666,
            "end_date": 1786,
            "year": 1666,
            "id": "T68"
        },
        {
            "bipartite": 1,
            "title": "Defensivallianz von Den Haag",
            "dates": "1666 X 15_25",
            "Treaty_Id": 69,
            "node_type": "treaty",
            "start_date": 1666,
            "end_date": 1786,
            "year": 1666,
            "id": "T69"
        },
        {
            "bipartite": 0,
            "sides": "Osnabrück",
            "Side_Id": 16,
            "node_type": "country",
            "start_date": 1666,
            "end_date": 1786,
            "id": "C16"
        },
        {
            "bipartite": 1,
            "title": "Präliminarvertrag von Den Haag",
            "dates": "1667 VII 6_16",
            "Treaty_Id": 70,
            "node_type": "treaty",
            "start_date": 1667,
            "end_date": 1786,
            "year": 1667,
            "id": "T70"
        },
        {
            "bipartite": 1,
            "title": "Freundschaftsvertrag von Den Haag",
            "dates": "1667 VII 18_28",
            "Treaty_Id": 71,
            "node_type": "treaty",
            "start_date": 1667,
            "end_date": 1786,
            "year": 1667,
            "id": "T71"
        },
        {
            "bipartite": 1,
            "title": "Beitrittsvertrag zum Frieden zwischen Großbritannien und den Generalstaaten (vom 1667 VII 7)",
            "dates": "1667 VIII 25",
            "Treaty_Id": 72,
            "node_type": "treaty",
            "start_date": 1667,
            "end_date": 1786,
            "year": 1667,
            "id": "T72"
        },
        {
            "bipartite": 1,
            "title": "Allianz von Den Haag",
            "dates": "1668 I 23",
            "Treaty_Id": 73,
            "node_type": "treaty",
            "start_date": 1668,
            "end_date": 1786,
            "year": 1668,
            "id": "T73"
        },
        {
            "bipartite": 1,
            "title": "Konvention über den eventuellen Beitritt Schwedens zur Allianz von Den Haag",
            "dates": "1668 I 13_23",
            "Treaty_Id": 74,
            "node_type": "treaty",
            "start_date": 1668,
            "end_date": 1786,
            "year": 1668,
            "id": "T74"
        },
        {
            "bipartite": 1,
            "title": "Konvention über den eventuellen Beitritt Schwedens zur Allianz von Den Haag",
            "dates": "1668 I 13_23",
            "Treaty_Id": 75,
            "node_type": "treaty",
            "start_date": 1668,
            "end_date": 1786,
            "year": 1668,
            "id": "T75"
        },
        {
            "bipartite": 1,
            "title": "Allianz von Den Haag",
            "dates": "1668 II 17",
            "Treaty_Id": 76,
            "node_type": "treaty",
            "start_date": 1668,
            "end_date": 1786,
            "year": 1668,
            "id": "T76"
        },
        {
            "bipartite": 1,
            "title": "Vertrag von Den Haag",
            "dates": "1668 III 16",
            "Treaty_Id": 77,
            "node_type": "treaty",
            "start_date": 1668,
            "end_date": 1786,
            "year": 1668,
            "id": "T77"
        },
        {
            "bipartite": 1,
            "title": "Allianz für einen Frieden mit Spanien von Saint-Germain-en-Laye",
            "dates": "1668 IV 15",
            "Treaty_Id": 78,
            "node_type": "treaty",
            "start_date": 1668,
            "end_date": 1786,
            "year": 1668,
            "id": "T78"
        },
        {
            "bipartite": 1,
            "title": "Tripelallianz von Westminster",
            "dates": "1668 IV 25_V 5",
            "Treaty_Id": 79,
            "node_type": "treaty",
            "start_date": 1668,
            "end_date": 1786,
            "year": 1668,
            "id": "T79"
        },
        {
            "bipartite": 1,
            "title": "Garantievertrag von Den Haag",
            "dates": "1669 V 7",
            "Treaty_Id": 80,
            "node_type": "treaty",
            "start_date": 1669,
            "end_date": 1786,
            "year": 1669,
            "id": "T80"
        },
        {
            "bipartite": 1,
            "title": "Allianz- und Handelsvertrag von Den Haag (mit Separatartikel)",
            "dates": "1669 VII 30 und 31",
            "Treaty_Id": 81,
            "node_type": "treaty",
            "start_date": 1669,
            "end_date": 1786,
            "year": 1669,
            "id": "T81"
        },
        {
            "bipartite": 1,
            "title": "Ausführungsvertrag von Den Haag",
            "dates": "1670 I 31",
            "Treaty_Id": 82,
            "node_type": "treaty",
            "start_date": 1670,
            "end_date": 1786,
            "year": 1670,
            "id": "T82"
        },
        {
            "bipartite": 1,
            "title": "Vertrag von Den Haag",
            "dates": "1671 VIII 18",
            "Treaty_Id": 83,
            "node_type": "treaty",
            "start_date": 1671,
            "end_date": 1786,
            "year": 1671,
            "id": "T83"
        },
        {
            "bipartite": 0,
            "sides": "Köln (Kurfürstentum)",
            "Side_Id": 18,
            "node_type": "country",
            "start_date": 1671,
            "end_date": 1786,
            "id": "C18"
        },
        {
            "bipartite": 1,
            "title": "Vertrag von Cölln an der Spree",
            "dates": "1672 V 6",
            "Treaty_Id": 84,
            "node_type": "treaty",
            "start_date": 1672,
            "end_date": 1786,
            "year": 1672,
            "id": "T84"
        },
        {
            "bipartite": 1,
            "title": "Defensivallianz von Den Haag",
            "dates": "1672 VII 25",
            "Treaty_Id": 85,
            "node_type": "treaty",
            "start_date": 1672,
            "end_date": 1786,
            "year": 1672,
            "id": "T85"
        },
        {
            "bipartite": 0,
            "sides": "Kaiser",
            "Side_Id": 19,
            "node_type": "country",
            "start_date": 1672,
            "end_date": 1786,
            "id": "C19"
        },
        {
            "bipartite": 1,
            "title": "Vertrag von Den Haag",
            "dates": "1672 IX 22",
            "Treaty_Id": 86,
            "node_type": "treaty",
            "start_date": 1672,
            "end_date": 1786,
            "year": 1672,
            "id": "T86"
        },
        {
            "bipartite": 1,
            "title": "Konvention zur Neutralität von Moers",
            "dates": "1673 III 30",
            "Treaty_Id": 87,
            "node_type": "treaty",
            "start_date": 1673,
            "end_date": 1786,
            "year": 1673,
            "id": "T87"
        },
        {
            "bipartite": 1,
            "title": "Freundschafts- und Handelsvertrag von Den Haag",
            "dates": "1673 V 2",
            "Treaty_Id": 88,
            "node_type": "treaty",
            "start_date": 1673,
            "end_date": 1786,
            "year": 1673,
            "id": "T88"
        },
        {
            "bipartite": 1,
            "title": "Defensivallianz von Kopenhagen",
            "dates": "1673 V 20",
            "Treaty_Id": 89,
            "node_type": "treaty",
            "start_date": 1673,
            "end_date": 1786,
            "year": 1673,
            "id": "T89"
        },
        {
            "bipartite": 1,
            "title": "Allianz von Den Haag",
            "dates": "1673 VII 1",
            "Treaty_Id": 90,
            "node_type": "treaty",
            "start_date": 1673,
            "end_date": 1786,
            "year": 1673,
            "id": "T90"
        },
        {
            "bipartite": 0,
            "sides": "Lothringen",
            "Side_Id": 20,
            "node_type": "country",
            "start_date": 1673,
            "end_date": 1786,
            "id": "C20"
        },
        {
            "bipartite": 1,
            "title": "Allianz von Den Haag",
            "dates": "1673 VIII 30",
            "Treaty_Id": 91,
            "node_type": "treaty",
            "start_date": 1673,
            "end_date": 1786,
            "year": 1673,
            "id": "T91"
        },
        {
            "bipartite": 1,
            "title": "Freundschafts- und Defensivallianz von Den Haag",
            "dates": "1673 VIII 30",
            "Treaty_Id": 92,
            "node_type": "treaty",
            "start_date": 1673,
            "end_date": 1786,
            "year": 1673,
            "id": "T92"
        },
        {
            "bipartite": 1,
            "title": "Konvention von Laon zur Neutralität von Moers",
            "dates": "1673 X 9",
            "Treaty_Id": 93,
            "node_type": "treaty",
            "start_date": 1673,
            "end_date": 1786,
            "year": 1673,
            "id": "T93"
        },
        {
            "bipartite": 1,
            "title": "Friedensvertrag von Westminster",
            "dates": "1674 II 19",
            "Treaty_Id": 94,
            "node_type": "treaty",
            "start_date": 1674,
            "end_date": 1786,
            "year": 1674,
            "id": "T94"
        },
        {
            "bipartite": 1,
            "title": "Deklaration von Den Haag",
            "dates": "1674 III 20",
            "Treaty_Id": 95,
            "node_type": "treaty",
            "start_date": 1674,
            "end_date": 1786,
            "year": 1674,
            "id": "T95"
        },
        {
            "bipartite": 1,
            "title": "Friedensvertrag von Köln",
            "dates": "1674 IV 22",
            "Treaty_Id": 96,
            "node_type": "treaty",
            "start_date": 1674,
            "end_date": 1786,
            "year": 1674,
            "id": "T96"
        },
        {
            "bipartite": 1,
            "title": "Friedensvertrag von Köln",
            "dates": "1674 V 11",
            "Treaty_Id": 97,
            "node_type": "treaty",
            "start_date": 1674,
            "end_date": 1786,
            "year": 1674,
            "id": "T97"
        },
        {
            "bipartite": 1,
            "title": "Verbund von Celle",
            "dates": "1674 VI 10_20",
            "Treaty_Id": 98,
            "node_type": "treaty",
            "start_date": 1674,
            "end_date": 1786,
            "year": 1674,
            "id": "T98"
        },
        {
            "bipartite": 0,
            "sides": "Braunschweig - Lüneburg - Wolfenbüttel",
            "Side_Id": 21,
            "node_type": "country",
            "start_date": 1674,
            "end_date": 1786,
            "id": "C21"
        },
        {
            "bipartite": 1,
            "title": "Allianz von Cölln a.d. Spree",
            "dates": "1674 VI 21 / VII 1",
            "Treaty_Id": 99,
            "node_type": "treaty",
            "start_date": 1674,
            "end_date": 1786,
            "year": 1674,
            "id": "T99"
        },
        {
            "bipartite": 1,
            "title": "Allianz von Den Haag",
            "dates": "1674 VII 10",
            "Treaty_Id": 100,
            "node_type": "treaty",
            "start_date": 1674,
            "end_date": 1786,
            "year": 1674,
            "id": "T100"
        },
        {
            "bipartite": 1,
            "title": "Handelsvertrag von London",
            "dates": "1674 XII 1",
            "Treaty_Id": 101,
            "node_type": "treaty",
            "start_date": 1674,
            "end_date": 1786,
            "year": 1674,
            "id": "T101"
        },
        {
            "bipartite": 1,
            "title": "Allianz von Den Haag",
            "dates": "1675 I 26",
            "Treaty_Id": 102,
            "node_type": "treaty",
            "start_date": 1675,
            "end_date": 1786,
            "year": 1675,
            "id": "T102"
        },
        {
            "bipartite": 1,
            "title": "Schiedsvertrag von London",
            "dates": "1675 III 8",
            "Treaty_Id": 103,
            "node_type": "treaty",
            "start_date": 1675,
            "end_date": 1786,
            "year": 1675,
            "id": "T103"
        },
        {
            "bipartite": 1,
            "title": "Vertrag von Den Haag",
            "dates": "1675 X 12",
            "Treaty_Id": 104,
            "node_type": "treaty",
            "start_date": 1675,
            "end_date": 1786,
            "year": 1675,
            "id": "T104"
        },
        {
            "bipartite": 1,
            "title": "Handels- und Seefahrtsvertrag von Stockholm",
            "dates": "1675 XI 26",
            "Treaty_Id": 105,
            "node_type": "treaty",
            "start_date": 1675,
            "end_date": 1786,
            "year": 1675,
            "id": "T105"
        },
        {
            "bipartite": 1,
            "title": "Beitritt zur Konvention von Freyr von 1675 X 25 zur Wiederherstellung des Handels",
            "dates": "1675 XII 17",
            "Treaty_Id": 106,
            "node_type": "treaty",
            "start_date": 1675,
            "end_date": 1786,
            "year": 1675,
            "id": "T106"
        },
        {
            "bipartite": 1,
            "title": "Vertrag von Den Haag",
            "dates": "1676 II 7",
            "Treaty_Id": 107,
            "node_type": "treaty",
            "start_date": 1676,
            "end_date": 1786,
            "year": 1676,
            "id": "T107"
        },
        {
            "bipartite": 1,
            "title": "Allianz von Den Haag",
            "dates": "1676 III 26",
            "Treaty_Id": 108,
            "node_type": "treaty",
            "start_date": 1676,
            "end_date": 1786,
            "year": 1676,
            "id": "T108"
        },
        {
            "bipartite": 0,
            "sides": "Pfalz - Neuburg",
            "Side_Id": 22,
            "node_type": "country",
            "start_date": 1676,
            "end_date": 1786,
            "id": "C22"
        },
        {
            "bipartite": 1,
            "title": "Defensivallianz von Den Haag",
            "dates": "1676 IV 26",
            "Treaty_Id": 109,
            "node_type": "treaty",
            "start_date": 1676,
            "end_date": 1786,
            "year": 1676,
            "id": "T109"
        },
        {
            "bipartite": 1,
            "title": "Verteidigungsbündnis von Den Haag",
            "dates": "1676 X 9",
            "Treaty_Id": 110,
            "node_type": "treaty",
            "start_date": 1676,
            "end_date": 1786,
            "year": 1676,
            "id": "T110"
        },
        {
            "bipartite": 1,
            "title": "Allianz von Den Haag",
            "dates": "1678 I 26",
            "Treaty_Id": 111,
            "node_type": "treaty",
            "start_date": 1678,
            "end_date": 1786,
            "year": 1678,
            "id": "T111"
        },
        {
            "bipartite": 1,
            "title": "Defensivallianz von Westminster",
            "dates": "1678 III 3",
            "Treaty_Id": 112,
            "node_type": "treaty",
            "start_date": 1678,
            "end_date": 1786,
            "year": 1678,
            "id": "T112"
        },
        {
            "bipartite": 1,
            "title": "Defensivvertrag von Cölln an der Spree",
            "dates": "1678 II 26_III 8",
            "Treaty_Id": 113,
            "node_type": "treaty",
            "start_date": 1678,
            "end_date": 1786,
            "year": 1678,
            "id": "T113"
        },
        {
            "bipartite": 1,
            "title": "Allianz von Den Haag",
            "dates": "1678 VII 25",
            "Treaty_Id": 114,
            "node_type": "treaty",
            "start_date": 1678,
            "end_date": 1786,
            "year": 1678,
            "id": "T114"
        },
        {
            "bipartite": 1,
            "title": "Friedensvertrag von Nijmegen",
            "dates": "1678 VIII 10",
            "Treaty_Id": 115,
            "node_type": "treaty",
            "start_date": 1678,
            "end_date": 1786,
            "year": 1678,
            "id": "T115"
        },
        {
            "bipartite": 1,
            "title": "Handels- und Schiffahrtsvertrag von Nijmegen",
            "dates": "1678 VIII 10",
            "Treaty_Id": 116,
            "node_type": "treaty",
            "start_date": 1678,
            "end_date": 1786,
            "year": 1678,
            "id": "T116"
        },
        {
            "bipartite": 1,
            "title": "Friedensvertrag von Nijmegen",
            "dates": "1679 X 2_12",
            "Treaty_Id": 117,
            "node_type": "treaty",
            "start_date": 1679,
            "end_date": 1786,
            "year": 1679,
            "id": "T117"
        },
        {
            "bipartite": 1,
            "title": "Handels- und Schiffahrtsvertrag von Nijmegen",
            "dates": "1679 X 12",
            "Treaty_Id": 118,
            "node_type": "treaty",
            "start_date": 1679,
            "end_date": 1786,
            "year": 1679,
            "id": "T118"
        },
        {
            "bipartite": 1,
            "title": "Erneuerung der Handelsvereinbarungen von Konstantinopel (Beta-Version)",
            "dates": "1680 IX 15 / 1091 AH",
            "Treaty_Id": 119,
            "node_type": "treaty",
            "start_date": 1680,
            "end_date": 1786,
            "year": 1680,
            "id": "T119"
        },
        {
            "bipartite": 0,
            "sides": "Osmanisches Reich",
            "Side_Id": 23,
            "node_type": "country",
            "start_date": 1680,
            "end_date": 1786,
            "id": "C23"
        },
        {
            "bipartite": 1,
            "title": "Bündnis von Den Haag",
            "dates": "1681 IX 30_X 10",
            "Treaty_Id": 120,
            "node_type": "treaty",
            "start_date": 1681,
            "end_date": 1786,
            "year": 1681,
            "id": "T120"
        },
        {
            "bipartite": 1,
            "title": "Spezialkonvention von Den Haag",
            "dates": "1683 III 8",
            "Treaty_Id": 121,
            "node_type": "treaty",
            "start_date": 1683,
            "end_date": 1786,
            "year": 1683,
            "id": "T121"
        },
        {
            "bipartite": 1,
            "title": "Allianz von Den Haag",
            "dates": "1683 III 18",
            "Treaty_Id": 122,
            "node_type": "treaty",
            "start_date": 1683,
            "end_date": 1786,
            "year": 1683,
            "id": "T122"
        },
        {
            "bipartite": 1,
            "title": "Waffenstillstand von Den Haag",
            "dates": "1684 VI 29",
            "Treaty_Id": 123,
            "node_type": "treaty",
            "start_date": 1684,
            "end_date": 1786,
            "year": 1684,
            "id": "T123"
        },
        {
            "bipartite": 1,
            "title": "Bestätigung und Erneuerung früherer Verträge",
            "dates": "1685 VIII 17",
            "Treaty_Id": 124,
            "node_type": "treaty",
            "start_date": 1685,
            "end_date": 1786,
            "year": 1685,
            "id": "T124"
        },
        {
            "bipartite": 1,
            "title": "Erneuerung früherer Verträge",
            "dates": "1686 I 12",
            "Treaty_Id": 125,
            "node_type": "treaty",
            "start_date": 1686,
            "end_date": 1786,
            "year": 1686,
            "id": "T125"
        },
        {
            "bipartite": 1,
            "title": "Erneuerung früherer Allianzen",
            "dates": "1688 VI 30",
            "Treaty_Id": 126,
            "node_type": "treaty",
            "start_date": 1688,
            "end_date": 1786,
            "year": 1688,
            "id": "T126"
        },
        {
            "bipartite": 1,
            "title": "Versprechen zur Einhaltung von Friedensverträgen",
            "dates": "1688 IX 4",
            "Treaty_Id": 127,
            "node_type": "treaty",
            "start_date": 1688,
            "end_date": 1786,
            "year": 1688,
            "id": "T127"
        },
        {
            "bipartite": 1,
            "title": "Konvention von Stockholm",
            "dates": "1688 IX 12",
            "Treaty_Id": 128,
            "node_type": "treaty",
            "start_date": 1688,
            "end_date": 1786,
            "year": 1688,
            "id": "T128"
        },
        {
            "bipartite": 1,
            "title": "Allianz von Whitehall",
            "dates": "1689 IV 29",
            "Treaty_Id": 129,
            "node_type": "treaty",
            "start_date": 1689,
            "end_date": 1786,
            "year": 1689,
            "id": "T129"
        },
        {
            "bipartite": 1,
            "title": "Offensiv- und Defensivallianz von Wien",
            "dates": "1689 V 12",
            "Treaty_Id": 130,
            "node_type": "treaty",
            "start_date": 1689,
            "end_date": 1786,
            "year": 1689,
            "id": "T130"
        },
        {
            "bipartite": 1,
            "title": "Vertrag von Whitehall",
            "dates": "1689 VIII 22",
            "Treaty_Id": 131,
            "node_type": "treaty",
            "start_date": 1689,
            "end_date": 1786,
            "year": 1689,
            "id": "T131"
        },
        {
            "bipartite": 1,
            "title": "Offensiv- und Defensivallianz von Westminster",
            "dates": "1689 VIII 24",
            "Treaty_Id": 132,
            "node_type": "treaty",
            "start_date": 1689,
            "end_date": 1786,
            "year": 1689,
            "id": "T132"
        },
        {
            "bipartite": 1,
            "title": "Defensivallianz von Kassel",
            "dates": "1690 I 27",
            "Treaty_Id": 133,
            "node_type": "treaty",
            "start_date": 1690,
            "end_date": 1786,
            "year": 1690,
            "id": "T133"
        },
        {
            "bipartite": 0,
            "sides": "Hessen - Kassel",
            "Side_Id": 24,
            "node_type": "country",
            "start_date": 1690,
            "end_date": 1786,
            "id": "C24"
        },
        {
            "bipartite": 1,
            "title": "Defensivallianz von Den Haag",
            "dates": "1691 V 14",
            "Treaty_Id": 134,
            "node_type": "treaty",
            "start_date": 1691,
            "end_date": 1786,
            "year": 1691,
            "id": "T134"
        },
        {
            "bipartite": 1,
            "title": "Konvention über aufgebrachte Schiffe",
            "dates": "1691 XI 19",
            "Treaty_Id": 135,
            "node_type": "treaty",
            "start_date": 1691,
            "end_date": 1786,
            "year": 1691,
            "id": "T135"
        },
        {
            "bipartite": 1,
            "title": "Konvention von Den Haag",
            "dates": "1692 V 23",
            "Treaty_Id": 136,
            "node_type": "treaty",
            "start_date": 1692,
            "end_date": 1786,
            "year": 1692,
            "id": "T136"
        },
        {
            "bipartite": 1,
            "title": "Vertrag von Mellé",
            "dates": "1692 VI 30",
            "Treaty_Id": 137,
            "node_type": "treaty",
            "start_date": 1692,
            "end_date": 1786,
            "year": 1692,
            "id": "T137"
        },
        {
            "bipartite": 0,
            "sides": "Braunschweig - Lüneburg - Calenberg",
            "Side_Id": 25,
            "node_type": "country",
            "start_date": 1692,
            "end_date": 1786,
            "id": "C25"
        },
        {
            "bipartite": 1,
            "title": "Vertrag von Den Haag",
            "dates": "1692 XII 22",
            "Treaty_Id": 138,
            "node_type": "treaty",
            "start_date": 1692,
            "end_date": 1786,
            "year": 1692,
            "id": "T138"
        },
        {
            "bipartite": 0,
            "sides": "Hannover",
            "Side_Id": 26,
            "node_type": "country",
            "start_date": 1692,
            "end_date": 1786,
            "id": "C26"
        },
        {
            "bipartite": 1,
            "title": "Konvention über aufgebrachte Schiffe",
            "dates": "1693 XI 25",
            "Treaty_Id": 139,
            "node_type": "treaty",
            "start_date": 1693,
            "end_date": 1786,
            "year": 1693,
            "id": "T139"
        },
        {
            "bipartite": 1,
            "title": "Bestätigung betr. 1691 V 14",
            "dates": "1694 V 21",
            "Treaty_Id": 140,
            "node_type": "treaty",
            "start_date": 1694,
            "end_date": 1786,
            "year": 1694,
            "id": "T140"
        },
        {
            "bipartite": 1,
            "title": "Vertrag von Den Haag",
            "dates": "1695 III 18",
            "Treaty_Id": 141,
            "node_type": "treaty",
            "start_date": 1695,
            "end_date": 1786,
            "year": 1695,
            "id": "T141"
        },
        {
            "bipartite": 1,
            "title": "Erneuerung betr. Defensivallianz 1689 V 12",
            "dates": "1695 VIII 8",
            "Treaty_Id": 142,
            "node_type": "treaty",
            "start_date": 1695,
            "end_date": 1786,
            "year": 1695,
            "id": "T142"
        },
        {
            "bipartite": 0,
            "sides": "Bayern",
            "Side_Id": 29,
            "node_type": "country",
            "start_date": 1695,
            "end_date": 1786,
            "id": "C29"
        },
        {
            "bipartite": 0,
            "sides": "Pfalz (Kurfürstentum)",
            "Side_Id": 28,
            "node_type": "country",
            "start_date": 1695,
            "end_date": 1786,
            "id": "C28"
        },
        {
            "bipartite": 0,
            "sides": "Savoyen - Piemont",
            "Side_Id": 27,
            "node_type": "country",
            "start_date": 1695,
            "end_date": 1786,
            "id": "C27"
        },
        {
            "bipartite": 1,
            "title": "Vertrag von Den Haag",
            "dates": "1696 V 14",
            "Treaty_Id": 143,
            "node_type": "treaty",
            "start_date": 1696,
            "end_date": 1786,
            "year": 1696,
            "id": "T143"
        },
        {
            "bipartite": 0,
            "sides": "Schleswig - Holstein - Gottorf",
            "Side_Id": 30,
            "node_type": "country",
            "start_date": 1696,
            "end_date": 1786,
            "id": "C30"
        },
        {
            "bipartite": 1,
            "title": "Defensivallianz von Den Haag",
            "dates": "1697 I 18",
            "Treaty_Id": 144,
            "node_type": "treaty",
            "start_date": 1697,
            "end_date": 1786,
            "year": 1697,
            "id": "T144"
        },
        {
            "bipartite": 1,
            "title": " Handels- und Schiffahrtsvertrag von Rijswijk",
            "dates": "1697 IX 20",
            "Treaty_Id": 145,
            "node_type": "treaty",
            "start_date": 1697,
            "end_date": 1786,
            "year": 1697,
            "id": "T145"
        },
        {
            "bipartite": 1,
            "title": "Friedensvertrag von Rijswijk",
            "dates": "1697 IX 20",
            "Treaty_Id": 146,
            "node_type": "treaty",
            "start_date": 1697,
            "end_date": 1786,
            "year": 1697,
            "id": "T146"
        },
        {
            "bipartite": 1,
            "title": "Erneuerung früherer Verträge",
            "dates": "1698 II 12",
            "Treaty_Id": 147,
            "node_type": "treaty",
            "start_date": 1698,
            "end_date": 1786,
            "year": 1698,
            "id": "T147"
        },
        {
            "bipartite": 1,
            "title": "Defensivallianz von Den Haag",
            "dates": "1698 V 4_14",
            "Treaty_Id": 148,
            "node_type": "treaty",
            "start_date": 1698,
            "end_date": 1786,
            "year": 1698,
            "id": "T148"
        },
        {
            "bipartite": 1,
            "title": "Teilungsvertrag über das spanische Erbe von Den Haag",
            "dates": "1698 X 11",
            "Treaty_Id": 149,
            "node_type": "treaty",
            "start_date": 1698,
            "end_date": 1786,
            "year": 1698,
            "id": "T149"
        },
        {
            "bipartite": 1,
            "title": "Vertrag von Paris zur Umsetzung des Handelsvertrags von 1697 IX 20",
            "dates": "1699 V 29",
            "Treaty_Id": 150,
            "node_type": "treaty",
            "start_date": 1699,
            "end_date": 1786,
            "year": 1699,
            "id": "T150"
        },
        {
            "bipartite": 1,
            "title": "Konvention von Nuys zur Neutralität von Moers",
            "dates": "1699 XI 14",
            "Treaty_Id": 151,
            "node_type": "treaty",
            "start_date": 1699,
            "end_date": 1786,
            "year": 1699,
            "id": "T151"
        },
        {
            "bipartite": 1,
            "title": "Defensivallianz von Den Haag und London",
            "dates": "1700 I 13_23 und 1700 I 20_30",
            "Treaty_Id": 152,
            "node_type": "treaty",
            "start_date": 1700,
            "end_date": 1786,
            "year": 1700,
            "id": "T152"
        },
        {
            "bipartite": 1,
            "title": "Neuer Teilungsvertrag über das spanische Erbe von London und Den Haag",
            "dates": "1700 III 3 und 1700 III 25",
            "Treaty_Id": 153,
            "node_type": "treaty",
            "start_date": 1700,
            "end_date": 1786,
            "year": 1700,
            "id": "T153"
        },
        {
            "bipartite": 1,
            "title": "Bündniserneuerung",
            "dates": "1700 VIII 31",
            "Treaty_Id": 154,
            "node_type": "treaty",
            "start_date": 1700,
            "end_date": 1786,
            "year": 1700,
            "id": "T154"
        },
        {
            "bipartite": 1,
            "title": "Defensivallianz von Den Haag",
            "dates": "1701 V 26",
            "Treaty_Id": 155,
            "node_type": "treaty",
            "start_date": 1701,
            "end_date": 1786,
            "year": 1701,
            "id": "T155"
        },
        {
            "bipartite": 1,
            "title": "Defensivallianz von Kopenhagen",
            "dates": "1701 VI 15",
            "Treaty_Id": 156,
            "node_type": "treaty",
            "start_date": 1701,
            "end_date": 1786,
            "year": 1701,
            "id": "T156"
        },
        {
            "bipartite": 1,
            "title": "Handelsvertrag von Kopenhagen",
            "dates": "1701 VI 15",
            "Treaty_Id": 157,
            "node_type": "treaty",
            "start_date": 1701,
            "end_date": 1786,
            "year": 1701,
            "id": "T157"
        },
        {
            "bipartite": 1,
            "title": "Allianzvertrag von Den Haag",
            "dates": "1701 IX 7",
            "Treaty_Id": 158,
            "node_type": "treaty",
            "start_date": 1701,
            "end_date": 1786,
            "year": 1701,
            "id": "T158"
        },
        {
            "bipartite": 1,
            "title": "Konvention von Den Haag",
            "dates": "1701 X 7",
            "Treaty_Id": 159,
            "node_type": "treaty",
            "start_date": 1701,
            "end_date": 1786,
            "year": 1701,
            "id": "T159"
        },
        {
            "bipartite": 1,
            "title": "Bündnis von Ahaus",
            "dates": "1701 X 17",
            "Treaty_Id": 160,
            "node_type": "treaty",
            "start_date": 1701,
            "end_date": 1786,
            "year": 1701,
            "id": "T160"
        },
        {
            "bipartite": 1,
            "title": "Allianz von Den Haag",
            "dates": "1701 XI 11",
            "Treaty_Id": 161,
            "node_type": "treaty",
            "start_date": 1701,
            "end_date": 1786,
            "year": 1701,
            "id": "T161"
        },
        {
            "bipartite": 1,
            "title": "Allianz von Den Haag",
            "dates": "1701 XII 30",
            "Treaty_Id": 162,
            "node_type": "treaty",
            "start_date": 1701,
            "end_date": 1786,
            "year": 1701,
            "id": "T162"
        },
        {
            "bipartite": 0,
            "sides": "Preußen",
            "Side_Id": 31,
            "node_type": "country",
            "start_date": 1701,
            "end_date": 1786,
            "id": "C31"
        },
        {
            "bipartite": 1,
            "title": "Konvention von Den Haag",
            "dates": "1702 I 5",
            "Treaty_Id": 163,
            "node_type": "treaty",
            "start_date": 1702,
            "end_date": 1786,
            "year": 1702,
            "id": "T163"
        },
        {
            "bipartite": 1,
            "title": "Defensivallianz von Den Haag",
            "dates": "1702 II 7",
            "Treaty_Id": 164,
            "node_type": "treaty",
            "start_date": 1702,
            "end_date": 1786,
            "year": 1702,
            "id": "T164"
        },
        {
            "bipartite": 1,
            "title": "Vertrag wider einen Friedenssschluss",
            "dates": "1702 IV 12",
            "Treaty_Id": 165,
            "node_type": "treaty",
            "start_date": 1702,
            "end_date": 1786,
            "year": 1702,
            "id": "T165"
        },
        {
            "bipartite": 1,
            "title": "Subsidienvertrag von Den Haag",
            "dates": "1703 III 13",
            "Treaty_Id": 166,
            "node_type": "treaty",
            "start_date": 1703,
            "end_date": 1786,
            "year": 1703,
            "id": "T166"
        },
        {
            "bipartite": 1,
            "title": "Allianzvertrag von Den Haag",
            "dates": "1703 V 5",
            "Treaty_Id": 167,
            "node_type": "treaty",
            "start_date": 1703,
            "end_date": 1786,
            "year": 1703,
            "id": "T167"
        },
        {
            "bipartite": 1,
            "title": "Offensiv- und Defensivallianz von Lissabon",
            "dates": "1703 V 16",
            "Treaty_Id": 168,
            "node_type": "treaty",
            "start_date": 1703,
            "end_date": 1786,
            "year": 1703,
            "id": "T168"
        },
        {
            "bipartite": 1,
            "title": "Erneuerung von Westminster",
            "dates": "1703 VI 9",
            "Treaty_Id": 169,
            "node_type": "treaty",
            "start_date": 1703,
            "end_date": 1786,
            "year": 1703,
            "id": "T169"
        },
        {
            "bipartite": 1,
            "title": "Defensivallianz von Den Haag",
            "dates": "1703 VIII 5_16",
            "Treaty_Id": 170,
            "node_type": "treaty",
            "start_date": 1703,
            "end_date": 1786,
            "year": 1703,
            "id": "T170"
        },
        {
            "bipartite": 1,
            "title": "Defensivallianz von Den Haag",
            "dates": "1703 VIII 5_16",
            "Treaty_Id": 171,
            "node_type": "treaty",
            "start_date": 1703,
            "end_date": 1786,
            "year": 1703,
            "id": "T171"
        },
        {
            "bipartite": 1,
            "title": "Zusatzartikel und Konvention von Den Haag",
            "dates": "1703 VIII 5_16",
            "Treaty_Id": 172,
            "node_type": "treaty",
            "start_date": 1703,
            "end_date": 1786,
            "year": 1703,
            "id": "T172"
        },
        {
            "bipartite": 1,
            "title": "Allianz von Den Haag",
            "dates": "1705 I 21",
            "Treaty_Id": 173,
            "node_type": "treaty",
            "start_date": 1705,
            "end_date": 1786,
            "year": 1705,
            "id": "T173"
        },
        {
            "bipartite": 1,
            "title": "Handelsvertrag von Lissabon",
            "dates": "1705 VIII 7",
            "Treaty_Id": 174,
            "node_type": "treaty",
            "start_date": 1705,
            "end_date": 1786,
            "year": 1705,
            "id": "T174"
        },
        {
            "bipartite": 1,
            "title": "Bündnisvertrag von Münster",
            "dates": "1708 VII 21",
            "Treaty_Id": 175,
            "node_type": "treaty",
            "start_date": 1708,
            "end_date": 1786,
            "year": 1708,
            "id": "T175"
        },
        {
            "bipartite": 1,
            "title": "Friedenspräliminarartikel von Den Haag",
            "dates": "1709 V 28",
            "Treaty_Id": 176,
            "node_type": "treaty",
            "start_date": 1709,
            "end_date": 1786,
            "year": 1709,
            "id": "T176"
        },
        {
            "bipartite": 1,
            "title": "Garantievertrag von Den Haag",
            "dates": "1709 X 29",
            "Treaty_Id": 177,
            "node_type": "treaty",
            "start_date": 1709,
            "end_date": 1786,
            "year": 1709,
            "id": "T177"
        },
        {
            "bipartite": 1,
            "title": "Vertrag von Den Haag über die Neutralität des Deutschen Reiches ",
            "dates": "1710 III 31",
            "Treaty_Id": 178,
            "node_type": "treaty",
            "start_date": 1710,
            "end_date": 1786,
            "year": 1710,
            "id": "T178"
        },
        {
            "bipartite": 1,
            "title": "Defensivallianz von Den Haag",
            "dates": "1710 VI 2",
            "Treaty_Id": 179,
            "node_type": "treaty",
            "start_date": 1710,
            "end_date": 1786,
            "year": 1710,
            "id": "T179"
        },
        {
            "bipartite": 0,
            "sides": "Paderborn",
            "Side_Id": 32,
            "node_type": "country",
            "start_date": 1710,
            "end_date": 1786,
            "id": "C32"
        },
        {
            "bipartite": 1,
            "title": "Vertrag von Den Haag über die Neutralität des Deutschen Reiches ",
            "dates": "1710 VIII 4",
            "Treaty_Id": 180,
            "node_type": "treaty",
            "start_date": 1710,
            "end_date": 1786,
            "year": 1710,
            "id": "T180"
        },
        {
            "bipartite": 1,
            "title": "Verlängerung des Allianzvertrages von Den Haag (1709 IX 7)",
            "dates": "1711 VIII [ohne Tag]",
            "Treaty_Id": 181,
            "node_type": "treaty",
            "start_date": 1711,
            "end_date": 1786,
            "year": 1711,
            "id": "T181"
        },
        {
            "bipartite": 0,
            "sides": "England",
            "Side_Id": 38,
            "node_type": "country",
            "start_date": 1711,
            "end_date": 1786,
            "id": "C38"
        },
        {
            "bipartite": 0,
            "sides": "Reichskreis",
            "Side_Id": 33,
            "node_type": "country",
            "start_date": 1711,
            "end_date": 1786,
            "id": "C33"
        },
        {
            "bipartite": 0,
            "sides": "Fränkischer",
            "Side_Id": 39,
            "node_type": "country",
            "start_date": 1711,
            "end_date": 1786,
            "id": "C39"
        },
        {
            "bipartite": 0,
            "sides": "Kurrheinischer",
            "Side_Id": 37,
            "node_type": "country",
            "start_date": 1711,
            "end_date": 1786,
            "id": "C37"
        },
        {
            "bipartite": 0,
            "sides": "Oberrheinischer",
            "Side_Id": 35,
            "node_type": "country",
            "start_date": 1711,
            "end_date": 1786,
            "id": "C35"
        },
        {
            "bipartite": 0,
            "sides": "Österreichischer",
            "Side_Id": 36,
            "node_type": "country",
            "start_date": 1711,
            "end_date": 1786,
            "id": "C36"
        },
        {
            "bipartite": 0,
            "sides": "Schwäbischer",
            "Side_Id": 34,
            "node_type": "country",
            "start_date": 1711,
            "end_date": 1786,
            "id": "C34"
        },
        {
            "bipartite": 1,
            "title": "Verlängerung betr. 1700 VIII 31",
            "dates": "1711 VIII 1",
            "Treaty_Id": 182,
            "node_type": "treaty",
            "start_date": 1711,
            "end_date": 1786,
            "year": 1711,
            "id": "T182"
        },
        {
            "bipartite": 1,
            "title": "Allianz von Den Haag",
            "dates": "1712 VI 21",
            "Treaty_Id": 183,
            "node_type": "treaty",
            "start_date": 1712,
            "end_date": 1786,
            "year": 1712,
            "id": "T183"
        },
        {
            "bipartite": 0,
            "sides": "Bern",
            "Side_Id": 40,
            "node_type": "country",
            "start_date": 1712,
            "end_date": 1786,
            "id": "C40"
        },
        {
            "bipartite": 1,
            "title": "Schiedsspruch von Mailand zur Schlichtung der Streitigkeiten zwischen Kaiser und Savoyen ",
            "dates": "1712 VI 27",
            "Treaty_Id": 184,
            "node_type": "treaty",
            "start_date": 1712,
            "end_date": 1786,
            "year": 1712,
            "id": "T184"
        },
        {
            "bipartite": 1,
            "title": "Schiedsspruch von Mailand zur Schlichtung der Streitigkeiten zwischen Kaiser und Savoyen (2. Exempl.) ",
            "dates": "1712 VI 27",
            "Treaty_Id": 185,
            "node_type": "treaty",
            "start_date": 1712,
            "end_date": 1786,
            "year": 1712,
            "id": "T185"
        },
        {
            "bipartite": 1,
            "title": "Konvention von London",
            "dates": "1712 XII 29",
            "Treaty_Id": 186,
            "node_type": "treaty",
            "start_date": 1712,
            "end_date": 1786,
            "year": 1712,
            "id": "T186"
        },
        {
            "bipartite": 1,
            "title": "Garantie betr. 1709 X 29",
            "dates": "1713 I 30",
            "Treaty_Id": 187,
            "node_type": "treaty",
            "start_date": 1713,
            "end_date": 1786,
            "year": 1713,
            "id": "T187"
        },
        {
            "bipartite": 1,
            "title": "Friedensvertrag von Utrecht",
            "dates": "1713 IV 11",
            "Treaty_Id": 188,
            "node_type": "treaty",
            "start_date": 1713,
            "end_date": 1786,
            "year": 1713,
            "id": "T188"
        },
        {
            "bipartite": 1,
            "title": "Handels- und Schiffahrtsvertrag von Utrecht",
            "dates": "1713 IV 11",
            "Treaty_Id": 189,
            "node_type": "treaty",
            "start_date": 1713,
            "end_date": 1786,
            "year": 1713,
            "id": "T189"
        },
        {
            "bipartite": 1,
            "title": "Defensivallianz von Den Haag",
            "dates": "1713 IV 19",
            "Treaty_Id": 190,
            "node_type": "treaty",
            "start_date": 1713,
            "end_date": 1786,
            "year": 1713,
            "id": "T190"
        },
        {
            "bipartite": 0,
            "sides": "Graubünden",
            "Side_Id": 41,
            "node_type": "country",
            "start_date": 1713,
            "end_date": 1786,
            "id": "C41"
        },
        {
            "bipartite": 1,
            "title": "Defensivallianz (Vgl. 1713 IV 19)",
            "dates": "1713 IV 9_20",
            "Treaty_Id": 191,
            "node_type": "treaty",
            "start_date": 1713,
            "end_date": 1786,
            "year": 1713,
            "id": "T191"
        },
        {
            "bipartite": 1,
            "title": "Deklaration von Utrecht",
            "dates": "1713 IV 28",
            "Treaty_Id": 192,
            "node_type": "treaty",
            "start_date": 1713,
            "end_date": 1786,
            "year": 1713,
            "id": "T192"
        },
        {
            "bipartite": 1,
            "title": "Konvention von Utrecht",
            "dates": "1713 V 12",
            "Treaty_Id": 193,
            "node_type": "treaty",
            "start_date": 1713,
            "end_date": 1786,
            "year": 1713,
            "id": "T193"
        },
        {
            "bipartite": 1,
            "title": "Vertrag über die Neutralität Luxemburgs",
            "dates": "1713 VI 11",
            "Treaty_Id": 194,
            "node_type": "treaty",
            "start_date": 1713,
            "end_date": 1786,
            "year": 1713,
            "id": "T194"
        },
        {
            "bipartite": 1,
            "title": "Provisorischer Handelsvertrag von Utrecht",
            "dates": "1713 VII 26",
            "Treaty_Id": 195,
            "node_type": "treaty",
            "start_date": 1713,
            "end_date": 1786,
            "year": 1713,
            "id": "T195"
        },
        {
            "bipartite": 1,
            "title": "Friedens- und Handelsvertrag von Utrecht",
            "dates": "1714 VI 26",
            "Treaty_Id": 196,
            "node_type": "treaty",
            "start_date": 1714,
            "end_date": 1786,
            "year": 1714,
            "id": "T196"
        },
        {
            "bipartite": 1,
            "title": "Vertrag über Marschordnung",
            "dates": "1714 VII 28",
            "Treaty_Id": 197,
            "node_type": "treaty",
            "start_date": 1714,
            "end_date": 1786,
            "year": 1714,
            "id": "T197"
        },
        {
            "bipartite": 1,
            "title": "Vertrag von Antwerpen",
            "dates": "1715 XI 15",
            "Treaty_Id": 198,
            "node_type": "treaty",
            "start_date": 1715,
            "end_date": 1786,
            "year": 1715,
            "id": "T198"
        },
        {
            "bipartite": 1,
            "title": "Bündniserneuerung",
            "dates": "1716 II 6",
            "Treaty_Id": 199,
            "node_type": "treaty",
            "start_date": 1716,
            "end_date": 1786,
            "year": 1716,
            "id": "T199"
        },
        {
            "bipartite": 1,
            "title": "Separatartikel zur Bündniserneuerung",
            "dates": "1716 IV 3",
            "Treaty_Id": 200,
            "node_type": "treaty",
            "start_date": 1716,
            "end_date": 1786,
            "year": 1716,
            "id": "T200"
        },
        {
            "bipartite": 1,
            "title": "Grenzkonvention von Venlo",
            "dates": "1716 XI 20",
            "Treaty_Id": 201,
            "node_type": "treaty",
            "start_date": 1716,
            "end_date": 1786,
            "year": 1716,
            "id": "T201"
        },
        {
            "bipartite": 1,
            "title": "Tripel-Allianz von Den Haag",
            "dates": "1717 I 4",
            "Treaty_Id": 202,
            "node_type": "treaty",
            "start_date": 1717,
            "end_date": 1786,
            "year": 1717,
            "id": "T202"
        },
        {
            "bipartite": 1,
            "title": "Quadrupel-Allianz von London",
            "dates": "1718 VIII 2",
            "Treaty_Id": 203,
            "node_type": "treaty",
            "start_date": 1718,
            "end_date": 1786,
            "year": 1718,
            "id": "T203"
        },
        {
            "bipartite": 0,
            "sides": "Reich",
            "Side_Id": 42,
            "node_type": "country",
            "start_date": 1718,
            "end_date": 1786,
            "id": "C42"
        },
        {
            "bipartite": 1,
            "title": "Konvention zur Umsetzung des Vertrags von Antwerpen",
            "dates": "1718 XII 22",
            "Treaty_Id": 204,
            "node_type": "treaty",
            "start_date": 1718,
            "end_date": 1786,
            "year": 1718,
            "id": "T204"
        },
        {
            "bipartite": 1,
            "title": "Vertrag über Truppendurchzug",
            "dates": "1722 VI 2",
            "Treaty_Id": 205,
            "node_type": "treaty",
            "start_date": 1722,
            "end_date": 1786,
            "year": 1722,
            "id": "T205"
        },
        {
            "bipartite": 1,
            "title": "Beitritt zu Allianzvertrag von Hannover",
            "dates": "1726 VIII 9",
            "Treaty_Id": 206,
            "node_type": "treaty",
            "start_date": 1726,
            "end_date": 1786,
            "year": 1726,
            "id": "T206"
        },
        {
            "bipartite": 1,
            "title": "Deklaration und Vertrag über das Europäische Gleichgewicht betr. 1726 VIII 9",
            "dates": "1726 VIII 9",
            "Treaty_Id": 207,
            "node_type": "treaty",
            "start_date": 1726,
            "end_date": 1786,
            "year": 1726,
            "id": "T207"
        },
        {
            "bipartite": 1,
            "title": "Separatartikel über den Beitritt Preussens betr. 1726 VIII 9",
            "dates": "1726 VIII 9",
            "Treaty_Id": 208,
            "node_type": "treaty",
            "start_date": 1726,
            "end_date": 1786,
            "year": 1726,
            "id": "T208"
        },
        {
            "bipartite": 1,
            "title": "Separatartikel über gegenseitige Hilfe betr. 1726 VIII 9",
            "dates": "1726 VIII 9",
            "Treaty_Id": 209,
            "node_type": "treaty",
            "start_date": 1726,
            "end_date": 1786,
            "year": 1726,
            "id": "T209"
        },
        {
            "bipartite": 1,
            "title": "Separatartikel zum Beitritt zum Allianzvertrag von 1725 IX 3",
            "dates": "1726 VIII 9",
            "Treaty_Id": 210,
            "node_type": "treaty",
            "start_date": 1726,
            "end_date": 1786,
            "year": 1726,
            "id": "T210"
        },
        {
            "bipartite": 1,
            "title": "Vertrag von Den Haag zur Übergabe der Insel Arguin und zum Handel in Afrika",
            "dates": "1727 I 13",
            "Treaty_Id": 211,
            "node_type": "treaty",
            "start_date": 1727,
            "end_date": 1786,
            "year": 1727,
            "id": "T211"
        },
        {
            "bipartite": 1,
            "title": "Präliminarfrieden von Paris",
            "dates": "1727 V 31",
            "Treaty_Id": 212,
            "node_type": "treaty",
            "start_date": 1727,
            "end_date": 1786,
            "year": 1727,
            "id": "T212"
        },
        {
            "bipartite": 1,
            "title": "Konvention von Madrid",
            "dates": "1728 III 6",
            "Treaty_Id": 213,
            "node_type": "treaty",
            "start_date": 1728,
            "end_date": 1786,
            "year": 1728,
            "id": "T213"
        },
        {
            "bipartite": 1,
            "title": "Bündniserneuerung",
            "dates": "1728 V 27",
            "Treaty_Id": 214,
            "node_type": "treaty",
            "start_date": 1728,
            "end_date": 1786,
            "year": 1728,
            "id": "T214"
        },
        {
            "bipartite": 1,
            "title": "Beitritt zum Allianzvertrag von Sevilla 1729 XI 9",
            "dates": "1729 XI 21",
            "Treaty_Id": 215,
            "node_type": "treaty",
            "start_date": 1729,
            "end_date": 1786,
            "year": 1729,
            "id": "T215"
        },
        {
            "bipartite": 1,
            "title": "Separatartikel über den Handel mit Ostindien betr. 1729 XI 9",
            "dates": "1729 XI 21",
            "Treaty_Id": 216,
            "node_type": "treaty",
            "start_date": 1729,
            "end_date": 1786,
            "year": 1729,
            "id": "T216"
        },
        {
            "bipartite": 1,
            "title": "Separatartikel über die Verfügbarkeit von Schiffen betr. 1729 XI 9",
            "dates": "1729 XI 21",
            "Treaty_Id": 217,
            "node_type": "treaty",
            "start_date": 1729,
            "end_date": 1786,
            "year": 1729,
            "id": "T217"
        },
        {
            "bipartite": 1,
            "title": "Konvention von Den Haag (beta-Version)",
            "dates": "1731 II 20",
            "Treaty_Id": 218,
            "node_type": "treaty",
            "start_date": 1731,
            "end_date": 1786,
            "year": 1731,
            "id": "T218"
        },
        {
            "bipartite": 1,
            "title": "Handelsvertrag von Kopenhagen",
            "dates": "1731 IX 3",
            "Treaty_Id": 219,
            "node_type": "treaty",
            "start_date": 1731,
            "end_date": 1786,
            "year": 1731,
            "id": "T219"
        },
        {
            "bipartite": 1,
            "title": "Beitritt von Den Haag betr. 1731 III 16",
            "dates": "1732 II 20",
            "Treaty_Id": 220,
            "node_type": "treaty",
            "start_date": 1732,
            "end_date": 1786,
            "year": 1732,
            "id": "T220"
        },
        {
            "bipartite": 1,
            "title": "Konvention von Den Haag zur Neutralität der Österreichischen Niederlande",
            "dates": "1733 XI 24",
            "Treaty_Id": 221,
            "node_type": "treaty",
            "start_date": 1733,
            "end_date": 1786,
            "year": 1733,
            "id": "T221"
        },
        {
            "bipartite": 1,
            "title": "Handelsvertrag von Versailles",
            "dates": "1739 XII 21",
            "Treaty_Id": 222,
            "node_type": "treaty",
            "start_date": 1739,
            "end_date": 1786,
            "year": 1739,
            "id": "T222"
        },
        {
            "bipartite": 1,
            "title": "Allianz von Den Haag",
            "dates": "1744 VII 4",
            "Treaty_Id": 223,
            "node_type": "treaty",
            "start_date": 1744,
            "end_date": 1786,
            "year": 1744,
            "id": "T223"
        },
        {
            "bipartite": 1,
            "title": "Allianz von Warschau /Quadrupelallianz",
            "dates": "1745 I 8",
            "Treaty_Id": 224,
            "node_type": "treaty",
            "start_date": 1745,
            "end_date": 1786,
            "year": 1745,
            "id": "T224"
        },
        {
            "bipartite": 0,
            "sides": "Oesterreich",
            "Side_Id": 44,
            "node_type": "country",
            "start_date": 1745,
            "end_date": 1786,
            "id": "C44"
        },
        {
            "bipartite": 0,
            "sides": "Sachsen / Polen",
            "Side_Id": 43,
            "node_type": "country",
            "start_date": 1745,
            "end_date": 1786,
            "id": "C43"
        },
        {
            "bipartite": 1,
            "title": "Subsidienvertrag von München",
            "dates": "1746 VII 21",
            "Treaty_Id": 225,
            "node_type": "treaty",
            "start_date": 1746,
            "end_date": 1786,
            "year": 1746,
            "id": "T225"
        },
        {
            "bipartite": 1,
            "title": "Provisorischer Subsidienvertrag",
            "dates": "1746 VIII 31",
            "Treaty_Id": 226,
            "node_type": "treaty",
            "start_date": 1746,
            "end_date": 1786,
            "year": 1746,
            "id": "T226"
        },
        {
            "bipartite": 1,
            "title": "Konvention von St. Petersburg",
            "dates": "1747 XI 19",
            "Treaty_Id": 227,
            "node_type": "treaty",
            "start_date": 1747,
            "end_date": 1786,
            "year": 1747,
            "id": "T227"
        },
        {
            "bipartite": 0,
            "sides": "Russland",
            "Side_Id": 45,
            "node_type": "country",
            "start_date": 1747,
            "end_date": 1786,
            "id": "C45"
        },
        {
            "bipartite": 1,
            "title": "Militärkonvention von Den Haag",
            "dates": "1748 I 26",
            "Treaty_Id": 228,
            "node_type": "treaty",
            "start_date": 1748,
            "end_date": 1786,
            "year": 1748,
            "id": "T228"
        },
        {
            "bipartite": 0,
            "sides": "Sardinien - Piemont",
            "Side_Id": 47,
            "node_type": "country",
            "start_date": 1748,
            "end_date": 1786,
            "id": "C47"
        },
        {
            "bipartite": 1,
            "title": "Vertrag über Stationierung von Truppen in den Niederlanden",
            "dates": "1748 I 26",
            "Treaty_Id": 229,
            "node_type": "treaty",
            "start_date": 1748,
            "end_date": 1786,
            "year": 1748,
            "id": "T229"
        },
        {
            "bipartite": 1,
            "title": "Präliminarfrieden von Aachen",
            "dates": "1748 IV 30",
            "Treaty_Id": 230,
            "node_type": "treaty",
            "start_date": 1748,
            "end_date": 1786,
            "year": 1748,
            "id": "T230"
        },
        {
            "bipartite": 1,
            "title": "Annahmen des Beitritts von Sardinien-Piemont zum Friedensvertrag von Aachen (1748 IV 30) mit Erklärungen der jeweiligen Minister zu Art. 2",
            "dates": "1748 V 31",
            "Treaty_Id": 231,
            "node_type": "treaty",
            "start_date": 1748,
            "end_date": 1786,
            "year": 1748,
            "id": "T231"
        },
        {
            "bipartite": 1,
            "title": "Digitaler Kartensatz zum Friedensvertrag von Aachen",
            "dates": "1748 X 18",
            "Treaty_Id": 232,
            "node_type": "treaty",
            "start_date": 1748,
            "end_date": 1786,
            "year": 1748,
            "id": "T232"
        },
        {
            "bipartite": 0,
            "sides": "Genua",
            "Side_Id": 46,
            "node_type": "country",
            "start_date": 1748,
            "end_date": 1786,
            "id": "C46"
        },
        {
            "bipartite": 0,
            "sides": "Modena",
            "Side_Id": 48,
            "node_type": "country",
            "start_date": 1748,
            "end_date": 1786,
            "id": "C48"
        },
        {
            "bipartite": 1,
            "title": "Friedensvertrag von Aachen",
            "dates": "1748 X 18",
            "Treaty_Id": 233,
            "node_type": "treaty",
            "start_date": 1748,
            "end_date": 1786,
            "year": 1748,
            "id": "T233"
        },
        {
            "bipartite": 1,
            "title": "Beitritt und Akzeptation, betr. 1748 X 18",
            "dates": "1748 X 28",
            "Treaty_Id": 234,
            "node_type": "treaty",
            "start_date": 1748,
            "end_date": 1786,
            "year": 1748,
            "id": "T234"
        },
        {
            "bipartite": 1,
            "title": "Konvention von Nizza, betr. Art. 8 des Friedensvertrags von Nizza (1748 X 18)",
            "dates": "1748 XII 2",
            "Treaty_Id": 235,
            "node_type": "treaty",
            "start_date": 1748,
            "end_date": 1786,
            "year": 1748,
            "id": "T235"
        },
        {
            "bipartite": 1,
            "title": "Konvention von Nizza über die Räumung von Gebieten, betr. Konvention vom 1748 XII 4",
            "dates": "1749 I 21",
            "Treaty_Id": 236,
            "node_type": "treaty",
            "start_date": 1749,
            "end_date": 1786,
            "year": 1749,
            "id": "T236"
        },
        {
            "bipartite": 1,
            "title": "Allianz von Neuhaus",
            "dates": "1750 II 27",
            "Treaty_Id": 237,
            "node_type": "treaty",
            "start_date": 1750,
            "end_date": 1786,
            "year": 1750,
            "id": "T237"
        },
        {
            "bipartite": 1,
            "title": "Subsidienvertrag von Hannover",
            "dates": "1750 VIII 11_22",
            "Treaty_Id": 238,
            "node_type": "treaty",
            "start_date": 1750,
            "end_date": 1786,
            "year": 1750,
            "id": "T238"
        },
        {
            "bipartite": 1,
            "title": "Allianz von Dresden",
            "dates": "1751 IX 13",
            "Treaty_Id": 239,
            "node_type": "treaty",
            "start_date": 1751,
            "end_date": 1786,
            "year": 1751,
            "id": "T239"
        },
        {
            "bipartite": 1,
            "title": "Handelsvertrag von Den Haag",
            "dates": "1753 VIII 27",
            "Treaty_Id": 240,
            "node_type": "treaty",
            "start_date": 1753,
            "end_date": 1786,
            "year": 1753,
            "id": "T240"
        },
        {
            "bipartite": 0,
            "sides": "Neapel - Sizilien",
            "Side_Id": 49,
            "node_type": "country",
            "start_date": 1753,
            "end_date": 1786,
            "id": "C49"
        },
        {
            "bipartite": 1,
            "title": "Vertrag von Mainz (und Konvention von Compiègne)",
            "dates": "1770 X 13",
            "Treaty_Id": 241,
            "node_type": "treaty",
            "start_date": 1770,
            "end_date": 1786,
            "year": 1770,
            "id": "T241"
        },
        {
            "bipartite": 1,
            "title": "Compiègner Aufhebungsvertrag über das Droit d' Aubaine",
            "dates": "1773 VII 23",
            "Treaty_Id": 242,
            "node_type": "treaty",
            "start_date": 1773,
            "end_date": 1786,
            "year": 1773,
            "id": "T242"
        },
        {
            "bipartite": 1,
            "title": "Beitritt zur Konvention über die bewaffnete Neutralität",
            "dates": "1780 XII 24",
            "Treaty_Id": 243,
            "node_type": "treaty",
            "start_date": 1780,
            "end_date": 1786,
            "year": 1780,
            "id": "T243"
        },
        {
            "bipartite": 1,
            "title": "Erklärung zum Beitritt zur Konvention über die bewaffnete Neutralität",
            "dates": "1781 II 8",
            "Treaty_Id": 244,
            "node_type": "treaty",
            "start_date": 1781,
            "end_date": 1786,
            "year": 1781,
            "id": "T244"
        },
        {
            "bipartite": 1,
            "title": "Konvention von Versailles zur Prisenregelung",
            "dates": "1781 V 1",
            "Treaty_Id": 245,
            "node_type": "treaty",
            "start_date": 1781,
            "end_date": 1786,
            "year": 1781,
            "id": "T245"
        },
        {
            "bipartite": 1,
            "title": "Erneuerung früherer Verträge",
            "dates": "1782 IV 19",
            "Treaty_Id": 246,
            "node_type": "treaty",
            "start_date": 1782,
            "end_date": 1786,
            "year": 1782,
            "id": "T246"
        },
        {
            "bipartite": 1,
            "title": "Friedenspräliminarien von Versailles",
            "dates": "1783 I 20",
            "Treaty_Id": 247,
            "node_type": "treaty",
            "start_date": 1783,
            "end_date": 1786,
            "year": 1783,
            "id": "T247"
        },
        {
            "bipartite": 1,
            "title": "Friedenspräliminarien von Paris",
            "dates": "1783 IX 2",
            "Treaty_Id": 248,
            "node_type": "treaty",
            "start_date": 1783,
            "end_date": 1786,
            "year": 1783,
            "id": "T248"
        },
        {
            "bipartite": 1,
            "title": "Friedensvertrag von Paris",
            "dates": "1784 V 20",
            "Treaty_Id": 249,
            "node_type": "treaty",
            "start_date": 1784,
            "end_date": 1786,
            "year": 1784,
            "id": "T249"
        },
        {
            "bipartite": 1,
            "title": "Vertrag von Bonn",
            "dates": "1784 X 30",
            "Treaty_Id": 250,
            "node_type": "treaty",
            "start_date": 1784,
            "end_date": 1786,
            "year": 1784,
            "id": "T250"
        },
        {
            "bipartite": 1,
            "title": "Friedenspräliminarien von Fontainebleau",
            "dates": "1785 IX 20",
            "Treaty_Id": 251,
            "node_type": "treaty",
            "start_date": 1785,
            "end_date": 1786,
            "year": 1785,
            "id": "T251"
        },
        {
            "bipartite": 1,
            "title": "Freundschafts- und Defensivvertrag von Fontainebleau",
            "dates": "1785 XI 10",
            "Treaty_Id": 252,
            "node_type": "treaty",
            "start_date": 1785,
            "end_date": 1786,
            "year": 1785,
            "id": "T252"
        }
    ],
    "links": [
        {
            "dates": "1577 II 12",
            "start_date": 1577,
            "end_date": 1786,
            "source": "T1",
            "target": "C1"
        },
        {
            "dates": "1577 II 12",
            "start_date": 1577,
            "end_date": 1786,
            "source": "T1",
            "target": "C2"
        },
        {
            "dates": "1579 VII 10/18",
            "start_date": 1579,
            "end_date": 1786,
            "source": "C1",
            "target": "T2"
        },
        {
            "dates": "1617 VII 19",
            "start_date": 1617,
            "end_date": 1786,
            "source": "C1",
            "target": "T3"
        },
        {
            "dates": "1619 XII 31",
            "start_date": 1619,
            "end_date": 1786,
            "source": "C1",
            "target": "T5"
        },
        {
            "dates": "1621 V 14",
            "start_date": 1621,
            "end_date": 1786,
            "source": "C1",
            "target": "T6"
        },
        {
            "dates": "1621 IX 30_X 10",
            "start_date": 1621,
            "end_date": 1786,
            "source": "C1",
            "target": "T7"
        },
        {
            "dates": "1622 III 10",
            "start_date": 1622,
            "end_date": 1786,
            "source": "C1",
            "target": "T8"
        },
        {
            "dates": "1623 VIII 5",
            "start_date": 1623,
            "end_date": 1786,
            "source": "C1",
            "target": "T9"
        },
        {
            "dates": "1624 VI 10",
            "start_date": 1624,
            "end_date": 1786,
            "source": "C1",
            "target": "T10"
        },
        {
            "dates": "1624 VI 15",
            "start_date": 1624,
            "end_date": 1786,
            "source": "C1",
            "target": "T11"
        },
        {
            "dates": "1625 IX 7_17",
            "start_date": 1625,
            "end_date": 1786,
            "source": "C1",
            "target": "T12"
        },
        {
            "dates": "1625 XII 9",
            "start_date": 1625,
            "end_date": 1786,
            "source": "C1",
            "target": "T13"
        },
        {
            "dates": "1627 VIII 28",
            "start_date": 1627,
            "end_date": 1786,
            "source": "C1",
            "target": "T14"
        },
        {
            "dates": "1630 VI 17",
            "start_date": 1630,
            "end_date": 1786,
            "source": "C1",
            "target": "T15"
        },
        {
            "dates": "1632 IV 2",
            "start_date": 1632,
            "end_date": 1786,
            "source": "C1",
            "target": "T16"
        },
        {
            "dates": "1634 IV 15",
            "start_date": 1634,
            "end_date": 1786,
            "source": "C1",
            "target": "T17"
        },
        {
            "dates": "1635 II 8",
            "start_date": 1635,
            "end_date": 1786,
            "source": "C1",
            "target": "T18"
        },
        {
            "dates": "1636 IX 4",
            "start_date": 1636,
            "end_date": 1786,
            "source": "C1",
            "target": "T19"
        },
        {
            "dates": "1636 IX 6",
            "start_date": 1636,
            "end_date": 1786,
            "source": "C1",
            "target": "T20"
        },
        {
            "dates": "1637 XII 17",
            "start_date": 1637,
            "end_date": 1786,
            "source": "C1",
            "target": "T21"
        },
        {
            "dates": "1639 III 24",
            "start_date": 1639,
            "end_date": 1786,
            "source": "C1",
            "target": "T22"
        },
        {
            "dates": "1640 IX 1",
            "start_date": 1640,
            "end_date": 1786,
            "source": "C1",
            "target": "T23"
        },
        {
            "dates": "1641 II 14",
            "start_date": 1641,
            "end_date": 1786,
            "source": "C1",
            "target": "T24"
        },
        {
            "dates": "1641 VI 12",
            "start_date": 1641,
            "end_date": 1786,
            "source": "C1",
            "target": "T25"
        },
        {
            "dates": "1643 III 30",
            "start_date": 1643,
            "end_date": 1786,
            "source": "C1",
            "target": "T26"
        },
        {
            "dates": "1643 VIII 30",
            "start_date": 1643,
            "end_date": 1786,
            "source": "C1",
            "target": "T27"
        },
        {
            "dates": "1644 II 29",
            "start_date": 1644,
            "end_date": 1786,
            "source": "C1",
            "target": "T28"
        },
        {
            "dates": "1644 III 1",
            "start_date": 1644,
            "end_date": 1786,
            "source": "C1",
            "target": "T29"
        },
        {
            "dates": "1645 III 10",
            "start_date": 1645,
            "end_date": 1786,
            "source": "C1",
            "target": "T31"
        },
        {
            "dates": "1645 III 27",
            "start_date": 1645,
            "end_date": 1786,
            "source": "C1",
            "target": "T32"
        },
        {
            "dates": "1645 IV 20",
            "start_date": 1645,
            "end_date": 1786,
            "source": "C1",
            "target": "T33"
        },
        {
            "dates": "1645 VIII 4",
            "start_date": 1645,
            "end_date": 1786,
            "source": "C1",
            "target": "T34"
        },
        {
            "dates": "1645 VIII 15",
            "start_date": 1645,
            "end_date": 1786,
            "source": "C1",
            "target": "T35"
        },
        {
            "dates": "1646 IV 6",
            "start_date": 1646,
            "end_date": 1786,
            "source": "C1",
            "target": "T36"
        },
        {
            "dates": "1646 IV 18",
            "start_date": 1646,
            "end_date": 1786,
            "source": "C1",
            "target": "T37"
        },
        {
            "dates": "1646 V 13",
            "start_date": 1646,
            "end_date": 1786,
            "source": "C1",
            "target": "T38"
        },
        {
            "dates": "1646 X 25",
            "start_date": 1646,
            "end_date": 1786,
            "source": "C1",
            "target": "T39"
        },
        {
            "dates": "1647 I 8",
            "start_date": 1647,
            "end_date": 1786,
            "source": "C1",
            "target": "T40"
        },
        {
            "dates": "1647 VII 29",
            "start_date": 1647,
            "end_date": 1786,
            "source": "C1",
            "target": "T41"
        },
        {
            "dates": "1647 XII 27",
            "start_date": 1647,
            "end_date": 1786,
            "source": "C1",
            "target": "T42"
        },
        {
            "dates": "1648 I 30",
            "start_date": 1648,
            "end_date": 1786,
            "source": "C1",
            "target": "T43"
        },
        {
            "dates": "1649 X 9",
            "start_date": 1649,
            "end_date": 1786,
            "source": "C1",
            "target": "T44"
        },
        {
            "dates": "1649 IX 29_X 9",
            "start_date": 1649,
            "end_date": 1786,
            "source": "C1",
            "target": "T45"
        },
        {
            "dates": "1653 II 18",
            "start_date": 1653,
            "end_date": 1786,
            "source": "C1",
            "target": "T46"
        },
        {
            "dates": "1654 IV 5",
            "start_date": 1654,
            "end_date": 1786,
            "source": "C1",
            "target": "T47"
        },
        {
            "dates": "1655 XI 23",
            "start_date": 1655,
            "end_date": 1786,
            "source": "C1",
            "target": "T48"
        },
        {
            "dates": "1656 IV 20",
            "start_date": 1656,
            "end_date": 1786,
            "source": "C1",
            "target": "T49"
        },
        {
            "dates": "1656 VIII 16",
            "start_date": 1656,
            "end_date": 1786,
            "source": "C1",
            "target": "T50"
        },
        {
            "dates": "1656 IX 1_11",
            "start_date": 1656,
            "end_date": 1786,
            "source": "C1",
            "target": "T51"
        },
        {
            "dates": "1657 VI 17_27",
            "start_date": 1657,
            "end_date": 1786,
            "source": "C1",
            "target": "T52"
        },
        {
            "dates": "1659 V 21",
            "start_date": 1659,
            "end_date": 1786,
            "source": "C1",
            "target": "T53"
        },
        {
            "dates": "1659 XI 29_XII 9",
            "start_date": 1659,
            "end_date": 1786,
            "source": "C1",
            "target": "T54"
        },
        {
            "dates": "1659 XII 9",
            "start_date": 1659,
            "end_date": 1786,
            "source": "C1",
            "target": "T55"
        },
        {
            "dates": "1660 VI 2_12",
            "start_date": 1660,
            "end_date": 1786,
            "source": "C1",
            "target": "T56"
        },
        {
            "dates": "1661 VIII 6",
            "start_date": 1661,
            "end_date": 1786,
            "source": "C1",
            "target": "T57"
        },
        {
            "dates": "1661 XII 26",
            "start_date": 1661,
            "end_date": 1786,
            "source": "C1",
            "target": "T58"
        },
        {
            "dates": "1662 IV 27",
            "start_date": 1662,
            "end_date": 1786,
            "source": "C1",
            "target": "T59"
        },
        {
            "dates": "1662 IX 14",
            "start_date": 1662,
            "end_date": 1786,
            "source": "C1",
            "target": "T60"
        },
        {
            "dates": "1664 IX 20",
            "start_date": 1664,
            "end_date": 1786,
            "source": "C1",
            "target": "T61"
        },
        {
            "dates": "1665 III 16",
            "start_date": 1665,
            "end_date": 1786,
            "source": "C1",
            "target": "T62"
        },
        {
            "dates": "1665 IX 19",
            "start_date": 1665,
            "end_date": 1786,
            "source": "C1",
            "target": "T63"
        },
        {
            "dates": "1666 II 11",
            "start_date": 1666,
            "end_date": 1786,
            "source": "C1",
            "target": "T64"
        },
        {
            "dates": "1666 II 16",
            "start_date": 1666,
            "end_date": 1786,
            "source": "C1",
            "target": "T65"
        },
        {
            "dates": "1666 II 16",
            "start_date": 1666,
            "end_date": 1786,
            "source": "C1",
            "target": "T66"
        },
        {
            "dates": "1666 IV 18",
            "start_date": 1666,
            "end_date": 1786,
            "source": "C1",
            "target": "T67"
        },
        {
            "dates": "1666 VII 17",
            "start_date": 1666,
            "end_date": 1786,
            "source": "C1",
            "target": "T68"
        },
        {
            "dates": "1666 X 15_25",
            "start_date": 1666,
            "end_date": 1786,
            "source": "C1",
            "target": "T69"
        },
        {
            "dates": "1667 VII 6_16",
            "start_date": 1667,
            "end_date": 1786,
            "source": "C1",
            "target": "T70"
        },
        {
            "dates": "1667 VII 18_28",
            "start_date": 1667,
            "end_date": 1786,
            "source": "C1",
            "target": "T71"
        },
        {
            "dates": "1667 VIII 25",
            "start_date": 1667,
            "end_date": 1786,
            "source": "C1",
            "target": "T72"
        },
        {
            "dates": "1668 I 23",
            "start_date": 1668,
            "end_date": 1786,
            "source": "C1",
            "target": "T73"
        },
        {
            "dates": "1668 I 13_23",
            "start_date": 1668,
            "end_date": 1786,
            "source": "C1",
            "target": "T74"
        },
        {
            "dates": "1668 I 13_23",
            "start_date": 1668,
            "end_date": 1786,
            "source": "C1",
            "target": "T75"
        },
        {
            "dates": "1668 II 17",
            "start_date": 1668,
            "end_date": 1786,
            "source": "C1",
            "target": "T76"
        },
        {
            "dates": "1668 III 16",
            "start_date": 1668,
            "end_date": 1786,
            "source": "C1",
            "target": "T77"
        },
        {
            "dates": "1668 IV 15",
            "start_date": 1668,
            "end_date": 1786,
            "source": "C1",
            "target": "T78"
        },
        {
            "dates": "1668 IV 25_V 5",
            "start_date": 1668,
            "end_date": 1786,
            "source": "C1",
            "target": "T79"
        },
        {
            "dates": "1669 V 7",
            "start_date": 1669,
            "end_date": 1786,
            "source": "C1",
            "target": "T80"
        },
        {
            "dates": "1669 VII 30 und 31",
            "start_date": 1669,
            "end_date": 1786,
            "source": "C1",
            "target": "T81"
        },
        {
            "dates": "1670 I 31",
            "start_date": 1670,
            "end_date": 1786,
            "source": "C1",
            "target": "T82"
        },
        {
            "dates": "1671 VIII 18",
            "start_date": 1671,
            "end_date": 1786,
            "source": "C1",
            "target": "T83"
        },
        {
            "dates": "1672 V 6",
            "start_date": 1672,
            "end_date": 1786,
            "source": "C1",
            "target": "T84"
        },
        {
            "dates": "1672 VII 25",
            "start_date": 1672,
            "end_date": 1786,
            "source": "C1",
            "target": "T85"
        },
        {
            "dates": "1672 IX 22",
            "start_date": 1672,
            "end_date": 1786,
            "source": "C1",
            "target": "T86"
        },
        {
            "dates": "1673 III 30",
            "start_date": 1673,
            "end_date": 1786,
            "source": "C1",
            "target": "T87"
        },
        {
            "dates": "1673 V 2",
            "start_date": 1673,
            "end_date": 1786,
            "source": "C1",
            "target": "T88"
        },
        {
            "dates": "1673 V 20",
            "start_date": 1673,
            "end_date": 1786,
            "source": "C1",
            "target": "T89"
        },
        {
            "dates": "1673 VII 1",
            "start_date": 1673,
            "end_date": 1786,
            "source": "C1",
            "target": "T90"
        },
        {
            "dates": "1673 VIII 30",
            "start_date": 1673,
            "end_date": 1786,
            "source": "C1",
            "target": "T91"
        },
        {
            "dates": "1673 VIII 30",
            "start_date": 1673,
            "end_date": 1786,
            "source": "C1",
            "target": "T92"
        },
        {
            "dates": "1673 X 9",
            "start_date": 1673,
            "end_date": 1786,
            "source": "C1",
            "target": "T93"
        },
        {
            "dates": "1674 II 19",
            "start_date": 1674,
            "end_date": 1786,
            "source": "C1",
            "target": "T94"
        },
        {
            "dates": "1674 III 20",
            "start_date": 1674,
            "end_date": 1786,
            "source": "C1",
            "target": "T95"
        },
        {
            "dates": "1674 IV 22",
            "start_date": 1674,
            "end_date": 1786,
            "source": "C1",
            "target": "T96"
        },
        {
            "dates": "1674 V 11",
            "start_date": 1674,
            "end_date": 1786,
            "source": "C1",
            "target": "T97"
        },
        {
            "dates": "1674 VI 10_20",
            "start_date": 1674,
            "end_date": 1786,
            "source": "C1",
            "target": "T98"
        },
        {
            "dates": "1674 VI 21 / VII 1",
            "start_date": 1674,
            "end_date": 1786,
            "source": "C1",
            "target": "T99"
        },
        {
            "dates": "1674 VII 10",
            "start_date": 1674,
            "end_date": 1786,
            "source": "C1",
            "target": "T100"
        },
        {
            "dates": "1674 XII 1",
            "start_date": 1674,
            "end_date": 1786,
            "source": "C1",
            "target": "T101"
        },
        {
            "dates": "1675 I 26",
            "start_date": 1675,
            "end_date": 1786,
            "source": "C1",
            "target": "T102"
        },
        {
            "dates": "1675 III 8",
            "start_date": 1675,
            "end_date": 1786,
            "source": "C1",
            "target": "T103"
        },
        {
            "dates": "1675 X 12",
            "start_date": 1675,
            "end_date": 1786,
            "source": "C1",
            "target": "T104"
        },
        {
            "dates": "1675 XI 26",
            "start_date": 1675,
            "end_date": 1786,
            "source": "C1",
            "target": "T105"
        },
        {
            "dates": "1675 XII 17",
            "start_date": 1675,
            "end_date": 1786,
            "source": "C1",
            "target": "T106"
        },
        {
            "dates": "1676 II 7",
            "start_date": 1676,
            "end_date": 1786,
            "source": "C1",
            "target": "T107"
        },
        {
            "dates": "1676 III 26",
            "start_date": 1676,
            "end_date": 1786,
            "source": "C1",
            "target": "T108"
        },
        {
            "dates": "1676 IV 26",
            "start_date": 1676,
            "end_date": 1786,
            "source": "C1",
            "target": "T109"
        },
        {
            "dates": "1676 X 9",
            "start_date": 1676,
            "end_date": 1786,
            "source": "C1",
            "target": "T110"
        },
        {
            "dates": "1678 I 26",
            "start_date": 1678,
            "end_date": 1786,
            "source": "C1",
            "target": "T111"
        },
        {
            "dates": "1678 III 3",
            "start_date": 1678,
            "end_date": 1786,
            "source": "C1",
            "target": "T112"
        },
        {
            "dates": "1678 II 26_III 8",
            "start_date": 1678,
            "end_date": 1786,
            "source": "C1",
            "target": "T113"
        },
        {
            "dates": "1678 VII 25",
            "start_date": 1678,
            "end_date": 1786,
            "source": "C1",
            "target": "T114"
        },
        {
            "dates": "1678 VIII 10",
            "start_date": 1678,
            "end_date": 1786,
            "source": "C1",
            "target": "T115"
        },
        {
            "dates": "1678 VIII 10",
            "start_date": 1678,
            "end_date": 1786,
            "source": "C1",
            "target": "T116"
        },
        {
            "dates": "1679 X 2_12",
            "start_date": 1679,
            "end_date": 1786,
            "source": "C1",
            "target": "T117"
        },
        {
            "dates": "1679 X 12",
            "start_date": 1679,
            "end_date": 1786,
            "source": "C1",
            "target": "T118"
        },
        {
            "dates": "1680 IX 15 / 1091 AH",
            "start_date": 1680,
            "end_date": 1786,
            "source": "C1",
            "target": "T119"
        },
        {
            "dates": "1681 IX 30_X 10",
            "start_date": 1681,
            "end_date": 1786,
            "source": "C1",
            "target": "T120"
        },
        {
            "dates": "1683 III 8",
            "start_date": 1683,
            "end_date": 1786,
            "source": "C1",
            "target": "T121"
        },
        {
            "dates": "1683 III 18",
            "start_date": 1683,
            "end_date": 1786,
            "source": "C1",
            "target": "T122"
        },
        {
            "dates": "1684 VI 29",
            "start_date": 1684,
            "end_date": 1786,
            "source": "C1",
            "target": "T123"
        },
        {
            "dates": "1685 VIII 17",
            "start_date": 1685,
            "end_date": 1786,
            "source": "C1",
            "target": "T124"
        },
        {
            "dates": "1686 I 12",
            "start_date": 1686,
            "end_date": 1786,
            "source": "C1",
            "target": "T125"
        },
        {
            "dates": "1688 VI 30",
            "start_date": 1688,
            "end_date": 1786,
            "source": "C1",
            "target": "T126"
        },
        {
            "dates": "1688 IX 4",
            "start_date": 1688,
            "end_date": 1786,
            "source": "C1",
            "target": "T127"
        },
        {
            "dates": "1688 IX 12",
            "start_date": 1688,
            "end_date": 1786,
            "source": "C1",
            "target": "T128"
        },
        {
            "dates": "1689 IV 29",
            "start_date": 1689,
            "end_date": 1786,
            "source": "C1",
            "target": "T129"
        },
        {
            "dates": "1689 V 12",
            "start_date": 1689,
            "end_date": 1786,
            "source": "C1",
            "target": "T130"
        },
        {
            "dates": "1689 VIII 22",
            "start_date": 1689,
            "end_date": 1786,
            "source": "C1",
            "target": "T131"
        },
        {
            "dates": "1689 VIII 24",
            "start_date": 1689,
            "end_date": 1786,
            "source": "C1",
            "target": "T132"
        },
        {
            "dates": "1690 I 27",
            "start_date": 1690,
            "end_date": 1786,
            "source": "C1",
            "target": "T133"
        },
        {
            "dates": "1691 V 14",
            "start_date": 1691,
            "end_date": 1786,
            "source": "C1",
            "target": "T134"
        },
        {
            "dates": "1691 XI 19",
            "start_date": 1691,
            "end_date": 1786,
            "source": "C1",
            "target": "T135"
        },
        {
            "dates": "1692 V 23",
            "start_date": 1692,
            "end_date": 1786,
            "source": "C1",
            "target": "T136"
        },
        {
            "dates": "1692 VI 30",
            "start_date": 1692,
            "end_date": 1786,
            "source": "C1",
            "target": "T137"
        },
        {
            "dates": "1692 XII 22",
            "start_date": 1692,
            "end_date": 1786,
            "source": "C1",
            "target": "T138"
        },
        {
            "dates": "1693 XI 25",
            "start_date": 1693,
            "end_date": 1786,
            "source": "C1",
            "target": "T139"
        },
        {
            "dates": "1694 V 21",
            "start_date": 1694,
            "end_date": 1786,
            "source": "C1",
            "target": "T140"
        },
        {
            "dates": "1695 III 18",
            "start_date": 1695,
            "end_date": 1786,
            "source": "C1",
            "target": "T141"
        },
        {
            "dates": "1695 VIII 8",
            "start_date": 1695,
            "end_date": 1786,
            "source": "C1",
            "target": "T142"
        },
        {
            "dates": "1696 V 14",
            "start_date": 1696,
            "end_date": 1786,
            "source": "C1",
            "target": "T143"
        },
        {
            "dates": "1697 I 18",
            "start_date": 1697,
            "end_date": 1786,
            "source": "C1",
            "target": "T144"
        },
        {
            "dates": "1697 IX 20",
            "start_date": 1697,
            "end_date": 1786,
            "source": "C1",
            "target": "T145"
        },
        {
            "dates": "1697 IX 20",
            "start_date": 1697,
            "end_date": 1786,
            "source": "C1",
            "target": "T146"
        },
        {
            "dates": "1698 II 12",
            "start_date": 1698,
            "end_date": 1786,
            "source": "C1",
            "target": "T147"
        },
        {
            "dates": "1698 V 4_14",
            "start_date": 1698,
            "end_date": 1786,
            "source": "C1",
            "target": "T148"
        },
        {
            "dates": "1698 X 11",
            "start_date": 1698,
            "end_date": 1786,
            "source": "C1",
            "target": "T149"
        },
        {
            "dates": "1699 V 29",
            "start_date": 1699,
            "end_date": 1786,
            "source": "C1",
            "target": "T150"
        },
        {
            "dates": "1699 XI 14",
            "start_date": 1699,
            "end_date": 1786,
            "source": "C1",
            "target": "T151"
        },
        {
            "dates": "1700 I 13_23 und 1700 I 20_30",
            "start_date": 1700,
            "end_date": 1786,
            "source": "C1",
            "target": "T152"
        },
        {
            "dates": "1700 III 3 und 1700 III 25",
            "start_date": 1700,
            "end_date": 1786,
            "source": "C1",
            "target": "T153"
        },
        {
            "dates": "1700 VIII 31",
            "start_date": 1700,
            "end_date": 1786,
            "source": "C1",
            "target": "T154"
        },
        {
            "dates": "1701 V 26",
            "start_date": 1701,
            "end_date": 1786,
            "source": "C1",
            "target": "T155"
        },
        {
            "dates": "1701 VI 15",
            "start_date": 1701,
            "end_date": 1786,
            "source": "C1",
            "target": "T156"
        },
        {
            "dates": "1701 VI 15",
            "start_date": 1701,
            "end_date": 1786,
            "source": "C1",
            "target": "T157"
        },
        {
            "dates": "1701 IX 7",
            "start_date": 1701,
            "end_date": 1786,
            "source": "C1",
            "target": "T158"
        },
        {
            "dates": "1701 X 7",
            "start_date": 1701,
            "end_date": 1786,
            "source": "C1",
            "target": "T159"
        },
        {
            "dates": "1701 X 17",
            "start_date": 1701,
            "end_date": 1786,
            "source": "C1",
            "target": "T160"
        },
        {
            "dates": "1701 XI 11",
            "start_date": 1701,
            "end_date": 1786,
            "source": "C1",
            "target": "T161"
        },
        {
            "dates": "1701 XII 30",
            "start_date": 1701,
            "end_date": 1786,
            "source": "C1",
            "target": "T162"
        },
        {
            "dates": "1702 I 5",
            "start_date": 1702,
            "end_date": 1786,
            "source": "C1",
            "target": "T163"
        },
        {
            "dates": "1702 II 7",
            "start_date": 1702,
            "end_date": 1786,
            "source": "C1",
            "target": "T164"
        },
        {
            "dates": "1702 IV 12",
            "start_date": 1702,
            "end_date": 1786,
            "source": "C1",
            "target": "T165"
        },
        {
            "dates": "1703 III 13",
            "start_date": 1703,
            "end_date": 1786,
            "source": "C1",
            "target": "T166"
        },
        {
            "dates": "1703 V 5",
            "start_date": 1703,
            "end_date": 1786,
            "source": "C1",
            "target": "T167"
        },
        {
            "dates": "1703 V 16",
            "start_date": 1703,
            "end_date": 1786,
            "source": "C1",
            "target": "T168"
        },
        {
            "dates": "1703 VI 9",
            "start_date": 1703,
            "end_date": 1786,
            "source": "C1",
            "target": "T169"
        },
        {
            "dates": "1703 VIII 5_16",
            "start_date": 1703,
            "end_date": 1786,
            "source": "C1",
            "target": "T170"
        },
        {
            "dates": "1703 VIII 5_16",
            "start_date": 1703,
            "end_date": 1786,
            "source": "C1",
            "target": "T171"
        },
        {
            "dates": "1703 VIII 5_16",
            "start_date": 1703,
            "end_date": 1786,
            "source": "C1",
            "target": "T172"
        },
        {
            "dates": "1705 I 21",
            "start_date": 1705,
            "end_date": 1786,
            "source": "C1",
            "target": "T173"
        },
        {
            "dates": "1705 VIII 7",
            "start_date": 1705,
            "end_date": 1786,
            "source": "C1",
            "target": "T174"
        },
        {
            "dates": "1708 VII 21",
            "start_date": 1708,
            "end_date": 1786,
            "source": "C1",
            "target": "T175"
        },
        {
            "dates": "1709 V 28",
            "start_date": 1709,
            "end_date": 1786,
            "source": "C1",
            "target": "T176"
        },
        {
            "dates": "1709 X 29",
            "start_date": 1709,
            "end_date": 1786,
            "source": "C1",
            "target": "T177"
        },
        {
            "dates": "1710 III 31",
            "start_date": 1710,
            "end_date": 1786,
            "source": "C1",
            "target": "T178"
        },
        {
            "dates": "1710 VI 2",
            "start_date": 1710,
            "end_date": 1786,
            "source": "C1",
            "target": "T179"
        },
        {
            "dates": "1710 VIII 4",
            "start_date": 1710,
            "end_date": 1786,
            "source": "C1",
            "target": "T180"
        },
        {
            "dates": "1711 VIII [ohne Tag]",
            "start_date": 1711,
            "end_date": 1786,
            "source": "C1",
            "target": "T181"
        },
        {
            "dates": "1711 VIII 1",
            "start_date": 1711,
            "end_date": 1786,
            "source": "C1",
            "target": "T182"
        },
        {
            "dates": "1712 VI 21",
            "start_date": 1712,
            "end_date": 1786,
            "source": "C1",
            "target": "T183"
        },
        {
            "dates": "1712 VI 27",
            "start_date": 1712,
            "end_date": 1786,
            "source": "C1",
            "target": "T184"
        },
        {
            "dates": "1712 VI 27",
            "start_date": 1712,
            "end_date": 1786,
            "source": "C1",
            "target": "T185"
        },
        {
            "dates": "1712 XII 29",
            "start_date": 1712,
            "end_date": 1786,
            "source": "C1",
            "target": "T186"
        },
        {
            "dates": "1713 I 30",
            "start_date": 1713,
            "end_date": 1786,
            "source": "C1",
            "target": "T187"
        },
        {
            "dates": "1713 IV 11",
            "start_date": 1713,
            "end_date": 1786,
            "source": "C1",
            "target": "T188"
        },
        {
            "dates": "1713 IV 11",
            "start_date": 1713,
            "end_date": 1786,
            "source": "C1",
            "target": "T189"
        },
        {
            "dates": "1713 IV 19",
            "start_date": 1713,
            "end_date": 1786,
            "source": "C1",
            "target": "T190"
        },
        {
            "dates": "1713 IV 9_20",
            "start_date": 1713,
            "end_date": 1786,
            "source": "C1",
            "target": "T191"
        },
        {
            "dates": "1713 IV 28",
            "start_date": 1713,
            "end_date": 1786,
            "source": "C1",
            "target": "T192"
        },
        {
            "dates": "1713 V 12",
            "start_date": 1713,
            "end_date": 1786,
            "source": "C1",
            "target": "T193"
        },
        {
            "dates": "1713 VI 11",
            "start_date": 1713,
            "end_date": 1786,
            "source": "C1",
            "target": "T194"
        },
        {
            "dates": "1713 VII 26",
            "start_date": 1713,
            "end_date": 1786,
            "source": "C1",
            "target": "T195"
        },
        {
            "dates": "1714 VI 26",
            "start_date": 1714,
            "end_date": 1786,
            "source": "C1",
            "target": "T196"
        },
        {
            "dates": "1714 VII 28",
            "start_date": 1714,
            "end_date": 1786,
            "source": "C1",
            "target": "T197"
        },
        {
            "dates": "1715 XI 15",
            "start_date": 1715,
            "end_date": 1786,
            "source": "C1",
            "target": "T198"
        },
        {
            "dates": "1716 II 6",
            "start_date": 1716,
            "end_date": 1786,
            "source": "C1",
            "target": "T199"
        },
        {
            "dates": "1716 IV 3",
            "start_date": 1716,
            "end_date": 1786,
            "source": "C1",
            "target": "T200"
        },
        {
            "dates": "1716 XI 20",
            "start_date": 1716,
            "end_date": 1786,
            "source": "C1",
            "target": "T201"
        },
        {
            "dates": "1717 I 4",
            "start_date": 1717,
            "end_date": 1786,
            "source": "C1",
            "target": "T202"
        },
        {
            "dates": "1718 VIII 2",
            "start_date": 1718,
            "end_date": 1786,
            "source": "C1",
            "target": "T203"
        },
        {
            "dates": "1718 XII 22",
            "start_date": 1718,
            "end_date": 1786,
            "source": "C1",
            "target": "T204"
        },
        {
            "dates": "1722 VI 2",
            "start_date": 1722,
            "end_date": 1786,
            "source": "C1",
            "target": "T205"
        },
        {
            "dates": "1726 VIII 9",
            "start_date": 1726,
            "end_date": 1786,
            "source": "C1",
            "target": "T206"
        },
        {
            "dates": "1726 VIII 9",
            "start_date": 1726,
            "end_date": 1786,
            "source": "C1",
            "target": "T207"
        },
        {
            "dates": "1726 VIII 9",
            "start_date": 1726,
            "end_date": 1786,
            "source": "C1",
            "target": "T208"
        },
        {
            "dates": "1726 VIII 9",
            "start_date": 1726,
            "end_date": 1786,
            "source": "C1",
            "target": "T209"
        },
        {
            "dates": "1726 VIII 9",
            "start_date": 1726,
            "end_date": 1786,
            "source": "C1",
            "target": "T210"
        },
        {
            "dates": "1727 I 13",
            "start_date": 1727,
            "end_date": 1786,
            "source": "C1",
            "target": "T211"
        },
        {
            "dates": "1727 V 31",
            "start_date": 1727,
            "end_date": 1786,
            "source": "C1",
            "target": "T212"
        },
        {
            "dates": "1728 III 6",
            "start_date": 1728,
            "end_date": 1786,
            "source": "C1",
            "target": "T213"
        },
        {
            "dates": "1728 V 27",
            "start_date": 1728,
            "end_date": 1786,
            "source": "C1",
            "target": "T214"
        },
        {
            "dates": "1729 XI 21",
            "start_date": 1729,
            "end_date": 1786,
            "source": "C1",
            "target": "T215"
        },
        {
            "dates": "1729 XI 21",
            "start_date": 1729,
            "end_date": 1786,
            "source": "C1",
            "target": "T216"
        },
        {
            "dates": "1729 XI 21",
            "start_date": 1729,
            "end_date": 1786,
            "source": "C1",
            "target": "T217"
        },
        {
            "dates": "1731 II 20",
            "start_date": 1731,
            "end_date": 1786,
            "source": "C1",
            "target": "T218"
        },
        {
            "dates": "1731 IX 3",
            "start_date": 1731,
            "end_date": 1786,
            "source": "C1",
            "target": "T219"
        },
        {
            "dates": "1732 II 20",
            "start_date": 1732,
            "end_date": 1786,
            "source": "C1",
            "target": "T220"
        },
        {
            "dates": "1733 XI 24",
            "start_date": 1733,
            "end_date": 1786,
            "source": "C1",
            "target": "T221"
        },
        {
            "dates": "1739 XII 21",
            "start_date": 1739,
            "end_date": 1786,
            "source": "C1",
            "target": "T222"
        },
        {
            "dates": "1744 VII 4",
            "start_date": 1744,
            "end_date": 1786,
            "source": "C1",
            "target": "T223"
        },
        {
            "dates": "1745 I 8",
            "start_date": 1745,
            "end_date": 1786,
            "source": "C1",
            "target": "T224"
        },
        {
            "dates": "1746 VII 21",
            "start_date": 1746,
            "end_date": 1786,
            "source": "C1",
            "target": "T225"
        },
        {
            "dates": "1746 VIII 31",
            "start_date": 1746,
            "end_date": 1786,
            "source": "C1",
            "target": "T226"
        },
        {
            "dates": "1747 XI 19",
            "start_date": 1747,
            "end_date": 1786,
            "source": "C1",
            "target": "T227"
        },
        {
            "dates": "1748 I 26",
            "start_date": 1748,
            "end_date": 1786,
            "source": "C1",
            "target": "T228"
        },
        {
            "dates": "1748 I 26",
            "start_date": 1748,
            "end_date": 1786,
            "source": "C1",
            "target": "T229"
        },
        {
            "dates": "1748 IV 30",
            "start_date": 1748,
            "end_date": 1786,
            "source": "C1",
            "target": "T230"
        },
        {
            "dates": "1748 V 31",
            "start_date": 1748,
            "end_date": 1786,
            "source": "C1",
            "target": "T231"
        },
        {
            "dates": "1748 X 18",
            "start_date": 1748,
            "end_date": 1786,
            "source": "C1",
            "target": "T232"
        },
        {
            "dates": "1748 X 18",
            "start_date": 1748,
            "end_date": 1786,
            "source": "C1",
            "target": "T233"
        },
        {
            "dates": "1748 X 28",
            "start_date": 1748,
            "end_date": 1786,
            "source": "C1",
            "target": "T234"
        },
        {
            "dates": "1748 XII 2",
            "start_date": 1748,
            "end_date": 1786,
            "source": "C1",
            "target": "T235"
        },
        {
            "dates": "1749 I 21",
            "start_date": 1749,
            "end_date": 1786,
            "source": "C1",
            "target": "T236"
        },
        {
            "dates": "1750 II 27",
            "start_date": 1750,
            "end_date": 1786,
            "source": "C1",
            "target": "T237"
        },
        {
            "dates": "1750 VIII 11_22",
            "start_date": 1750,
            "end_date": 1786,
            "source": "C1",
            "target": "T238"
        },
        {
            "dates": "1751 IX 13",
            "start_date": 1751,
            "end_date": 1786,
            "source": "C1",
            "target": "T239"
        },
        {
            "dates": "1753 VIII 27",
            "start_date": 1753,
            "end_date": 1786,
            "source": "C1",
            "target": "T240"
        },
        {
            "dates": "1770 X 13",
            "start_date": 1770,
            "end_date": 1786,
            "source": "C1",
            "target": "T241"
        },
        {
            "dates": "1773 VII 23",
            "start_date": 1773,
            "end_date": 1786,
            "source": "C1",
            "target": "T242"
        },
        {
            "dates": "1780 XII 24",
            "start_date": 1780,
            "end_date": 1786,
            "source": "C1",
            "target": "T243"
        },
        {
            "dates": "1781 II 8",
            "start_date": 1781,
            "end_date": 1786,
            "source": "C1",
            "target": "T244"
        },
        {
            "dates": "1781 V 1",
            "start_date": 1781,
            "end_date": 1786,
            "source": "C1",
            "target": "T245"
        },
        {
            "dates": "1782 IV 19",
            "start_date": 1782,
            "end_date": 1786,
            "source": "C1",
            "target": "T246"
        },
        {
            "dates": "1783 I 20",
            "start_date": 1783,
            "end_date": 1786,
            "source": "C1",
            "target": "T247"
        },
        {
            "dates": "1783 IX 2",
            "start_date": 1783,
            "end_date": 1786,
            "source": "C1",
            "target": "T248"
        },
        {
            "dates": "1784 V 20",
            "start_date": 1784,
            "end_date": 1786,
            "source": "C1",
            "target": "T249"
        },
        {
            "dates": "1784 X 30",
            "start_date": 1784,
            "end_date": 1786,
            "source": "C1",
            "target": "T250"
        },
        {
            "dates": "1785 IX 20",
            "start_date": 1785,
            "end_date": 1786,
            "source": "C1",
            "target": "T251"
        },
        {
            "dates": "1785 XI 10",
            "start_date": 1785,
            "end_date": 1786,
            "source": "C1",
            "target": "T252"
        },
        {
            "dates": "1579 VII 10/18",
            "start_date": 1579,
            "end_date": 1786,
            "source": "C2",
            "target": "T2"
        },
        {
            "dates": "1647 I 8",
            "start_date": 1647,
            "end_date": 1786,
            "source": "C2",
            "target": "T40"
        },
        {
            "dates": "1647 XII 27",
            "start_date": 1647,
            "end_date": 1786,
            "source": "C2",
            "target": "T42"
        },
        {
            "dates": "1648 I 30",
            "start_date": 1648,
            "end_date": 1786,
            "source": "C2",
            "target": "T43"
        },
        {
            "dates": "1661 XII 26",
            "start_date": 1661,
            "end_date": 1786,
            "source": "C2",
            "target": "T58"
        },
        {
            "dates": "1664 IX 20",
            "start_date": 1664,
            "end_date": 1786,
            "source": "C2",
            "target": "T61"
        },
        {
            "dates": "1673 VII 1",
            "start_date": 1673,
            "end_date": 1786,
            "source": "C2",
            "target": "T90"
        },
        {
            "dates": "1673 VIII 30",
            "start_date": 1673,
            "end_date": 1786,
            "source": "C2",
            "target": "T92"
        },
        {
            "dates": "1674 VI 10_20",
            "start_date": 1674,
            "end_date": 1786,
            "source": "C2",
            "target": "T98"
        },
        {
            "dates": "1674 VI 21 / VII 1",
            "start_date": 1674,
            "end_date": 1786,
            "source": "C2",
            "target": "T99"
        },
        {
            "dates": "1674 VII 10",
            "start_date": 1674,
            "end_date": 1786,
            "source": "C2",
            "target": "T100"
        },
        {
            "dates": "1675 I 26",
            "start_date": 1675,
            "end_date": 1786,
            "source": "C2",
            "target": "T102"
        },
        {
            "dates": "1675 X 12",
            "start_date": 1675,
            "end_date": 1786,
            "source": "C2",
            "target": "T104"
        },
        {
            "dates": "1676 II 7",
            "start_date": 1676,
            "end_date": 1786,
            "source": "C2",
            "target": "T107"
        },
        {
            "dates": "1676 III 26",
            "start_date": 1676,
            "end_date": 1786,
            "source": "C2",
            "target": "T108"
        },
        {
            "dates": "1676 IV 26",
            "start_date": 1676,
            "end_date": 1786,
            "source": "C2",
            "target": "T109"
        },
        {
            "dates": "1676 X 9",
            "start_date": 1676,
            "end_date": 1786,
            "source": "C2",
            "target": "T110"
        },
        {
            "dates": "1683 III 18",
            "start_date": 1683,
            "end_date": 1786,
            "source": "C2",
            "target": "T122"
        },
        {
            "dates": "1695 VIII 8",
            "start_date": 1695,
            "end_date": 1786,
            "source": "C2",
            "target": "T142"
        },
        {
            "dates": "1714 VI 26",
            "start_date": 1714,
            "end_date": 1786,
            "source": "C2",
            "target": "T196"
        },
        {
            "dates": "1727 V 31",
            "start_date": 1727,
            "end_date": 1786,
            "source": "C2",
            "target": "T212"
        },
        {
            "dates": "1728 III 6",
            "start_date": 1728,
            "end_date": 1786,
            "source": "C2",
            "target": "T213"
        },
        {
            "dates": "1748 X 18",
            "start_date": 1748,
            "end_date": 1786,
            "source": "C2",
            "target": "T232"
        },
        {
            "dates": "1748 XII 2",
            "start_date": 1748,
            "end_date": 1786,
            "source": "C2",
            "target": "T235"
        },
        {
            "dates": "1749 I 21",
            "start_date": 1749,
            "end_date": 1786,
            "source": "C2",
            "target": "T236"
        },
        {
            "dates": "1617 VII 19",
            "start_date": 1617,
            "end_date": 1786,
            "source": "T3",
            "target": "C3"
        },
        {
            "dates": "1619 VI 2",
            "start_date": 1619,
            "end_date": 1786,
            "source": "T4",
            "target": "C5"
        },
        {
            "dates": "1619 VI 2",
            "start_date": 1619,
            "end_date": 1786,
            "source": "T4",
            "target": "C6"
        },
        {
            "dates": "1644 XI 14",
            "start_date": 1644,
            "end_date": 1786,
            "source": "C5",
            "target": "T30"
        },
        {
            "dates": "1624 VI 15",
            "start_date": 1624,
            "end_date": 1786,
            "source": "C6",
            "target": "T11"
        },
        {
            "dates": "1625 IX 7_17",
            "start_date": 1625,
            "end_date": 1786,
            "source": "C6",
            "target": "T12"
        },
        {
            "dates": "1625 XII 9",
            "start_date": 1625,
            "end_date": 1786,
            "source": "C6",
            "target": "T13"
        },
        {
            "dates": "1654 IV 5",
            "start_date": 1654,
            "end_date": 1786,
            "source": "C6",
            "target": "T47"
        },
        {
            "dates": "1659 V 21",
            "start_date": 1659,
            "end_date": 1786,
            "source": "C6",
            "target": "T53"
        },
        {
            "dates": "1662 IX 14",
            "start_date": 1662,
            "end_date": 1786,
            "source": "C6",
            "target": "T60"
        },
        {
            "dates": "1666 VII 17",
            "start_date": 1666,
            "end_date": 1786,
            "source": "C6",
            "target": "T68"
        },
        {
            "dates": "1668 I 23",
            "start_date": 1668,
            "end_date": 1786,
            "source": "C6",
            "target": "T73"
        },
        {
            "dates": "1668 I 13_23",
            "start_date": 1668,
            "end_date": 1786,
            "source": "C6",
            "target": "T74"
        },
        {
            "dates": "1668 I 13_23",
            "start_date": 1668,
            "end_date": 1786,
            "source": "C6",
            "target": "T75"
        },
        {
            "dates": "1668 II 17",
            "start_date": 1668,
            "end_date": 1786,
            "source": "C6",
            "target": "T76"
        },
        {
            "dates": "1668 IV 15",
            "start_date": 1668,
            "end_date": 1786,
            "source": "C6",
            "target": "T78"
        },
        {
            "dates": "1668 IV 25_V 5",
            "start_date": 1668,
            "end_date": 1786,
            "source": "C6",
            "target": "T79"
        },
        {
            "dates": "1669 V 7",
            "start_date": 1669,
            "end_date": 1786,
            "source": "C6",
            "target": "T80"
        },
        {
            "dates": "1670 I 31",
            "start_date": 1670,
            "end_date": 1786,
            "source": "C6",
            "target": "T82"
        },
        {
            "dates": "1674 II 19",
            "start_date": 1674,
            "end_date": 1786,
            "source": "C6",
            "target": "T94"
        },
        {
            "dates": "1674 XII 1",
            "start_date": 1674,
            "end_date": 1786,
            "source": "C6",
            "target": "T101"
        },
        {
            "dates": "1675 III 8",
            "start_date": 1675,
            "end_date": 1786,
            "source": "C6",
            "target": "T103"
        },
        {
            "dates": "1678 I 26",
            "start_date": 1678,
            "end_date": 1786,
            "source": "C6",
            "target": "T111"
        },
        {
            "dates": "1678 III 3",
            "start_date": 1678,
            "end_date": 1786,
            "source": "C6",
            "target": "T112"
        },
        {
            "dates": "1678 VII 25",
            "start_date": 1678,
            "end_date": 1786,
            "source": "C6",
            "target": "T114"
        },
        {
            "dates": "1685 VIII 17",
            "start_date": 1685,
            "end_date": 1786,
            "source": "C6",
            "target": "T124"
        },
        {
            "dates": "1689 IV 29",
            "start_date": 1689,
            "end_date": 1786,
            "source": "C6",
            "target": "T129"
        },
        {
            "dates": "1689 VIII 22",
            "start_date": 1689,
            "end_date": 1786,
            "source": "C6",
            "target": "T131"
        },
        {
            "dates": "1689 VIII 24",
            "start_date": 1689,
            "end_date": 1786,
            "source": "C6",
            "target": "T132"
        },
        {
            "dates": "1692 VI 30",
            "start_date": 1692,
            "end_date": 1786,
            "source": "C6",
            "target": "T137"
        },
        {
            "dates": "1692 XII 22",
            "start_date": 1692,
            "end_date": 1786,
            "source": "C6",
            "target": "T138"
        },
        {
            "dates": "1694 V 21",
            "start_date": 1694,
            "end_date": 1786,
            "source": "C6",
            "target": "T140"
        },
        {
            "dates": "1695 III 18",
            "start_date": 1695,
            "end_date": 1786,
            "source": "C6",
            "target": "T141"
        },
        {
            "dates": "1695 VIII 8",
            "start_date": 1695,
            "end_date": 1786,
            "source": "C6",
            "target": "T142"
        },
        {
            "dates": "1696 V 14",
            "start_date": 1696,
            "end_date": 1786,
            "source": "C6",
            "target": "T143"
        },
        {
            "dates": "1698 V 4_14",
            "start_date": 1698,
            "end_date": 1786,
            "source": "C6",
            "target": "T148"
        },
        {
            "dates": "1698 X 11",
            "start_date": 1698,
            "end_date": 1786,
            "source": "C6",
            "target": "T149"
        },
        {
            "dates": "1700 I 13_23 und 1700 I 20_30",
            "start_date": 1700,
            "end_date": 1786,
            "source": "C6",
            "target": "T152"
        },
        {
            "dates": "1700 III 3 und 1700 III 25",
            "start_date": 1700,
            "end_date": 1786,
            "source": "C6",
            "target": "T153"
        },
        {
            "dates": "1701 VI 15",
            "start_date": 1701,
            "end_date": 1786,
            "source": "C6",
            "target": "T156"
        },
        {
            "dates": "1701 IX 7",
            "start_date": 1701,
            "end_date": 1786,
            "source": "C6",
            "target": "T158"
        },
        {
            "dates": "1701 X 7",
            "start_date": 1701,
            "end_date": 1786,
            "source": "C6",
            "target": "T159"
        },
        {
            "dates": "1701 XI 11",
            "start_date": 1701,
            "end_date": 1786,
            "source": "C6",
            "target": "T161"
        },
        {
            "dates": "1701 XII 30",
            "start_date": 1701,
            "end_date": 1786,
            "source": "C6",
            "target": "T162"
        },
        {
            "dates": "1702 IV 12",
            "start_date": 1702,
            "end_date": 1786,
            "source": "C6",
            "target": "T165"
        },
        {
            "dates": "1703 V 5",
            "start_date": 1703,
            "end_date": 1786,
            "source": "C6",
            "target": "T167"
        },
        {
            "dates": "1703 V 16",
            "start_date": 1703,
            "end_date": 1786,
            "source": "C6",
            "target": "T168"
        },
        {
            "dates": "1703 VI 9",
            "start_date": 1703,
            "end_date": 1786,
            "source": "C6",
            "target": "T169"
        },
        {
            "dates": "1703 VIII 5_16",
            "start_date": 1703,
            "end_date": 1786,
            "source": "C6",
            "target": "T170"
        },
        {
            "dates": "1703 VIII 5_16",
            "start_date": 1703,
            "end_date": 1786,
            "source": "C6",
            "target": "T171"
        },
        {
            "dates": "1703 VIII 5_16",
            "start_date": 1703,
            "end_date": 1786,
            "source": "C6",
            "target": "T172"
        },
        {
            "dates": "1709 V 28",
            "start_date": 1709,
            "end_date": 1786,
            "source": "C6",
            "target": "T176"
        },
        {
            "dates": "1709 X 29",
            "start_date": 1709,
            "end_date": 1786,
            "source": "C6",
            "target": "T177"
        },
        {
            "dates": "1710 III 31",
            "start_date": 1710,
            "end_date": 1786,
            "source": "C6",
            "target": "T178"
        },
        {
            "dates": "1710 VIII 4",
            "start_date": 1710,
            "end_date": 1786,
            "source": "C6",
            "target": "T180"
        },
        {
            "dates": "1712 VI 27",
            "start_date": 1712,
            "end_date": 1786,
            "source": "C6",
            "target": "T184"
        },
        {
            "dates": "1712 VI 27",
            "start_date": 1712,
            "end_date": 1786,
            "source": "C6",
            "target": "T185"
        },
        {
            "dates": "1712 XII 29",
            "start_date": 1712,
            "end_date": 1786,
            "source": "C6",
            "target": "T186"
        },
        {
            "dates": "1713 I 30",
            "start_date": 1713,
            "end_date": 1786,
            "source": "C6",
            "target": "T187"
        },
        {
            "dates": "1713 VII 26",
            "start_date": 1713,
            "end_date": 1786,
            "source": "C6",
            "target": "T195"
        },
        {
            "dates": "1715 XI 15",
            "start_date": 1715,
            "end_date": 1786,
            "source": "C6",
            "target": "T198"
        },
        {
            "dates": "1716 II 6",
            "start_date": 1716,
            "end_date": 1786,
            "source": "C6",
            "target": "T199"
        },
        {
            "dates": "1716 IV 3",
            "start_date": 1716,
            "end_date": 1786,
            "source": "C6",
            "target": "T200"
        },
        {
            "dates": "1717 I 4",
            "start_date": 1717,
            "end_date": 1786,
            "source": "C6",
            "target": "T202"
        },
        {
            "dates": "1718 VIII 2",
            "start_date": 1718,
            "end_date": 1786,
            "source": "C6",
            "target": "T203"
        },
        {
            "dates": "1718 XII 22",
            "start_date": 1718,
            "end_date": 1786,
            "source": "C6",
            "target": "T204"
        },
        {
            "dates": "1726 VIII 9",
            "start_date": 1726,
            "end_date": 1786,
            "source": "C6",
            "target": "T207"
        },
        {
            "dates": "1726 VIII 9",
            "start_date": 1726,
            "end_date": 1786,
            "source": "C6",
            "target": "T208"
        },
        {
            "dates": "1726 VIII 9",
            "start_date": 1726,
            "end_date": 1786,
            "source": "C6",
            "target": "T209"
        },
        {
            "dates": "1726 VIII 9",
            "start_date": 1726,
            "end_date": 1786,
            "source": "C6",
            "target": "T210"
        },
        {
            "dates": "1727 V 31",
            "start_date": 1727,
            "end_date": 1786,
            "source": "C6",
            "target": "T212"
        },
        {
            "dates": "1728 III 6",
            "start_date": 1728,
            "end_date": 1786,
            "source": "C6",
            "target": "T213"
        },
        {
            "dates": "1728 V 27",
            "start_date": 1728,
            "end_date": 1786,
            "source": "C6",
            "target": "T214"
        },
        {
            "dates": "1729 XI 21",
            "start_date": 1729,
            "end_date": 1786,
            "source": "C6",
            "target": "T216"
        },
        {
            "dates": "1731 II 20",
            "start_date": 1731,
            "end_date": 1786,
            "source": "C6",
            "target": "T218"
        },
        {
            "dates": "1744 VII 4",
            "start_date": 1744,
            "end_date": 1786,
            "source": "C6",
            "target": "T223"
        },
        {
            "dates": "1745 I 8",
            "start_date": 1745,
            "end_date": 1786,
            "source": "C6",
            "target": "T224"
        },
        {
            "dates": "1746 VII 21",
            "start_date": 1746,
            "end_date": 1786,
            "source": "C6",
            "target": "T225"
        },
        {
            "dates": "1746 VIII 31",
            "start_date": 1746,
            "end_date": 1786,
            "source": "C6",
            "target": "T226"
        },
        {
            "dates": "1748 I 26",
            "start_date": 1748,
            "end_date": 1786,
            "source": "C6",
            "target": "T228"
        },
        {
            "dates": "1748 I 26",
            "start_date": 1748,
            "end_date": 1786,
            "source": "C6",
            "target": "T229"
        },
        {
            "dates": "1748 IV 30",
            "start_date": 1748,
            "end_date": 1786,
            "source": "C6",
            "target": "T230"
        },
        {
            "dates": "1748 V 31",
            "start_date": 1748,
            "end_date": 1786,
            "source": "C6",
            "target": "T231"
        },
        {
            "dates": "1748 X 18",
            "start_date": 1748,
            "end_date": 1786,
            "source": "C6",
            "target": "T232"
        },
        {
            "dates": "1748 X 18",
            "start_date": 1748,
            "end_date": 1786,
            "source": "C6",
            "target": "T233"
        },
        {
            "dates": "1748 XII 2",
            "start_date": 1748,
            "end_date": 1786,
            "source": "C6",
            "target": "T235"
        },
        {
            "dates": "1749 I 21",
            "start_date": 1749,
            "end_date": 1786,
            "source": "C6",
            "target": "T236"
        },
        {
            "dates": "1750 VIII 11_22",
            "start_date": 1750,
            "end_date": 1786,
            "source": "C6",
            "target": "T238"
        },
        {
            "dates": "1751 IX 13",
            "start_date": 1751,
            "end_date": 1786,
            "source": "C6",
            "target": "T239"
        },
        {
            "dates": "1783 I 20",
            "start_date": 1783,
            "end_date": 1786,
            "source": "C6",
            "target": "T247"
        },
        {
            "dates": "1783 IX 2",
            "start_date": 1783,
            "end_date": 1786,
            "source": "C6",
            "target": "T248"
        },
        {
            "dates": "1784 V 20",
            "start_date": 1784,
            "end_date": 1786,
            "source": "C6",
            "target": "T249"
        },
        {
            "dates": "1619 XII 31",
            "start_date": 1619,
            "end_date": 1786,
            "source": "T5",
            "target": "C4"
        },
        {
            "dates": "1621 V 14",
            "start_date": 1621,
            "end_date": 1786,
            "source": "T6",
            "target": "C7"
        },
        {
            "dates": "1621 IX 30_X 10",
            "start_date": 1621,
            "end_date": 1786,
            "source": "C7",
            "target": "T7"
        },
        {
            "dates": "1625 XII 9",
            "start_date": 1625,
            "end_date": 1786,
            "source": "C7",
            "target": "T13"
        },
        {
            "dates": "1649 X 9",
            "start_date": 1649,
            "end_date": 1786,
            "source": "C7",
            "target": "T44"
        },
        {
            "dates": "1649 IX 29_X 9",
            "start_date": 1649,
            "end_date": 1786,
            "source": "C7",
            "target": "T45"
        },
        {
            "dates": "1653 II 18",
            "start_date": 1653,
            "end_date": 1786,
            "source": "C7",
            "target": "T46"
        },
        {
            "dates": "1656 VIII 16",
            "start_date": 1656,
            "end_date": 1786,
            "source": "C7",
            "target": "T50"
        },
        {
            "dates": "1657 VI 17_27",
            "start_date": 1657,
            "end_date": 1786,
            "source": "C7",
            "target": "T52"
        },
        {
            "dates": "1666 II 11",
            "start_date": 1666,
            "end_date": 1786,
            "source": "C7",
            "target": "T64"
        },
        {
            "dates": "1666 X 15_25",
            "start_date": 1666,
            "end_date": 1786,
            "source": "C7",
            "target": "T69"
        },
        {
            "dates": "1673 V 20",
            "start_date": 1673,
            "end_date": 1786,
            "source": "C7",
            "target": "T89"
        },
        {
            "dates": "1674 VII 10",
            "start_date": 1674,
            "end_date": 1786,
            "source": "C7",
            "target": "T100"
        },
        {
            "dates": "1701 VI 15",
            "start_date": 1701,
            "end_date": 1786,
            "source": "C7",
            "target": "T156"
        },
        {
            "dates": "1701 VI 15",
            "start_date": 1701,
            "end_date": 1786,
            "source": "C7",
            "target": "T157"
        },
        {
            "dates": "1731 IX 3",
            "start_date": 1731,
            "end_date": 1786,
            "source": "C7",
            "target": "T219"
        },
        {
            "dates": "1622 III 10",
            "start_date": 1622,
            "end_date": 1786,
            "source": "T8",
            "target": "C8"
        },
        {
            "dates": "1632 IV 2",
            "start_date": 1632,
            "end_date": 1786,
            "source": "C8",
            "target": "T16"
        },
        {
            "dates": "1636 IX 4",
            "start_date": 1636,
            "end_date": 1786,
            "source": "C8",
            "target": "T19"
        },
        {
            "dates": "1666 II 16",
            "start_date": 1666,
            "end_date": 1786,
            "source": "C8",
            "target": "T65"
        },
        {
            "dates": "1666 II 16",
            "start_date": 1666,
            "end_date": 1786,
            "source": "C8",
            "target": "T66"
        },
        {
            "dates": "1666 X 15_25",
            "start_date": 1666,
            "end_date": 1786,
            "source": "C8",
            "target": "T69"
        },
        {
            "dates": "1672 V 6",
            "start_date": 1672,
            "end_date": 1786,
            "source": "C8",
            "target": "T84"
        },
        {
            "dates": "1674 VI 21 / VII 1",
            "start_date": 1674,
            "end_date": 1786,
            "source": "C8",
            "target": "T99"
        },
        {
            "dates": "1678 II 26_III 8",
            "start_date": 1678,
            "end_date": 1786,
            "source": "C8",
            "target": "T113"
        },
        {
            "dates": "1688 VI 30",
            "start_date": 1688,
            "end_date": 1786,
            "source": "C8",
            "target": "T126"
        },
        {
            "dates": "1695 VIII 8",
            "start_date": 1695,
            "end_date": 1786,
            "source": "C8",
            "target": "T142"
        },
        {
            "dates": "1700 VIII 31",
            "start_date": 1700,
            "end_date": 1786,
            "source": "C8",
            "target": "T154"
        },
        {
            "dates": "1623 VIII 5",
            "start_date": 1623,
            "end_date": 1786,
            "source": "T9",
            "target": "C9"
        },
        {
            "dates": "1624 VI 10",
            "start_date": 1624,
            "end_date": 1786,
            "source": "T10",
            "target": "C10"
        },
        {
            "dates": "1627 VIII 28",
            "start_date": 1627,
            "end_date": 1786,
            "source": "C10",
            "target": "T14"
        },
        {
            "dates": "1630 VI 17",
            "start_date": 1630,
            "end_date": 1786,
            "source": "C10",
            "target": "T15"
        },
        {
            "dates": "1634 IV 15",
            "start_date": 1634,
            "end_date": 1786,
            "source": "C10",
            "target": "T17"
        },
        {
            "dates": "1635 II 8",
            "start_date": 1635,
            "end_date": 1786,
            "source": "C10",
            "target": "T18"
        },
        {
            "dates": "1636 IX 6",
            "start_date": 1636,
            "end_date": 1786,
            "source": "C10",
            "target": "T20"
        },
        {
            "dates": "1637 XII 17",
            "start_date": 1637,
            "end_date": 1786,
            "source": "C10",
            "target": "T21"
        },
        {
            "dates": "1639 III 24",
            "start_date": 1639,
            "end_date": 1786,
            "source": "C10",
            "target": "T22"
        },
        {
            "dates": "1641 II 14",
            "start_date": 1641,
            "end_date": 1786,
            "source": "C10",
            "target": "T24"
        },
        {
            "dates": "1643 III 30",
            "start_date": 1643,
            "end_date": 1786,
            "source": "C10",
            "target": "T26"
        },
        {
            "dates": "1643 VIII 30",
            "start_date": 1643,
            "end_date": 1786,
            "source": "C10",
            "target": "T27"
        },
        {
            "dates": "1644 II 29",
            "start_date": 1644,
            "end_date": 1786,
            "source": "C10",
            "target": "T28"
        },
        {
            "dates": "1644 III 1",
            "start_date": 1644,
            "end_date": 1786,
            "source": "C10",
            "target": "T29"
        },
        {
            "dates": "1645 III 10",
            "start_date": 1645,
            "end_date": 1786,
            "source": "C10",
            "target": "T31"
        },
        {
            "dates": "1645 IV 20",
            "start_date": 1645,
            "end_date": 1786,
            "source": "C10",
            "target": "T33"
        },
        {
            "dates": "1646 IV 6",
            "start_date": 1646,
            "end_date": 1786,
            "source": "C10",
            "target": "T36"
        },
        {
            "dates": "1646 IV 18",
            "start_date": 1646,
            "end_date": 1786,
            "source": "C10",
            "target": "T37"
        },
        {
            "dates": "1646 V 13",
            "start_date": 1646,
            "end_date": 1786,
            "source": "C10",
            "target": "T38"
        },
        {
            "dates": "1647 VII 29",
            "start_date": 1647,
            "end_date": 1786,
            "source": "C10",
            "target": "T41"
        },
        {
            "dates": "1659 V 21",
            "start_date": 1659,
            "end_date": 1786,
            "source": "C10",
            "target": "T53"
        },
        {
            "dates": "1662 IV 27",
            "start_date": 1662,
            "end_date": 1786,
            "source": "C10",
            "target": "T59"
        },
        {
            "dates": "1666 VII 17",
            "start_date": 1666,
            "end_date": 1786,
            "source": "C10",
            "target": "T68"
        },
        {
            "dates": "1668 IV 15",
            "start_date": 1668,
            "end_date": 1786,
            "source": "C10",
            "target": "T78"
        },
        {
            "dates": "1673 III 30",
            "start_date": 1673,
            "end_date": 1786,
            "source": "C10",
            "target": "T87"
        },
        {
            "dates": "1673 X 9",
            "start_date": 1673,
            "end_date": 1786,
            "source": "C10",
            "target": "T93"
        },
        {
            "dates": "1678 VIII 10",
            "start_date": 1678,
            "end_date": 1786,
            "source": "C10",
            "target": "T115"
        },
        {
            "dates": "1678 VIII 10",
            "start_date": 1678,
            "end_date": 1786,
            "source": "C10",
            "target": "T116"
        },
        {
            "dates": "1684 VI 29",
            "start_date": 1684,
            "end_date": 1786,
            "source": "C10",
            "target": "T123"
        },
        {
            "dates": "1697 IX 20",
            "start_date": 1697,
            "end_date": 1786,
            "source": "C10",
            "target": "T145"
        },
        {
            "dates": "1697 IX 20",
            "start_date": 1697,
            "end_date": 1786,
            "source": "C10",
            "target": "T146"
        },
        {
            "dates": "1698 X 11",
            "start_date": 1698,
            "end_date": 1786,
            "source": "C10",
            "target": "T149"
        },
        {
            "dates": "1699 V 29",
            "start_date": 1699,
            "end_date": 1786,
            "source": "C10",
            "target": "T150"
        },
        {
            "dates": "1699 XI 14",
            "start_date": 1699,
            "end_date": 1786,
            "source": "C10",
            "target": "T151"
        },
        {
            "dates": "1700 III 3 und 1700 III 25",
            "start_date": 1700,
            "end_date": 1786,
            "source": "C10",
            "target": "T153"
        },
        {
            "dates": "1709 V 28",
            "start_date": 1709,
            "end_date": 1786,
            "source": "C10",
            "target": "T176"
        },
        {
            "dates": "1713 IV 11",
            "start_date": 1713,
            "end_date": 1786,
            "source": "C10",
            "target": "T188"
        },
        {
            "dates": "1713 IV 11",
            "start_date": 1713,
            "end_date": 1786,
            "source": "C10",
            "target": "T189"
        },
        {
            "dates": "1713 IV 28",
            "start_date": 1713,
            "end_date": 1786,
            "source": "C10",
            "target": "T192"
        },
        {
            "dates": "1713 V 12",
            "start_date": 1713,
            "end_date": 1786,
            "source": "C10",
            "target": "T193"
        },
        {
            "dates": "1713 VI 11",
            "start_date": 1713,
            "end_date": 1786,
            "source": "C10",
            "target": "T194"
        },
        {
            "dates": "1717 I 4",
            "start_date": 1717,
            "end_date": 1786,
            "source": "C10",
            "target": "T202"
        },
        {
            "dates": "1718 VIII 2",
            "start_date": 1718,
            "end_date": 1786,
            "source": "C10",
            "target": "T203"
        },
        {
            "dates": "1726 VIII 9",
            "start_date": 1726,
            "end_date": 1786,
            "source": "C10",
            "target": "T207"
        },
        {
            "dates": "1726 VIII 9",
            "start_date": 1726,
            "end_date": 1786,
            "source": "C10",
            "target": "T208"
        },
        {
            "dates": "1726 VIII 9",
            "start_date": 1726,
            "end_date": 1786,
            "source": "C10",
            "target": "T209"
        },
        {
            "dates": "1726 VIII 9",
            "start_date": 1726,
            "end_date": 1786,
            "source": "C10",
            "target": "T210"
        },
        {
            "dates": "1727 I 13",
            "start_date": 1727,
            "end_date": 1786,
            "source": "C10",
            "target": "T211"
        },
        {
            "dates": "1727 V 31",
            "start_date": 1727,
            "end_date": 1786,
            "source": "C10",
            "target": "T212"
        },
        {
            "dates": "1728 III 6",
            "start_date": 1728,
            "end_date": 1786,
            "source": "C10",
            "target": "T213"
        },
        {
            "dates": "1733 XI 24",
            "start_date": 1733,
            "end_date": 1786,
            "source": "C10",
            "target": "T221"
        },
        {
            "dates": "1739 XII 21",
            "start_date": 1739,
            "end_date": 1786,
            "source": "C10",
            "target": "T222"
        },
        {
            "dates": "1748 IV 30",
            "start_date": 1748,
            "end_date": 1786,
            "source": "C10",
            "target": "T230"
        },
        {
            "dates": "1748 V 31",
            "start_date": 1748,
            "end_date": 1786,
            "source": "C10",
            "target": "T231"
        },
        {
            "dates": "1748 X 18",
            "start_date": 1748,
            "end_date": 1786,
            "source": "C10",
            "target": "T232"
        },
        {
            "dates": "1748 X 18",
            "start_date": 1748,
            "end_date": 1786,
            "source": "C10",
            "target": "T233"
        },
        {
            "dates": "1748 XII 2",
            "start_date": 1748,
            "end_date": 1786,
            "source": "C10",
            "target": "T235"
        },
        {
            "dates": "1749 I 21",
            "start_date": 1749,
            "end_date": 1786,
            "source": "C10",
            "target": "T236"
        },
        {
            "dates": "1773 VII 23",
            "start_date": 1773,
            "end_date": 1786,
            "source": "C10",
            "target": "T242"
        },
        {
            "dates": "1781 V 1",
            "start_date": 1781,
            "end_date": 1786,
            "source": "C10",
            "target": "T245"
        },
        {
            "dates": "1783 I 20",
            "start_date": 1783,
            "end_date": 1786,
            "source": "C10",
            "target": "T247"
        },
        {
            "dates": "1785 XI 10",
            "start_date": 1785,
            "end_date": 1786,
            "source": "C10",
            "target": "T252"
        },
        {
            "dates": "1640 IX 1",
            "start_date": 1640,
            "end_date": 1786,
            "source": "T23",
            "target": "C11"
        },
        {
            "dates": "1645 VIII 15",
            "start_date": 1645,
            "end_date": 1786,
            "source": "C11",
            "target": "T35"
        },
        {
            "dates": "1656 IX 1_11",
            "start_date": 1656,
            "end_date": 1786,
            "source": "C11",
            "target": "T51"
        },
        {
            "dates": "1659 XI 29_XII 9",
            "start_date": 1659,
            "end_date": 1786,
            "source": "C11",
            "target": "T54"
        },
        {
            "dates": "1659 XII 9",
            "start_date": 1659,
            "end_date": 1786,
            "source": "C11",
            "target": "T55"
        },
        {
            "dates": "1660 VI 2_12",
            "start_date": 1660,
            "end_date": 1786,
            "source": "C11",
            "target": "T56"
        },
        {
            "dates": "1665 III 16",
            "start_date": 1665,
            "end_date": 1786,
            "source": "C11",
            "target": "T62"
        },
        {
            "dates": "1666 VII 17",
            "start_date": 1666,
            "end_date": 1786,
            "source": "C11",
            "target": "T68"
        },
        {
            "dates": "1667 VII 6_16",
            "start_date": 1667,
            "end_date": 1786,
            "source": "C11",
            "target": "T70"
        },
        {
            "dates": "1667 VII 18_28",
            "start_date": 1667,
            "end_date": 1786,
            "source": "C11",
            "target": "T71"
        },
        {
            "dates": "1668 I 13_23",
            "start_date": 1668,
            "end_date": 1786,
            "source": "C11",
            "target": "T74"
        },
        {
            "dates": "1668 I 13_23",
            "start_date": 1668,
            "end_date": 1786,
            "source": "C11",
            "target": "T75"
        },
        {
            "dates": "1668 IV 25_V 5",
            "start_date": 1668,
            "end_date": 1786,
            "source": "C11",
            "target": "T79"
        },
        {
            "dates": "1669 V 7",
            "start_date": 1669,
            "end_date": 1786,
            "source": "C11",
            "target": "T80"
        },
        {
            "dates": "1670 I 31",
            "start_date": 1670,
            "end_date": 1786,
            "source": "C11",
            "target": "T82"
        },
        {
            "dates": "1673 V 2",
            "start_date": 1673,
            "end_date": 1786,
            "source": "C11",
            "target": "T88"
        },
        {
            "dates": "1675 XI 26",
            "start_date": 1675,
            "end_date": 1786,
            "source": "C11",
            "target": "T105"
        },
        {
            "dates": "1679 X 2_12",
            "start_date": 1679,
            "end_date": 1786,
            "source": "C11",
            "target": "T117"
        },
        {
            "dates": "1679 X 12",
            "start_date": 1679,
            "end_date": 1786,
            "source": "C11",
            "target": "T118"
        },
        {
            "dates": "1681 IX 30_X 10",
            "start_date": 1681,
            "end_date": 1786,
            "source": "C11",
            "target": "T120"
        },
        {
            "dates": "1683 III 8",
            "start_date": 1683,
            "end_date": 1786,
            "source": "C11",
            "target": "T121"
        },
        {
            "dates": "1683 III 18",
            "start_date": 1683,
            "end_date": 1786,
            "source": "C11",
            "target": "T122"
        },
        {
            "dates": "1686 I 12",
            "start_date": 1686,
            "end_date": 1786,
            "source": "C11",
            "target": "T125"
        },
        {
            "dates": "1688 IX 12",
            "start_date": 1688,
            "end_date": 1786,
            "source": "C11",
            "target": "T128"
        },
        {
            "dates": "1691 XI 19",
            "start_date": 1691,
            "end_date": 1786,
            "source": "C11",
            "target": "T135"
        },
        {
            "dates": "1693 XI 25",
            "start_date": 1693,
            "end_date": 1786,
            "source": "C11",
            "target": "T139"
        },
        {
            "dates": "1698 II 12",
            "start_date": 1698,
            "end_date": 1786,
            "source": "C11",
            "target": "T147"
        },
        {
            "dates": "1698 V 4_14",
            "start_date": 1698,
            "end_date": 1786,
            "source": "C11",
            "target": "T148"
        },
        {
            "dates": "1700 I 13_23 und 1700 I 20_30",
            "start_date": 1700,
            "end_date": 1786,
            "source": "C11",
            "target": "T152"
        },
        {
            "dates": "1701 X 7",
            "start_date": 1701,
            "end_date": 1786,
            "source": "C11",
            "target": "T159"
        },
        {
            "dates": "1703 VIII 5_16",
            "start_date": 1703,
            "end_date": 1786,
            "source": "C11",
            "target": "T170"
        },
        {
            "dates": "1703 VIII 5_16",
            "start_date": 1703,
            "end_date": 1786,
            "source": "C11",
            "target": "T171"
        },
        {
            "dates": "1703 VIII 5_16",
            "start_date": 1703,
            "end_date": 1786,
            "source": "C11",
            "target": "T172"
        },
        {
            "dates": "1641 VI 12",
            "start_date": 1641,
            "end_date": 1786,
            "source": "T25",
            "target": "C12"
        },
        {
            "dates": "1644 XI 14",
            "start_date": 1644,
            "end_date": 1786,
            "source": "C12",
            "target": "T30"
        },
        {
            "dates": "1645 III 27",
            "start_date": 1645,
            "end_date": 1786,
            "source": "C12",
            "target": "T32"
        },
        {
            "dates": "1661 VIII 6",
            "start_date": 1661,
            "end_date": 1786,
            "source": "C12",
            "target": "T57"
        },
        {
            "dates": "1669 VII 30 und 31",
            "start_date": 1669,
            "end_date": 1786,
            "source": "C12",
            "target": "T81"
        },
        {
            "dates": "1692 V 23",
            "start_date": 1692,
            "end_date": 1786,
            "source": "C12",
            "target": "T136"
        },
        {
            "dates": "1703 V 16",
            "start_date": 1703,
            "end_date": 1786,
            "source": "C12",
            "target": "T168"
        },
        {
            "dates": "1705 VIII 7",
            "start_date": 1705,
            "end_date": 1786,
            "source": "C12",
            "target": "T174"
        },
        {
            "dates": "1645 VIII 4",
            "start_date": 1645,
            "end_date": 1786,
            "source": "T34",
            "target": "C13"
        },
        {
            "dates": "1646 X 25",
            "start_date": 1646,
            "end_date": 1786,
            "source": "T39",
            "target": "C14"
        },
        {
            "dates": "1665 IX 19",
            "start_date": 1665,
            "end_date": 1786,
            "source": "T63",
            "target": "C15"
        },
        {
            "dates": "1666 X 15_25",
            "start_date": 1666,
            "end_date": 1786,
            "source": "C15",
            "target": "T69"
        },
        {
            "dates": "1667 VIII 25",
            "start_date": 1667,
            "end_date": 1786,
            "source": "C15",
            "target": "T72"
        },
        {
            "dates": "1668 III 16",
            "start_date": 1668,
            "end_date": 1786,
            "source": "C15",
            "target": "T77"
        },
        {
            "dates": "1674 VI 10_20",
            "start_date": 1674,
            "end_date": 1786,
            "source": "C15",
            "target": "T98"
        },
        {
            "dates": "1695 VIII 8",
            "start_date": 1695,
            "end_date": 1786,
            "source": "C15",
            "target": "T142"
        },
        {
            "dates": "1697 I 18",
            "start_date": 1697,
            "end_date": 1786,
            "source": "C15",
            "target": "T144"
        },
        {
            "dates": "1666 IV 18",
            "start_date": 1666,
            "end_date": 1786,
            "source": "T67",
            "target": "C17"
        },
        {
            "dates": "1674 IV 22",
            "start_date": 1674,
            "end_date": 1786,
            "source": "C17",
            "target": "T96"
        },
        {
            "dates": "1675 X 12",
            "start_date": 1675,
            "end_date": 1786,
            "source": "C17",
            "target": "T104"
        },
        {
            "dates": "1676 X 9",
            "start_date": 1676,
            "end_date": 1786,
            "source": "C17",
            "target": "T110"
        },
        {
            "dates": "1695 III 18",
            "start_date": 1695,
            "end_date": 1786,
            "source": "C17",
            "target": "T141"
        },
        {
            "dates": "1695 VIII 8",
            "start_date": 1695,
            "end_date": 1786,
            "source": "C17",
            "target": "T142"
        },
        {
            "dates": "1701 X 17",
            "start_date": 1701,
            "end_date": 1786,
            "source": "C17",
            "target": "T160"
        },
        {
            "dates": "1703 III 13",
            "start_date": 1703,
            "end_date": 1786,
            "source": "C17",
            "target": "T166"
        },
        {
            "dates": "1703 V 5",
            "start_date": 1703,
            "end_date": 1786,
            "source": "C17",
            "target": "T167"
        },
        {
            "dates": "1708 VII 21",
            "start_date": 1708,
            "end_date": 1786,
            "source": "C17",
            "target": "T175"
        },
        {
            "dates": "1710 VI 2",
            "start_date": 1710,
            "end_date": 1786,
            "source": "C17",
            "target": "T179"
        },
        {
            "dates": "1666 X 15_25",
            "start_date": 1666,
            "end_date": 1786,
            "source": "T69",
            "target": "C16"
        },
        {
            "dates": "1667 VIII 25",
            "start_date": 1667,
            "end_date": 1786,
            "source": "C16",
            "target": "T72"
        },
        {
            "dates": "1668 III 16",
            "start_date": 1668,
            "end_date": 1786,
            "source": "C16",
            "target": "T77"
        },
        {
            "dates": "1675 I 26",
            "start_date": 1675,
            "end_date": 1786,
            "source": "C16",
            "target": "T102"
        },
        {
            "dates": "1676 II 7",
            "start_date": 1676,
            "end_date": 1786,
            "source": "C16",
            "target": "T107"
        },
        {
            "dates": "1671 VIII 18",
            "start_date": 1671,
            "end_date": 1786,
            "source": "T83",
            "target": "C18"
        },
        {
            "dates": "1674 V 11",
            "start_date": 1674,
            "end_date": 1786,
            "source": "C18",
            "target": "T97"
        },
        {
            "dates": "1722 VI 2",
            "start_date": 1722,
            "end_date": 1786,
            "source": "C18",
            "target": "T205"
        },
        {
            "dates": "1744 VII 4",
            "start_date": 1744,
            "end_date": 1786,
            "source": "C18",
            "target": "T223"
        },
        {
            "dates": "1750 II 27",
            "start_date": 1750,
            "end_date": 1786,
            "source": "C18",
            "target": "T237"
        },
        {
            "dates": "1770 X 13",
            "start_date": 1770,
            "end_date": 1786,
            "source": "C18",
            "target": "T241"
        },
        {
            "dates": "1782 IV 19",
            "start_date": 1782,
            "end_date": 1786,
            "source": "C18",
            "target": "T246"
        },
        {
            "dates": "1784 X 30",
            "start_date": 1784,
            "end_date": 1786,
            "source": "C18",
            "target": "T250"
        },
        {
            "dates": "1672 VII 25",
            "start_date": 1672,
            "end_date": 1786,
            "source": "T85",
            "target": "C19"
        },
        {
            "dates": "1672 IX 22",
            "start_date": 1672,
            "end_date": 1786,
            "source": "C19",
            "target": "T86"
        },
        {
            "dates": "1673 VII 1",
            "start_date": 1673,
            "end_date": 1786,
            "source": "C19",
            "target": "T90"
        },
        {
            "dates": "1673 VIII 30",
            "start_date": 1673,
            "end_date": 1786,
            "source": "C19",
            "target": "T91"
        },
        {
            "dates": "1674 VI 10_20",
            "start_date": 1674,
            "end_date": 1786,
            "source": "C19",
            "target": "T98"
        },
        {
            "dates": "1674 VI 21 / VII 1",
            "start_date": 1674,
            "end_date": 1786,
            "source": "C19",
            "target": "T99"
        },
        {
            "dates": "1674 VII 10",
            "start_date": 1674,
            "end_date": 1786,
            "source": "C19",
            "target": "T100"
        },
        {
            "dates": "1675 I 26",
            "start_date": 1675,
            "end_date": 1786,
            "source": "C19",
            "target": "T102"
        },
        {
            "dates": "1676 II 7",
            "start_date": 1676,
            "end_date": 1786,
            "source": "C19",
            "target": "T107"
        },
        {
            "dates": "1683 III 18",
            "start_date": 1683,
            "end_date": 1786,
            "source": "C19",
            "target": "T122"
        },
        {
            "dates": "1688 IX 4",
            "start_date": 1688,
            "end_date": 1786,
            "source": "C19",
            "target": "T127"
        },
        {
            "dates": "1689 V 12",
            "start_date": 1689,
            "end_date": 1786,
            "source": "C19",
            "target": "T130"
        },
        {
            "dates": "1695 III 18",
            "start_date": 1695,
            "end_date": 1786,
            "source": "C19",
            "target": "T141"
        },
        {
            "dates": "1695 VIII 8",
            "start_date": 1695,
            "end_date": 1786,
            "source": "C19",
            "target": "T142"
        },
        {
            "dates": "1701 IX 7",
            "start_date": 1701,
            "end_date": 1786,
            "source": "C19",
            "target": "T158"
        },
        {
            "dates": "1702 IV 12",
            "start_date": 1702,
            "end_date": 1786,
            "source": "C19",
            "target": "T165"
        },
        {
            "dates": "1703 V 5",
            "start_date": 1703,
            "end_date": 1786,
            "source": "C19",
            "target": "T167"
        },
        {
            "dates": "1703 V 16",
            "start_date": 1703,
            "end_date": 1786,
            "source": "C19",
            "target": "T168"
        },
        {
            "dates": "1709 V 28",
            "start_date": 1709,
            "end_date": 1786,
            "source": "C19",
            "target": "T176"
        },
        {
            "dates": "1710 III 31",
            "start_date": 1710,
            "end_date": 1786,
            "source": "C19",
            "target": "T178"
        },
        {
            "dates": "1710 VIII 4",
            "start_date": 1710,
            "end_date": 1786,
            "source": "C19",
            "target": "T180"
        },
        {
            "dates": "1712 VI 27",
            "start_date": 1712,
            "end_date": 1786,
            "source": "C19",
            "target": "T184"
        },
        {
            "dates": "1712 VI 27",
            "start_date": 1712,
            "end_date": 1786,
            "source": "C19",
            "target": "T185"
        },
        {
            "dates": "1715 XI 15",
            "start_date": 1715,
            "end_date": 1786,
            "source": "C19",
            "target": "T198"
        },
        {
            "dates": "1718 XII 22",
            "start_date": 1718,
            "end_date": 1786,
            "source": "C19",
            "target": "T204"
        },
        {
            "dates": "1728 III 6",
            "start_date": 1728,
            "end_date": 1786,
            "source": "C19",
            "target": "T213"
        },
        {
            "dates": "1785 IX 20",
            "start_date": 1785,
            "end_date": 1786,
            "source": "C19",
            "target": "T251"
        },
        {
            "dates": "1673 VII 1",
            "start_date": 1673,
            "end_date": 1786,
            "source": "T90",
            "target": "C20"
        },
        {
            "dates": "1695 VIII 8",
            "start_date": 1695,
            "end_date": 1786,
            "source": "C20",
            "target": "T142"
        },
        {
            "dates": "1674 VI 10_20",
            "start_date": 1674,
            "end_date": 1786,
            "source": "T98",
            "target": "C21"
        },
        {
            "dates": "1691 V 14",
            "start_date": 1691,
            "end_date": 1786,
            "source": "C21",
            "target": "T134"
        },
        {
            "dates": "1694 V 21",
            "start_date": 1694,
            "end_date": 1786,
            "source": "C21",
            "target": "T140"
        },
        {
            "dates": "1676 III 26",
            "start_date": 1676,
            "end_date": 1786,
            "source": "T108",
            "target": "C22"
        },
        {
            "dates": "1676 IV 26",
            "start_date": 1676,
            "end_date": 1786,
            "source": "C22",
            "target": "T109"
        },
        {
            "dates": "1680 IX 15 / 1091 AH",
            "start_date": 1680,
            "end_date": 1786,
            "source": "T119",
            "target": "C23"
        },
        {
            "dates": "1690 I 27",
            "start_date": 1690,
            "end_date": 1786,
            "source": "T133",
            "target": "C24"
        },
        {
            "dates": "1702 I 5",
            "start_date": 1702,
            "end_date": 1786,
            "source": "C24",
            "target": "T163"
        },
        {
            "dates": "1702 II 7",
            "start_date": 1702,
            "end_date": 1786,
            "source": "C24",
            "target": "T164"
        },
        {
            "dates": "1692 VI 30",
            "start_date": 1692,
            "end_date": 1786,
            "source": "T137",
            "target": "C25"
        },
        {
            "dates": "1692 XII 22",
            "start_date": 1692,
            "end_date": 1786,
            "source": "T138",
            "target": "C26"
        },
        {
            "dates": "1695 VIII 8",
            "start_date": 1695,
            "end_date": 1786,
            "source": "C26",
            "target": "T142"
        },
        {
            "dates": "1697 I 18",
            "start_date": 1697,
            "end_date": 1786,
            "source": "C26",
            "target": "T144"
        },
        {
            "dates": "1695 VIII 8",
            "start_date": 1695,
            "end_date": 1786,
            "source": "T142",
            "target": "C29"
        },
        {
            "dates": "1695 VIII 8",
            "start_date": 1695,
            "end_date": 1786,
            "source": "T142",
            "target": "C28"
        },
        {
            "dates": "1695 VIII 8",
            "start_date": 1695,
            "end_date": 1786,
            "source": "T142",
            "target": "C27"
        },
        {
            "dates": "1746 VII 21",
            "start_date": 1746,
            "end_date": 1786,
            "source": "C29",
            "target": "T225"
        },
        {
            "dates": "1750 VIII 11_22",
            "start_date": 1750,
            "end_date": 1786,
            "source": "C29",
            "target": "T238"
        },
        {
            "dates": "1701 V 26",
            "start_date": 1701,
            "end_date": 1786,
            "source": "C28",
            "target": "T155"
        },
        {
            "dates": "1705 I 21",
            "start_date": 1705,
            "end_date": 1786,
            "source": "C27",
            "target": "T173"
        },
        {
            "dates": "1712 VI 27",
            "start_date": 1712,
            "end_date": 1786,
            "source": "C27",
            "target": "T184"
        },
        {
            "dates": "1712 VI 27",
            "start_date": 1712,
            "end_date": 1786,
            "source": "C27",
            "target": "T185"
        },
        {
            "dates": "1696 V 14",
            "start_date": 1696,
            "end_date": 1786,
            "source": "T143",
            "target": "C30"
        },
        {
            "dates": "1701 XII 30",
            "start_date": 1701,
            "end_date": 1786,
            "source": "T162",
            "target": "C31"
        },
        {
            "dates": "1711 VIII 1",
            "start_date": 1711,
            "end_date": 1786,
            "source": "C31",
            "target": "T182"
        },
        {
            "dates": "1714 VII 28",
            "start_date": 1714,
            "end_date": 1786,
            "source": "C31",
            "target": "T197"
        },
        {
            "dates": "1716 XI 20",
            "start_date": 1716,
            "end_date": 1786,
            "source": "C31",
            "target": "T201"
        },
        {
            "dates": "1710 VI 2",
            "start_date": 1710,
            "end_date": 1786,
            "source": "T179",
            "target": "C32"
        },
        {
            "dates": "1711 VIII [ohne Tag]",
            "start_date": 1711,
            "end_date": 1786,
            "source": "T181",
            "target": "C38"
        },
        {
            "dates": "1711 VIII [ohne Tag]",
            "start_date": 1711,
            "end_date": 1786,
            "source": "T181",
            "target": "C33"
        },
        {
            "dates": "1711 VIII [ohne Tag]",
            "start_date": 1711,
            "end_date": 1786,
            "source": "T181",
            "target": "C39"
        },
        {
            "dates": "1711 VIII [ohne Tag]",
            "start_date": 1711,
            "end_date": 1786,
            "source": "T181",
            "target": "C37"
        },
        {
            "dates": "1711 VIII [ohne Tag]",
            "start_date": 1711,
            "end_date": 1786,
            "source": "T181",
            "target": "C35"
        },
        {
            "dates": "1711 VIII [ohne Tag]",
            "start_date": 1711,
            "end_date": 1786,
            "source": "T181",
            "target": "C36"
        },
        {
            "dates": "1711 VIII [ohne Tag]",
            "start_date": 1711,
            "end_date": 1786,
            "source": "T181",
            "target": "C34"
        },
        {
            "dates": "1712 VI 21",
            "start_date": 1712,
            "end_date": 1786,
            "source": "T183",
            "target": "C40"
        },
        {
            "dates": "1713 IV 19",
            "start_date": 1713,
            "end_date": 1786,
            "source": "T190",
            "target": "C41"
        },
        {
            "dates": "1713 IV 9_20",
            "start_date": 1713,
            "end_date": 1786,
            "source": "C41",
            "target": "T191"
        },
        {
            "dates": "1718 VIII 2",
            "start_date": 1718,
            "end_date": 1786,
            "source": "T203",
            "target": "C42"
        },
        {
            "dates": "1727 V 31",
            "start_date": 1727,
            "end_date": 1786,
            "source": "C42",
            "target": "T212"
        },
        {
            "dates": "1745 I 8",
            "start_date": 1745,
            "end_date": 1786,
            "source": "T224",
            "target": "C44"
        },
        {
            "dates": "1745 I 8",
            "start_date": 1745,
            "end_date": 1786,
            "source": "T224",
            "target": "C43"
        },
        {
            "dates": "1746 VIII 31",
            "start_date": 1746,
            "end_date": 1786,
            "source": "C44",
            "target": "T226"
        },
        {
            "dates": "1748 I 26",
            "start_date": 1748,
            "end_date": 1786,
            "source": "C44",
            "target": "T228"
        },
        {
            "dates": "1748 X 18",
            "start_date": 1748,
            "end_date": 1786,
            "source": "C44",
            "target": "T232"
        },
        {
            "dates": "1748 XII 2",
            "start_date": 1748,
            "end_date": 1786,
            "source": "C44",
            "target": "T235"
        },
        {
            "dates": "1749 I 21",
            "start_date": 1749,
            "end_date": 1786,
            "source": "C44",
            "target": "T236"
        },
        {
            "dates": "1751 IX 13",
            "start_date": 1751,
            "end_date": 1786,
            "source": "C43",
            "target": "T239"
        },
        {
            "dates": "1747 XI 19",
            "start_date": 1747,
            "end_date": 1786,
            "source": "T227",
            "target": "C45"
        },
        {
            "dates": "1748 I 26",
            "start_date": 1748,
            "end_date": 1786,
            "source": "T228",
            "target": "C47"
        },
        {
            "dates": "1748 I 26",
            "start_date": 1748,
            "end_date": 1786,
            "source": "C47",
            "target": "T229"
        },
        {
            "dates": "1748 V 31",
            "start_date": 1748,
            "end_date": 1786,
            "source": "C47",
            "target": "T231"
        },
        {
            "dates": "1748 X 18",
            "start_date": 1748,
            "end_date": 1786,
            "source": "C47",
            "target": "T232"
        },
        {
            "dates": "1748 XII 2",
            "start_date": 1748,
            "end_date": 1786,
            "source": "C47",
            "target": "T235"
        },
        {
            "dates": "1749 I 21",
            "start_date": 1749,
            "end_date": 1786,
            "source": "C47",
            "target": "T236"
        },
        {
            "dates": "1748 X 18",
            "start_date": 1748,
            "end_date": 1786,
            "source": "T232",
            "target": "C46"
        },
        {
            "dates": "1748 X 18",
            "start_date": 1748,
            "end_date": 1786,
            "source": "T232",
            "target": "C48"
        },
        {
            "dates": "1748 X 28",
            "start_date": 1748,
            "end_date": 1786,
            "source": "C46",
            "target": "T234"
        },
        {
            "dates": "1748 XII 2",
            "start_date": 1748,
            "end_date": 1786,
            "source": "C46",
            "target": "T235"
        },
        {
            "dates": "1749 I 21",
            "start_date": 1749,
            "end_date": 1786,
            "source": "C46",
            "target": "T236"
        },
        {
            "dates": "1748 XII 2",
            "start_date": 1748,
            "end_date": 1786,
            "source": "C48",
            "target": "T235"
        },
        {
            "dates": "1749 I 21",
            "start_date": 1749,
            "end_date": 1786,
            "source": "C48",
            "target": "T236"
        },
        {
            "dates": "1753 VIII 27",
            "start_date": 1753,
            "end_date": 1786,
            "source": "T240",
            "target": "C49"
        }
    ]
}

# kiara\kiara_plugin.network_analysis\examples\data\JSON\Readme.md
# A collection of sample JSON files for testing


### peacetreaties
This file was created by [Lena Jaskov](https://github.com/yaslena). It is is based on data that is available online in a database focused on European peace treaties from 1450 to 1789 (https://www.ieg-friedensvertraege.de/vertraege).

The database was created as part of a project that was funded by the DFG (German Research Foundation)
and contains a selection of some 1800 treaties. One extra treaty between Tsarist Russia and the Qing Empire (1689) was manually added by Lena Jaskov.

The data sample here only shows a selection of that data, i.e. filterd on treaties that involve Russia.
There are only edges between treaties and countries. If a country was involved in a treaty, there will be an edge between that country and the respective treaty.

The data was first scraped into a table in a first step. The original scraped table (CSV) can be found in this [repository](https://github.com/DHARPA-Project/kiara_plugin.network_analysis/blob/develop/examples/data/treaties). The scraping code can be found [here](https://github.com/yaslena/WebScraping).
In a second step the data from the scraping process was restructured to fit the requirements of creating a dynamic bipartite graph with networkX.
Code for restructuring the table data and for generating a bipartite graph with python networkX can be found [here](https://github.com/yaslena/NetworkAnalysis).
The restructured data was converted into JSON with networkX for visualization in JavaScript on [Observable](https://observablehq.com/@yaslena/dynamic-network-graph).

The data represents a **two-mode** graph that is **dynamic** ('time-specific'), **bipartite**, **undirected** and **unweighted**. The bipartite graph makes it possible to create a **projected** **one-mode** graph that would consist of only countries as nodes and have connections between them when they have a teaty in common.

- 120 nodes (two types, countries and treaties)
- 203 edges (undirected)

### radicaltranslations

This file is too large, it is therefore located in a different [repository](https://github.com/DHARPA-Project/kiara.examples/tree/main/examples/data/network_analysis/JSON).

This network is based on the work of The Radical Translations Project (https://radicaltranslations.org/).

An interactive visualisation can be found here:

https://observablehq.com/@jmiguelv/radical-translations-agents-network-visualisation

The visualisation shows a network of – *person (f), person (m), person (u), organisation, place, serial publication* – nodes, and how they are connected to one another via the relationships *based in (place), edited, knows, member of, published, published in (place), translated*. The size of the nodes corresponds to the number of connections the node has.

This network represents a **multi-mode network** with **several node-types** (person, organisation, place, serial publication) and **several edge (or relationship) types** (based_in, edited, knows, member of, published, translated..)

The dataset is a **json file** that seems to be the output of the the project's online database (https://radicaltranslations.org/about/database/). There is also **zip file** available for download on the website. The zip file contains several CSV files, one for each of the main data types (Agents, Events, Places and Resources) in the project.

- 1495 nodes (multiple types)
- 6675 edges (multiple types)


# kiara\kiara_plugin.network_analysis\examples\data\quakers\Readme.md
This data comes from a programming historian tutorial on network analysis with python.
For more context see the Readme file under quakers [here](https://github.com/DHARPA-Project/kiara_plugin.network_analysis/tree/develop/examples/data/gexf).


# kiara\kiara_plugin.network_analysis\examples\data\treaties\Readme.md
This data was created by [Lena Jaskov](https://github.com/yaslena).
For more context see under *peacetreaties* [here](https://github.com/DHARPA-Project/kiara_plugin.network_analysis/blob/develop/examples/data/JSON/Readme.md).


# kiara\kiara_plugin.network_analysis\examples\jobs\create_journals_network.yaml
operation: "${this_dir}/../pipelines/create_network_graph.yaml"
inputs:
  edges_file: "${this_dir}/../data/journals/JournalEdges1902.csv"
  nodes_file: "${this_dir}/../data/journals/JournalNodes1902.csv"
doc: |
    Create a network graph of historical medical Journals and the relations between them.

    Input will be taken from 2 csv files, containing nodes and edges, respectively.

save:
  network_data: journals_network


# kiara\kiara_plugin.network_analysis\examples\jobs\create_simple_disconnected_network.yaml
operation: "${this_dir}/../pipelines/create_network_graph.yaml"
inputs:
  edges_file: "${this_dir}/../data/simple_networks/two_components/SampleEdges.csv"
  nodes_file: "${this_dir}/../data/simple_networks/two_components/SampleNodes.csv"
doc: |
  Create a simple 6 node network with 2 components.
save:
  network_data: simple_disconnected


# kiara\kiara_plugin.network_analysis\examples\jobs\create_simple_network.yaml
operation: "${this_dir}/../pipelines/create_network_graph.yaml"
inputs:
  edges_file: "${this_dir}/../data/simple_networks/connected/SampleEdges.csv"
  nodes_file: "${this_dir}/../data/simple_networks/connected/SampleNodes.csv"
save:
  network_data: simple_network


# kiara\kiara_plugin.network_analysis\examples\jobs\Readme.md
A folder to place example pipelines that are relevant for this plugin. It can be used subsequently for unit tests, and in documentation generation.


# kiara\kiara_plugin.network_analysis\examples\pipelines\create_network_graph.yaml
pipeline_name: create_network_graph
doc: Onboard network data
steps:
  - module_type: import.local.file
    step_id: import_edges_file
  - module_type: create.table.from.file
    step_id: create_edges_table
    input_links:
      file: import_edges_file.file
  - module_type: import.local.file
    step_id: import_nodes_file
  - module_type: create.table.from.file
    step_id: create_nodes_table
    input_links:
      file: import_nodes_file.file
  - module_type: assemble.network_data
    step_id: assemble_network_data
    input_links:
      edges: create_edges_table.table
      nodes: create_nodes_table.table

input_aliases:
  import_edges_file.path: edges_file
  import_nodes_file.path: nodes_file
  assemble_network_data.source_column: source_column
  assemble_network_data.target_column: target_column
  assemble_network_data.edges_column_map: edges_column_map
  assemble_network_data.id_column: id_column
  assemble_network_data.label_column: label_column
  assemble_network_data.nodes_column_map: nodes_column_map
  create_edges_table.first_row_is_header: first_edges_row_is_header
  create_nodes_table.first_row_is_header: first_nodes_row_is_header
output_aliases:
  assemble_network_data.network_data: network_data

#defaults:
#  edges_file: "${pipeline_dir}/../data/journals/JournalEdges1902.csv"
#  nodes_file: "${pipeline_dir}/../data/journals/JournalNodes1902.csv"


# kiara\kiara_plugin.network_analysis\examples\pipelines\Readme.md
A folder to place example pipelines that are relevant for this plugin. It can be used subsequently for unit tests, and in documentation generation.


# kiara\kiara_plugin.network_analysis\scripts\documentation\gen_api_doc_pages.py
# -*- coding: utf-8 -*-

"""Generate the code reference pages and navigation."""

from pathlib import Path

import mkdocs_gen_files

nav = mkdocs_gen_files.Nav()

for path in sorted(Path("src").rglob("*.py")):
    module_path = path.relative_to("src").with_suffix("")
    doc_path = path.relative_to("src").with_suffix(".md")
    full_doc_path = Path("reference", doc_path)

    parts = list(module_path.parts)

    if parts[-1] == "__init__":
        parts = parts[:-1]
    elif parts[-1] == "__main__":
        continue

    nav[parts] = doc_path  #

    with mkdocs_gen_files.open(full_doc_path, "w") as fd:
        ident = ".".join(parts)
        print("::: " + ident, file=fd)

    mkdocs_gen_files.set_edit_path(full_doc_path, path)

with mkdocs_gen_files.open("reference/SUMMARY.md", "w") as nav_file:  #
    nav_file.writelines(nav.build_literate_nav())  #


# kiara\kiara_plugin.network_analysis\scripts\documentation\gen_info_pages.py
# -*- coding: utf-8 -*-
#  Copyright (c) 2022-2022, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import builtins

from kiara.context import Kiara, KiaraContextInfo
from kiara.doc.gen_info_pages import generate_detail_pages

pkg_name = "kiara_plugin.network_analysis"

kiara: Kiara = Kiara.instance()
context_info = KiaraContextInfo.create_from_kiara_instance(
    kiara=kiara, package_filter=pkg_name
)

generate_detail_pages(context_info=context_info)

builtins.plugin_package_context_info = context_info


# kiara\kiara_plugin.network_analysis\scripts\documentation\gen_module_doc.py
# -*- coding: utf-8 -*-
import os

import mkdocs_gen_files

from kiara.context import Kiara

kiara = Kiara.instance()

modules_file_path = os.path.join("modules_list.md")
modules_page_content = """# Available module types

This page contains a list of all available *Kiara* module types, and their details.

!!! note
The formatting here will be improved later on, for now this should be enough to get the important details of each module type.

"""

BASE_PACKAGE = "kiara_plugin.network_analysis"


for module_type in kiara.module_mgmt.find_modules_for_package(
    BASE_PACKAGE, include_pipelines=False
).keys():

    if module_type == "pipeline":
        continue

    modules_page_content = modules_page_content + f"## ``{module_type}``\n\n"
    modules_page_content = (
        modules_page_content
        + "```\n{{ get_module_info('"
        + module_type
        + "') }}\n```\n\n"
    )

with mkdocs_gen_files.open(modules_file_path, "w") as f:
    f.write(modules_page_content)

pipelines_file_path = os.path.join("pipelines_list.md")
pipelines_page_content = """# Available pipeline module types

This page contains a list of all available *Kiara* pipeline module types, and their details.

!!! note
The formatting here will be improved later on, for now this should be enough to get the important details of each module type.

"""

for module_type in kiara.module_mgmt.find_modules_for_package(
    BASE_PACKAGE, include_core_modules=False
):

    if module_type == "pipeline":
        continue

    pipelines_page_content = pipelines_page_content + f"## ``{module_type}``\n\n"
    pipelines_page_content = (
        pipelines_page_content
        + "```\n{{ get_module_info('"
        + module_type
        + "') }}\n```\n\n"
    )

with mkdocs_gen_files.open(pipelines_file_path, "w") as f:
    f.write(pipelines_page_content)


# kiara\kiara_plugin.network_analysis\src\kiara_plugin\network_analysis\defaults.py
# -*- coding: utf-8 -*-
#  Copyright (c) 2022, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)
from enum import Enum
from typing import Literal

NODE_ID_COLUMN_NAME = "_node_id"
EDGE_ID_COLUMN_NAME = "_edge_id"
SOURCE_COLUMN_NAME = "_source"
TARGET_COLUMN_NAME = "_target"
LABEL_COLUMN_NAME = "_label"

COMPONENT_ID_COLUMN_NAME = "component_id"
IS_CUTPOINT_COLUMN_NAME = "is_cut_point"

EDGES_TABLE_NAME = "edges"
NODES_TABLE_NAME = "nodes"


DEFAULT_NETWORK_DATA_CHUNK_SIZE = 1024

NODE_ID_ALIAS_NAMES = ["id", "node_id"]
LABEL_ALIAS_NAMES = ["label", "node_label"]

SOURCE_COLUMN_ALIAS_NAMES = ["source", "sources", "source_id", "from", "sender"]
TARGET_COLUMN_ALIAS_NAMES = ["target", "targets", "target_id", "to", "receiver"]

ATTRIBUTE_PROPERTY_KEY = "attribute_property"

# WEIGHT_COLUMN_ALIAS_NAMES = [
#     "weight",
#     "weights",
#     "edge_weight",
#     "edge_weights",
#     "strength",
#     "strengths",
# ]

COUNT_DIRECTED_COLUMN_NAME = "_count_dup_directed"
COUNT_UNDIRECTED_COLUMN_NAME = "_count_dup_undirected"
COUNT_IDX_DIRECTED_COLUMN_NAME = "_idx_dup_directed"
COUNT_IDX_UNDIRECTED_COLUMN_NAME = "_idx_dup_undirected"

IN_DIRECTED_COLUMN_NAME = "_in_edges"
OUT_DIRECTED_COLUMN_NAME = "_out_edges"
CONNECTIONS_COLUMN_NAME = "_count_edges"

IN_DIRECTED_MULTI_COLUMN_NAME = "_in_edges_multi"
OUT_DIRECTED_MULTI_COLUMN_NAME = "_out_edges_multi"
CONNECTIONS_MULTI_COLUMN_NAME = "_count_edges_multi"


RANKING_TABLE_NAME = "ranking"
RANKING_COLUNN_NAME = "_rank"
RANKING_VALUE_COLUMN_NAME = "_value"

AUTO_CALCULATED_EDGE_COLUMNS = [
    EDGE_ID_COLUMN_NAME,
    SOURCE_COLUMN_NAME,
    TARGET_COLUMN_NAME,
    COUNT_DIRECTED_COLUMN_NAME,
    COUNT_IDX_DIRECTED_COLUMN_NAME,
    COUNT_UNDIRECTED_COLUMN_NAME,
    COUNT_IDX_UNDIRECTED_COLUMN_NAME,
]


AUTO_CALCULATED_NODE_COLUMNS = [
    NODE_ID_COLUMN_NAME,
    LABEL_COLUMN_NAME,
    IN_DIRECTED_COLUMN_NAME,
    OUT_DIRECTED_COLUMN_NAME,
    IN_DIRECTED_MULTI_COLUMN_NAME,
    OUT_DIRECTED_MULTI_COLUMN_NAME,
]


class NetworkDataTableType(Enum):
    EDGES = EDGES_TABLE_NAME
    NODES = NODES_TABLE_NAME


class GraphType(Enum):
    UNDIRECTED = "undirected"
    DIRECTED = "directed"
    DIRECTED_MULTI = "directed_multi"
    UNDIRECTED_MULTI = "undirected_multi"


NODE_ID_TEXT = """The unique id for the node.

This is a unique integer identifier (counting up from 0) and is automatically generated by kiara, for each `network_data` value.
"""
NODE_LABEL_TEXT = """The label for the node.

This is a (potentially non-unique) (ideally) human meaningful lable for the node, mostly used in visualizations. Depending on
how the 'network_data' was created, this could be a name, title, etc. If no such label was available or specified
by the user, the node id will be used as label.
"""

NODE_COUNT_EDGES_TEXT = """The number of edges that are connected to this node if the network_data is interpreted as a non-multi graph

Both incoming and outgoing edges are counted, which means that the number is valid for both directed and undirected graphs.
."""
NODE_COUNT_EDGES_MULTI_TEXT = """The number of edges that are connected to this node if the network_data is interpreted as a multi graph

Both incoming and outgoing edges are counted, which means that the number is valid for both directed and undirected graphs."""
NODE_COUNT_IN_EDGES_TEXT = """The number of incoming edges that are connected to this node if the network_data is interpreted as a non-multi graph."""
NODE_COUNT_IN_EDGES_MULTI_TEXT = """The number of incoming edges that are connected to this node if the network_data is interpreted as a multi graph."""
NODE_COUNT_OUT_EDGES_TEXT = """The number of outgoing edges that are connected to this node if the network_data is interpreted as a non-multi graph."""
NODE_COUNT_OUT_EDGES_MULTI_TEXT = """The number of outgoing edges that are connected to this node if the network_data is interpreted as a multi graph."""

EDGE_ID_TEXT = """The unique id for the edge.

This is a unique integer identifier (counting up from 0) and is automatically generated by kiara, for each `network_data` value.
"""
EDGE_SOURCE_TEXT = """The node id of the source for an edge."""
EDGE_TARGET_TEXT = """The node id of the target for an edge."""

EDGE_COUNT_DUP_DIRECTED_TEXT = """The number of edges that have the same source/target combination as this (incl. this), if the network_data is interpreted as directed multi graph.
"""
EDGE_IDX_DUP_DIRECTED_TEXT = """A unique index for this edge within its set of duplicates, if the network_data is interpreted as directed multi graph.

This is a unique integer identifier in combination with (_source/_target), counting up from 1. The order of the edges within this set is not guaranteed.
"""
EDGE_COUNT_DUP_UNDIRECTED_TEXT = """The number of edges that have the same source/target combination as this (incl. this), if the network_data is interpreted as undirected multi graph."""
EDGE_IDX_DUP_UNDIRECTED_TEXT = """A unique index for this edge within its set of duplicates, if the network_data is interpreted as undirected multi graph.

This is a unique integer identifier in combination with (_source/_target), counting up from 1. The order of the edges within this set is not guaranteed.
"""


ALLOWED_AGGREGATION_FUNCTIONS = {
    "group_by": "Don't aggregate on this column, but keep it as is and use it in the group by clause.",
    "any_val": "Returns the first non-null value",
    "avg": "Calculates the average value for all tuples in arg.",
    "bool_and": "Returns TRUE if every input value is TRUE, otherwise FALSE.",
    "bool_or": "Returns TRUE if any input value is TRUE, otherwise FALSE.",
    "count": "Returns the number of input values.",
    "favg": "Calculates the average using a more accurate floating point summation (Kahan Sum).",
    "first": "Returns the first value of a column.",
    "fsum": "Calculates the sum using a more accurate floating point summation (Kahan Sum).",
    "histogram": "Returns a LIST of STRUCTs with the fields bucket and count.",
    "last": "Returns the last value of a column.",
    "list": "Returns a LIST containing all the values of a column.",
    "max": "Returns the maximum value present in the column.",
    "min": "Returns the minimum value present in the column.",
    "product": "Returns the product of all tuples in the column.",
    "string_agg_comma": "Concatenates the column string values with a comma separator.",
    "sum": "Calculates the sum value for all tuples in arg.",
}
AGGREGATION_FUNCTION_NAME = Literal[tuple(ALLOWED_AGGREGATION_FUNCTIONS.keys())]  # type: ignore


DEFAULT_UNWEIGHTED_NODE_DEGREE_COLUMN_NAME = "_degree_unweighted"
UNWEIGHTED_NODE_DEGREE_TEXT = (
    """The degree of a node is the number of edges connected to the node."""
)
UNWEIGHTED_DEGREE_CENTRALITY_COLUMN_NAME = "_degree_centrality"
UNWEIGHTED_DEGREE_CENTRALITY_MULTI_COLUMN_NAME = "_degree_centrality_multi"
UNWEIGHTED_DEGREE_CENTRALITY_TEXT = """The degree centrality values are normalized by dividing the degree of a node by the maximum possible degree in a simple graph n-1 where n is the number of nodes in the graph. For multigraphs or graphs with self loops the maximum degree might be higher than n-1 and values of degree centrality greater than 1 are possible."""


# kiara\kiara_plugin.network_analysis\src\kiara_plugin\network_analysis\utils.py
# -*- coding: utf-8 -*-
#  Copyright (c) 2022, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)
from typing import (
    TYPE_CHECKING,
    Any,
    Dict,
    Hashable,
    Iterable,
    List,
    Tuple,
    Union,
)

from kiara.exceptions import KiaraException
from kiara_plugin.network_analysis.defaults import (
    CONNECTIONS_COLUMN_NAME,
    CONNECTIONS_MULTI_COLUMN_NAME,
    COUNT_DIRECTED_COLUMN_NAME,
    COUNT_IDX_DIRECTED_COLUMN_NAME,
    COUNT_IDX_UNDIRECTED_COLUMN_NAME,
    COUNT_UNDIRECTED_COLUMN_NAME,
    EDGE_ID_COLUMN_NAME,
    IN_DIRECTED_COLUMN_NAME,
    IN_DIRECTED_MULTI_COLUMN_NAME,
    LABEL_COLUMN_NAME,
    NODE_ID_COLUMN_NAME,
    OUT_DIRECTED_COLUMN_NAME,
    OUT_DIRECTED_MULTI_COLUMN_NAME,
    SOURCE_COLUMN_NAME,
    TARGET_COLUMN_NAME,
    UNWEIGHTED_DEGREE_CENTRALITY_COLUMN_NAME,
    UNWEIGHTED_DEGREE_CENTRALITY_MULTI_COLUMN_NAME,
)

if TYPE_CHECKING:
    import networkx as nx
    import polars as pl
    import pyarrow as pa
    from sqlalchemy import MetaData, Table  # noqa


def extract_networkx_nodes_as_table(
    graph: "nx.Graph",
    label_attr_name: Union[str, None, Iterable[str]] = None,
    ignore_attributes: Union[None, Iterable[str]] = None,
) -> Tuple["pa.Table", Dict[Hashable, int]]:
    """Extract the nodes of a networkx graph as a pyarrow table.

    Arguments:
        graph: the networkx graph
        label_attr_name: the name of the node attribute that should be used as label. If None, the node id is used.
        ignore_attributes: a list of node attributes that should be ignored and not added to the table

    Returns:
        a tuple with the table and a map containing the original node id as key and the newly created internal node id (int) as value
    """
    # adapted from networx code
    # License: 3-clause BSD license
    # Copyright (C) 2004-2022, NetworkX Developers

    import pyarrow as pa

    # nan = float("nan")

    nodes: Dict[str, List[Any]] = {
        NODE_ID_COLUMN_NAME: [],
        LABEL_COLUMN_NAME: [],
    }
    nodes_map = {}

    for i, (node_id, node_data) in enumerate(graph.nodes(data=True)):
        nodes[NODE_ID_COLUMN_NAME].append(i)
        if label_attr_name is None:
            nodes[LABEL_COLUMN_NAME].append(str(node_id))
        elif isinstance(label_attr_name, str):
            label = node_data.get(label_attr_name, None)
            if label:
                nodes[LABEL_COLUMN_NAME].append(str(label))
            else:
                nodes[LABEL_COLUMN_NAME].append(str(node_id))
        else:
            label_final = None
            for label in label_attr_name:
                label_final = node_data.get(label, None)
                if label_final:
                    break
            if not label_final:
                label_final = node_id
            nodes[LABEL_COLUMN_NAME].append(str(label_final))

        nodes_map[node_id] = i
        for k in node_data.keys():
            if ignore_attributes and k in ignore_attributes:
                continue

            if k.startswith("_"):
                raise KiaraException(
                    "Graph contains node column name starting with '_'. This is reserved for internal use, and not allowed."
                )

            v = node_data.get(k, None)
            nodes.setdefault(k, []).append(v)

    nodes_table = pa.Table.from_pydict(mapping=nodes)

    return nodes_table, nodes_map


def extract_networkx_edges_as_table(
    graph: "nx.Graph", node_id_map: Dict[Hashable, int]
) -> "pa.Table":
    """Extract the edges of this graph as a pyarrow table.

    The provided `node_id_map` might be modified if a node id is not yet in the map.

    Args:
        graph: The graph to extract edges from.
        node_id_map: A mapping from (original) node ids to (kiara-internal) (integer) node-ids.
    """

    # adapted from networx code
    # License: 3-clause BSD license
    # Copyright (C) 2004-2022, NetworkX Developers

    import pyarrow as pa

    if node_id_map is None:
        node_id_map = {}

    # nan = float("nan")

    max_node_id = max(node_id_map.values())  # TODO: could we just use len(node_id_map)?
    edge_columns: Dict[str, List[int]] = {
        SOURCE_COLUMN_NAME: [],
        TARGET_COLUMN_NAME: [],
    }

    for source, target, edge_data in graph.edges(data=True):
        if source not in node_id_map.keys():
            max_node_id += 1
            node_id_map[source] = max_node_id
        if target not in node_id_map.keys():
            max_node_id += 1
            node_id_map[target] = max_node_id

        edge_columns[SOURCE_COLUMN_NAME].append(node_id_map[source])
        edge_columns[TARGET_COLUMN_NAME].append(node_id_map[target])

        for k in edge_data.keys():
            if k.startswith("_"):
                raise KiaraException(
                    "Graph contains edge column name starting with '_'. This is reserved for internal use, and not allowed."
                )

            v = edge_data.get(k, None)
            edge_columns.setdefault(k, []).append(v)

    edges_table = pa.Table.from_pydict(mapping=edge_columns)

    return edges_table


def augment_nodes_table_with_connection_counts(
    nodes_table: Union["pa.Table", "pl.DataFrame"],
    edges_table: Union["pa.Table", "pl.DataFrame"],
) -> "pa.Table":

    import duckdb

    try:
        nodes_column_names = nodes_table.column_names  # type: ignore
    except Exception:
        nodes_column_names = nodes_table.columns  # type: ignore

    node_attr_columns = [x for x in nodes_column_names if not x.startswith("_")]
    if node_attr_columns:
        other_columns = ", " + ", ".join(node_attr_columns)
    else:
        other_columns = ""

    # we can avoid 'COUNT(*)' calls in the following  query
    nodes_table_rows = len(nodes_table)
    print(nodes_table_rows)

    query = f"""
    SELECT
         {NODE_ID_COLUMN_NAME},
         {LABEL_COLUMN_NAME},
         COALESCE(e1.{IN_DIRECTED_COLUMN_NAME}, 0) + COALESCE(e3.{OUT_DIRECTED_COLUMN_NAME}, 0) as {CONNECTIONS_COLUMN_NAME},
         (COALESCE(e1._in_edges, 0) + COALESCE(e3._out_edges, 0)) / {nodes_table_rows} AS _degree_centrality,
         COALESCE(e2.{IN_DIRECTED_MULTI_COLUMN_NAME}, 0) + COALESCE(e4.{OUT_DIRECTED_MULTI_COLUMN_NAME}, 0) as {CONNECTIONS_MULTI_COLUMN_NAME},
         COALESCE(e1.{IN_DIRECTED_COLUMN_NAME}, 0) as {IN_DIRECTED_COLUMN_NAME},
         COALESCE(e2.{IN_DIRECTED_MULTI_COLUMN_NAME}, 0) as {IN_DIRECTED_MULTI_COLUMN_NAME},
         COALESCE(e3.{OUT_DIRECTED_COLUMN_NAME}, 0) as {OUT_DIRECTED_COLUMN_NAME},
         COALESCE(e4.{OUT_DIRECTED_MULTI_COLUMN_NAME}, 0) as {OUT_DIRECTED_MULTI_COLUMN_NAME}
         {other_columns}
         FROM nodes_table n
         left join
           (SELECT {TARGET_COLUMN_NAME}, {COUNT_IDX_DIRECTED_COLUMN_NAME}, COUNT(*) as {IN_DIRECTED_COLUMN_NAME} from edges_table GROUP BY {TARGET_COLUMN_NAME}, {COUNT_IDX_DIRECTED_COLUMN_NAME}) e1
           on n.{NODE_ID_COLUMN_NAME} = e1.{TARGET_COLUMN_NAME} and e1.{COUNT_IDX_DIRECTED_COLUMN_NAME} = 1
         left join
           (SELECT {TARGET_COLUMN_NAME}, COUNT(*) as {IN_DIRECTED_MULTI_COLUMN_NAME} from edges_table GROUP BY {TARGET_COLUMN_NAME}) e2
           on n.{NODE_ID_COLUMN_NAME} = e2.{TARGET_COLUMN_NAME}
         left join
           (SELECT {SOURCE_COLUMN_NAME}, {COUNT_IDX_DIRECTED_COLUMN_NAME}, COUNT(*) as {OUT_DIRECTED_COLUMN_NAME} from edges_table GROUP BY {SOURCE_COLUMN_NAME}, {COUNT_IDX_DIRECTED_COLUMN_NAME}) e3
           on n.{NODE_ID_COLUMN_NAME} = e3.{SOURCE_COLUMN_NAME} and e3.{COUNT_IDX_DIRECTED_COLUMN_NAME} = 1
         left join
           (SELECT {SOURCE_COLUMN_NAME}, COUNT(*) as {OUT_DIRECTED_MULTI_COLUMN_NAME} from edges_table GROUP BY {SOURCE_COLUMN_NAME}) e4
           on n.{NODE_ID_COLUMN_NAME} = e4.{SOURCE_COLUMN_NAME}
        ORDER BY {NODE_ID_COLUMN_NAME}
    """

    print(query)
    nodes_table_result = duckdb.sql(query)

    centrality_query = f"""
    SELECT
         {NODE_ID_COLUMN_NAME},
         {LABEL_COLUMN_NAME},
         {CONNECTIONS_COLUMN_NAME},
         {CONNECTIONS_COLUMN_NAME} / (SELECT COUNT(*) FROM nodes_table_result) AS {UNWEIGHTED_DEGREE_CENTRALITY_COLUMN_NAME},
         {CONNECTIONS_MULTI_COLUMN_NAME},
         {CONNECTIONS_MULTI_COLUMN_NAME} / (SELECT COUNT(*) FROM nodes_table_result) AS {UNWEIGHTED_DEGREE_CENTRALITY_MULTI_COLUMN_NAME},
         {IN_DIRECTED_COLUMN_NAME},
         {IN_DIRECTED_MULTI_COLUMN_NAME},
         {OUT_DIRECTED_COLUMN_NAME},
         {OUT_DIRECTED_MULTI_COLUMN_NAME}
         {other_columns}
    FROM nodes_table_result
    """
    print(centrality_query)

    result = duckdb.sql(centrality_query)

    nodes_table_augmented = result.arrow()
    return nodes_table_augmented


def augment_edges_table_with_id_and_weights(
    edges_table: Union["pa.Table", "pl.DataFrame"]
) -> "pa.Table":
    """Augment the edges table with additional pre-computed columns for directed and undirected weights.."""

    import duckdb

    try:
        column_names = edges_table.column_names  # type: ignore
    except Exception:
        column_names = edges_table.columns  # type: ignore

    edge_attr_columns = [x for x in column_names if not x.startswith("_")]
    if edge_attr_columns:
        other_columns = ", " + ", ".join(edge_attr_columns)
    else:
        other_columns = ""

    query = f"""
    SELECT
      ROW_NUMBER() OVER () -1 as {EDGE_ID_COLUMN_NAME},
      {SOURCE_COLUMN_NAME},
      {TARGET_COLUMN_NAME},
      COUNT(*) OVER (PARTITION BY {SOURCE_COLUMN_NAME}, {TARGET_COLUMN_NAME}) as {COUNT_DIRECTED_COLUMN_NAME},
      ROW_NUMBER(*) OVER (PARTITION BY {SOURCE_COLUMN_NAME}, {TARGET_COLUMN_NAME}) as {COUNT_IDX_DIRECTED_COLUMN_NAME},
      COUNT(*) OVER (PARTITION BY LEAST({SOURCE_COLUMN_NAME}, {TARGET_COLUMN_NAME}), GREATEST({SOURCE_COLUMN_NAME}, {TARGET_COLUMN_NAME})) as {COUNT_UNDIRECTED_COLUMN_NAME},
      ROW_NUMBER(*) OVER (PARTITION BY LEAST({SOURCE_COLUMN_NAME}, {TARGET_COLUMN_NAME}), GREATEST({SOURCE_COLUMN_NAME}, {TARGET_COLUMN_NAME})) as {COUNT_IDX_UNDIRECTED_COLUMN_NAME}
      {other_columns}
    FROM edges_table"""

    result = duckdb.sql(query)
    edges_table_augmented = result.arrow()

    return edges_table_augmented


# kiara\kiara_plugin.network_analysis\src\kiara_plugin\network_analysis\__init__.py
# -*- coding: utf-8 -*-

"""Top-level package for kiara_plugin.network_analysis."""


import os

from kiara.utils.class_loading import (
    KiaraEntryPointItem,
    find_data_types_under,
    find_kiara_model_classes_under,
    find_kiara_modules_under,
    find_pipeline_base_path_for_module,
)

__author__ = """Markus Binsteiner"""
__email__ = "markus@frkl.io"

KIARA_METADATA = {
    "authors": [],
    "description": "Kiara modules for: network_analysis",
    "references": {
        "source_repo": {
            "desc": "The module package git repository.",
            "url": "https://github.com/DHARPA-Project/kiara_plugin.network_analysis",
        },
        "documentation": {
            "desc": "The url for the module package documentation.",
            "url": "https://DHARPA-Project.github.io/kiara_plugin.network_analysis/",
        },
    },
    "tags": ["network_analysis"],
    "labels": {"package": "kiara_plugin.network_analysis"},
}

find_modules: KiaraEntryPointItem = (
    find_kiara_modules_under,
    "kiara_plugin.network_analysis.modules",
)
find_model_classes: KiaraEntryPointItem = (
    find_kiara_model_classes_under,
    "kiara_plugin.network_analysis.models",
)
find_data_types: KiaraEntryPointItem = (
    find_data_types_under,
    "kiara_plugin.network_analysis.data_types",
)
find_pipelines: KiaraEntryPointItem = (
    find_pipeline_base_path_for_module,
    "kiara_plugin.network_analysis.pipelines",
    KIARA_METADATA,
)

try:
    from kiara_plugin.streamlit import find_kiara_streamlit_components_under

    find_kiara_streamlit_components: KiaraEntryPointItem = (
        find_kiara_streamlit_components_under,
        "kiara_plugin.network_analysis.streamlit.components",
    )
except Exception:
    find_kiara_streamlit_components = list


def get_version():
    from importlib.metadata import PackageNotFoundError, version

    try:
        # Change here if project is renamed and does not equal the package name
        dist_name = __name__
        __version__ = version(dist_name)
    except PackageNotFoundError:

        try:
            version_file = os.path.join(os.path.dirname(__file__), "version.txt")

            if os.path.exists(version_file):
                with open(version_file, encoding="utf-8") as vf:
                    __version__ = vf.read()
            else:
                __version__ = "unknown"

        except (Exception):
            pass

        if __version__ is None:
            __version__ = "unknown"

    return __version__


# kiara\kiara_plugin.network_analysis\src\kiara_plugin\network_analysis\data_types\__init__.py
# -*- coding: utf-8 -*-

"""This module contains the value type classes that are used in the ``kiara_plugin.network_analysis`` package.
"""
from typing import Any, ClassVar, List, Mapping, Type, Union

from rich.console import Group

from kiara.defaults import DEFAULT_PRETTY_PRINT_CONFIG
from kiara.exceptions import KiaraException
from kiara.models.values.value import Value
from kiara.utils.output import ArrowTabularWrap
from kiara_plugin.network_analysis.defaults import (
    CONNECTIONS_COLUMN_NAME,
    CONNECTIONS_MULTI_COLUMN_NAME,
    COUNT_DIRECTED_COLUMN_NAME,
    COUNT_IDX_DIRECTED_COLUMN_NAME,
    COUNT_IDX_UNDIRECTED_COLUMN_NAME,
    COUNT_UNDIRECTED_COLUMN_NAME,
    EDGE_ID_COLUMN_NAME,
    EDGES_TABLE_NAME,
    IN_DIRECTED_COLUMN_NAME,
    IN_DIRECTED_MULTI_COLUMN_NAME,
    LABEL_COLUMN_NAME,
    NODE_ID_COLUMN_NAME,
    NODES_TABLE_NAME,
    OUT_DIRECTED_COLUMN_NAME,
    OUT_DIRECTED_MULTI_COLUMN_NAME,
    SOURCE_COLUMN_NAME,
    TARGET_COLUMN_NAME,
)
from kiara_plugin.network_analysis.models import NetworkData
from kiara_plugin.tabular.data_types.tables import TablesType
from kiara_plugin.tabular.models.tables import KiaraTables


class NetworkDataType(TablesType):
    """Data that can be assembled into a graph.

    This data type extends the 'tables' type from the [kiara_plugin.tabular](https://github.com/DHARPA-Project/kiara_plugin.tabular) plugin, restricting the allowed tables to one called 'edges',
    and one called 'nodes'.
    """

    _data_type_name: ClassVar[str] = "network_data"
    _cached_doc: ClassVar[Union[str, None]] = None

    @classmethod
    def python_class(cls) -> Type:
        return NetworkData  # type: ignore

    @classmethod
    def type_doc(cls) -> str:

        if cls._cached_doc:
            return cls._cached_doc

        from kiara_plugin.network_analysis.models.metadata import (
            EDGE_COUNT_DUP_DIRECTED_COLUMN_METADATA,
            EDGE_COUNT_DUP_UNDIRECTED_COLUMN_METADATA,
            EDGE_ID_COLUMN_METADATA,
            EDGE_IDX_DUP_DIRECTED_COLUMN_METADATA,
            EDGE_IDX_DUP_UNDIRECTED_COLUMN_METADATA,
            EDGE_SOURCE_COLUMN_METADATA,
            EDGE_TARGET_COLUMN_METADATA,
            NODE_COUND_EDGES_MULTI_COLUMN_METADATA,
            NODE_COUNT_EDGES_COLUMN_METADATA,
            NODE_COUNT_IN_EDGES_COLUMN_METADATA,
            NODE_COUNT_IN_EDGES_MULTI_COLUMN_METADATA,
            NODE_COUNT_OUT_EDGES_COLUMN_METADATA,
            NODE_COUNT_OUT_EDGES_MULTI_COLUMN_METADATA,
            NODE_ID_COLUMN_METADATA,
            NODE_LABEL_COLUMN_METADATA,
        )

        edge_properties = {}
        edge_properties[EDGE_ID_COLUMN_NAME] = EDGE_ID_COLUMN_METADATA.doc.full_doc
        edge_properties[SOURCE_COLUMN_NAME] = EDGE_SOURCE_COLUMN_METADATA.doc.full_doc
        edge_properties[TARGET_COLUMN_NAME] = EDGE_TARGET_COLUMN_METADATA.doc.full_doc
        edge_properties[
            COUNT_DIRECTED_COLUMN_NAME
        ] = EDGE_COUNT_DUP_DIRECTED_COLUMN_METADATA.doc.full_doc
        edge_properties[
            COUNT_IDX_DIRECTED_COLUMN_NAME
        ] = EDGE_IDX_DUP_DIRECTED_COLUMN_METADATA.doc.full_doc
        edge_properties[
            COUNT_UNDIRECTED_COLUMN_NAME
        ] = EDGE_COUNT_DUP_UNDIRECTED_COLUMN_METADATA.doc.full_doc
        edge_properties[
            COUNT_IDX_UNDIRECTED_COLUMN_NAME
        ] = EDGE_IDX_DUP_UNDIRECTED_COLUMN_METADATA.doc.full_doc

        properties_node = {}
        properties_node[NODE_ID_COLUMN_NAME] = NODE_ID_COLUMN_METADATA.doc.full_doc
        properties_node[LABEL_COLUMN_NAME] = NODE_LABEL_COLUMN_METADATA.doc.full_doc
        properties_node[
            CONNECTIONS_COLUMN_NAME
        ] = NODE_COUNT_EDGES_COLUMN_METADATA.doc.full_doc
        properties_node[
            CONNECTIONS_MULTI_COLUMN_NAME
        ] = NODE_COUND_EDGES_MULTI_COLUMN_METADATA.doc.full_doc
        properties_node[
            IN_DIRECTED_COLUMN_NAME
        ] = NODE_COUNT_IN_EDGES_COLUMN_METADATA.doc.full_doc
        properties_node[
            IN_DIRECTED_MULTI_COLUMN_NAME
        ] = NODE_COUNT_IN_EDGES_MULTI_COLUMN_METADATA.doc.full_doc
        properties_node[
            OUT_DIRECTED_COLUMN_NAME
        ] = NODE_COUNT_OUT_EDGES_COLUMN_METADATA.doc.full_doc
        properties_node[
            OUT_DIRECTED_MULTI_COLUMN_NAME
        ] = NODE_COUNT_OUT_EDGES_MULTI_COLUMN_METADATA.doc.full_doc

        edge_properties_str = "\n\n".join(
            f"***{key}***:\n\n{value}" for key, value in edge_properties.items()
        )
        node_properties_str = "\n\n".join(
            f"***{key}***:\n\n{value}" for key, value in properties_node.items()
        )

        doc = cls.__doc__
        doc_tables = f"""

## Edges
The 'edges' table contains the following columns:

{edge_properties_str}

## Nodes

The 'nodes' table contains the following columns:

{node_properties_str}

"""

        cls._cached_doc = f"{doc}\n\n{doc_tables}"
        return cls._cached_doc

    def parse_python_obj(self, data: Any) -> NetworkData:

        if isinstance(data, KiaraTables):
            if EDGES_TABLE_NAME not in data.tables.keys():
                raise KiaraException(
                    f"Can't import network data: no '{EDGES_TABLE_NAME}' table found"
                )

            if NODES_TABLE_NAME not in data.tables.keys():
                raise KiaraException(
                    f"Can't import network data: no '{NODES_TABLE_NAME}' table found"
                )

            # return NetworkData(
            #     tables={
            #         EDGES_TABLE_NAME: data.tables[EDGES_TABLE_NAME],
            #         NODES_TABLE_NAME: data.tables[NODES_TABLE_NAME],
            #     },
            #
            # )
            return NetworkData.create_network_data(
                edges_table=data.tables[EDGES_TABLE_NAME].arrow_table,
                nodes_table=data.tables[NODES_TABLE_NAME].arrow_table,
                augment_tables=False,
            )

        if not isinstance(data, NetworkData):
            raise KiaraException(
                f"Can't parse object to network data: invalid type '{type(data)}'."
            )

        return data

    def _validate(cls, value: Any) -> None:
        if not isinstance(value, NetworkData):
            raise ValueError(
                f"Invalid type '{type(value)}': must be of 'NetworkData' (or a sub-class)."
            )

        network_data: NetworkData = value

        table_names = network_data.table_names
        if EDGES_TABLE_NAME not in table_names:
            raise Exception(
                f"Invalid 'network_data' value: database does not contain table '{EDGES_TABLE_NAME}'."
            )
        if NODES_TABLE_NAME not in table_names:
            raise Exception(
                f"Invalid 'network_data' value: database does not contain table '{NODES_TABLE_NAME}'."
            )

        edges_columns = network_data.edges.column_names
        if SOURCE_COLUMN_NAME not in edges_columns:
            raise Exception(
                f"Invalid 'network_data' value: 'edges' table does not contain a '{SOURCE_COLUMN_NAME}' column. Available columns: {', '.join(edges_columns)}."
            )
        if TARGET_COLUMN_NAME not in edges_columns:
            raise Exception(
                f"Invalid 'network_data' value: 'edges' table does not contain a '{TARGET_COLUMN_NAME}' column. Available columns: {', '.join(edges_columns)}."
            )

        nodes_columns = network_data.nodes.column_names
        if NODE_ID_COLUMN_NAME not in nodes_columns:
            raise Exception(
                f"Invalid 'network_data' value: 'nodes' table does not contain a '{NODE_ID_COLUMN_NAME}' column. Available columns: {', '.join(nodes_columns)}."
            )
        if LABEL_COLUMN_NAME not in nodes_columns:
            raise Exception(
                f"Invalid 'network_data' value: 'nodes' table does not contain a '{LABEL_COLUMN_NAME}' column. Available columns: {', '.join(nodes_columns)}."
            )

    def pretty_print_as__terminal_renderable(
        self, value: Value, render_config: Mapping[str, Any]
    ) -> Any:

        max_rows = render_config.get(
            "max_no_rows", DEFAULT_PRETTY_PRINT_CONFIG["max_no_rows"]
        )
        max_row_height = render_config.get(
            "max_row_height", DEFAULT_PRETTY_PRINT_CONFIG["max_row_height"]
        )
        max_cell_length = render_config.get(
            "max_cell_length", DEFAULT_PRETTY_PRINT_CONFIG["max_cell_length"]
        )

        half_lines: Union[int, None] = None
        if max_rows:
            half_lines = int(max_rows / 2)

        network_data: NetworkData = value.data

        result: List[Any] = [""]

        nodes_atw = ArrowTabularWrap(network_data.nodes.arrow_table)
        nodes_pretty = nodes_atw.as_terminal_renderable(
            rows_head=half_lines,
            rows_tail=half_lines,
            max_row_height=max_row_height,
            max_cell_length=max_cell_length,
        )
        result.append(f"[b]{NODES_TABLE_NAME}[/b]")
        result.append(nodes_pretty)

        edges_atw = ArrowTabularWrap(network_data.edges.arrow_table)
        edges_pretty = edges_atw.as_terminal_renderable(
            rows_head=half_lines,
            rows_tail=half_lines,
            max_row_height=max_row_height,
            max_cell_length=max_cell_length,
        )
        result.append(f"[b]{EDGES_TABLE_NAME}[/b]")
        result.append(edges_pretty)

        return Group(*result)


# kiara\kiara_plugin.network_analysis\src\kiara_plugin\network_analysis\models\inputs.py
# -*- coding: utf-8 -*-
from typing import Any, ClassVar, Dict, Union

from pydantic import Field, model_validator

from kiara.models import KiaraModel
from kiara_plugin.core_types.defaults import DEFAULT_MODEL_KEY
from kiara_plugin.network_analysis.defaults import AGGREGATION_FUNCTION_NAME


class AttributeMapStrategy(KiaraModel):

    _kiara_model_id: ClassVar = "input.network_analysis_attribute_map_transformation"

    @model_validator(mode="before")
    @classmethod
    def pre_validate_model(cls, values: Dict[str, Any]):

        if len(values) == 1 and DEFAULT_MODEL_KEY in values.keys():

            token = values[DEFAULT_MODEL_KEY]
            source_column_name = None
            if "=" in token:

                target_column_name, func_token = token.split("=", maxsplit=1)

                if "(" in func_token:

                    func, source_column_name = func_token.split("(", maxsplit=1)
                    if source_column_name == ")":
                        raise ValueError(
                            f"Invalid function definition, empty source column name: {func_token}. Use like: `{func}(YOUR_SOURCE_COLUMN_NAME)"
                        )
                    source_column_name = source_column_name.strip()
                    func = func.strip()
                    if not source_column_name.endswith(")"):
                        raise ValueError(
                            f"Invalid function definition, missing closing parenthesis: {func_token}"
                        )

                    source_column_name = source_column_name[:-1]
                else:
                    source_column_name = func_token.strip()
                    func = None

            else:
                target_column_name = token
                func = None

            if not source_column_name:
                source_column_name = target_column_name
            result = {
                "target_column_name": target_column_name,
                "source_column_name": source_column_name,
                "transform_function": func.lower() if func else None,
            }

            return result

        if not values.get("target_column_name", None):
            raise ValueError("No 'target_column_name' specified.")
        if not values.get("source_column_name", None):
            values["source_column_name"] = values["target_column_name"]

        values["transform_function"] = (
            values["transform_function"].lower()
            if values["transform_function"]
            else None
        )
        return values

    target_column_name: str = Field(
        description="The name of the attribute in the resulting network_data instance."
    )
    source_column_name: str = Field(
        description="The name of the attribute (or attributes) in the source network_data instance. Defaults to 'target_column_name'."
    )
    transform_function: Union[AGGREGATION_FUNCTION_NAME, None] = Field(  # type: ignore
        description="The name of the function to apply to the attribute(s).",
        default=None,
    )


# kiara\kiara_plugin.network_analysis\src\kiara_plugin\network_analysis\models\metadata.py
# -*- coding: utf-8 -*-
from typing import ClassVar

from pydantic import Field, field_validator

from kiara.models import KiaraModel
from kiara.models.documentation import DocumentationMetadataModel
from kiara_plugin.network_analysis.defaults import (
    EDGE_COUNT_DUP_DIRECTED_TEXT,
    EDGE_COUNT_DUP_UNDIRECTED_TEXT,
    EDGE_IDX_DUP_DIRECTED_TEXT,
    EDGE_IDX_DUP_UNDIRECTED_TEXT,
    EDGE_SOURCE_TEXT,
    EDGE_TARGET_TEXT,
    NODE_COUNT_EDGES_MULTI_TEXT,
    NODE_COUNT_EDGES_TEXT,
    NODE_COUNT_IN_EDGES_MULTI_TEXT,
    NODE_COUNT_IN_EDGES_TEXT,
    NODE_COUNT_OUT_EDGES_MULTI_TEXT,
    NODE_COUNT_OUT_EDGES_TEXT,
    NODE_ID_TEXT,
    NODE_LABEL_TEXT,
    UNWEIGHTED_DEGREE_CENTRALITY_TEXT,
)


class NetworkNodeAttributeMetadata(KiaraModel):

    _kiara_model_id: ClassVar = "metadata.network_node_attribute"

    doc: DocumentationMetadataModel = Field(
        description="Explanation what this attribute is about.",
        default_factory=DocumentationMetadataModel.create,
    )
    computed_attribute: bool = Field(
        description="Whether this is the default attribute that is always automatically added by kiara.",
        default=False,
    )

    @field_validator("doc", mode="before")
    @classmethod
    def validate_doc(cls, value):
        return DocumentationMetadataModel.create(value)


class NetworkEdgeAttributeMetadata(KiaraModel):

    _kiara_model_id: ClassVar = "metadata.network_edge_attribute"

    doc: DocumentationMetadataModel = Field(
        description="Explanation what this attribute is about.",
        default_factory=DocumentationMetadataModel.create,
    )
    computed_attribute: bool = Field(
        description="Whether this is the computed attribute that is automatically added by kiara.",
        default=False,
    )

    @field_validator("doc", mode="before")
    @classmethod
    def validate_doc(cls, value):
        return DocumentationMetadataModel.create(value)


NODE_ID_COLUMN_METADATA = NetworkNodeAttributeMetadata(doc=NODE_ID_TEXT, computed_attribute=True)  # type: ignore
NODE_LABEL_COLUMN_METADATA = NetworkNodeAttributeMetadata(doc=NODE_LABEL_TEXT, computed_attribute=True)  # type: ignore

NODE_COUNT_EDGES_COLUMN_METADATA = NetworkNodeAttributeMetadata(doc=NODE_COUNT_EDGES_TEXT, computed_attribute=True)  # type: ignore
NODE_DEGREE_COLUMN_METADATA = NetworkEdgeAttributeMetadata(
    doc=UNWEIGHTED_DEGREE_CENTRALITY_TEXT, computed_attribute=True
)  # type: ignore
NODE_COUND_EDGES_MULTI_COLUMN_METADATA = NetworkNodeAttributeMetadata(doc=NODE_COUNT_EDGES_MULTI_TEXT, computed_attribute=True)  # type: ignore
NODE_DEGREE_MULTI_COLUMN_METADATA = NetworkEdgeAttributeMetadata(
    doc=UNWEIGHTED_DEGREE_CENTRALITY_TEXT, computed_attribute=True
)  # type: ignore

NODE_COUNT_IN_EDGES_COLUMN_METADATA = NetworkNodeAttributeMetadata(doc=NODE_COUNT_IN_EDGES_TEXT, computed_attribute=True)  # type: ignore
NODE_COUNT_IN_EDGES_MULTI_COLUMN_METADATA = NetworkNodeAttributeMetadata(doc=NODE_COUNT_IN_EDGES_MULTI_TEXT, computed_attribute=True)  # type: ignore
NODE_COUNT_OUT_EDGES_COLUMN_METADATA = NetworkEdgeAttributeMetadata(doc=NODE_COUNT_OUT_EDGES_TEXT, computed_attribute=True)  # type: ignore
NODE_COUNT_OUT_EDGES_MULTI_COLUMN_METADATA = NetworkEdgeAttributeMetadata(doc=NODE_COUNT_OUT_EDGES_MULTI_TEXT, computed_attribute=True)  # type: ignore

EDGE_ID_COLUMN_METADATA = NetworkEdgeAttributeMetadata(doc="The unique id for the edge.", computed_attribute=True)  # type: ignore
EDGE_SOURCE_COLUMN_METADATA = NetworkEdgeAttributeMetadata(doc=EDGE_SOURCE_TEXT, computed_attribute=True)  # type: ignore
EDGE_TARGET_COLUMN_METADATA = NetworkEdgeAttributeMetadata(doc=EDGE_TARGET_TEXT, computed_attribute=True)  # type: ignore

EDGE_COUNT_DUP_DIRECTED_COLUMN_METADATA = NetworkEdgeAttributeMetadata(
    doc=EDGE_COUNT_DUP_DIRECTED_TEXT, computed_attribute=True
)
EDGE_IDX_DUP_DIRECTED_COLUMN_METADATA = NetworkEdgeAttributeMetadata(
    doc=EDGE_IDX_DUP_DIRECTED_TEXT, computed_attribute=True
)
EDGE_COUNT_DUP_UNDIRECTED_COLUMN_METADATA = NetworkEdgeAttributeMetadata(
    doc=EDGE_COUNT_DUP_UNDIRECTED_TEXT, computed_attribute=True
)
EDGE_IDX_DUP_UNDIRECTED_COLUMN_METADATA = NetworkEdgeAttributeMetadata(
    doc=EDGE_IDX_DUP_UNDIRECTED_TEXT, computed_attribute=True
)


# kiara\kiara_plugin.network_analysis\src\kiara_plugin\network_analysis\models\__init__.py
# -*- coding: utf-8 -*-

"""This module contains the metadata (and other) models that are used in the ``kiara_plugin.network_analysis`` package.

Those models are convenience wrappers that make it easier for *kiara* to find, create, manage and version metadata -- but also
other type of models -- that is attached to data, as well as *kiara* modules.

Metadata models must be a sub-class of [kiara.metadata.MetadataModel][kiara.metadata.MetadataModel]. Other models usually
sub-class a pydantic BaseModel or implement custom base classes.
"""
from typing import (
    TYPE_CHECKING,
    ClassVar,
    Dict,
    Iterable,
    List,
    Literal,
    Protocol,
    Type,
    TypeVar,
    Union,
)

from pydantic import BaseModel, Field

from kiara.exceptions import KiaraException
from kiara.models import KiaraModel
from kiara.models.values.value import Value
from kiara.models.values.value_metadata import ValueMetadata
from kiara_plugin.network_analysis.defaults import (
    ATTRIBUTE_PROPERTY_KEY,
    CONNECTIONS_COLUMN_NAME,
    CONNECTIONS_MULTI_COLUMN_NAME,
    COUNT_DIRECTED_COLUMN_NAME,
    COUNT_IDX_DIRECTED_COLUMN_NAME,
    COUNT_IDX_UNDIRECTED_COLUMN_NAME,
    COUNT_UNDIRECTED_COLUMN_NAME,
    EDGE_ID_COLUMN_NAME,
    EDGES_TABLE_NAME,
    IN_DIRECTED_COLUMN_NAME,
    IN_DIRECTED_MULTI_COLUMN_NAME,
    LABEL_COLUMN_NAME,
    NODE_ID_COLUMN_NAME,
    NODES_TABLE_NAME,
    OUT_DIRECTED_COLUMN_NAME,
    OUT_DIRECTED_MULTI_COLUMN_NAME,
    SOURCE_COLUMN_NAME,
    TARGET_COLUMN_NAME,
    UNWEIGHTED_DEGREE_CENTRALITY_COLUMN_NAME,
    UNWEIGHTED_DEGREE_CENTRALITY_MULTI_COLUMN_NAME,
    GraphType,
)
from kiara_plugin.network_analysis.utils import (
    augment_edges_table_with_id_and_weights,
    augment_nodes_table_with_connection_counts,
    extract_networkx_edges_as_table,
    extract_networkx_nodes_as_table,
)
from kiara_plugin.tabular.models.tables import KiaraTables

if TYPE_CHECKING:
    import networkx as nx
    import pyarrow as pa
    import rustworkx as rx

    from kiara_plugin.tabular.models.table import KiaraTable

NETWORKX_GRAPH_TYPE = TypeVar("NETWORKX_GRAPH_TYPE", bound="nx.Graph")
RUSTWORKX_GRAPH_TYPE = TypeVar("RUSTWORKX_GRAPH_TYPE", "rx.PyGraph", "rx.PyDiGraph")


class NodesCallback(Protocol):
    def __call__(self, _node_id: int, **kwargs) -> None:
        ...


class EdgesCallback(Protocol):
    def __call__(self, _source: int, _target: int, **kwargs) -> None:
        ...


class NetworkData(KiaraTables):
    """A wrapper class to access and query network datasets.

    This class provides different ways to access the underlying network data, most notably via sql and as rustworkx (also networkx) Graph object.

    Internally, network data is stored as 2 Arrow tables with the edges stored in a table called 'edges' and the nodes in a table called 'nodes'. The edges table must have (at least) the following columns: '_source', '_target'. The nodes table must have (at least) the following columns: '_id' (integer), '_label' (string).

    By convention, kiara will add columns prefixed with an underscore if the values in it have internal 'meaning', normal/original attributes are stored in columns without that prefix.
    """

    _kiara_model_id: ClassVar = "instance.network_data"

    @classmethod
    def create_augmented(
        cls,
        network_data: "NetworkData",
        additional_edges_columns: Union[None, Dict[str, "pa.Array"]] = None,
        additional_nodes_columns: Union[None, Dict[str, "pa.Array"]] = None,
        nodes_column_metadata: Union[Dict[str, Dict[str, KiaraModel]], None] = None,
        edges_column_metadata: Union[Dict[str, Dict[str, KiaraModel]], None] = None,
    ) -> "NetworkData":
        """Create a new network_data instance, augmented with additional columns.

        This won't re-compute any of the automatically generated columns (starting with '_').
        """

        nodes_table = network_data.nodes.arrow_table
        edges_table = network_data.edges.arrow_table

        # nodes_table = pa.Table.from_arrays(orig_nodes_table.columns, schema=orig_nodes_table.schema)
        # edges_table = pa.Table.from_arrays(orig_edges_table.columns, schema=orig_edges_table.schema)

        if additional_edges_columns is not None:
            for col_name, col_data in additional_edges_columns.items():
                edges_table = edges_table.append_column(col_name, col_data)

        if additional_nodes_columns is not None:
            for col_name, col_data in additional_nodes_columns.items():
                nodes_table = nodes_table.append_column(col_name, col_data)

        new_network_data = NetworkData.create_network_data(
            nodes_table=nodes_table,
            edges_table=edges_table,
            augment_tables=False,
            nodes_column_metadata=nodes_column_metadata,
            edges_column_metadata=edges_column_metadata,
        )

        return new_network_data

    @classmethod
    def create_network_data(
        cls,
        nodes_table: "pa.Table",
        edges_table: "pa.Table",
        augment_tables: bool = True,
        nodes_column_metadata: Union[Dict[str, Dict[str, KiaraModel]], None] = None,
        edges_column_metadata: Union[Dict[str, Dict[str, KiaraModel]], None] = None,
    ) -> "NetworkData":
        """Create a `NetworkData` instance from two Arrow tables.

        This method requires the nodes to have an "_id' column (int) as well as a '_label' one (utf8).
        The edges table needs at least a '_source' (int) and '_target' (int) column.

        This method can augment both tables with additional columns that are required for the internal representation (id, counts, etc).

        If you specify additional metadata, it will be attached to the columns of the tables. This is useful if you want to add metadata that was part of the original data, and that you want to be available when the data is used in a network analysis (for example existing weight data). The format of that additional metadata is a dict of dicts, with the
        root key the column name, the second key the property name, and the value the property value.

        Arguments:
            nodes_table: the table containing the nodes data
            edges_table: the table containing the edges data
            augment_tables: whether to augment the tables with pre-processed edge/node metadata (in most cases you want to do this, except if you know the metadata is already present and correct)
            nodes_column_metadata: additional metadata to attach to the nodes table columns
            edges_column_metadata: additional metadata to attach to the edges table columns
        """

        from kiara_plugin.network_analysis.models.metadata import (
            EDGE_COUNT_DUP_DIRECTED_COLUMN_METADATA,
            EDGE_COUNT_DUP_UNDIRECTED_COLUMN_METADATA,
            EDGE_ID_COLUMN_METADATA,
            EDGE_IDX_DUP_DIRECTED_COLUMN_METADATA,
            EDGE_IDX_DUP_UNDIRECTED_COLUMN_METADATA,
            EDGE_SOURCE_COLUMN_METADATA,
            EDGE_TARGET_COLUMN_METADATA,
            NODE_COUND_EDGES_MULTI_COLUMN_METADATA,
            NODE_COUNT_EDGES_COLUMN_METADATA,
            NODE_COUNT_IN_EDGES_COLUMN_METADATA,
            NODE_COUNT_IN_EDGES_MULTI_COLUMN_METADATA,
            NODE_COUNT_OUT_EDGES_COLUMN_METADATA,
            NODE_COUNT_OUT_EDGES_MULTI_COLUMN_METADATA,
            NODE_DEGREE_COLUMN_METADATA,
            NODE_DEGREE_MULTI_COLUMN_METADATA,
            NODE_ID_COLUMN_METADATA,
            NODE_LABEL_COLUMN_METADATA,
        )

        if augment_tables:
            edges_table = augment_edges_table_with_id_and_weights(edges_table)
            nodes_table = augment_nodes_table_with_connection_counts(
                nodes_table, edges_table
            )

        if edges_table.column(SOURCE_COLUMN_NAME).null_count > 0:
            raise KiaraException(
                msg="Can't assemble network data.",
                details="Source column in edges table contains null values.",
            )
        if edges_table.column(TARGET_COLUMN_NAME).null_count > 0:
            raise KiaraException(
                msg="Can't assemble network data.",
                details="Target column in edges table contains null values.",
            )

        network_data: NetworkData = cls.create_tables(
            {NODES_TABLE_NAME: nodes_table, EDGES_TABLE_NAME: edges_table}
        )

        # set default column metadata
        network_data.edges.set_column_metadata(
            EDGE_ID_COLUMN_NAME,
            ATTRIBUTE_PROPERTY_KEY,
            EDGE_ID_COLUMN_METADATA,
            overwrite_existing=False,
        )
        network_data.edges.set_column_metadata(
            SOURCE_COLUMN_NAME,
            ATTRIBUTE_PROPERTY_KEY,
            EDGE_SOURCE_COLUMN_METADATA,
            overwrite_existing=False,
        )
        network_data.edges.set_column_metadata(
            TARGET_COLUMN_NAME,
            ATTRIBUTE_PROPERTY_KEY,
            EDGE_TARGET_COLUMN_METADATA,
            overwrite_existing=False,
        )
        network_data.edges.set_column_metadata(
            COUNT_DIRECTED_COLUMN_NAME,
            ATTRIBUTE_PROPERTY_KEY,
            EDGE_COUNT_DUP_DIRECTED_COLUMN_METADATA,
            overwrite_existing=False,
        )
        network_data.edges.set_column_metadata(
            COUNT_IDX_DIRECTED_COLUMN_NAME,
            ATTRIBUTE_PROPERTY_KEY,
            EDGE_IDX_DUP_DIRECTED_COLUMN_METADATA,
            overwrite_existing=False,
        )
        network_data.edges.set_column_metadata(
            COUNT_UNDIRECTED_COLUMN_NAME,
            ATTRIBUTE_PROPERTY_KEY,
            EDGE_COUNT_DUP_UNDIRECTED_COLUMN_METADATA,
            overwrite_existing=False,
        )
        network_data.edges.set_column_metadata(
            COUNT_IDX_UNDIRECTED_COLUMN_NAME,
            ATTRIBUTE_PROPERTY_KEY,
            EDGE_IDX_DUP_UNDIRECTED_COLUMN_METADATA,
            overwrite_existing=False,
        )

        network_data.nodes.set_column_metadata(
            NODE_ID_COLUMN_NAME,
            ATTRIBUTE_PROPERTY_KEY,
            NODE_ID_COLUMN_METADATA,
            overwrite_existing=False,
        )
        network_data.nodes.set_column_metadata(
            LABEL_COLUMN_NAME,
            ATTRIBUTE_PROPERTY_KEY,
            NODE_LABEL_COLUMN_METADATA,
            overwrite_existing=False,
        )
        network_data.nodes.set_column_metadata(
            CONNECTIONS_COLUMN_NAME,
            ATTRIBUTE_PROPERTY_KEY,
            NODE_COUNT_EDGES_COLUMN_METADATA,
            overwrite_existing=False,
        )
        network_data.nodes.set_column_metadata(
            UNWEIGHTED_DEGREE_CENTRALITY_COLUMN_NAME,
            ATTRIBUTE_PROPERTY_KEY,
            NODE_DEGREE_COLUMN_METADATA,
            overwrite_existing=False,
        )
        network_data.nodes.set_column_metadata(
            CONNECTIONS_MULTI_COLUMN_NAME,
            ATTRIBUTE_PROPERTY_KEY,
            NODE_COUND_EDGES_MULTI_COLUMN_METADATA,
            overwrite_existing=False,
        )
        network_data.nodes.set_column_metadata(
            UNWEIGHTED_DEGREE_CENTRALITY_MULTI_COLUMN_NAME,
            ATTRIBUTE_PROPERTY_KEY,
            NODE_DEGREE_MULTI_COLUMN_METADATA,
            overwrite_existing=False,
        )
        network_data.nodes.set_column_metadata(
            IN_DIRECTED_COLUMN_NAME,
            ATTRIBUTE_PROPERTY_KEY,
            NODE_COUNT_IN_EDGES_COLUMN_METADATA,
            overwrite_existing=False,
        )
        network_data.nodes.set_column_metadata(
            IN_DIRECTED_MULTI_COLUMN_NAME,
            ATTRIBUTE_PROPERTY_KEY,
            NODE_COUNT_IN_EDGES_MULTI_COLUMN_METADATA,
            overwrite_existing=False,
        )
        network_data.nodes.set_column_metadata(
            OUT_DIRECTED_COLUMN_NAME,
            ATTRIBUTE_PROPERTY_KEY,
            NODE_COUNT_OUT_EDGES_COLUMN_METADATA,
            overwrite_existing=False,
        )
        network_data.nodes.set_column_metadata(
            OUT_DIRECTED_MULTI_COLUMN_NAME,
            ATTRIBUTE_PROPERTY_KEY,
            NODE_COUNT_OUT_EDGES_MULTI_COLUMN_METADATA,
            overwrite_existing=False,
        )

        if nodes_column_metadata is not None:
            for col_name, col_meta in nodes_column_metadata.items():
                for prop_name, prop_value in col_meta.items():
                    network_data.nodes.set_column_metadata(
                        col_name, prop_name, prop_value, overwrite_existing=True
                    )
        if edges_column_metadata is not None:
            for col_name, col_meta in edges_column_metadata.items():
                for prop_name, prop_value in col_meta.items():
                    network_data.edges.set_column_metadata(
                        col_name, prop_name, prop_value, overwrite_existing=True
                    )

        return network_data

    @classmethod
    def from_filtered_nodes(
        cls, network_data: "NetworkData", nodes_list: List[int]
    ) -> "NetworkData":
        """Create a new, filtered instance of this class using a source network, and a list of node ids to include.

        Nodes/edges containing a node id not in the list will be removed from the resulting network data.

        Arguments:
            network_data: the source network data
            nodes_list: the list of node ids to include in the filtered network data
        """

        import duckdb
        import polars as pl

        node_columns = [NODE_ID_COLUMN_NAME, LABEL_COLUMN_NAME]
        for column_name, metadata in network_data.nodes.column_metadata.items():
            attr_prop: Union[None, NetworkNodeAttributeMetadata] = metadata.get(  # type: ignore
                ATTRIBUTE_PROPERTY_KEY, None
            )
            if attr_prop is None or not attr_prop.computed_attribute:
                node_columns.append(column_name)

        node_list_str = ", ".join([str(n) for n in nodes_list])

        nodes_table = network_data.nodes.arrow_table  # noqa
        nodes_query = f"SELECT {', '.join(node_columns)} FROM nodes_table n WHERE n.{NODE_ID_COLUMN_NAME} IN ({node_list_str})"

        nodes_result = duckdb.sql(nodes_query).pl()

        edges_table = network_data.edges.arrow_table  # noqa
        edge_columns = [SOURCE_COLUMN_NAME, TARGET_COLUMN_NAME]
        for column_name, metadata in network_data.edges.column_metadata.items():
            attr_prop = metadata.get(ATTRIBUTE_PROPERTY_KEY, None)  # type: ignore
            if attr_prop is None or not attr_prop.computed_attribute:
                edge_columns.append(column_name)

        edges_query = f"SELECT {', '.join(edge_columns)} FROM edges_table WHERE {SOURCE_COLUMN_NAME} IN ({node_list_str}) OR {TARGET_COLUMN_NAME} IN ({node_list_str})"

        edges_result = duckdb.sql(edges_query).pl()

        nodes_idx_colum = range(len(nodes_result))
        old_idx_column = nodes_result[NODE_ID_COLUMN_NAME]

        repl_map = dict(zip(old_idx_column.to_list(), nodes_idx_colum))
        nodes_result = nodes_result.with_columns(
            pl.col(NODE_ID_COLUMN_NAME).map_dict(repl_map)
        )

        edges_result = edges_result.with_columns(
            pl.col(SOURCE_COLUMN_NAME).map_dict(repl_map),
            pl.col(TARGET_COLUMN_NAME).map_dict(repl_map),
        )

        filtered = NetworkData.create_network_data(
            nodes_table=nodes_result, edges_table=edges_result
        )
        return filtered

    @classmethod
    def create_from_networkx_graph(
        cls,
        graph: "nx.Graph",
        label_attr_name: Union[str, None] = None,
        ignore_node_attributes: Union[Iterable[str], None] = None,
    ) -> "NetworkData":
        """Create a `NetworkData` instance from a networkx Graph object.

        Arguments:
            graph: the networkx graph instance
            label_attr_name: the name of the node attribute that contains the node label (if None, the node id is used as label)
            ignore_node_attributes: a list of node attributes that should be ignored and not added to the table

        """

        # TODO: should we also index nodes/edges attributes?

        nodes_table, node_id_map = extract_networkx_nodes_as_table(
            graph=graph,
            label_attr_name=label_attr_name,
            ignore_attributes=ignore_node_attributes,
        )

        edges_table = extract_networkx_edges_as_table(graph, node_id_map)

        network_data = NetworkData.create_network_data(
            nodes_table=nodes_table, edges_table=edges_table
        )

        return network_data

    @property
    def edges(self) -> "KiaraTable":
        """Return the edges table."""

        return self.tables[EDGES_TABLE_NAME]

    @property
    def nodes(self) -> "KiaraTable":
        """Return the nodes table."""

        return self.tables[NODES_TABLE_NAME]

    @property
    def num_nodes(self):
        """Return the number of nodes in the network data."""

        return self.nodes.num_rows

    @property
    def num_edges(self):
        """Return the number of edges in the network data."""

        return self.edges.num_rows

    def query_edges(
        self, sql_query: str, relation_name: str = EDGES_TABLE_NAME
    ) -> "pa.Table":
        """Query the edges table using SQL.

        The table name to use in the query defaults to 'edges', but can be changed using the 'relation_name' argument.
        """

        import duckdb

        con = duckdb.connect()
        edges = self.edges.arrow_table  # noqa: F841
        if relation_name != EDGES_TABLE_NAME:
            sql_query = sql_query.replace(relation_name, EDGES_TABLE_NAME)

        result = con.execute(sql_query)
        return result.arrow()

    def query_nodes(
        self, sql_query: str, relation_name: str = NODES_TABLE_NAME
    ) -> "pa.Table":
        """Query the nodes table using SQL.

        The table name to use in the query defaults to 'nodes', but can be changed using the 'relation_name' argument.
        """

        import duckdb

        con = duckdb.connect()
        nodes = self.nodes.arrow_table  # noqa
        if relation_name != NODES_TABLE_NAME:
            sql_query = sql_query.replace(relation_name, NODES_TABLE_NAME)

        result = con.execute(sql_query)
        return result.arrow()

    def _calculate_node_attributes(
        self, incl_node_attributes: Union[bool, str, Iterable[str]]
    ) -> List[str]:
        """Calculate the node attributes that should be included in the output."""

        if incl_node_attributes is False:
            node_attr_names: List[str] = [NODE_ID_COLUMN_NAME, LABEL_COLUMN_NAME]
        else:
            all_node_attr_names: List[str] = self.nodes.column_names  # type: ignore
            if incl_node_attributes is True:
                node_attr_names = [NODE_ID_COLUMN_NAME]
                node_attr_names.extend((x for x in all_node_attr_names if x != NODE_ID_COLUMN_NAME))  # type: ignore
            elif isinstance(incl_node_attributes, str):
                if incl_node_attributes not in all_node_attr_names:
                    raise KiaraException(
                        f"Can't include node attribute {incl_node_attributes}: not part of the available attributes ({', '.join(all_node_attr_names)})."
                    )
                node_attr_names = [NODE_ID_COLUMN_NAME, incl_node_attributes]
            else:
                node_attr_names = [NODE_ID_COLUMN_NAME]
                for attr_name in incl_node_attributes:
                    if incl_node_attributes not in all_node_attr_names:
                        raise KiaraException(
                            f"Can't include node attribute {incl_node_attributes}: not part of the available attributes ({', '.join(all_node_attr_names)})."
                        )
                    node_attr_names.append(attr_name)  # type: ignore

        return node_attr_names

    def _calculate_edge_attributes(
        self, incl_edge_attributes: Union[bool, str, Iterable[str]]
    ) -> List[str]:
        """Calculate the edge attributes that should be included in the output."""

        if incl_edge_attributes is False:
            edge_attr_names: List[str] = [SOURCE_COLUMN_NAME, TARGET_COLUMN_NAME]
        else:
            all_edge_attr_names: List[str] = self.edges.column_names  # type: ignore
            if incl_edge_attributes is True:
                edge_attr_names = [SOURCE_COLUMN_NAME, TARGET_COLUMN_NAME]
                edge_attr_names.extend(
                    (
                        x
                        for x in all_edge_attr_names
                        if x not in (SOURCE_COLUMN_NAME, TARGET_COLUMN_NAME)
                    )
                )  # type: ignore
            elif isinstance(incl_edge_attributes, str):
                if incl_edge_attributes not in all_edge_attr_names:
                    raise KiaraException(
                        f"Can't include edge attribute {incl_edge_attributes}: not part of the available attributes ({', '.join(all_edge_attr_names)})."
                    )
                edge_attr_names = [
                    SOURCE_COLUMN_NAME,
                    TARGET_COLUMN_NAME,
                    incl_edge_attributes,
                ]
            else:
                edge_attr_names = [SOURCE_COLUMN_NAME, TARGET_COLUMN_NAME]
                for attr_name in incl_edge_attributes:
                    if incl_edge_attributes not in all_edge_attr_names:
                        raise KiaraException(
                            f"Can't include edge attribute {incl_edge_attributes}: not part of the available attributes ({', '.join(all_edge_attr_names)})."
                        )
                    edge_attr_names.append(attr_name)  # type: ignore

        return edge_attr_names

    def retrieve_graph_data(
        self,
        nodes_callback: Union[NodesCallback, None] = None,
        edges_callback: Union[EdgesCallback, None] = None,
        incl_node_attributes: Union[bool, str, Iterable[str]] = False,
        incl_edge_attributes: Union[bool, str, Iterable[str]] = False,
        omit_self_loops: bool = False,
    ):
        """Retrieve graph data from the sqlite database, and call the specified callbacks for each node and edge.

        First the nodes will be processed, then the edges, if that does not suit your needs you can just use this method twice, and set the callback you don't need to None.

        The nodes_callback will be called with the following arguments:
            - node_id: the id of the node (int)
            - if False: nothing else
            - if True: all node attributes, in the order they are defined in the table schema
            - if str: the value of the specified node attribute
            - if Iterable[str]: the values of the specified node attributes, in the order they are specified

        The edges_callback will be called with the following aruments:
            - source_id: the id of the source node (int)
            - target_id: the id of the target node (int)
            - if False: nothing else
            - if True: all edge attributes, in the order they are defined in the table schema
            - if str: the value of the specified edge attribute
            - if Iterable[str]: the values of the specified edge attributes, in the order they are specified

        """

        if nodes_callback is not None:
            node_attr_names = self._calculate_node_attributes(incl_node_attributes)

            nodes_df = self.nodes.to_polars_dataframe()
            for row in nodes_df.select(*node_attr_names).rows(named=True):
                nodes_callback(**row)  # type: ignore

        if edges_callback is not None:
            edge_attr_names = self._calculate_edge_attributes(incl_edge_attributes)

            edges_df = self.edges.to_polars_dataframe()
            for row in edges_df.select(*edge_attr_names).rows(named=True):
                if (
                    omit_self_loops
                    and row[SOURCE_COLUMN_NAME] == row[TARGET_COLUMN_NAME]
                ):
                    continue
                edges_callback(**row)  # type: ignore

    def as_networkx_graph(
        self,
        graph_type: Type[NETWORKX_GRAPH_TYPE],
        incl_node_attributes: Union[bool, str, Iterable[str]] = False,
        incl_edge_attributes: Union[bool, str, Iterable[str]] = False,
        omit_self_loops: bool = False,
    ) -> NETWORKX_GRAPH_TYPE:
        """Return the network data as a networkx graph object.

        Arguments:
            graph_type: the networkx Graph class to use
            incl_node_attributes: if True, all node attributes are included in the graph, if False, none are, otherwise the specified attributes are included
            incl_edge_attributes: if True, all edge attributes are included in the graph, if False, none are, otherwise the specified attributes are included
            omit_self_loops: if False, self-loops are included in the graph, otherwise they are not added to the resulting graph (nodes that are only connected to themselves are still included)

        """

        graph: NETWORKX_GRAPH_TYPE = graph_type()

        def add_node(_node_id: int, **attrs):
            graph.add_node(_node_id, **attrs)

        def add_edge(_source: int, _target: int, **attrs):
            graph.add_edge(_source, _target, **attrs)

        self.retrieve_graph_data(
            nodes_callback=add_node,
            edges_callback=add_edge,
            incl_node_attributes=incl_node_attributes,
            incl_edge_attributes=incl_edge_attributes,
            omit_self_loops=omit_self_loops,
        )

        return graph

    def as_rustworkx_graph(
        self,
        graph_type: Type[RUSTWORKX_GRAPH_TYPE],
        multigraph: bool = False,
        incl_node_attributes: Union[bool, str, Iterable[str]] = False,
        incl_edge_attributes: Union[bool, str, Iterable[str]] = False,
        omit_self_loops: bool = False,
        attach_node_id_map: bool = False,
    ) -> RUSTWORKX_GRAPH_TYPE:
        """
        Return the network data as a rustworkx graph object.

        Be aware that the node ids in the rustworks graph might not match up with the values of the _node_id column of
        the original network_data. The original _node_id will be set as an attribute (`_node_id`) on the nodes.

        Arguments:
            graph_type: the rustworkx Graph class to use
            multigraph: if True, a Multi(Di)Graph is returned, otherwise a normal (Di)Graph
            incl_node_attributes: if True, all node attributes are included in the graph, if False, none are, otherwise the specified attributes are included
            incl_edge_attributes: if True, all edge attributes are included in the graph, if False, none are, otherwise the specified attributes are included
            omit_self_loops: if False, self-loops are included in the graph, otherwise they are not added to the resulting graph (nodes that are only connected to themselves are still included)
            attach_node_id_map: if True, add the dict describing how the rustworkx graph node ids (key) are mapped to the original node id of the network data, under the 'node_id_map' key in the graph's attributes
        """

        from bidict import bidict

        graph = graph_type(multigraph=multigraph)

        # rustworkx uses 0-based integer indexes, so we don't neeed to look up the node ids (unless we want to
        # include node attributes)

        self._calculate_node_attributes(incl_node_attributes)[1:]
        self._calculate_edge_attributes(incl_edge_attributes)[2:]

        # we can use a 'global' dict here because we know the nodes are processed before the edges
        node_map: bidict = bidict()

        def add_node(_node_id: int, **attrs):
            data = {NODE_ID_COLUMN_NAME: _node_id}
            data.update(attrs)

            graph_node_id = graph.add_node(data)

            node_map[graph_node_id] = _node_id
            # if not _node_id == graph_node_id:
            #     raise Exception("Internal error: node ids don't match")

        def add_edge(_source: int, _target: int, **attrs):

            source = node_map[_source]
            target = node_map[_target]
            if not attrs:
                graph.add_edge(source, target, None)
            else:
                graph.add_edge(source, target, attrs)

        self.retrieve_graph_data(
            nodes_callback=add_node,
            edges_callback=add_edge,
            incl_node_attributes=incl_node_attributes,
            incl_edge_attributes=incl_edge_attributes,
            omit_self_loops=omit_self_loops,
        )

        if attach_node_id_map:
            graph.attrs = {"node_id_map": node_map}  # type: ignore

        return graph


class GraphProperties(BaseModel):
    """Properties of graph data, if interpreted as a specific graph type."""

    number_of_edges: int = Field(description="The number of edges.")
    parallel_edges: int = Field(
        description="The number of parallel edges (if 'multi' graph type).", default=0
    )


class NetworkGraphProperties(ValueMetadata):
    """Network data stats."""

    _metadata_key: ClassVar[str] = "network_data"

    number_of_nodes: int = Field(description="Number of nodes in the network graph.")
    properties_by_graph_type: Dict[  # type: ignore
        Literal[
            GraphType.DIRECTED.value,
            GraphType.UNDIRECTED.value,
            GraphType.UNDIRECTED_MULTI.value,
            GraphType.DIRECTED_MULTI.value,
        ],
        GraphProperties,
    ] = Field(description="Properties of the network data, by graph type.")
    number_of_self_loops: int = Field(
        description="Number of edges where source and target point to the same node."
    )

    @classmethod
    def retrieve_supported_data_types(cls) -> Iterable[str]:
        return ["network_data"]

    @classmethod
    def create_value_metadata(cls, value: Value) -> "NetworkGraphProperties":

        network_data: NetworkData = value.data

        num_rows = network_data.num_nodes
        num_edges = network_data.num_edges

        # query_num_edges_directed = f"SELECT COUNT(*) FROM (SELECT DISTINCT {SOURCE_COLUMN_NAME}, {TARGET_COLUMN_NAME} FROM {EDGES_TABLE_NAME})"
        query_num_edges_directed = f"SELECT COUNT(*) FROM {EDGES_TABLE_NAME} WHERE {COUNT_IDX_DIRECTED_COLUMN_NAME} = 1"

        num_edges_directed_result = network_data.query_edges(query_num_edges_directed)
        num_edges_directed = num_edges_directed_result.columns[0][0].as_py()

        query_num_edges_undirected = f"SELECT COUNT(*) FROM {EDGES_TABLE_NAME} WHERE {COUNT_IDX_UNDIRECTED_COLUMN_NAME} = 1"
        num_edges_undirected_result = network_data.query_edges(
            query_num_edges_undirected
        )
        num_edges_undirected = num_edges_undirected_result.columns[0][0].as_py()

        self_loop_query = f"SELECT count(*) FROM {EDGES_TABLE_NAME} WHERE {SOURCE_COLUMN_NAME} = {TARGET_COLUMN_NAME}"
        self_loop_result = network_data.query_edges(self_loop_query)
        num_self_loops = self_loop_result.columns[0][0].as_py()

        num_parallel_edges_directed_query = f"SELECT COUNT(*) FROM {EDGES_TABLE_NAME} WHERE {COUNT_IDX_DIRECTED_COLUMN_NAME} = 2"
        num_parallel_edges_directed_result = network_data.query_edges(
            num_parallel_edges_directed_query
        )
        num_parallel_edges_directed = num_parallel_edges_directed_result.columns[0][
            0
        ].as_py()

        num_parallel_edges_undirected_query = f"SELECT COUNT(*) FROM {EDGES_TABLE_NAME} WHERE {COUNT_IDX_UNDIRECTED_COLUMN_NAME} = 2"
        num_parallel_edges_undirected_result = network_data.query_edges(
            num_parallel_edges_undirected_query
        )
        num_parallel_edges_undirected = num_parallel_edges_undirected_result.columns[0][
            0
        ].as_py()

        directed_props = GraphProperties(number_of_edges=num_edges_directed)
        undirected_props = GraphProperties(number_of_edges=num_edges_undirected)
        directed_multi_props = GraphProperties(
            number_of_edges=num_edges, parallel_edges=num_parallel_edges_directed
        )
        undirected_multi_props = GraphProperties(
            number_of_edges=num_edges, parallel_edges=num_parallel_edges_undirected
        )

        props = {
            GraphType.DIRECTED.value: directed_props,
            GraphType.DIRECTED_MULTI.value: directed_multi_props,
            GraphType.UNDIRECTED.value: undirected_props,
            GraphType.UNDIRECTED_MULTI.value: undirected_multi_props,
        }

        result = cls(
            number_of_nodes=num_rows,
            properties_by_graph_type=props,
            number_of_self_loops=num_self_loops,
        )
        return result


# kiara\kiara_plugin.network_analysis\src\kiara_plugin\network_analysis\modules\components.py
# -*- coding: utf-8 -*-
from typing import TYPE_CHECKING, Any, Dict

from kiara.api import KiaraModule, ValueMap, ValueMapSchema
from kiara.exceptions import KiaraException
from kiara_plugin.network_analysis.defaults import (
    ATTRIBUTE_PROPERTY_KEY,
    COMPONENT_ID_COLUMN_NAME,
    IS_CUTPOINT_COLUMN_NAME,
)
from kiara_plugin.network_analysis.models import NetworkData
from kiara_plugin.network_analysis.models.metadata import NetworkNodeAttributeMetadata

if TYPE_CHECKING:
    from kiara.models import KiaraModel

KIARA_METADATA = {
    "authors": [
        {"name": "Lena Jaskov", "email": "helena.jaskov@uni.lu"},
        {"name": "Caitlin Burge", "email": "caitlin.burge@uni.lu"},
        {"name": "Markus Binsteiner", "email": "markus@frkl.io"},
    ],
    "description": "Modules related to extracting components from network data.",
}

COMPONENT_COLUMN_TEXT = """The id of the component the node is part of.

If all nodes are connected, all nodes will have '0' as value in the component_id field. Otherwise, the nodes will be assigned 'component_id'-s according to the component they belong to, with the largest component having '0' as component_id, the second largest '1' and so on. If two components have the same size, who gets the higher component_id is not determinate."""
COMPONENT_COLUMN_METADATA = NetworkNodeAttributeMetadata(doc=COMPONENT_COLUMN_TEXT, computed_attribute=True)  # type: ignore

CUT_POINTS_TEXT = """Whether the node is a cut point or not."""
CUT_POINTS_COLUMN_METADATA = NetworkNodeAttributeMetadata(doc=COMPONENT_COLUMN_TEXT, computed_attribute=True)  # type: ignore


class CalculateComponentModule(KiaraModule):
    """Calculate component information for this network data.

    This module analyses network data and checks if it contains clusters, and if so, how many. If all nodes are connected, all nodes will have '0' as value in the component_id field.

    Otherwise, the nodes will be assigned 'component_id'-s according to the component they belong to, with the  largest component having '0' as component_id, the second largest '1' and so on. If two components have the same size, who gets the higher component_id is not determinate.
    """

    _module_type_name = "network_data.calculate_components"

    def create_inputs_schema(
        self,
    ) -> ValueMapSchema:
        result = {
            "network_data": {
                "type": "network_data",
                "doc": "The network data to analyze.",
            }
        }
        return result

    def create_outputs_schema(
        self,
    ) -> ValueMapSchema:
        result: Dict[str, Dict[str, Any]] = {}

        result["network_data"] = {
            "type": "network_data",
            "doc": "The network_data, with a new column added to the nodes table, indicating the component the node belongs to.",
        }

        result["number_of_components"] = {
            "type": "integer",
            "doc": "The number of components in the graph.",
        }

        result["is_connected"] = {
            "type": "boolean",
            "doc": "Whether the graph is connected or not.",
        }
        return result

    def process(self, inputs: ValueMap, outputs: ValueMap):
        import pyarrow as pa
        import rustworkx as rx

        network_value = inputs.get_value_obj("network_data")
        network_data: NetworkData = network_value.data

        # TODO: maybe this can be done directly in sql, without networx, which would be faster and better
        # for memory usage
        undir_graph = network_data.as_rustworkx_graph(
            graph_type=rx.PyGraph,
            multigraph=False,
            omit_self_loops=False,
            attach_node_id_map=True,
        )
        undir_components = rx.connected_components(undir_graph)  # type: ignore

        nodes_columns_metadata: Dict[str, Dict[str, KiaraModel]] = {
            COMPONENT_ID_COLUMN_NAME: {
                ATTRIBUTE_PROPERTY_KEY: COMPONENT_COLUMN_METADATA
            }
        }

        if len(undir_components) == 1:

            nodes = network_data.nodes.arrow_table
            components_column = pa.array([0] * len(nodes), type=pa.int64())
            nodes = nodes.append_column(COMPONENT_ID_COLUMN_NAME, components_column)

            network_data = NetworkData.create_network_data(
                nodes_table=nodes,
                edges_table=network_data.edges.arrow_table,
                augment_tables=False,
                nodes_column_metadata=nodes_columns_metadata,
            )
            outputs.set_values(
                network_data=network_data,
                number_of_components=1,
                is_connected=True,
            )
            return

        number_of_components = len(undir_components)
        is_connected = False
        node_id_map = undir_graph.attrs["node_id_map"]  # type: ignore

        node_components = {}
        for idx, component in enumerate(
            sorted(undir_components, key=len, reverse=True)
        ):
            for node in component:
                node_id = node_id_map[node]
                node_components[node_id] = idx

        if len(node_components) != network_data.num_nodes:
            raise KiaraException(
                "Number of nodes in component map does not match number of nodes in network data. This is most likely a bug."
            )

        components_column = pa.array(
            (node_components[node_id] for node_id in sorted(node_components.keys())),
            type=pa.int64(),
        )

        nodes = network_data.nodes.arrow_table
        nodes = nodes.append_column(COMPONENT_ID_COLUMN_NAME, components_column)
        network_data = NetworkData.create_network_data(
            nodes_table=nodes,
            edges_table=network_data.edges.arrow_table,
            augment_tables=False,
            nodes_column_metadata=nodes_columns_metadata,
        )
        outputs.set_values(
            is_connected=is_connected,
            number_of_components=number_of_components,
            network_data=network_data,
        )


class CutPointsList(KiaraModule):
    """Create a list of nodes that are cut-points.
    Cut-points are any node in a network whose removal disconnects members of the network, creating one or more new distinct components.

    Uses the [rustworkx.articulation_points](https://qiskit.org/documentation/retworkx/dev/apiref/rustworkx.articulation_points.html#rustworkx-articulation-points) function.
    """

    _module_type_name = "network_data.extract_cut_points"

    def create_inputs_schema(self):
        return {
            "network_data": {
                "type": "network_data",
                "doc": "The network graph being queried.",
            }
        }

    def create_outputs_schema(self):
        return {
            "network_data": {
                "type": "network_data",
                "doc": """The network_data, with a new column added to the nodes table, indicating whether the node is a cut-point or not. The column is named 'is_cut_point' and is of type 'boolean'.""",
            }
        }

    def process(self, inputs, outputs) -> None:

        import pyarrow as pa
        import rustworkx as rx

        network_value = inputs.get_value_obj("network_data")
        network_data: NetworkData = network_value.data

        # TODO: maybe this can be done directly in sql, without networx, which would be faster and better
        # for memory usage
        undir_graph = network_data.as_rustworkx_graph(
            graph_type=rx.PyGraph,
            multigraph=False,
            omit_self_loops=False,
            attach_node_id_map=True,
        )

        node_id_map = undir_graph.attrs["node_id_map"]  # type: ignore

        cut_points = rx.articulation_points(undir_graph)  # type: ignore
        translated_cut_points = [node_id_map[x] for x in cut_points]
        if not cut_points:
            raise NotImplementedError()
        cut_points_column = [
            x in translated_cut_points
            for x in range(0, network_data.num_nodes)  # noqa: PIE808
        ]

        nodes = network_data.nodes.arrow_table
        nodes = nodes.append_column(
            IS_CUTPOINT_COLUMN_NAME, pa.array(cut_points_column, type=pa.bool_())
        )

        nodes_columns_metadata: Dict[str, Dict[str, KiaraModel]] = {
            IS_CUTPOINT_COLUMN_NAME: {
                ATTRIBUTE_PROPERTY_KEY: CUT_POINTS_COLUMN_METADATA
            }
        }

        network_data = NetworkData.create_network_data(
            nodes_table=nodes,
            edges_table=network_data.edges.arrow_table,
            augment_tables=False,
            nodes_column_metadata=nodes_columns_metadata,
        )
        outputs.set_values(network_data=network_data)


# kiara\kiara_plugin.network_analysis\src\kiara_plugin\network_analysis\modules\create.py
# -*- coding: utf-8 -*-
from typing import Any, Dict, List, Mapping, Union

from pydantic import Field

from kiara.api import KiaraModule, ValueMap, ValueMapSchema
from kiara.exceptions import KiaraProcessingException
from kiara.models.filesystem import (
    KiaraFile,
)
from kiara.models.module import KiaraModuleConfig
from kiara.models.module.jobs import JobLog
from kiara.models.values.value import Value
from kiara.modules.included_core_modules.create_from import (
    CreateFromModule,
    CreateFromModuleConfig,
)
from kiara_plugin.network_analysis.defaults import (
    LABEL_ALIAS_NAMES,
    LABEL_COLUMN_NAME,
    NODE_ID_ALIAS_NAMES,
    NODE_ID_COLUMN_NAME,
    SOURCE_COLUMN_ALIAS_NAMES,
    SOURCE_COLUMN_NAME,
    TARGET_COLUMN_ALIAS_NAMES,
    TARGET_COLUMN_NAME,
)
from kiara_plugin.network_analysis.models import NetworkData
from kiara_plugin.tabular.models.table import KiaraTable

KIARA_METADATA = {
    "authors": [
        {"name": "Lena Jaskov", "email": "helena.jaskov@uni.lu"},
        {"name": "Markus Binsteiner", "email": "markus@frkl.io"},
    ],
    "description": "Modules to create/export network data.",
}


class CreateNetworkDataModuleConfig(CreateFromModuleConfig):
    ignore_errors: bool = Field(
        description="Whether to ignore convert errors and omit the failed items.",
        default=False,
    )


class CreateNetworkDataModule(CreateFromModule):
    _module_type_name = "create.network_data"
    _config_cls = CreateNetworkDataModuleConfig

    def create__network_data__from__file(self, source_value: Value) -> Any:
        """Create a table from a file, trying to auto-determine the format of said file.

        Supported file formats (at the moment):
        - gml
        - gexf
        - graphml (uses the standard xml library present in Python, which is insecure - see xml for additional information. Only parse GraphML files you trust)
        - pajek
        - leda
        - graph6
        - sparse6
        """

        source_file: KiaraFile = source_value.data
        # the name of the attribute kiara should use to populate the node labels
        label_attr_name: Union[str, None] = None
        # attributes to ignore when creating the node table,
        # mostly useful if we know that the file contains attributes that are not relevant for the network
        # or for 'label', if we don't want to duplicate the information in '_label' and 'label'
        ignore_node_attributes = None

        if source_file.file_name.endswith(".gml"):
            import networkx as nx

            # we use 'lable="id"' here because networkx is fussy about labels being unique and non-null
            # we use the 'label' attribute for the node labels manually later
            graph = nx.read_gml(source_file.path, label="id")
            label_attr_name = "label"
            ignore_node_attributes = ["label"]

        elif source_file.file_name.endswith(".gexf"):
            import networkx as nx

            graph = nx.read_gexf(source_file.path)
        elif source_file.file_name.endswith(".graphml"):
            import networkx as nx

            graph = nx.read_graphml(source_file.path)
        elif source_file.file_name.endswith(".pajek") or source_file.file_name.endswith(
            ".net"
        ):
            import networkx as nx

            graph = nx.read_pajek(source_file.path)
        elif source_file.file_name.endswith(".leda"):
            import networkx as nx

            graph = nx.read_leda(source_file.path)
        elif source_file.file_name.endswith(
            ".graph6"
        ) or source_file.file_name.endswith(".g6"):
            import networkx as nx

            graph = nx.read_graph6(source_file.path)
        elif source_file.file_name.endswith(
            ".sparse6"
        ) or source_file.file_name.endswith(".s6"):
            import networkx as nx

            graph = nx.read_sparse6(source_file.path)
        else:
            supported_file_estensions = [
                "gml",
                "gexf",
                "graphml",
                "pajek",
                "leda",
                "graph6",
                "g6",
                "sparse6",
                "s6",
            ]

            msg = f"Can't create network data for unsupported format of file: {source_file.file_name}. Supported file extensions: {', '.join(supported_file_estensions)}"

            raise KiaraProcessingException(msg)

        return NetworkData.create_from_networkx_graph(
            graph=graph,
            label_attr_name=label_attr_name,
            ignore_node_attributes=ignore_node_attributes,
        )


class AssembleNetworkDataModuleConfig(KiaraModuleConfig):
    node_id_column_aliases: List[str] = Field(
        description="Alias strings to test (in order) for auto-detecting the node id column.",
        default=NODE_ID_ALIAS_NAMES,
    )  # pydantic should handle that correctly (deepcopy) -- and anyway, it's immutable (hopefully)
    label_column_aliases: List[str] = Field(
        description="Alias strings to test (in order) for auto-detecting the node label column.",
        default=LABEL_ALIAS_NAMES,
    )
    source_column_aliases: List[str] = Field(
        description="Alias strings to test (in order) for auto-detecting the source column.",
        default=SOURCE_COLUMN_ALIAS_NAMES,
    )
    target_column_aliases: List[str] = Field(
        description="Alias strings to test (in order) for auto-detecting the target column.",
        default=TARGET_COLUMN_ALIAS_NAMES,
    )


class AssembleGraphFromTablesModule(KiaraModule):
    """Create a 'network_data' instance from one or two tables.

    This module needs at least one table as input, providing the edges of the resulting network data set.
    If no further table is created, basic node information will be automatically created by using unique values from the edges source and target columns.

    If no `source_column_name` (and/or `target_column_name`) is provided, *kiara* will try to auto-detect the most likely of the existing columns to use. If that is not possible, an error will be raised.
    """

    _module_type_name = "assemble.network_data"
    _config_cls = AssembleNetworkDataModuleConfig

    def create_inputs_schema(
        self,
    ) -> ValueMapSchema:
        inputs: Mapping[str, Any] = {
            "edges": {
                "type": "table",
                "doc": "A table that contains the edges data.",
                "optional": False,
            },
            "source_column": {
                "type": "string",
                "doc": "The name of the source column name in the edges table.",
                "optional": True,
            },
            "target_column": {
                "type": "string",
                "doc": "The name of the target column name in the edges table.",
                "optional": True,
            },
            "edges_column_map": {
                "type": "dict",
                "doc": "An optional map of original column name to desired.",
                "optional": True,
            },
            "nodes": {
                "type": "table",
                "doc": "A table that contains the nodes data.",
                "optional": True,
            },
            "id_column": {
                "type": "string",
                "doc": "The name (before any potential column mapping) of the node-table column that contains the node identifier (used in the edges table).",
                "optional": True,
            },
            "label_column": {
                "type": "string",
                "doc": "The name of a column that contains the node label (before any potential column name mapping). If not specified, the value of the id value will be used as label.",
                "optional": True,
            },
            "nodes_column_map": {
                "type": "dict",
                "doc": "An optional map of original column name to desired.",
                "optional": True,
            },
        }
        return inputs

    def create_outputs_schema(
        self,
    ) -> ValueMapSchema:
        outputs: Mapping[str, Any] = {
            "network_data": {"type": "network_data", "doc": "The network/graph data."}
        }
        return outputs

    def process(self, inputs: ValueMap, outputs: ValueMap, job_log: JobLog) -> None:

        import polars as pl

        # process nodes
        nodes = inputs.get_value_obj("nodes")

        # the nodes column map can be used to rename attribute columns in the nodes table
        nodes_column_map: Dict[str, str] = inputs.get_value_data("nodes_column_map")
        if nodes_column_map is None:
            nodes_column_map = {}

        # we need to process the nodes first, because if we have nodes, we need to create the node id map that translates from the original
        # id to the new, internal, integer-based one

        if nodes.is_set:

            job_log.add_log("processing nodes table")

            nodes_table: KiaraTable = nodes.data
            assert nodes_table is not None

            nodes_column_names = nodes_table.column_names

            # the most important column is the id column, which is the only one that we absolutely need to have
            id_column_name = inputs.get_value_data("id_column")

            if id_column_name is None:
                # try to auto-detect the id column
                column_names_to_test = self.get_config_value("node_id_column_aliases")
                for col_name in nodes_column_names:
                    if col_name.lower() in column_names_to_test:
                        id_column_name = col_name
                        break

                job_log.add_log(f"auto-detected id column: {id_column_name}")
                if id_column_name is None:
                    raise KiaraProcessingException(
                        f"Could not auto-determine id column name. Please specify one manually, using one of: {', '.join(nodes_column_names)}"
                    )

            if id_column_name not in nodes_column_names:
                raise KiaraProcessingException(
                    f"Could not find id column '{id_column_name}' in the nodes table. Please specify a valid column name manually, using one of: {', '.join(nodes_column_names)}"
                )

            nodes_column_map[id_column_name] = NODE_ID_COLUMN_NAME
            if id_column_name in nodes_column_map.keys():
                if nodes_column_map[id_column_name] != NODE_ID_COLUMN_NAME:
                    raise KiaraProcessingException(
                        f"Existing mapping of id column name '{id_column_name}' is not mapped to '{NODE_ID_COLUMN_NAME}' in the 'nodes_column_map' input."
                    )
            else:
                nodes_column_map[id_column_name] = NODE_ID_COLUMN_NAME

            # the label is optional, if not specified, we try to auto-detect it. If not possible, we will use the (stringified) id column as label.
            label_column_name = inputs.get_value_data("label_column")
            if label_column_name is None:
                job_log.add_log("auto-detecting label column")
                column_names_to_test = self.get_config_value("label_column_aliases")
                for col_name in nodes_column_names:
                    if col_name.lower() in column_names_to_test:
                        label_column_name = col_name
                        job_log.add_log(
                            f"auto-detected label column: {label_column_name}"
                        )
                        break

            if label_column_name and label_column_name not in nodes_column_names:
                raise KiaraProcessingException(
                    f"Could not find id column '{id_column_name}' in the nodes table. Please specify a valid column name manually, using one of: {', '.join(nodes_column_names)}"
                )

            nodes_arrow_dataframe = nodes_table.to_polars_dataframe()

        else:
            nodes_arrow_dataframe = None
            label_column_name = None

        # process edges

        job_log.add_log("processing edges table")
        edges = inputs.get_value_obj("edges")
        edges_table: KiaraTable = edges.data
        edges_source_column_name = inputs.get_value_data("source_column")
        edges_target_column_name = inputs.get_value_data("target_column")

        edges_arrow_dataframe = edges_table.to_polars_dataframe()
        edges_column_names = edges_arrow_dataframe.columns

        if edges_source_column_name is None:
            job_log.add_log("auto-detecting source column")
            column_names_to_test = self.get_config_value("source_column_aliases")
            for item in edges_column_names:
                if item.lower() in column_names_to_test:
                    edges_source_column_name = item
                    job_log.add_log(
                        f"auto-detected source column: {edges_source_column_name}"
                    )
                    break

        if edges_target_column_name is None:
            job_log.add_log("auto-detecting target column")
            column_names_to_test = self.get_config_value("target_column_aliases")
            for item in edges_column_names:
                if item.lower() in column_names_to_test:
                    edges_target_column_name = item
                    job_log.add_log(
                        f"auto-detected target column: {edges_target_column_name}"
                    )
                    break

        if not edges_source_column_name or not edges_target_column_name:
            if not edges_source_column_name and not edges_target_column_name:
                if len(edges_column_names) == 2:
                    job_log.add_log(
                        "using first two columns as source and target columns"
                    )
                    edges_source_column_name = edges_column_names[0]
                    edges_target_column_name = edges_column_names[1]
                else:
                    raise KiaraProcessingException(
                        f"Could not auto-detect source and target column names. Please specify them manually using one of: {', '.join(edges_column_names)}."
                    )

            if not edges_source_column_name:
                raise KiaraProcessingException(
                    f"Could not auto-detect source column name. Please specify it manually using one of: {', '.join(edges_column_names)}."
                )

            if not edges_target_column_name:
                raise KiaraProcessingException(
                    f"Could not auto-detect target column name. Please specify it manually using one of: {', '.join(edges_column_names)}."
                )

        edges_column_map: Dict[str, str] = inputs.get_value_data("edges_column_map")
        if edges_column_map is None:
            edges_column_map = {}

        if edges_source_column_name in edges_column_map.keys():
            if edges_column_map[edges_source_column_name] != SOURCE_COLUMN_NAME:
                raise KiaraProcessingException(
                    f"Existing mapping of source column name '{edges_source_column_name}' is not mapped to '{SOURCE_COLUMN_NAME}' in the 'edges_column_map' input."
                )
        else:
            edges_column_map[edges_source_column_name] = SOURCE_COLUMN_NAME

        if edges_target_column_name in edges_column_map.keys():
            if edges_column_map[edges_target_column_name] == SOURCE_COLUMN_NAME:
                raise KiaraProcessingException(
                    msg="Edges and source column names can't be the same."
                )
            if edges_column_map[edges_target_column_name] != TARGET_COLUMN_NAME:
                raise KiaraProcessingException(
                    f"Existing mapping of target column name '{edges_target_column_name}' is not mapped to '{TARGET_COLUMN_NAME}' in the 'edges_column_map' input."
                )
        else:
            edges_column_map[edges_target_column_name] = TARGET_COLUMN_NAME

        if edges_source_column_name not in edges_column_names:
            raise KiaraProcessingException(
                f"Edges table does not contain source column '{edges_source_column_name}'. Choose one of: {', '.join(edges_column_names)}."
            )
        if edges_target_column_name not in edges_column_names:
            raise KiaraProcessingException(
                f"Edges table does not contain target column '{edges_target_column_name}'. Choose one of: {', '.join(edges_column_names)}."
            )

        source_column_old = edges_arrow_dataframe.get_column(edges_source_column_name)
        target_column_old = edges_arrow_dataframe.get_column(edges_target_column_name)

        job_log.add_log("generating node id map and nodes table")
        # fill out the node id map
        unique_node_ids_old = (
            pl.concat([source_column_old, target_column_old], rechunk=False)
            .unique()
            .sort()
        )

        if nodes_arrow_dataframe is None:
            new_node_ids = range(0, len(unique_node_ids_old))  # noqa: PIE808
            node_id_map = dict(zip(unique_node_ids_old, new_node_ids))
            # node_id_map = {
            #     node_id: new_node_id
            #     for node_id, new_node_id in
            # }

            nodes_arrow_dataframe = pl.DataFrame(
                {
                    NODE_ID_COLUMN_NAME: new_node_ids,
                    LABEL_COLUMN_NAME: (str(x) for x in unique_node_ids_old),
                    "id": unique_node_ids_old,
                }
            )

        else:
            id_column_old = nodes_arrow_dataframe.get_column(id_column_name)
            unique_node_ids_nodes_table = id_column_old.unique().sort()

            if len(unique_node_ids_old) > len(unique_node_ids_nodes_table):
                ~(unique_node_ids_old.is_in(unique_node_ids_nodes_table))
                raise NotImplementedError("MISSING NODE IDS NOT IMPLEMENTED YET")
            else:
                new_node_ids = range(0, len(id_column_old))  # noqa: PIE808
                node_id_map = dict(zip(id_column_old, new_node_ids))
                # node_id_map = {
                #     node_id: new_node_id
                #     for node_id, new_node_id in
                # }
                new_idx_series = pl.Series(
                    name=NODE_ID_COLUMN_NAME, values=new_node_ids
                )
                nodes_arrow_dataframe.insert_at_idx(0, new_idx_series)

                if not label_column_name:
                    label_column_name = NODE_ID_COLUMN_NAME

                # we create a copy of the label column, and stringify its items

                label_column = nodes_arrow_dataframe.get_column(
                    label_column_name
                ).rename(LABEL_COLUMN_NAME)
                if label_column.dtype != pl.Utf8:
                    label_column = label_column.cast(pl.Utf8)

                if label_column.null_count() != 0:
                    raise KiaraProcessingException(
                        f"Label column '{label_column_name}' contains null values. This is not allowed."
                    )

                nodes_arrow_dataframe = nodes_arrow_dataframe.insert_at_idx(
                    1, label_column
                )

        # TODO: deal with different types if node ids are strings or integers
        try:
            source_column_mapped = source_column_old.map_dict(
                node_id_map, default=None
            ).rename(SOURCE_COLUMN_NAME)
        except Exception:
            raise KiaraProcessingException(
                "Could not map node ids onto edges source column.  In most cases the issue is that your node ids have a different data type in your nodes table as in the source column of your edges table."
            )

        if source_column_mapped.is_null().any():
            raise KiaraProcessingException(
                "The source column contains values that are not mapped in the nodes table."
            )

        try:
            target_column_mapped = target_column_old.map_dict(
                node_id_map, default=None
            ).rename(TARGET_COLUMN_NAME)
        except Exception:
            raise KiaraProcessingException(
                "Could not map node ids onto edges source column.  In most cases the issue is that your node ids have a different data type in your nodes table as in the target column of your edges table."
            )

        if target_column_mapped.is_null().any():
            raise KiaraProcessingException(
                "The target column contains values that are not mapped in the nodes table."
            )

        edges_arrow_dataframe.insert_at_idx(0, source_column_mapped)
        edges_arrow_dataframe.insert_at_idx(1, target_column_mapped)

        edges_arrow_dataframe = edges_arrow_dataframe.drop(edges_source_column_name)
        edges_arrow_dataframe = edges_arrow_dataframe.drop(edges_target_column_name)

        edges_arrow_table = edges_arrow_dataframe.to_arrow()
        # edges_table_augmented = augment_edges_table_with_weights(edges_arrow_dataframe)

        # # TODO: also index the other columns?
        # edges_data_schema = create_sqlite_schema_data_from_arrow_table(
        #     table=edges_arrow_dataframe,
        #     index_columns=[SOURCE_COLUMN_NAME, TARGET_COLUMN_NAME],
        #     column_map=edges_column_map,
        # )

        nodes_arrow_table = nodes_arrow_dataframe.to_arrow()

        job_log.add_log("creating network data instance")
        network_data = NetworkData.create_network_data(
            nodes_table=nodes_arrow_table, edges_table=edges_arrow_table
        )

        outputs.set_value("network_data", network_data)


# class FilteredNetworkDataModule(KiaraModule):
#     """Create a new network_data instance from an existing one, using only a sub-set of nodes and/or edges."""
#
#     def create_inputs_schema(
#         self,
#     ) -> ValueMapSchema:
#         return {}
#
#     def create_outputs_schema(
#         self,
#     ) -> ValueMapSchema:
#         return {}


# kiara\kiara_plugin.network_analysis\src\kiara_plugin\network_analysis\modules\export.py
# -*- coding: utf-8 -*-
import os

from kiara.modules.included_core_modules.export_as import DataExportModule
from kiara_plugin.network_analysis.models import NetworkData

KIARA_METADATA = {
    "authors": [{"name": "Markus Binsteiner", "email": "markus@frkl.io"}],
    "description": "Modules related to extracting components from network data.",
}


class ExportNetworkDataModule(DataExportModule):
    """Export network data items."""

    _module_type_name = "export.network_data"

    def export__network_data__as__graphml_file(
        self, value: NetworkData, base_path: str, name: str
    ):
        """Export network data as graphml file."""

        import networkx as nx

        target_path = os.path.join(base_path, f"{name}.graphml")

        # TODO: can't just assume digraph
        graph: nx.Graph = value.as_networkx_graph(
            nx.DiGraph, incl_node_attributes=True, incl_edge_attributes=True
        )
        nx.write_graphml(graph, target_path)

        return {"files": target_path}

    def export__network_data__as__gexf_file(
        self, value: NetworkData, base_path: str, name: str
    ):
        """Export network data as gexf file."""

        import networkx as nx

        target_path = os.path.join(base_path, f"{name}.gexf")

        # TODO: can't just assume digraph
        graph: nx.Graph = value.as_networkx_graph(
            nx.DiGraph, incl_node_attributes=True, incl_edge_attributes=True
        )
        nx.write_gexf(graph, target_path)

        return {"files": target_path}

    def export__network_data__as__adjlist_file(
        self, value: NetworkData, base_path: str, name: str
    ):
        """Export network data as adjacency list file."""

        import networkx as nx

        target_path = os.path.join(base_path, f"{name}.adjlist")

        # TODO: can't just assume digraph
        graph: nx.Graph = value.as_networkx_graph(
            nx.DiGraph, incl_node_attributes=True, incl_edge_attributes=True
        )
        nx.write_adjlist(graph, target_path)

        return {"files": target_path}

    def export__network_data__as__multiline_adjlist_file(
        self, value: NetworkData, base_path: str, name: str
    ):
        """Export network data as multiline adjacency list file."""

        import networkx as nx

        target_path = os.path.join(base_path, f"{name}.adjlist_multiline")

        # TODO: can't just assume digraph
        graph: nx.Graph = value.as_networkx_graph(
            nx.DiGraph, incl_node_attributes=True, incl_edge_attributes=True
        )
        nx.write_multiline_adjlist(graph, target_path)

        return {"files": target_path}

    def export__network_data__as__edgelist_file(
        self, value: NetworkData, base_path: str, name: str
    ):
        """Export network data as edgelist file."""

        import networkx as nx

        target_path = os.path.join(base_path, f"{name}.edge_list")

        # TODO: can't just assume digraph
        graph: nx.Graph = value.as_networkx_graph(
            nx.DiGraph, incl_node_attributes=True, incl_edge_attributes=True
        )
        nx.write_edgelist(graph, target_path)

        return {"files": target_path}

    def export__network_data__as__network_text_file(
        self, value: NetworkData, base_path: str, name: str
    ):
        """Export network data as network text file (with a '.network' extension)."""

        import networkx as nx

        target_path = os.path.join(base_path, f"{name}.network")

        # TODO: can't just assume digraph
        graph: nx.Graph = value.as_networkx_graph(
            nx.DiGraph, incl_node_attributes=True, incl_edge_attributes=True
        )
        nx.write_network_text(graph, target_path)

        return {"files": target_path}


# kiara\kiara_plugin.network_analysis\src\kiara_plugin\network_analysis\modules\filters.py
# -*- coding: utf-8 -*-
from typing import Any, Dict, Mapping, Union

from kiara.exceptions import KiaraProcessingException
from kiara.models.values.value import Value
from kiara.modules import ValueMapSchema
from kiara.modules.included_core_modules.filter import FilterModule
from kiara_plugin.network_analysis.defaults import (
    COMPONENT_ID_COLUMN_NAME,
    NODE_ID_COLUMN_NAME,
)
from kiara_plugin.network_analysis.models import NetworkData


class TableFiltersModule(FilterModule):

    _module_type_name = "network_data.filters"

    @classmethod
    def retrieve_supported_type(cls) -> Union[Dict[str, Any], str]:

        return "network_data"

    def create_filter_inputs(self, filter_name: str) -> Union[None, ValueMapSchema]:

        if filter_name == "component":
            return {
                "component_id": {
                    "type": "string",
                    "doc": "The id of the component to extract.",
                    "default": "0",
                },
                "component_column": {
                    "type": "string",
                    "doc": "The name of the colum that contains the component id.",
                    "default": COMPONENT_ID_COLUMN_NAME,
                },
            }

        return None

    def filter__component(self, value: Value, filter_inputs: Mapping[str, Any]):
        """Retrieve a single sub-component from a network data object."""

        component_id = filter_inputs["component_id"]
        component_column = filter_inputs["component_column"]

        network_data: NetworkData = value.data

        if component_column not in network_data.nodes.column_names:
            msg = f"Component column `{component_column}` not valid for this network_data instance.\n\nAvailable column names:\n\n"

            for attr in network_data.nodes.column_names:
                msg += f"  - {attr}\n"

            if component_column == COMPONENT_ID_COLUMN_NAME:
                msg = f"{msg}\n\nTry to run the `network_data.extract_components` module on your network_data before using this module."

            raise KiaraProcessingException(msg)

        # network_data.nodes.arrow_table.column(component_column).type
        # filter_item = pa.scalar(component_id, type=pa.int32())

        query = f"select {NODE_ID_COLUMN_NAME} from nodes where {component_column} = '{component_id}'"
        node_result = network_data.query_nodes(query)

        network_data = NetworkData.from_filtered_nodes(
            network_data=network_data,
            nodes_list=node_result.column(NODE_ID_COLUMN_NAME).to_pylist(),
        )

        return network_data


# kiara\kiara_plugin.network_analysis\src\kiara_plugin\network_analysis\modules\rendering.py
# -*- coding: utf-8 -*-
# from kiara.models.rendering import RenderValueResult
# from kiara.modules.included_core_modules.render_value import RenderValueModule
#
# KIARA_METADATA = {
#     "authors": [
#        {"name": "Markus Binsteiner", "email": "markus@frkl.io"}
#     ],
#     "description": "Modules related to extracting components from network data.",
# }
# class RenderNetworkModule(RenderValueModule):
#     _module_type_name = "render.network_data.for.web"
#
#     def render__network_data__as__html(
#         self, value: Value, render_config: Mapping[str, Any]
#     ):
#         input_number_of_rows = render_config.get("number_of_rows", 20)
#         input_row_offset = render_config.get("row_offset", 0)
#
#         table_name = render_config.get("table_name", None)
#
#         wrap, data_related_scenes = self.preprocess_database(
#             value=value,
#             table_name=table_name,
#             input_number_of_rows=input_number_of_rows,
#             input_row_offset=input_row_offset,
#         )
#         pretty = wrap.as_html(max_row_height=1)
#
#         result = RenderValueResult(
#             value_id=value.value_id,
#             render_config=render_config,
#             render_manifest=self.manifest.manifest_hash,
#             rendered=pretty,
#             related_scenes=data_related_scenes,
#         )
#         return result


# kiara\kiara_plugin.network_analysis\src\kiara_plugin\network_analysis\modules\__init__.py
# -*- coding: utf-8 -*-
from typing import Any, Dict, List, Mapping, Union

from kiara.api import KiaraModule, ValueMap, ValueMapSchema
from kiara.exceptions import KiaraProcessingException
from kiara_plugin.core_types.data_types.models import KiaraModelList
from kiara_plugin.network_analysis.defaults import (
    ALLOWED_AGGREGATION_FUNCTIONS,
    SOURCE_COLUMN_NAME,
    TARGET_COLUMN_NAME,
)
from kiara_plugin.network_analysis.models import NetworkData
from kiara_plugin.network_analysis.models.inputs import AttributeMapStrategy

KIARA_METADATA = {
    "authors": [
        {"name": "Lena Jaskov", "email": "helena.jaskov@uni.lu"},
        {"name": "Markus Binsteiner", "email": "markus@frkl.io"},
    ],
    "description": "Modules related to extracting components from network data.",
    "tags": ["network", "network analysis", "network_graphs"],
}


def generate_redefine_edges_doc():
    REDEFINE_EDGES_DOC = """Redefine edges by merging duplicate edges and applying aggregation functions to certain edge attributes.

The main use cases for this::

  - 'flatten' a multigraph into a non-multigraph one
  - independently aggregate existing edge attribute columns into the new network_data instance
  - let the user specify columns that will be included in the 'group_by' clause of the generated sql query (in which case we don't create a not-multigraph, but a multigraph with a different set of parallel edges)
  - rename existing edge attributes

By default, this operation does not copy existing any attributes, so every edge attribute that should be contained in the resulting, new network_data instance needs to be specified in the 'attributes' input.
Automatically computed attributes/columns (those that start with '_') can be used as source columns, but need to be renamed to not start with "_".

If no target_column is specified, the original source column name is used. If no transform function is specified, the column data is copied as is.

### Available transformation functions:

"""

    funcs_doc = ""
    for name, doc in ALLOWED_AGGREGATION_FUNCTIONS.items():
        funcs_doc = f"{funcs_doc}\n - ***{name}***: {doc}"

    doc = f"{REDEFINE_EDGES_DOC}\n\n{funcs_doc}"

    EXAMPLES = """### Examples

In the commandline, *kiara* will parse each input string as a column transformation. If the string does not contain a '=', the column will not be renamed, and the default transformation will be applied (COUNT).

```
kiara run network_data.redefine_edges network_data=simple attributes=weight
```

If you want to use a transformation or rename an attribute, you can specify the details after a '=':

```
kiara run network_data.redefine_edges network_data=simple 'attributes=new_time_column_name=time' 'attributes=sum_weight=SUM(weight)'
```

This example would copy all contents of the original 'time' column to a new column named 'new_time_column_name' (with a list of all 'time' values of a specific source/target combination), and create a column 'sum_weight' that contains the sum of all 'weight' values of duplicate edges.

If using this operation from Python or some other way, the 'column' inputs for the previous two examples would look like:

```
inputs:
  network_data: ...
  attributes:
    - target_column_name: weight
      source_column_name: weight
      transform_function: sum
    - target_column_name: time
      source_column_name: new_time_column_name
      transform_function: list
```

and

```
inputs:
  network_data: ...
  attributes:
    - target_column_name: new_time_column_name
      source_column_name: time
      transform_function: list
    - target_column_name: count_weight
      source_column_name: weight
      transform_function: count
```
"""

    doc = f"{doc}\n\n{EXAMPLES}"
    return doc


class RedefineNetworkEdgesModule(KiaraModule):
    """Redefine edges by merging duplicate edges and applying aggregation functions to certain edge attributes."""

    _module_type_name = "network_data.redefine_edges"

    KIARA_METADATA = {
        "references": {
            "discussion": {
                "url": "https://github.com/DHARPA-Project/kiara_plugin.network_analysis/discussions/23"
            }
        }
    }

    @classmethod
    def type_doc(cls):

        return generate_redefine_edges_doc()

    def create_inputs_schema(
        self,
    ) -> ValueMapSchema:
        result: Mapping[str, Mapping[str, Any]] = {
            "network_data": {
                "type": "network_data",
                "doc": "The network data to flatten.",
            },
            "attributes": {
                "type": "kiara_model_list",
                "type_config": {
                    "kiara_model_id": "input.network_analysis_attribute_map_transformation"
                },
                "doc": "A list of specs on how to map existing attributes onto the target network edge data.",
                "optional": True,
            },
        }
        return result

    def create_outputs_schema(
        self,
    ) -> ValueMapSchema:
        result: Dict[str, Dict[str, Any]] = {}
        result["network_data"] = {
            "type": "network_data",
            "doc": "The network_data, with a new column added to the nodes table, indicating the component the node belongs to.",
        }

        return result

    def process(self, inputs: ValueMap, outputs: ValueMap):

        import duckdb

        network_data_obj = inputs.get_value_obj("network_data")
        network_data: NetworkData = network_data_obj.data

        # needed for sql query later
        edges_table = network_data.edges.arrow_table  # noqa

        attr_map_strategies: Union[
            None, KiaraModelList[AttributeMapStrategy]
        ] = inputs.get_value_data("attributes")

        if attr_map_strategies:

            invalid_columns = set()
            for strategy in attr_map_strategies.list_items:

                # if strategy.source_column_name == SOURCE_COLUMN_NAME:
                #     raise KiaraProcessingException(
                #         msg=f"Can't redefine edges with provided attribute map: the source column name '{SOURCE_COLUMN_NAME}' is reserved."
                #     )

                if strategy.source_column_name == TARGET_COLUMN_NAME:
                    raise KiaraProcessingException(
                        msg=f"Can't redefine edges with provided attribute map: the target column name '{TARGET_COLUMN_NAME}' is reserved."
                    )

                if strategy.target_column_name.startswith("_"):
                    raise KiaraProcessingException(
                        msg=f"Can't redefine edges with provided column map: the target column name '{strategy.target_column_name}' starts with an underscore, which is reserved for automatically computed edge attributes."
                    )

                if strategy.source_column_name not in network_data.edges.column_names:
                    invalid_columns.add(strategy.source_column_name)

            if invalid_columns:

                msg = f"Can't redefine edges with provided attribute map strategies: the following columns are not available in the network data: {', '.join(invalid_columns)}"

                msg = f"{msg}\n\nAvailable column names:\n\n"
                for col_name in (
                    x for x in network_data.edges.column_names if not x.startswith("_")
                ):
                    msg = f"{msg}\n - {col_name}"
                raise KiaraProcessingException(msg=msg)

        sql_tokens: List[str] = []
        group_bys = [SOURCE_COLUMN_NAME, TARGET_COLUMN_NAME]
        if attr_map_strategies:
            for strategy in attr_map_strategies.list_items:

                if not strategy.transform_function:
                    # column_type = edges_table.field(strategy.source_column_name).type
                    # if pa.types.is_integer(column_type) or pa.types.is_floating(
                    #     column_type
                    # ):
                    #     transform_function = "SUM"
                    # else:
                    #     transform_function = "LIST"
                    transform_function = "COUNT"
                else:
                    transform_function = strategy.transform_function

                transform_function = transform_function.lower()
                if transform_function == "group_by":
                    group_bys.append(strategy.source_column_name)
                    sql_token = None
                elif transform_function == "string_agg_comma":
                    sql_token = f"STRING_AGG({strategy.source_column_name}, ',') as {strategy.target_column_name}"
                else:
                    sql_token = f"{transform_function.upper()}({strategy.source_column_name}) as {strategy.target_column_name}"
                if sql_token:
                    sql_tokens.append(sql_token)

        query = f"""
        SELECT
            {', '.join(group_bys)},
            {', '.join(sql_tokens)}
        FROM edges_table
        GROUP BY {', '.join(group_bys)}
        """

        result = duckdb.sql(query)
        new_edges_table = result.arrow()
        network_data = NetworkData.create_network_data(
            nodes_table=network_data.nodes.arrow_table,
            edges_table=new_edges_table,
            augment_tables=True,
        )
        outputs.set_values(network_data=network_data)


# kiara\kiara_plugin.network_analysis\src\kiara_plugin\network_analysis\pipelines\__init__.py
# -*- coding: utf-8 -*-

"""Default (empty) module that is used as a base path for pipelines contained in this package.
"""


# kiara\kiara_plugin.network_analysis\src\kiara_plugin\network_analysis\streamlit\__init__.py


# kiara\kiara_plugin.network_analysis\src\kiara_plugin\network_analysis\streamlit\components\data_import.py
# -*- coding: utf-8 -*-
from typing import TYPE_CHECKING, Union

from kiara.interfaces.python_api import JobDesc
from kiara_plugin.network_analysis.defaults import (
    LABEL_ALIAS_NAMES,
    NODE_ID_ALIAS_NAMES,
    SOURCE_COLUMN_ALIAS_NAMES,
    TARGET_COLUMN_ALIAS_NAMES,
)
from kiara_plugin.streamlit.components.data_import import (
    DataImportComponent,
    DataImportOptions,
)

if TYPE_CHECKING:
    from kiara_plugin.streamlit.api import KiaraStreamlitAPI


class NetworkDataImportComponent(DataImportComponent):

    _component_name = "import_network_data"
    _examples = [{"doc": "The default network_data onboarding component.", "args": {}}]  # type: ignore

    @classmethod
    def get_data_type(cls) -> str:
        return "network_data"

    def render_onboarding_page(
        self, st: "KiaraStreamlitAPI", options: DataImportOptions
    ) -> Union[None, JobDesc]:

        _key = options.create_key("import", "network_data")

        with st.expander(label="Select a table containing the edges", expanded=True):
            key = options.create_key("import", "network_data", "from", "table", "edges")
            selected_edges_table = self.get_component("select_table").render(
                st=st, key=key, add_import_widget=True
            )

        with st.expander(
            label="Select a table containing (optional) node information",
            expanded=False,
        ):
            key = options.create_key("import", "network_data", "from", "table", "nodes")
            selected_nodes_table = self.get_component("select_table").render(
                st=st, key=key, add_import_widget=True, add_no_value_option=True
            )

        with st.expander(label="Assemble options", expanded=True):
            key_column, value_column = st.columns([1, 5])
            # with key_column:
            #     st.write("Edges table")
            # with value_column:
            #     if selected_edges_table:
            #         st.kiara.preview_table(selected_edges_table, height=200)
            #     else:
            #         st.write("*-- no edges table selected --*")

            key_column, value_column = st.columns([1, 5])
            with key_column:
                st.write("Edge table options")
            with value_column:
                if selected_edges_table:
                    available_edge_coluns = selected_edges_table.data.column_names
                else:
                    available_edge_coluns = []
                edge_columns = st.columns([1, 1])
                with edge_columns[0]:
                    default = 0
                    for idx, column in enumerate(available_edge_coluns):
                        if column.lower() in SOURCE_COLUMN_ALIAS_NAMES:
                            default = idx
                            break
                    edge_source_column = st.selectbox(
                        "Source column name",
                        available_edge_coluns,
                        key=f"{_key}_edge_source_column",
                        index=default,
                    )
                with edge_columns[1]:
                    default = 0
                    for idx, column in enumerate(available_edge_coluns):
                        if column.lower() in TARGET_COLUMN_ALIAS_NAMES:
                            default = idx
                            break

                    edge_target_column = st.selectbox(
                        "Source target name",
                        available_edge_coluns,
                        key=f"{_key}_edge_target_column",
                        index=default,
                    )

            key_column, value_column = st.columns([1, 5])
            with key_column:
                st.write("Node table options")
            with value_column:
                if selected_nodes_table:
                    available_node_coluns = selected_nodes_table.data.column_names
                else:
                    available_node_coluns = []
                node_columns = st.columns([1, 1])
                with node_columns[0]:
                    default = 0
                    for idx, column in enumerate(available_node_coluns):
                        if column.lower() in NODE_ID_ALIAS_NAMES:
                            default = idx
                            break

                    node_id_column = st.selectbox(
                        "Node ID column name",
                        available_node_coluns,
                        key=f"{_key}_node_id_column",
                        index=default,
                    )
                with node_columns[1]:
                    default = 0
                    for idx, column in enumerate(available_node_coluns):
                        if column.lower() in LABEL_ALIAS_NAMES:
                            default = idx
                            break

                    label_column = st.selectbox(
                        "Label column name",
                        available_node_coluns,
                        key=f"{_key}_label_column",
                        index=default,
                    )

        if not selected_edges_table:
            return None

        inputs = {}
        inputs["edges"] = selected_edges_table.value_id
        inputs["nodes"] = (
            selected_nodes_table.value_id if selected_nodes_table else None
        )
        inputs["source_column"] = edge_source_column
        inputs["target_column"] = edge_target_column
        inputs["id_column"] = node_id_column
        inputs["label_column"] = label_column

        job_desc = {
            "operation": "assemble.network_data",
            "inputs": inputs,
            "doc": "Assemble a network_data value.",
        }
        return JobDesc(**job_desc)


# kiara\kiara_plugin.network_analysis\src\kiara_plugin\network_analysis\streamlit\components\__init__.py
# -*- coding: utf-8 -*-
from typing import Iterable

from streamlit.delta_generator import DeltaGenerator

from kiara_plugin.network_analysis.defaults import (
    EDGE_ID_COLUMN_NAME,
    EDGES_TABLE_NAME,
    LABEL_COLUMN_NAME,
    NODE_ID_COLUMN_NAME,
    NODES_TABLE_NAME,
    SOURCE_COLUMN_NAME,
    TARGET_COLUMN_NAME,
)
from kiara_plugin.streamlit.components.preview import PreviewComponent, PreviewOptions


class NetworkDataPreview(PreviewComponent):
    """Preview a value of type 'network data'.

    Currently, this displays a graph, as well as the nodes and edges tables. The graph is only a preview, and takes a while to render depending on the network data size, this will replaced at some point.
    """

    _component_name = "preview_network_data"

    @classmethod
    def get_data_type(cls) -> str:
        return "network_data"

    def render_preview(self, st: DeltaGenerator, options: PreviewOptions):

        import networkx as nx
        import streamlit.components.v1 as components
        from pyvis.network import Network

        from kiara_plugin.network_analysis.models import NetworkData

        _value = self.api.get_value(options.value)

        if options.show_properties:
            tab_names = ["Nodes", "Edges", "Graph", "Value properties"]
        else:
            tab_names = ["Nodes", "Edges", "Graph"]

        network_data: NetworkData = _value.data
        tabs = st.tabs(tab_names)

        with tabs[0]:
            nodes_table = network_data.get_table(NODES_TABLE_NAME)
            _callback, _key = self._create_session_store_callback(
                options, "preview", "network_data", "nodes"
            )
            show_internal_columns = tabs[0].checkbox(
                "Show computed columns", value=False, key=_key, on_change=_callback
            )
            if show_internal_columns:
                exclude_columns: Iterable[str] = []
            else:
                exclude_columns = (
                    x
                    for x in nodes_table.column_names
                    if x not in [NODE_ID_COLUMN_NAME, LABEL_COLUMN_NAME]
                    and x.startswith("_")
                )
            tabs[0].dataframe(
                nodes_table.to_pandas_dataframe(exclude_columns=exclude_columns),
                use_container_width=True,
                hide_index=True,
            )

        with tabs[1]:
            edges_table = network_data.get_table(EDGES_TABLE_NAME)
            _callback, _key = self._create_session_store_callback(
                options, "preview", "network_data", "edges"
            )
            show_internal_columns = tabs[1].checkbox(
                "Show computed columns", value=False, key=_key, on_change=_callback
            )
            if show_internal_columns:
                exclude_columns = []
            else:
                exclude_columns = (
                    x
                    for x in edges_table.column_names
                    if x
                    not in [EDGE_ID_COLUMN_NAME, SOURCE_COLUMN_NAME, TARGET_COLUMN_NAME]
                    and x.startswith("_")
                )
            tabs[1].dataframe(
                edges_table.to_pandas_dataframe(exclude_columns=exclude_columns),
                use_container_width=True,
                hide_index=True,
            )

        # graph
        with tabs[2]:
            _callback, _key = self._create_session_store_callback(
                options, "preview", "network_data", "graphs"
            )
            graph_types = ["non-directed", "directed"]
            graph_type = tabs[2].radio(
                "Graph type", graph_types, key=_key, on_change=_callback
            )
            if graph_type == "non-directed":
                graph = network_data.as_networkx_graph(nx.Graph)
            else:
                graph = network_data.as_networkx_graph(nx.DiGraph)

            for node_id, data in graph.nodes(data=True):
                data["label"] = data.pop(LABEL_COLUMN_NAME)

            vis_graph = Network(
                height="400px", width="100%", bgcolor="#222222", font_color="white"
            )
            vis_graph.from_nx(graph)
            vis_graph.repulsion(
                node_distance=420,
                central_gravity=0.33,
                spring_length=110,
                spring_strength=0.10,
                damping=0.95,
            )

            html = vis_graph.generate_html()
            components.html(html, height=435)
        if options.show_properties:
            with tabs[3]:
                comp = self.get_component("display_value_properties")
                comp.render_func(tabs[3])(value=options.value)


# kiara\kiara_plugin.network_analysis\tests\conftest.py
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
    Dummy conftest.py for kiara_plugin.language_processing.

    If you don't know what this is for, just leave it empty.
    Read more about conftest.py under:
    https://pytest.org/latest/plugins.html
"""

import os
import tempfile
import uuid
from pathlib import Path

import pytest

from kiara.context import KiaraConfig
from kiara.interfaces.python_api import KiaraAPI
from kiara.interfaces.python_api.models.job import JobTest
from kiara.utils.testing import get_tests_for_job, list_job_descs

ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
JOBS_FOLDER = Path(os.path.join(ROOT_DIR, "examples", "jobs"))


def create_temp_dir():
    session_id = str(uuid.uuid4())
    TEMP_DIR = Path(os.path.join(tempfile.gettempdir(), "kiara_tests"))

    instance_path = os.path.join(
        TEMP_DIR.resolve().absolute(), f"instance_{session_id}"
    )
    return instance_path


@pytest.fixture
def kiara_api() -> KiaraAPI:

    instance_path = create_temp_dir()
    kc = KiaraConfig.create_in_folder(instance_path)
    api = KiaraAPI(kc)
    return api


@pytest.fixture(params=list_job_descs(JOBS_FOLDER))
def example_job_test(request, kiara_api) -> JobTest:

    job_tests_folder = Path(os.path.join(ROOT_DIR, "tests", "job_tests"))

    job_desc = request.param
    tests = get_tests_for_job(
        job_alias=job_desc.job_alias, job_tests_folder=job_tests_folder
    )

    job_test = JobTest(kiara_api=kiara_api, job_desc=job_desc, tests=tests)
    return job_test


@pytest.fixture
def example_data_folder() -> Path:
    return Path(os.path.join(ROOT_DIR, "examples", "data"))


@pytest.fixture
def example_pipelines_folder() -> Path:
    return Path(os.path.join(ROOT_DIR, "examples", "pipelines"))


@pytest.fixture()
def tests_resources_folder() -> Path:
    return Path(os.path.join(ROOT_DIR, "tests"))


# kiara\kiara_plugin.network_analysis\tests\test_job_descs.py
# -*- coding: utf-8 -*-
from kiara.interfaces.python_api.models.job import JobTest

"""Auto-generated tests that use job descriptions in the 'examples/jobs' folder and run them.

To test against the outputs of those jobs, add files into subfolders that are called the same as the job (minus the file extension), under the `tests/job_tests` folder.

To test values directly, add a file called `outputs.json` or `outputs.yaml` into that folder, containing a 'dict'
data structure with the value attribute to test as key and the expected value as value.

Most likely you will want to test against a value property, which would be done like so (in yaml):

```yaml
network_data::properties::metadata.graph_properties::number_of_self_loops: 1
```

The format is:
```yaml
<output field name>::properties::<property name>::[<property attribute>]::[<more_sub_keys>]: <expected value>
```

In case of scalars, you can also test against the value directly:
```
output_string::data: "some string"
```

If the results are too complex to test against in this way, you can also write Python code. Add a file with a random
name and a `.py` extension (output.py is a good choice if you only have one). In that file, each function will be run
against the job results. You can control which arguments will be passed to the function by naming the arguments:

- `kiara_api`: a kiara api instance will be passed in
- 'outputs`: the whole result of the job will be passed in (of type `ValueMap`)
- the field name of the value you are interested in (e.g. `table`, `network_data`, depends on the job)

Specifying any other argument name will throw an error.
"""


def test_job_desc(example_job_test: JobTest):

    example_job_test.run_tests()


# kiara\kiara_plugin.network_analysis\tests\test_kiara_modules_default.py
#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""Tests for `kiara_plugin.network_analysis` package."""

import pytest  # noqa

import kiara_plugin.network_analysis


def test_assert():

    assert kiara_plugin.network_analysis.get_version() is not None


# kiara\kiara_plugin.network_analysis\tests\job_tests\create_journals_network\outputs.py
# -*- coding: utf-8 -*-
from kiara.models.values.value import Value
from kiara_plugin.network_analysis.models import NetworkGraphProperties


def check_properties(network_data: Value):

    properties: NetworkGraphProperties = network_data.get_property_data(
        "metadata.network_data"
    )
    assert (
        properties.number_of_nodes == 276
    ), f"Invalid number of nodes: {properties.number_of_nodes} != 276"


# kiara\kiara_plugin.network_analysis\tests\job_tests\create_journals_network\outputs.yaml
network_data::properties::metadata.network_data::number_of_self_loops: 1
network_data::properties::metadata.network_data::number_of_nodes: 276
network_data::properties::metadata.network_data::properties_by_graph_type::undirected::number_of_edges: 313


# kiara\NetworkAnalysis\README.md
# Network Analysis

Network analysis examines relationships among entities, such as persons, institutions, documents, or words. Navigating between multiple scales, one can use network analysis to describe and draw conclusions about relational properties of individual entities, of subsets of entities, and of entire networks. Thus, network analysis is well suited to approach research questions that focus on relationships and offers a way to integrate micro and macro perspectives.

## Table of contents

1. [The DHARPA Project](#1-the-dharpa-project)
2. [Methodological considerations](#2-methodological-considerations)

   2.1 [Networks are abstractions](#21-networks-are-abstractions)

   2.2 [When should I use Network Analysis?](#22-when-should-i-use-network-analysis)
   
   2.3 [Methods and data](#23-methods-and-data)

   2.4 [Terminology](#24-terminology)
  
3. [Getting started](#3-getting-started)
4. [Conclusions](#4-conclusions)
5.  [Remarks](#5-remarks)
6.  [License](#6-license)
7.  [Links](#7-links)
8.  [References](#8-references)
9.  [The team](#9-the-team)
10. [How to cite](#10-how-to-cite)

## 1. The DHARPA Project
While the ‘digital humanities moment’ has yielded great accomplishments and enthusiastic interdisciplinary cooperations across the humanities and between the humanities and the sciences, concerns have been raised about the little transparency in digital practices as well as the difficulty of replicating studies due to the lack of data access or standardised practices as well as unclear methodological processes (Faull et al 2016; Jakacki et al 2016, 2015; O’ Sullivan 2019). Such concerns have for instance led scholars to claim that digital humanities is still in “search of a methodology” (Dobson 2019) and the metaphor of the 'black box' has started to be used (Smith 2014) to describe the apparent loss of human agency in the digital reseach process. This could be to some extent due to the fact that traditional historical inquiry itself has in some ways been like a “Mechanical Turk,” with the decisions and interventions made by the researcher hidden from view and only the well-oiled and seemingly autonomous product on display. The [**DHARPA Project**](https://dharpa-project.github.io/) (Digital History Advanced Research Projects Accelerator) aims to reverse this trend. We want to encourage historians and digital humanities scholars to lift the lid, to show how the application of their expertise works in tandem with technology to produce knowledge, how even digitally enabled research is not a product but a process, reliant on the critical engagement of the scholar. 

In this workflow, we aim to promote a self-reflective analysis of the interaction of technology and humanities practice and we use network analysis as a case study.

## 2. Methodological considerations

Almost everything can be represented in the form of a network. That does not mean that it will be useful or insightful. In the following, I will outline some basic considerations that need to be taken into account when considering network analysis as a method and introduce the basic teminology.

## 2.1 Networks are abstractions

Network graphs and the databases that are usually used to build them concentrate on one or a few sort of relationships between a limited set of actors, deliberately ignoring the fact that these actors necessarily have other relationships among themselves and with outsiders. Choices in "boundary specification" (whom do we observe? which ties among them? at what time(s)?) heavily constrain the sort of questions that can be analyzed by network analysis. This implies, on the one hand, that interpretations based on network data should be careful not to reify notions such as centrality or isolation, that are always relative to a choice of relations and actors observed; on the other hand, nothing prevents us to study some sorts of relations thanks to some historical sources, even if they generally do not systematically record all the sorts of relations that we would be interested in (Lemercier, 2015).

## 2.2 When should I use Network Analysis?

Network Analysis is a collection of methods that can be useful to study complex relational patterns. If you are interested in studying such patterns and have already identified them in your sources, then there is a good chance that network analysis will help you to explore and analyse them in greater depth. You might still need to think more carefully about what exactly the entities (nodes) are that you want to study and how exactly the relationships (edges) between them can be expressed and defined. The clearer your conception of these elements, the more solid your network analysis project will be.

In order to be able to do network analysis, your data will need to be in a certain shape. Typically, the data would be retrieved from a database. Some table data might also be easily tweaked into an appropriate format, but if your starting point is a collection of unstructured text (like .txt files) then some preprocessing steps will be necessary to parse the data into a format suitable for network analysis before you can start analysing anything.

A good guide to help answering the question whether or not it is worthwhile and feasible to engage with network analysis can be found here: https://cvcedhlab.hypotheses.org/125

## 2.3 Methods and data

Some methods, like calculating centralities, or detecting or discovering communities in a network, will be appropriate for all sorts of datasets and a variety of research questions. It does not matter if the network is small or large, if it constitutes a social network or a text network. These methods can provide insights that are otherwise difficult to obtain and can raise new research questions. However, the meaning and interpretation of the results of these methods can be very specific to the individual network. In the following, I will describe approaches to calculate centralities and to detect communities in a realtively abstract way. Ideas on how some of the results can be interpreted in light of a case specific research question can be found in the various Jupyter notebooks in this repository.

As with many other methodological choices in network analysis, the choice of the graph type (directed, undirected etc.) can substantially affect the results of centrality measures and community detection outcomes. Which choice or combination makes the most sense will depend on the research question. Some researchers will want to investigate different aspects of their network and explore their data in the form of different graph types. Others will have a more narrow research question and focus on one set of methods tailored to one particular graph type.
Generally speaking, research questions focusing on the analysis of "structure" will indicate a preference for an undirected graph, while the focus on "flow" will indicate a directed network and imply the use of the associated methods. Those are not necessarily mutually exclusive, and a more general research question would entail that a researcher investigates both structure and flow.

### Calculating Centralities

One of the most common operations in network analysis is to identify important nodes.
What makes a node important depends on your network and your notion of importance.
An important node might be a node that has the most connections. A node
with one connection has a degree of one and degree centrality is a way for calculating a normalized value for this measure. 
Another kind of “importance” is betweenness-centrality. It indicates a node
that lies between two distinct parts of a network (A node through which most "shortest paths" pass through). A node with high betweenness centrality
can indicate that the node is a “broker” or a “gatekeeper”. 

Here are some (but not all!) of the more popular centrality measures:

**Closeness centrality** is measuring how close a node is located with respect to every other node in the network.

**Degree centrality** is a measure to identify the nodes which have the most connections in the network, they are often interpreted as **hubs**.

**Betweenness centrality** looks at all the shortest paths that pass through a particular node. It is expressed on a scale of 0 to 1 and is good at finding nodes that connect two otherwise disparate parts of a network. 
In contrast to a **hub**, this sort of node is often referred to as a **broker** because the value can be interpreted as indicating a node with an active brokering role in the network.

**Eigenvector centrality** considers if a node has many direct connections on its own, but it also considers how it is connected to other highly connected nodes. It’s calculated as a value from 0 to 1: the closer to one, the greater the centrality. In information networks, eigenvector centrality is useful for understanding which nodes can get information to many other nodes quickly. If you know a lot of well-connected people, you could spread a message very efficiently.

It is important to note that all these different labels (hubs, brokers) constitute **interpretations** of the obtained numbers and generate assumptions about the roles of individual actors in a network.
Edge **weights** can significantly alter some of these centrality measures and yield very different rankings in terms of centrality. Because weights are interpreted as the "connection strength" in some measures and as "distance" (or cost) in other measures, it is important to adjust calculations according to the type of network. For example, this can imply that it might be appropriate to invert the egde weights when calculating weighted betweenness centrality in an information network. (See this related discussion on centralities in weighted graphs for an example and a more detailed explanation: https://github.com/DHARPA-Project/kiara_plugin.network_analysis/discussions/24)

Depending on the choice of centrality, very different nodes in a network can be identified as "important" in one way or other (Ortiz-Arroyo, 2010):

![image](https://user-images.githubusercontent.com/53467834/173307418-51d741b3-fff3-4708-ad4e-987653534d29.png)


### Detecting Communities

Community detection is a popular operation in network analysis, especially with social
networks like social media groups, but it also has its uses with text networks.
This method idetifies groups of nodes that belong together based on one or several properties of the graph.
By partitioning the graph into distinct communities, it is possible to detect node characteristics that are otherwise not recorded (as node attributes) or not easily discernible.
If nothing else, this method can help to raise new questions about the network and initiate a search for the unknown common denominators among the nodes that were placed in the same community according to the partition of the community detection algorithm.

A common approach is to identify communities by searching for node clusters in a network that have a notably higher density. These density clusters can then be interpreted as distinct communities within the network.

There are specialized methods to detect communities in **directed networks**. In most cases, those methods would rely on analyzing flow rather than structure. Depending on the data and research question, it can be very misleading to use community detection algorithms on directed graphs that have been originally developed for undirected graphs ("It is clear that ignoring edge directionality and considering the graph as undirected is not a meaningful
way to cluster directed networks as it fails to capture the asymmetric relationships implied by the edges
of a directed network." (Malliaros & Vazirgiannis, 2013)).
A flow-based community detection algorithm (like the "infomap" algorithm) can, for example, detect citation patterns in citation networks.

It is also possible to search for **overlapping communities** where nodes belong to more than one group. Some methods can be found here: https://cdlib.readthedocs.io/en/latest/overview.html

## 2.4 Terminology

|          Term         |                                                                                                                                                   Description                                                                                                                                                  |
|:---------------------:|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|
| Actor                 | The entity which is described by a node, e.g. a person, an institution etc.                                                                                                                                                                                                                                    |
| Broker                | A node which is positioned e.g. between two clusters and can act as a “bottleneck”                                                                                                                                                                                                                             |
| Community             | A set of nodes which are relatively more connected to each other than to the rest of the graph                                                                                                                                                                                                                 |
| Co-occurrence network | A network in which the edge between nodes is based on the fact that both  appear together in the same context, usually in texts. An example would  be two people who are mentioned in the same paragraph.                                                                                                      |
| Component             | A connected part of the network. Networks often consist of multiple disconnected components. An isolated node can be considered to be a component.                                                                                                                                                             |
| Degree of a node      | The number of edges connected to this node. Variants include in-degree/out-degree, which counts the number of ingoing and outgoing edges in a directed network. Sometimes indicated by the size of the  sphere representing the node. Also called degree centrality when normalized.                           |
| Diameter of a network | The shortest distance between the two most distant nodes in the network i.e. the largest shortest path lenght.                                                                                                                                                                                                 |
| Directed network      | A network in which the edges are directional, e.g. when actor A sends a letter to actor B.                                                                                                                                                                                                                     |
| Dyad                  | A group of two connected nodes, the smallest possible network                                                                                                                                                                                                                                                  |
| Edge                  | Connection, also called link, arc, or tie between nodes.                                                                                                                                                                                                                                                       |
| Edge attribute        | Data which describes a certain aspect of an edge, for example how often two actors speak to each other.                                                                                                                                                                                                        |
| Ego network           | A network which contains all connections of an actor to their alteri and usually also the connections between the alteri. A classic example is a correspondence network derived from the collected letters of a single individual.                                                                             |
| Graph                 | (in a network and math. context): objects which are connected by links. However, often used interchangeably with “network” even by experts.                                                                                                                                                                    |
| Homophily             | The tendency of nodes to become connected to other nodes that are similar under a certain definition of similarity.                                                                                                                                                                                            |
| Hub                   | A node with a degree far higher than the average in a network.                                                                                                                                                                                                                                                 |
| Matrix                | A way of representing a network where there is a row and a column for each node, and the values in the cells indicate whether an edge exists between a pair of nodes.                                                                                                                                          |
| Multi-mode network    | A network with more than one node type, for example author-nodes, book-nodes, and subject-nodes; victim-nodes, suspect-nodes, crime-scene-nodes, and crime-weapon nodes.                                                                                                                                       |
| Node                  | Sometimes called a vertex. Refers to the object which is connected to other objects in a graph.                                                                                                                                                                                                                |
| Node attribute        | Data which describes a certain aspect of a node, for example an actor’s age or gender.                                                                                                                                                                                                                         |
| Node centrality       | Describes the extent to which a node is connected to other nodes within a  network. Various algorithms exist to describe different aspects of such  connectivity. To some extent centrality can be linked to abstract  notions such as “influence”, “power” or “importance”.                                   |
| One-mode network      | Also "unipartitite network". A network of just one  node type, in contrast to bipartite or multi-mode network.                                                                                                                                                                                                 |
| Projection            | Usually "projection of a bipartite network". When a bipartite network is projected, one of the node types is transformed into an edge. For example, instead of two people-nodes being  connected to a place-node, they are now connected to each other, and the place-node  becomes the edge connecting them.  |
| Shortest path         | The shortest path (fewest number of steps) between two  nodes in the network.                                                                                                                                                                                                                                  |
| Weighted network      | A network in which each edge has a numerical weight attached to it, indicating the strength (or intensity) of the connection. In an unweighted network all edges are assumed to have a weight of 1.                                                                                                            |                                                                               |

For sources see the [links](#7-links) section.

## 6. License
See COPYING file in this repository.

## 7. Links

### Introductions to Network Basics

http://www.scottbot.net/HIAL/index.html@p=6279.html

### Network terminology
https://cvcedhlab.hypotheses.org/106

https://folgerpedia.folger.edu/Glossary_of_network_analysis_terms

### Temporal (dynamic) networks
https://www.youtube.com/watch?v=f8EqqzzuoFs

## 8. References

Marten Düring et al., eds., Handbuch Historische Netzwerkforschung: Grundlagen Und Anwendungen, Schriften des Kulturwissenschaftlichen Instituts Essen (KWI) zur Methodenforschung Band 1 (Berlin: LIT, 2016).

Claire Lemercier, “Formal Network Methods in History: Why and How?,” in Social Networks, Political Institutions, and Rural Societies, ed. Georg Fertig, Rural history in Europe 11 (Turnhout, Belgium: Brepols, 2015).

Fragkiskos D. Malliaros and Michalis Vazirgiannis, “Clustering and Community Detection in Directed Networks: A Survey,” Physics Reports 533, no. 4 (2013): 95–142, accessed June 13, 2022, https://linkinghub.elsevier.com/retrieve/pii/S0370157313002822.

Mark Newman, Networks: An Introduction (Oxford University Press, 2010), accessed December 3, 2020, https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780199206650.001.0001/acprof-9780199206650.

Daniel Ortiz-Arroyo, “Discovering Sets of Key Players in Social Networks,” in Computational Social Network Analysis, ed. Ajith Abraham, Aboul-Ella Hassanien, and Vaclav Sná¿el (London: Springer London, 2010), 27–47, accessed March 17, 2021, http://link.springer.com/10.1007/978-1-84882-229-0_2.

Deryc T. Painter, Bryan C. Daniels, and Jürgen Jost, “Network Analysis for the Digital Humanities: Principles, Problems, Extensions,” Isis 110, no. 3 (August 30, 2019): 538–554, accessed March 3, 2021, https://www.journals.uchicago.edu/doi/full/10.1086/705532.

John Scott, Social Network Analysis - SAGE Research Methods, 2017, accessed March 11, 2022, https://methods.sagepub.com/book/social-network-analysis-4e.

John Scott and Peter Carrington, The SAGE Handbook of Social Network Analysis (London, United Kingdom: SAGE Publications Ltd, 2014), accessed March 11, 2022, http://methods.sagepub.com/book/the-sage-handbook-of-social-network-analysis.

Stanley Wasserman and Katherine Faust, Social Network Analysis: Methods and Applications, Structural analysis in the social sciences 8 (Cambridge ; New York: Cambridge University Press, 1994).

Katharina A. Zweig, Network Analysis Literacy: A Practical Approach to the Analysis of Networks, 1st ed. 2016., Lecture Notes in Social Networks (Vienna: Springer Vienna : Imprint: Springer, 2016), https://link-springer-com.proxy.bnl.lu/book/10.1007/978-3-7091-0741-6.

## 9. The team

## 10. How to cite
To cite the repository, please use the following format according to the APA style guide:

Jaskov, Helena. 2022. *Network Analysis in Python* (v. 1.0.0). University of Luxembourg. https://github.com/DHARPA-Project/NetworkAnalysis


# kiara\TopicModelling-\README.md
# Topic Modelling with Gensim - A workflow for the Humanities

As digitally available textual repositories are becoming larger and larger, the relevance of distant reading for the humanities has grown exponentially. Traditional close reading methods are no longer sufficient to analyse such unprecedented mass of digital data, therefore humanities scholars are confronted more and more with the challenge of having to use quantitative techniques in their research. One such quantitative technique is Topic Modelling (TM), a computational, statistical method to discover patterns and topics in large collections of unstructured text. With this repository, the [**DHARPA Project**](https://dharpa-project.github.io/) (Digital History Advanced Research Projects Accelerator) aims to offer a step-by-step guide to a comprehensive and generalizable TM workflow that could be applied transversely across different datasets. The intention is to make the TM technique more transparent and accessible for humanities scholars, assisting them in taking up an active role in the digital analysis process and claiming ownership of their interventions. This workflow is partially based on [Viola and Verheul](https://academic.oup.com/dsh/advance-article/doi/10.1093/llc/fqz068/5601610) (2019).

*Workflow created by [Dr Lorella Viola](https://www.c2dh.uni.lu/de/people/lorella-viola) and [Mariella de Crouy-Chanel](https://www.c2dh.uni.lu/people/mariella-de-crouy-chanel)*

## Table of contents

1. [The DHARPA Project](#1-the-dharpa-project)
2. [Topic Modelling](#2-topic-modelling)

   2.1 [What is a topic?](#21-what-is-a-topic)
   
   2.2 [When should I use TM?](#22-when-should-i-use-tm)
  
3. [Getting started](#3-getting-started)
4. [Installation](#4-installation)
5. [A critical approach to preparing the data](#5-a-critical-approach-to-preparing-the-data)

   5.1 [Preparing the data (1 of 2)](#51-preparing-the-data-1-of-2)
   
   5.2 [Preparing the data (2 of 2)](#52-preparing-the-data-2-of-2)
   
6. [Building the topic model](#6-building-the-topic-model)

   6.1 [Determining the 'optimal' number of topics](#61-determining-the-optimal-number-of-topics)
   
   6.2 [Perplexity and coherence](#62-perplexity-and-coherence)
   
7. [Understanding the topics through visualisation](#7-understanding-the-topics-through-visualisation)
8. [Topics' distribution](#8-topics-distribution)
9. [Historicise the topics](#9-historicise-the-topics)

   9.1 [Time is continuous (there are no gaps in my data)](#91-time-is-continuous-there-are-no-gaps-in-my-data)
   
   9.2 [Time is discrete (there are gaps in my data)](#92-time-is-discrete-there-are-gaps-in-my-data)
   
10. [Conclusions](#10-conclusions)
11. [Remarks](#11-remarks)
12. [License](#12-license)
13. [Links](#13-links)
14. [References](#14-references)
15. [The team](#15-the-team)
16. [How to cite](#16-how-to-cite)

## 1. The DHARPA Project
While the ‘digital humanities moment’ has yielded great accomplishments and enthusiastic interdisciplinary cooperations across the humanities and between the humanities and the sciences, concerns have been raised about the little transparency in digital practices as well as the difficulty of replicating studies due to the lack of data access or standardised practices as well as unclear methodological processes (Faull et al 2016; Jakacki et al 2016, 2015; O’ Sullivan 2019). Such concerns have for instance led scholars to claim that digital humanities is still in “search of a methodology” (Dobson 2019) and the metaphor of the 'black box' has started to be used (Smith 2014) to describe the apparent loss of human agency in the digital reseach process. This could be to some extent due to the fact that traditional historical inquiry itself has in some ways been like a “Mechanical Turk,” with the decisions and interventions made by the researcher hidden from view and only the well-oiled and seemingly autonomous product on display. The DHARPA Project aims to reverse this trend. We want to encourage historians and digital humanities scholars to lift the lid, to show how the application of their expertise works in tandem with technology to produce knowledge, how even digitally enabled research is not a product but a process, reliant on the critical engagement of the scholar. In this workflow, we aim to promote a self-reflective analysis of the interaction of technology and humanities practice and we use TM as a case study.   

## 2. Topic Modelling
Before talking about TM and how it works, it is worth spending a few words on what is intended by *topic*. This will also help to clarify how the TM algorithm works, when researchers could consider apply it to their data and how they should understand the output.

## 2.1 What is a topic?
A topic is understood as a set of terms that occur together in a statistically significant way to form a cluster of words. According to this logic, a text can be understood as the combination of such clusters of words, where each cluster is made of words mathematically likely to appear together (Steyvers and Griffiths 2007). The model assumes that a corpus has a fixed number of founding topics and that these topics compose each document of the corpus to varying degrees (Lee 2019). Using contextual information, topic models distinguish when words are used with multiple meanings in different contexts; this ultimately means that the words are also clustered according to similar uses. What happens in practice is that TM runs statistical calculations multiple times until it determines the most likely distribution of words into clusters, i.e. into topics. The procedure guarantees impartial results in terms of which words will appear in each topic, as the topics emerge from the algorithm’s identification of patterns and trends in the texts, rather than the potentially subjective interpretation of the semantic meaning of the words in each document. In this sense, there is no intervention from the researcher. 

## 2.2 When should I use TM?
This analytical tool works best with large collections of unstructured text (i.e., without any machine-readable annotations) and when the main purpose is to obtain a general overview of the topics discussed in a corpus. As long as it is unstructured, the corpus can be just about anything (e.g., emails, newspapers' headlines, newspapers' articles, a standard .txt document). For this, TM is an excellent distant reading technique that may be used as a data exploration method, for instance to categorise documents within a collection without having to read them all. A practical example of its use may be libraries that need to label digital collections for archiving purposes. Its potential, however, is most fully reached when working in tandem with expert knowledge. To help clarify the application of TM and show how it works in practice, in this workflow we will apply TM to real data. Specifically, we will use a subset of *ChroniclItaly 3.0* (Viola 2020), a collection of Italian immigrant newspapers published in the United States between 1898 and 1936.

## 3. Getting started
There are many variations of the TM algorithm and numerous programs and techniques to implement them. The rationale behind all of them, however, is the same: using statistical modelling to discover topics in a textual collection. Among these very many techniques, Latent Dirichlet Allocation (LDA - [Blei, Ng and Jordan 2003](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)) is perhaps the most widely used. In this workflow, we show two ways of using LDA with Python: Gensim, a Python library for topic modelling, document indexing and similarity retrieval which also implements LDA and the Gensim's implementation of Mallet [(McCallum, 2002)](http://mallet.cs.umass.edu), a natural language processing toolkit that uses machine learning applications to work with unstructured texts. Mallet as a software is a widely used TM tool both for research and teaching purposes (Nelson 2010; Graham et al., 2012, 2016), especially in the humanities. In this way, we aim to offer researchers a way to engage critically with the TM methodology; by comparing techniques and results, we promote the full critical engagement of the scholar along each step of the digital research process.

Gensim depends on the Python packages NumPy and Scipy, which must be installed prior to installing Gensim. Although in principle scholars are free to use whichever TM program they feel more comfortable with, a major advantage of using Gensim is its memory efficiency, significantly higher than other software which have often issues handling big data. More information about Gensim, including the offical documentation, can be found [here](https://pypi.org/project/gensim/).

## 4. Installation
Please install Python 3.6 or higher to run the notebook. The notebook makes use
of the dependencies found in `requirements.txt`. Install the dependencies with 
pip by running the following line of code in your terminal.

```sh
pip install -r requirements.txt
```
## 5. A critical approach to preparing the data
Deciding which of the pre-processing operations should be performed depends on many factors such as the language of the dataset, the type of data, the individual research question(s). It is therefore paramount that this step is tackled **critically** by the researcher as each one of their interventions will have consequences on how the TM algorithm will process the data and therefore on the results.

## 5.1 Preparing the data (1 of 2)
This step is concerned with tokenization, lowercasing, stemming, lemmatization, removing stopwords and words with less than three characters, removing noise (e.g., numbers, punctuation marks, special characters). In principle, the entire step is optional as the train-topic command will work on whichever version of the dataset is used. In reality, however, pre-processing the data is key to the analysis for several reasons. First and foremost, it will likely make the TM results more reliable and more interpretable. For instance, running TM on languages that are rich in articles, pronouns, prepositions, etc., will almost certainly result in poorly interpretable topics. Second, pre-processing the data will remove most OCR mistakes which are always present in digital textual collections to various degrees. This is especially true for corpora such as historical datasets, repositories of underdocumented languages, or digitised archives from handwritten texts. Third, it will reduce the size of the collection thus decreasing the required processing power. Fourth, it is *de facto* a data exploration step which will allow the researcher to look more closely at their data.

 Here's a short explanation of each operation:
- **Tokenization**: Split the text into sentences and/or the sentences into words. In other words, this operation establishes the word boundaries (i.e., tokens) a very helpful way of finding patterns. It is also the typical step prior to stemming and lemmatization; 
- **Lowercasing**: Lowercase the words. This operation is a double-edged sword. It can be effective at yielding potentially better results in the case of relatively small datasets or datatsets with a high percentage of OCR mistakes. For instance, if lowercasing is not performed, the algorithm will treat *USA*, *Usa*, *usa*, *UsA*, *uSA*, etc. as distinct tokens, even though they may all refer to the same entity. On the other hand, if the dataset does not contain such OCR mistakes, then it may become difficult to distinguish between homonyms and make interpreting the topics much harder;
- **Stemming/Lemmatization**: Reduce inflection in words (e.g. states --> state). Although similar, stemming should not be confused with lemmatization. While the latter reduces the inflected word to the actual root form (e.g., better --> good), the former outputs a canonical form of the original word (e.g., past and future tense --> present tense), and not the grammatical root. Performing or not either of these operations is very much dependant on the dataset's language as in terms of TM, they may not affect the output *per se*;
- **Removing stopwords and words with less than three characters**: Remove low information words. These are typically words such as articles, pronouns, prepositions, conjunctions, etc. which are not semantically salient. There are numerous stopword lists available for many, though not all, languages which can be easily adapted to the individual researcher's needs. Removing words with less than three characters may additionally remove many OCR mistakes. Both these operations have the dual advantage of yielding more reliable results while reducing the size of the dataset, thus in turn reducing the required processing power. This step can therefore hardly be considered optional in TM;
- **Noise removal**: Remove elements such as punctuation marks, special characters, numbers, html formatting, etc. This operation is again concerned with removing elements that may not be relevant to the text analysis and in fact interfere with it. Depending on the dataset and research question, this operation can become essential.

Each one of these interventions will need to be quantitatively and qualitatively tested and assed by the researcher every time before deciding which ones to actually perform. This is of course true not just for TM, but in general for all NLP tasks. It is important to remember that each one of these steps is an additional layer of text manipualation and has direct, heavy consequences on the data and therefore on the results. It is critical that researchers assess carefully to what degree they want to intervene on their data. For this reason, this part of the digital analysis should not be considered as separate from the analysis of the results or from the results themselves. On the contrary, it is an **integral part** of the entire digital research process. 

## 5.2 Preparing the data (2 of 2)
This step is concerned with transforming the textual data in a format that will serve as an input for training the Gensim LDA model. What happens in practice is that the documents in the collection are converted into a vector representation called Bag of Words (BOW). A BOW is a way to represent the occurrence of words within a document without considering any structural information (e.g., grammar) other than whether known words occur or not in the documents. The intuition behind the BOW model is based on the semantic theory of language usage (Harris, 1954: 156) according to which words that are used and occur in the same contexts tend to purport similar meanings. If the meaning of a word can be inferred by its context, the opposite is true as well: words found in different contexts tend to purport different meanings.

A BOW model involves two things: **1) a dictionary** of known words (i.e., tokens) and **2) a measure** of the presence of such known words. In practice, the dictionary converts the text into numbers by indexing the words. Consider the following example: let's assume I want to create a dictionary for BOW of the following sentence:

```sh
My name is Lorella. What is your name
```
The dictionary will be

```sh
My name is Lorella. What is your name
0   1    2    3      4    2   5    1
```

Now the BOW model will be a list of (word_id, word_frequency) 2-tuples like this:

```sh
[[(0, 1), (1, 2), (2, 2), (3, 1), (5, 1)]]
```

There are different ways to design the dictionary and to calculate the presence of words. For instance, it is possible to map the text as it is by using sheer frequency (as in the example above), by calculating  inverse of document frequency (TF–IDF) or by using collocations (bigrams). **TF-IDF** is a statistical calculation that aims to reflect how important a word is (i.e., weight) to a document in a collection of texts (Rajaraman & Ullman 2011). The TF–IDF value increases proportionally to the number of times a word appears in the document in relation to the number of documents in the corpus that contain that word. This calculation accounts for the fact that some words appear more frequently in general and therefore, their weight is relative. **Bigrams** are contiguous sequences of two items from a text and they may provide a way to identify meaningful collocations. It should be said, however, that bigrams *per se* do not necessairily entail meaningful results, as not all consecutive words automatically reflect phrases. Thus, they need to be paired with methods to filter for the most meaningful sequences that will then be more likely to be collocations. 

Choosing one method over another determines the complexity of the model and, as any other step, ultimately impacts the results. In this workflow, we will experiment with both TF-IDF and bigrams to compare the different outputs and critically assess each method.

## 6. Building the topic model
Once the corpus and the dictionary have been created, the LDA model can be trained. The only parameter that still needs to be provided is the number of topics, which of course is not known in advance and therefore, the first time, will be arbitrary. 

## 6.1 Determining the 'optimal' number of topics
In literature, there is disagreement about how the number of topics should be determined. Some researchers confide in statistical methods (i.e. perplexity, coherence score) to determine the number of topics that is mathematically more accurate. However, some have found that 'mathematically more accurate' does not automatically entail that the topics will be more interpretable (Jacobi et al., 2015, p. 7). Other researchers prefer running the train-topics command multiple times with a different number of topics and then compare the different models' composition and topics' variety before settling for a more interpretable number (i.e., model). The latter approach allows the researcher to examine the various topic structures carefully before determining the number of topics that seems to offer the most coherent thematic breakdown of the corpus. In this sense, this approach relies heavily on the expert's knowledge. Others yet build many LDA models with different numbers of topics and simply pick the one that gives the highest coherence value. Once more, the critical understanding of how each choice will impact on the results is essential for informed and responsible decision making.   

## 6.2 Perplexity and coherence
Model perplexity and topic coherence are two ways to measure the statistical quality of a topic model as it is believed, though not unanimously, that a higher statistical quality would yield more interpretable topics. **Model perplexity** (also known as predictive likelihood) predicts the likelihood of new (unseen) text to appear based on a pre-trained model. The lower the perplexity value, the better the model predicts the sample, in this case, the words that appear in each topic. However, studies have shown that optimizing a language model for perplexity does not necessairily increase interpretability, as perplexity and human judgment are often not correlated, and sometimes even slightly anti-correlated (Jacobi et al., 2015, p. 7). **Topic Coherence** was developed to compensate for this shortcoming and it has become popular over the years. In reality, what the method does is modelling human judgement by scoring the composition of the topics based on how interpretable they are, i.e., coherent  (Röder, Both and Hinneburg 2015). If the coherence score increases as the number of topics increases, for example, that would suggest that the most interpretable model is the one that gave the highest coherence value before flattening out. In this workflow, we are using the topic coherence method.

## 7. Understanding the topics through visualisation
An effective way to analyse and interpret the topics is by visualising the terms' distributions associated with each topic. In this notebook, we implemented an interactive visualisation tool that helps users to assess the *quality* of individual topics and facilitates the overall interpretability. 

In order to interpret the topics, the first thing to consider is that each topic is a combination of keywords and that each keyword contributes a certain 'weight' to that topic. Understanding the weight of each keyword is crucial to understand how important that keyword is to that topic. The tool provides this information by visualising each keyword's *saliency* (Chuang et. al 2012), defined as the relationship between the likelihood that the observed word *w* was generated by the latent topic T and the likelihood that any randomly-selected word *w'* was generated by topic T. In other words, the saliency represents how informative a specific term is for interpreting a topic, versus a randomly-selected term. The way saliency is visualised is by showing each keyword's overall frequency (in blue) in comparison with the estimated term frequency within the selected topic (in orange). Filtering terms by saliency is helpful for a relatively rapid classification and disambiguation of the topics (ibid.) as well as for the identification of non-informative topics lacking salient terms. 

The terms are also ranked according to *relevance* (Sievert & Shirley 2014). Relevance takes into account the ratio between the probability of a term occurring in a topic and its marginal probability across the corpus (called *lift*). The quantification of the relevance measure results in a value (called apha value) and it ranges from 0 to 1. Setting the alpha value to 1 results in the ranking of terms in decreasing order according to their topic-specific probability. Setting the alpha value to 0 ranks terms solely by their *lift*. Observations conducted by Sievert & Shirley (2014) have shown that setting the alpha value to 0.6, that is in decreasing order of probability, increases the topics' interpretability. In this notebook, the alpha value is therefore set to 0.6.

Finally, it is also possible to obtain a general visualisation of the topics' weight. This is indicated by the size of the circles' areas (i.e., the larger the areas, the higher the weight of the topics in the corpus). The combined visualisation of information about saliency, relevance and weight allows for a relatively rapid and finer analysis of the semantic space shared by the topics, ultimately improving interpretability. 

## 8. Topics' distribution
Once the topics have been interpreted and possibly labeled for convenience, the next natural step would be to examine their distribution per document and over the collection. This allows the researcher to identify discursive patterns thoughout the documents, for instance to discover how widely certain topics were or were not discussed. If the visualisation provided a general overview of the structural composition of the topics - instrumental to their understanding and interpretation - the topics' distribution will reveal their discursive and 'behavioural' quality, for instance by evidencing potential over- or underrepresentations of some topics over others as well as their different distribution within each document and over the whole corpus. Potentially meaningful insights can also be obtained by calculating the distribution across subcollections (e.g., different newspapers' titles, different books) and comparing the results. 

In this notebook, the topics' distribution for each document is calculated through the Gensim's function *inference*

```sh
gensim.models.LdaModel.inference
```

This is calculated by dividing a topic's weight by the weight of all the topics for each document. 

For the topics' distribution over the entire collection, this is 'normalised' in the sense that it is calculated by taking into account the lenght of each document (i.e., number of words). The calculation yields more accurate results, especially when working with collections from different sources with heterogenous documents. 

## 9. Historicise the topics
When working with historical collections of timestamped documents, a step that bears great relevant for humanities scholars who try to answer historical questions is the possibility to plot the topics' distribution over time, i.e., historicise the topics. This truly effective operation in terms of revealing patterns and continuities over time can be performed in various ways depending on a number of factors of both technical and theoretical nature. For instance, standard LDA modelling tools such as MALLET or Gensim do not provide this functionality; sometimes, the nature of the data itself may not allow for meaningful historicisations (e.g., many time gaps in the collection); moreover, the way time is understood by the researcher (either as continuous or discrete) will determine the way the topics can be historicised. Thus, depending on these determining factors, there are different strategies to perform and visualise this step which necessarily require the critical intervention of the researcher.

## 9.1 Time is continuous (there are no gaps in my data)
In this case, lines would be a good way for identifying spikes in discourse and for depicting the relationship between the various discourses in a corpus. The lines visualise the average topic weight aggregated over time, that is it is computed by adding all of the weights for a given topic in a time period and dividing by the total number of documents in that time period. The avearge so calculated will likely show sharp peaks and falls and could be recommended for instance in the case of research questions aiming to identify specific sudden changes in a type of discourse. 

In the case of timeseries data, or data that is produced on regular intervals, the method of a rolling mean is typically used for capturing the general trend of a topic over time. If on the one hand this technique is very helpful for finding a time trend, on the other it flattens the topics' evolution to a smooth, possibly artificial, trend. So once again, the choice of one calculation over another depends on the dataset and the research question. As a rule of thumb, if the research enquiry relates to the long-term trend of a topic, then the rolling mean serves the purpose. If the research question is about a type of discourse at a particular point in time or over shorter periods, then computing the average would be recommended.

## 9.2 Time is discrete (there are gaps in my data)
Like in the case of time as continuous, if there are gaps in the dataset or time is conceived as discrete, the aggregation over time should be computed by calculating the average topic weight aggregated over time. The difference with the previous approach concerns the way the averages are displayed which could for instance be done by choosing to use a bar chart instead of lines. This would still show peaks and discontinuities in the topics' trends while encouraging to think of time as discrete. For the same reason, the rolling mean would not be recommended.

## 10. Conclusions
## 11. Remarks
## 12. License
## 13. Links
## 14. References
## 15. The team
## 16. How to cite
To cite the repository, please use the following format according to the APA style guide:

Viola, Lorella and de Crouy-Chanel, Mariella. 2020. *Topic Modelling with Gensim. A workflow for the Humanities* (v. 1.0.0). University of Luxembourg. https://github.com/DHARPA-Project/TopicModelling-


# kiara\TopicModelling-\requirements.txt
blis==0.7.3
catalogue==1.0.0
click==7.1.2
cymem==2.0.4
funcy==1.15
future==0.18.2
gensim==3.8.3
joblib==0.17.0
murmurhash==1.0.4
nltk==3.5
numpy==1.19.4
pandas==1.1.4
plac==1.1.3
preshed==3.0.4
pytz==2020.4
regex==2020.11.13
scipy==1.5.4
smart-open==4.0.1
spacy==2.3.4
srsly==1.0.4
thinc==7.4.3
wasabi==0.8.0


# kiara\TopicModelling-\vis-files\readme.md
These files contain the visualisations that are displayed in the topic modelling notebook.
This was a second experiment to include visualisations created in React within a notebook.
It works fine but the set-up is too complicated (it requires to add many files in the notebook) and time-consuming to use.
Since then, things can be done with the new "prototyping interface".


# kiara\TopicModelling-\vis-files\tm_coherence_chart.js
d3.csv('/nbextensions/coherence_values.csv').then(data => {
    const {
      LineChart,
      Line,
      Label,
      XAxis,
      YAxis,
      CartesianGrid,
      Tooltip,
      Legend
    } = Recharts;

    const CustomTooltip = ({
      active,
      payload,
      label
    }) => {
      if (active) {
        return React.createElement("div", {
          className: "custom-tooltip"
        }, React.createElement("p", null, React.createElement("span", {
          className: "label"
        }, " ", `${payload[0].name}: `, "  "), React.createElement("span", {
          className: "desc"
        }, " ", `${nrformat(payload[0].value)}`, " ")), React.createElement("p", null, React.createElement("span", {
          className: "label"
        }, " ", `Number of topics: `, "  "), React.createElement("span", {
          className: "desc"
        }, " ", `${label}`, " ")));
      }

      return null;
    };

    function nrformat(num) {
      num = +num;
      return num.toFixed(3);
    }

    ReactDOM.render( React.createElement(LineChart, {
      width: 600,
      height: 300,
      data: data,
      margin: {
        top: 5,
        right: 30,
        left: 20,
        bottom: 10
      }
    }, React.createElement(CartesianGrid, {
      strokeDasharray: "3 3"
    }), React.createElement(XAxis, {
      dataKey: "Number of topics",
      padding: {
        left: 20,
        right: 20
      }
    }, React.createElement(Label, {
      value: "Number of topics",
      offset: -7,
      position: "insideBottom"
    })), React.createElement(YAxis, {
      dataKey: "Coherence",
      padding: {
        top: 20,
        bottom: 20
      }
    }, React.createElement(Label, {
      value: "Coherence",
      offset: 3,
      position: "insideLeft",
      angle: -90
    })), React.createElement(Tooltip, {
      content: React.createElement(CustomTooltip, null),
      animationDuration: 20
    }, " "), React.createElement(Line, {
      type: "monotone",
      dataKey: "Coherence",
      stroke: "#4682b4",
      activeDot: {
        r: 8
      }
    })), document.querySelector('#coherence-chart'));
    });

# kiara\TopicModelling-\vis-files\tm_coherence_table.js
d3.csv('nbextensions/coherence_table.csv', removeSpaces).then(data => {
  data['nr_topics'] = data['Number of topics'];

  class TempTable extends React.Component {
    constructor(props) {
      super(props);
      this.state = {
        Topics: '4'
      };
      this.handleChange = this.handleChange.bind(this);
    }

    handleChange(event) {
      this.setState({
        Topics: event.target.value
      });
    }

    render() {
      const dataset = data;
      var entries = d3.nest().key(function (d) {
        return d.Number_of_topics;
      }).entries(dataset);
      var user_select = this.state.Topics;
      var data_filtered = dataset.filter(function (d) {
        return d.Number_of_topics === user_select;
      });
      return React.createElement("div", null, React.createElement("select", {
        className: "ui selection dropdown",
        value: this.state.Topics,
        onChange: this.handleChange
      }, entries.map(item => React.createElement("option", {
        key: item.key,
        value: item.key
      }, "Number of topics: ", item.key))), React.createElement("table", {
        className: "ui collapsing table unstackable"
      }, React.createElement("thead", null, React.createElement("tr", null, React.createElement("th", null, "Topic"), React.createElement("th", null, "Topic words"))), React.createElement("tbody", null, data_filtered.map(item => React.createElement("tr", null, React.createElement("td", {
        "data-label": "Topic"
      }, item['Topic']), React.createElement("td", {
        "data-label": "Topic words"
      }, item['Topic_words']))))));
    }

  }

  ReactDOM.render( React.createElement(TempTable, null), document.querySelector('#coherence-table'));
});

function removeSpaces(d) {
  Object.keys(d).forEach(function (origProp) {
    const noSpace = replaceAll(origProp, " ", "_");
    d[noSpace] = d[origProp];
  });
  return d;
}

;

function replaceAll(string, search, replace) {
  return string.split(search).join(replace);
}

# kiara\TopicModelling-\vis-files\tm_distribution_year_rolling.js
d3.csv('/nbextensions/rolling_mean.csv').then(data => {
    const cols1 = data.columns.slice(2);
    const cols = cols1.map(item => {
      const nr = item.match(/(\d+)/);
      const top_nr = +nr[0];
      return `Topic ${top_nr}`;
    });
    const colors = cols.map((item, index) => {
      d3.interpolateTurbo((index + 1) / cols.length);
    });
    const {
      LineChart,
      Line,
      Label,
      Cell,
      XAxis,
      YAxis,
      CartesianGrid,
      Tooltip,
      Legend
    } = Recharts;
    ReactDOM.render( React.createElement(LineChart, {
      width: 600,
      height: 400,
      data: data,
      margin: {
        top: 30,
        right: 30,
        left: 20,
        bottom: 10
      }
    }, React.createElement(CartesianGrid, {
      strokeDasharray: "3 3"
    }), React.createElement(XAxis, {
      dataKey: "year",
      padding: {
        left: 30,
        right: 30
      }
    }), React.createElement(YAxis, null), React.createElement(Tooltip, null), React.createElement(Legend, null), cols1.map((item, index) => React.createElement(Line, {
      type: "monotone",
      dataKey: item,
      stroke: d3.interpolateRdYlBu((index + 1) / cols.length)
    }))), document.querySelector('#rolling-mean-chart'));
  });

# kiara\TopicModelling-\vis-files\tm_publication_dist.js
d3.csv('/nbextensions/distribution_per_publication.csv').then(data => {
  const cols = data.columns.slice(2);
  const colors = cols.map((item, index) => {
    d3.interpolateTurbo((index + 1) / cols.length);
  });
  const {
    BarChart,
    Bar,
    Label,
    Cell,
    XAxis,
    YAxis,
    CartesianGrid,
    Tooltip,
    Legend
  } = Recharts;
  ReactDOM.render( React.createElement(BarChart, {
    width: 600,
    height: 300,
    data: data,
    margin: {
      top: 30,
      right: 30,
      left: 20,
      bottom: 10
    }
  }, React.createElement(CartesianGrid, {
    strokeDasharray: "3 3"
  }), React.createElement(XAxis, {
    dataKey: "publication_name"
  }), React.createElement(YAxis, null), React.createElement(Tooltip, null), React.createElement(Legend, null), cols.map((item, index) => React.createElement(Bar, {
    dataKey: item,
    fill: d3.interpolateRdYlBu((index + 1) / cols.length)
  }))), document.querySelector('#pub-distribution-chart'));
});

# kiara\TopicModelling-\vis-files\tm_topic_dist_year.js
d3.csv('/nbextensions/distribution_per_year.csv').then(data => {
    const cols1 = data.columns.slice(2);
    const cols = cols1.map(item => {
      const nr = item.match(/(\d+)/);
      const top_nr = +nr[0];
      return `Topic ${top_nr}`;
    });
    const colors = cols.map((item, index) => {
      d3.interpolateTurbo((index + 1) / cols.length);
    });
    const {
      LineChart,
      Line,
      Label,
      Cell,
      XAxis,
      YAxis,
      CartesianGrid,
      Tooltip,
      Legend
    } = Recharts;
    ReactDOM.render( React.createElement(LineChart, {
      width: 600,
      height: 400,
      data: data,
      margin: {
        top: 30,
        right: 30,
        left: 20,
        bottom: 10
      }
    }, React.createElement(CartesianGrid, {
      strokeDasharray: "3 3"
    }), React.createElement(XAxis, {
      dataKey: "year",
      padding: {
        left: 30,
        right: 30
      }
    }), React.createElement(YAxis, null), React.createElement(Tooltip, null), React.createElement(Legend, null), cols1.map((item, index) => React.createElement(Line, {
      type: "monotone",
      dataKey: item,
      stroke: d3.interpolateRdYlBu((index + 1) / cols.length)
    }))), document.querySelector('#year-distribution-chart'));
  });

# kiara\TopicModelling-\vis-files\tm_topic_dist_year_bar.js
d3.csv('/nbextensions/distribution_per_year.csv').then(data => {
    const cols = data.columns.slice(2);
    const colors = cols.map((item, index) => {
      d3.interpolateTurbo((index + 1) / cols.length);
    });
    const {
      BarChart,
      Bar,
      Label,
      Cell,
      XAxis,
      YAxis,
      CartesianGrid,
      Tooltip,
      Legend
    } = Recharts;
    ReactDOM.render( React.createElement(BarChart, {
      width: 1000,
      height: 400,
      data: data,
      margin: {
        top: 30,
        right: 30,
        left: 20,
        bottom: 10
      }
    }, React.createElement(CartesianGrid, {
      strokeDasharray: "3 3"
    }), React.createElement(XAxis, {
      dataKey: "year"
    }), React.createElement(YAxis, null), React.createElement(Tooltip, null), React.createElement(Legend, null), cols.map((item, index) => React.createElement(Bar, {
      dataKey: item,
      fill: d3.interpolateRdYlBu((index + 1) / cols.length)
    }))), document.querySelector('#pub-distribution-year-bar'));
  });

# kiara\TopicModelling-\vis-files\tm_topic_proportion_chart.js
d3.csv('/nbextensions/topic_distribution.csv', processDataChart).then(data => {

    const {
      BarChart,
      Bar,
      Label,
      Cell,
      XAxis,
      YAxis,
      CartesianGrid,
      Tooltip,
      Legend
    } = Recharts;
  
    const CustomTooltip = ({
      active,
      payload,
      label
    }) => {
      if (active) {
        return React.createElement("div", {
          className: "custom-tooltip"
        }, React.createElement("p", null, React.createElement("span", {
          className: "label"
        }, " ", ` Topic ${label}: ${nrformat(payload[0].value)} %`, " ")));
      }
  
      return null;
  
      function nrformat(num) {
        num = +num;
        return num.toFixed(2);
      }
    };
  
    ReactDOM.render( React.createElement(BarChart, {
      width: 600,
      height: 300,
      data: data,
      margin: {
        top: 5,
        right: 30,
        left: 20,
        bottom: 10
      }
    }, React.createElement(CartesianGrid, {
      strokeDasharray: "3 3"
    }), React.createElement(XAxis, {
      dataKey: "topics",
      padding: {
        left: 20,
        right: 20
      }
    }, React.createElement(Label, {
      value: "Topics",
      offset: -7,
      position: "insideBottom"
    })), React.createElement(YAxis, {
      dataKey: data.topics
    }, React.createElement(Label, {
      value: "Weight (%)",
      offset: 12,
      position: "insideLeft",
      angle: -90
    })), React.createElement(Tooltip, {
      content: React.createElement(CustomTooltip, null)
    }), React.createElement(Bar, {
      dataKey: "weight",
      fill: "steelblue"
    })), document.querySelector('#topic-prop-chart'));
  });

  function processDataChart(d) {
    if (d.topics) {
        var nr = d.topics.match(/(\d+)/);
        var nr = +nr[0] + 1 
        d.topics = nr
        d.weight = +d.weight
    }
    return d
}

# kiara\TopicModelling-\vis-files\tm_1\2.64b0803b.chunk.js
/*! For license information please see 2.64b0803b.chunk.js.LICENSE.txt */
(this["webpackJsonpvisualisation-d3"]=this["webpackJsonpvisualisation-d3"]||[]).push([[2],[function(e,t,n){"use strict";e.exports=n(29)},,function(e,t,n){"use strict";n.d(t,"b",(function(){return m})),n.d(t,"c",(function(){return g})),n.d(t,"d",(function(){return Zn})),n.d(t,"a",(function(){return br})),n.d(t,"e",(function(){return pi}));var r=function(e,t){return e<t?-1:e>t?1:e>=t?0:NaN},i=function(e){var t;return 1===e.length&&(t=e,e=function(e,n){return r(t(e),n)}),{left:function(t,n,r,i){for(null==r&&(r=0),null==i&&(i=t.length);r<i;){var o=r+i>>>1;e(t[o],n)<0?r=o+1:i=o}return r},right:function(t,n,r,i){for(null==r&&(r=0),null==i&&(i=t.length);r<i;){var o=r+i>>>1;e(t[o],n)>0?i=o:r=o+1}return r}}};var o=i(r),u=o.right,a=(o.left,u);var l=Array.prototype,c=(l.slice,l.map,Math.sqrt(50)),s=Math.sqrt(10),f=Math.sqrt(2),d=function(e,t,n){var r,i,o,u,a=-1;if(n=+n,(e=+e)===(t=+t)&&n>0)return[e];if((r=t<e)&&(i=e,e=t,t=i),0===(u=p(e,t,n))||!isFinite(u))return[];if(u>0)for(e=Math.ceil(e/u),t=Math.floor(t/u),o=new Array(i=Math.ceil(t-e+1));++a<i;)o[a]=(e+a)*u;else for(e=Math.floor(e*u),t=Math.ceil(t*u),o=new Array(i=Math.ceil(e-t+1));++a<i;)o[a]=(e-a)/u;return r&&o.reverse(),o};function p(e,t,n){var r=(t-e)/Math.max(0,n),i=Math.floor(Math.log(r)/Math.LN10),o=r/Math.pow(10,i);return i>=0?(o>=c?10:o>=s?5:o>=f?2:1)*Math.pow(10,i):-Math.pow(10,-i)/(o>=c?10:o>=s?5:o>=f?2:1)}function h(e,t,n){var r=Math.abs(t-e)/Math.max(0,n),i=Math.pow(10,Math.floor(Math.log(r)/Math.LN10)),o=r/i;return o>=c?i*=10:o>=s?i*=5:o>=f&&(i*=2),t<e?-i:i}var m=function(e,t){var n,r,i=e.length,o=-1;if(null==t){for(;++o<i;)if(null!=(n=e[o])&&n>=n)for(r=n;++o<i;)null!=(n=e[o])&&n>r&&(r=n)}else for(;++o<i;)if(null!=(n=t(e[o],o,e))&&n>=n)for(r=n;++o<i;)null!=(n=t(e[o],o,e))&&n>r&&(r=n);return r},g=function(e,t){var n,r,i=e.length,o=-1;if(null==t){for(;++o<i;)if(null!=(n=e[o])&&n>=n)for(r=n;++o<i;)null!=(n=e[o])&&r>n&&(r=n)}else for(;++o<i;)if(null!=(n=t(e[o],o,e))&&n>=n)for(r=n;++o<i;)null!=(n=t(e[o],o,e))&&r>n&&(r=n);return r};Array.prototype.slice;var v={value:function(){}};function y(){for(var e,t=0,n=arguments.length,r={};t<n;++t){if(!(e=arguments[t]+"")||e in r||/[\s.]/.test(e))throw new Error("illegal type: "+e);r[e]=[]}return new b(r)}function b(e){this._=e}function w(e,t){return e.trim().split(/^|\s+/).map((function(e){var n="",r=e.indexOf(".");if(r>=0&&(n=e.slice(r+1),e=e.slice(0,r)),e&&!t.hasOwnProperty(e))throw new Error("unknown type: "+e);return{type:e,name:n}}))}function x(e,t){for(var n,r=0,i=e.length;r<i;++r)if((n=e[r]).name===t)return n.value}function _(e,t,n){for(var r=0,i=e.length;r<i;++r)if(e[r].name===t){e[r]=v,e=e.slice(0,r).concat(e.slice(r+1));break}return null!=n&&e.push({name:t,value:n}),e}b.prototype=y.prototype={constructor:b,on:function(e,t){var n,r=this._,i=w(e+"",r),o=-1,u=i.length;if(!(arguments.length<2)){if(null!=t&&"function"!==typeof t)throw new Error("invalid callback: "+t);for(;++o<u;)if(n=(e=i[o]).type)r[n]=_(r[n],e.name,t);else if(null==t)for(n in r)r[n]=_(r[n],e.name,null);return this}for(;++o<u;)if((n=(e=i[o]).type)&&(n=x(r[n],e.name)))return n},copy:function(){var e={},t=this._;for(var n in t)e[n]=t[n].slice();return new b(e)},call:function(e,t){if((n=arguments.length-2)>0)for(var n,r,i=new Array(n),o=0;o<n;++o)i[o]=arguments[o+2];if(!this._.hasOwnProperty(e))throw new Error("unknown type: "+e);for(o=0,n=(r=this._[e]).length;o<n;++o)r[o].value.apply(t,i)},apply:function(e,t,n){if(!this._.hasOwnProperty(e))throw new Error("unknown type: "+e);for(var r=this._[e],i=0,o=r.length;i<o;++i)r[i].value.apply(t,n)}};var k=y;function T(){}var E=function(e){return null==e?T:function(){return this.querySelector(e)}};function C(){return[]}var S=function(e){return null==e?C:function(){return this.querySelectorAll(e)}},M=function(e){return function(){return this.matches(e)}},N=function(e){return new Array(e.length)};function P(e,t){this.ownerDocument=e.ownerDocument,this.namespaceURI=e.namespaceURI,this._next=null,this._parent=e,this.__data__=t}P.prototype={constructor:P,appendChild:function(e){return this._parent.insertBefore(e,this._next)},insertBefore:function(e,t){return this._parent.insertBefore(e,t)},querySelector:function(e){return this._parent.querySelector(e)},querySelectorAll:function(e){return this._parent.querySelectorAll(e)}};function A(e,t,n,r,i,o){for(var u,a=0,l=t.length,c=o.length;a<c;++a)(u=t[a])?(u.__data__=o[a],r[a]=u):n[a]=new P(e,o[a]);for(;a<l;++a)(u=t[a])&&(i[a]=u)}function R(e,t,n,r,i,o,u){var a,l,c,s={},f=t.length,d=o.length,p=new Array(f);for(a=0;a<f;++a)(l=t[a])&&(p[a]=c="$"+u.call(l,l.__data__,a,t),c in s?i[a]=l:s[c]=l);for(a=0;a<d;++a)(l=s[c="$"+u.call(e,o[a],a,o)])?(r[a]=l,l.__data__=o[a],s[c]=null):n[a]=new P(e,o[a]);for(a=0;a<f;++a)(l=t[a])&&s[p[a]]===l&&(i[a]=l)}function D(e,t){return e<t?-1:e>t?1:e>=t?0:NaN}var O="http://www.w3.org/1999/xhtml",z={svg:"http://www.w3.org/2000/svg",xhtml:O,xlink:"http://www.w3.org/1999/xlink",xml:"http://www.w3.org/XML/1998/namespace",xmlns:"http://www.w3.org/2000/xmlns/"},L=function(e){var t=e+="",n=t.indexOf(":");return n>=0&&"xmlns"!==(t=e.slice(0,n))&&(e=e.slice(n+1)),z.hasOwnProperty(t)?{space:z[t],local:e}:e};function U(e){return function(){this.removeAttribute(e)}}function F(e){return function(){this.removeAttributeNS(e.space,e.local)}}function I(e,t){return function(){this.setAttribute(e,t)}}function j(e,t){return function(){this.setAttributeNS(e.space,e.local,t)}}function H(e,t){return function(){var n=t.apply(this,arguments);null==n?this.removeAttribute(e):this.setAttribute(e,n)}}function V(e,t){return function(){var n=t.apply(this,arguments);null==n?this.removeAttributeNS(e.space,e.local):this.setAttributeNS(e.space,e.local,n)}}var $=function(e){return e.ownerDocument&&e.ownerDocument.defaultView||e.document&&e||e.defaultView};function W(e){return function(){this.style.removeProperty(e)}}function Q(e,t,n){return function(){this.style.setProperty(e,t,n)}}function q(e,t,n){return function(){var r=t.apply(this,arguments);null==r?this.style.removeProperty(e):this.style.setProperty(e,r,n)}}function B(e,t){return e.style.getPropertyValue(t)||$(e).getComputedStyle(e,null).getPropertyValue(t)}function Y(e){return function(){delete this[e]}}function X(e,t){return function(){this[e]=t}}function K(e,t){return function(){var n=t.apply(this,arguments);null==n?delete this[e]:this[e]=n}}function Z(e){return e.trim().split(/^|\s+/)}function J(e){return e.classList||new G(e)}function G(e){this._node=e,this._names=Z(e.getAttribute("class")||"")}function ee(e,t){for(var n=J(e),r=-1,i=t.length;++r<i;)n.add(t[r])}function te(e,t){for(var n=J(e),r=-1,i=t.length;++r<i;)n.remove(t[r])}function ne(e){return function(){ee(this,e)}}function re(e){return function(){te(this,e)}}function ie(e,t){return function(){(t.apply(this,arguments)?ee:te)(this,e)}}G.prototype={add:function(e){this._names.indexOf(e)<0&&(this._names.push(e),this._node.setAttribute("class",this._names.join(" ")))},remove:function(e){var t=this._names.indexOf(e);t>=0&&(this._names.splice(t,1),this._node.setAttribute("class",this._names.join(" ")))},contains:function(e){return this._names.indexOf(e)>=0}};function oe(){this.textContent=""}function ue(e){return function(){this.textContent=e}}function ae(e){return function(){var t=e.apply(this,arguments);this.textContent=null==t?"":t}}function le(){this.innerHTML=""}function ce(e){return function(){this.innerHTML=e}}function se(e){return function(){var t=e.apply(this,arguments);this.innerHTML=null==t?"":t}}function fe(){this.nextSibling&&this.parentNode.appendChild(this)}function de(){this.previousSibling&&this.parentNode.insertBefore(this,this.parentNode.firstChild)}function pe(e){return function(){var t=this.ownerDocument,n=this.namespaceURI;return n===O&&t.documentElement.namespaceURI===O?t.createElement(e):t.createElementNS(n,e)}}function he(e){return function(){return this.ownerDocument.createElementNS(e.space,e.local)}}var me=function(e){var t=L(e);return(t.local?he:pe)(t)};function ge(){return null}function ve(){var e=this.parentNode;e&&e.removeChild(this)}function ye(){var e=this.cloneNode(!1),t=this.parentNode;return t?t.insertBefore(e,this.nextSibling):e}function be(){var e=this.cloneNode(!0),t=this.parentNode;return t?t.insertBefore(e,this.nextSibling):e}var we={},xe=null;"undefined"!==typeof document&&("onmouseenter"in document.documentElement||(we={mouseenter:"mouseover",mouseleave:"mouseout"}));function _e(e,t,n){return e=ke(e,t,n),function(t){var n=t.relatedTarget;n&&(n===this||8&n.compareDocumentPosition(this))||e.call(this,t)}}function ke(e,t,n){return function(r){var i=xe;xe=r;try{e.call(this,this.__data__,t,n)}finally{xe=i}}}function Te(e){return e.trim().split(/^|\s+/).map((function(e){var t="",n=e.indexOf(".");return n>=0&&(t=e.slice(n+1),e=e.slice(0,n)),{type:e,name:t}}))}function Ee(e){return function(){var t=this.__on;if(t){for(var n,r=0,i=-1,o=t.length;r<o;++r)n=t[r],e.type&&n.type!==e.type||n.name!==e.name?t[++i]=n:this.removeEventListener(n.type,n.listener,n.capture);++i?t.length=i:delete this.__on}}}function Ce(e,t,n){var r=we.hasOwnProperty(e.type)?_e:ke;return function(i,o,u){var a,l=this.__on,c=r(t,o,u);if(l)for(var s=0,f=l.length;s<f;++s)if((a=l[s]).type===e.type&&a.name===e.name)return this.removeEventListener(a.type,a.listener,a.capture),this.addEventListener(a.type,a.listener=c,a.capture=n),void(a.value=t);this.addEventListener(e.type,c,n),a={type:e.type,name:e.name,value:t,listener:c,capture:n},l?l.push(a):this.__on=[a]}}function Se(e,t,n){var r=$(e),i=r.CustomEvent;"function"===typeof i?i=new i(t,n):(i=r.document.createEvent("Event"),n?(i.initEvent(t,n.bubbles,n.cancelable),i.detail=n.detail):i.initEvent(t,!1,!1)),e.dispatchEvent(i)}function Me(e,t){return function(){return Se(this,e,t)}}function Ne(e,t){return function(){return Se(this,e,t.apply(this,arguments))}}var Pe=[null];function Ae(e,t){this._groups=e,this._parents=t}function Re(){return new Ae([[document.documentElement]],Pe)}Ae.prototype=Re.prototype={constructor:Ae,select:function(e){"function"!==typeof e&&(e=E(e));for(var t=this._groups,n=t.length,r=new Array(n),i=0;i<n;++i)for(var o,u,a=t[i],l=a.length,c=r[i]=new Array(l),s=0;s<l;++s)(o=a[s])&&(u=e.call(o,o.__data__,s,a))&&("__data__"in o&&(u.__data__=o.__data__),c[s]=u);return new Ae(r,this._parents)},selectAll:function(e){"function"!==typeof e&&(e=S(e));for(var t=this._groups,n=t.length,r=[],i=[],o=0;o<n;++o)for(var u,a=t[o],l=a.length,c=0;c<l;++c)(u=a[c])&&(r.push(e.call(u,u.__data__,c,a)),i.push(u));return new Ae(r,i)},filter:function(e){"function"!==typeof e&&(e=M(e));for(var t=this._groups,n=t.length,r=new Array(n),i=0;i<n;++i)for(var o,u=t[i],a=u.length,l=r[i]=[],c=0;c<a;++c)(o=u[c])&&e.call(o,o.__data__,c,u)&&l.push(o);return new Ae(r,this._parents)},data:function(e,t){if(!e)return h=new Array(this.size()),s=-1,this.each((function(e){h[++s]=e})),h;var n,r=t?R:A,i=this._parents,o=this._groups;"function"!==typeof e&&(n=e,e=function(){return n});for(var u=o.length,a=new Array(u),l=new Array(u),c=new Array(u),s=0;s<u;++s){var f=i[s],d=o[s],p=d.length,h=e.call(f,f&&f.__data__,s,i),m=h.length,g=l[s]=new Array(m),v=a[s]=new Array(m);r(f,d,g,v,c[s]=new Array(p),h,t);for(var y,b,w=0,x=0;w<m;++w)if(y=g[w]){for(w>=x&&(x=w+1);!(b=v[x])&&++x<m;);y._next=b||null}}return(a=new Ae(a,i))._enter=l,a._exit=c,a},enter:function(){return new Ae(this._enter||this._groups.map(N),this._parents)},exit:function(){return new Ae(this._exit||this._groups.map(N),this._parents)},join:function(e,t,n){var r=this.enter(),i=this,o=this.exit();return r="function"===typeof e?e(r):r.append(e+""),null!=t&&(i=t(i)),null==n?o.remove():n(o),r&&i?r.merge(i).order():i},merge:function(e){for(var t=this._groups,n=e._groups,r=t.length,i=n.length,o=Math.min(r,i),u=new Array(r),a=0;a<o;++a)for(var l,c=t[a],s=n[a],f=c.length,d=u[a]=new Array(f),p=0;p<f;++p)(l=c[p]||s[p])&&(d[p]=l);for(;a<r;++a)u[a]=t[a];return new Ae(u,this._parents)},order:function(){for(var e=this._groups,t=-1,n=e.length;++t<n;)for(var r,i=e[t],o=i.length-1,u=i[o];--o>=0;)(r=i[o])&&(u&&4^r.compareDocumentPosition(u)&&u.parentNode.insertBefore(r,u),u=r);return this},sort:function(e){function t(t,n){return t&&n?e(t.__data__,n.__data__):!t-!n}e||(e=D);for(var n=this._groups,r=n.length,i=new Array(r),o=0;o<r;++o){for(var u,a=n[o],l=a.length,c=i[o]=new Array(l),s=0;s<l;++s)(u=a[s])&&(c[s]=u);c.sort(t)}return new Ae(i,this._parents).order()},call:function(){var e=arguments[0];return arguments[0]=this,e.apply(null,arguments),this},nodes:function(){var e=new Array(this.size()),t=-1;return this.each((function(){e[++t]=this})),e},node:function(){for(var e=this._groups,t=0,n=e.length;t<n;++t)for(var r=e[t],i=0,o=r.length;i<o;++i){var u=r[i];if(u)return u}return null},size:function(){var e=0;return this.each((function(){++e})),e},empty:function(){return!this.node()},each:function(e){for(var t=this._groups,n=0,r=t.length;n<r;++n)for(var i,o=t[n],u=0,a=o.length;u<a;++u)(i=o[u])&&e.call(i,i.__data__,u,o);return this},attr:function(e,t){var n=L(e);if(arguments.length<2){var r=this.node();return n.local?r.getAttributeNS(n.space,n.local):r.getAttribute(n)}return this.each((null==t?n.local?F:U:"function"===typeof t?n.local?V:H:n.local?j:I)(n,t))},style:function(e,t,n){return arguments.length>1?this.each((null==t?W:"function"===typeof t?q:Q)(e,t,null==n?"":n)):B(this.node(),e)},property:function(e,t){return arguments.length>1?this.each((null==t?Y:"function"===typeof t?K:X)(e,t)):this.node()[e]},classed:function(e,t){var n=Z(e+"");if(arguments.length<2){for(var r=J(this.node()),i=-1,o=n.length;++i<o;)if(!r.contains(n[i]))return!1;return!0}return this.each(("function"===typeof t?ie:t?ne:re)(n,t))},text:function(e){return arguments.length?this.each(null==e?oe:("function"===typeof e?ae:ue)(e)):this.node().textContent},html:function(e){return arguments.length?this.each(null==e?le:("function"===typeof e?se:ce)(e)):this.node().innerHTML},raise:function(){return this.each(fe)},lower:function(){return this.each(de)},append:function(e){var t="function"===typeof e?e:me(e);return this.select((function(){return this.appendChild(t.apply(this,arguments))}))},insert:function(e,t){var n="function"===typeof e?e:me(e),r=null==t?ge:"function"===typeof t?t:E(t);return this.select((function(){return this.insertBefore(n.apply(this,arguments),r.apply(this,arguments)||null)}))},remove:function(){return this.each(ve)},clone:function(e){return this.select(e?be:ye)},datum:function(e){return arguments.length?this.property("__data__",e):this.node().__data__},on:function(e,t,n){var r,i,o=Te(e+""),u=o.length;if(!(arguments.length<2)){for(a=t?Ce:Ee,null==n&&(n=!1),r=0;r<u;++r)this.each(a(o[r],t,n));return this}var a=this.node().__on;if(a)for(var l,c=0,s=a.length;c<s;++c)for(r=0,l=a[c];r<u;++r)if((i=o[r]).type===l.type&&i.name===l.name)return l.value},dispatch:function(e,t){return this.each(("function"===typeof t?Ne:Me)(e,t))}};var De=Re;var Oe=function(e,t,n){e.prototype=t.prototype=n,n.constructor=e};function ze(e,t){var n=Object.create(e.prototype);for(var r in t)n[r]=t[r];return n}function Le(){}var Ue="\\s*([+-]?\\d+)\\s*",Fe="\\s*([+-]?\\d*\\.?\\d+(?:[eE][+-]?\\d+)?)\\s*",Ie="\\s*([+-]?\\d*\\.?\\d+(?:[eE][+-]?\\d+)?)%\\s*",je=/^#([0-9a-f]{3,8})$/,He=new RegExp("^rgb\\("+[Ue,Ue,Ue]+"\\)$"),Ve=new RegExp("^rgb\\("+[Ie,Ie,Ie]+"\\)$"),$e=new RegExp("^rgba\\("+[Ue,Ue,Ue,Fe]+"\\)$"),We=new RegExp("^rgba\\("+[Ie,Ie,Ie,Fe]+"\\)$"),Qe=new RegExp("^hsl\\("+[Fe,Ie,Ie]+"\\)$"),qe=new RegExp("^hsla\\("+[Fe,Ie,Ie,Fe]+"\\)$"),Be={aliceblue:15792383,antiquewhite:16444375,aqua:65535,aquamarine:8388564,azure:15794175,beige:16119260,bisque:16770244,black:0,blanchedalmond:16772045,blue:255,blueviolet:9055202,brown:10824234,burlywood:14596231,cadetblue:6266528,chartreuse:8388352,chocolate:13789470,coral:16744272,cornflowerblue:6591981,cornsilk:16775388,crimson:14423100,cyan:65535,darkblue:139,darkcyan:35723,darkgoldenrod:12092939,darkgray:11119017,darkgreen:25600,darkgrey:11119017,darkkhaki:12433259,darkmagenta:9109643,darkolivegreen:5597999,darkorange:16747520,darkorchid:10040012,darkred:9109504,darksalmon:15308410,darkseagreen:9419919,darkslateblue:4734347,darkslategray:3100495,darkslategrey:3100495,darkturquoise:52945,darkviolet:9699539,deeppink:16716947,deepskyblue:49151,dimgray:6908265,dimgrey:6908265,dodgerblue:2003199,firebrick:11674146,floralwhite:16775920,forestgreen:2263842,fuchsia:16711935,gainsboro:14474460,ghostwhite:16316671,gold:16766720,goldenrod:14329120,gray:8421504,green:32768,greenyellow:11403055,grey:8421504,honeydew:15794160,hotpink:16738740,indianred:13458524,indigo:4915330,ivory:16777200,khaki:15787660,lavender:15132410,lavenderblush:16773365,lawngreen:8190976,lemonchiffon:16775885,lightblue:11393254,lightcoral:15761536,lightcyan:14745599,lightgoldenrodyellow:16448210,lightgray:13882323,lightgreen:9498256,lightgrey:13882323,lightpink:16758465,lightsalmon:16752762,lightseagreen:2142890,lightskyblue:8900346,lightslategray:7833753,lightslategrey:7833753,lightsteelblue:11584734,lightyellow:16777184,lime:65280,limegreen:3329330,linen:16445670,magenta:16711935,maroon:8388608,mediumaquamarine:6737322,mediumblue:205,mediumorchid:12211667,mediumpurple:9662683,mediumseagreen:3978097,mediumslateblue:8087790,mediumspringgreen:64154,mediumturquoise:4772300,mediumvioletred:13047173,midnightblue:1644912,mintcream:16121850,mistyrose:16770273,moccasin:16770229,navajowhite:16768685,navy:128,oldlace:16643558,olive:8421376,olivedrab:7048739,orange:16753920,orangered:16729344,orchid:14315734,palegoldenrod:15657130,palegreen:10025880,paleturquoise:11529966,palevioletred:14381203,papayawhip:16773077,peachpuff:16767673,peru:13468991,pink:16761035,plum:14524637,powderblue:11591910,purple:8388736,rebeccapurple:6697881,red:16711680,rosybrown:12357519,royalblue:4286945,saddlebrown:9127187,salmon:16416882,sandybrown:16032864,seagreen:3050327,seashell:16774638,sienna:10506797,silver:12632256,skyblue:8900331,slateblue:6970061,slategray:7372944,slategrey:7372944,snow:16775930,springgreen:65407,steelblue:4620980,tan:13808780,teal:32896,thistle:14204888,tomato:16737095,turquoise:4251856,violet:15631086,wheat:16113331,white:16777215,whitesmoke:16119285,yellow:16776960,yellowgreen:10145074};function Ye(){return this.rgb().formatHex()}function Xe(){return this.rgb().formatRgb()}function Ke(e){var t,n;return e=(e+"").trim().toLowerCase(),(t=je.exec(e))?(n=t[1].length,t=parseInt(t[1],16),6===n?Ze(t):3===n?new tt(t>>8&15|t>>4&240,t>>4&15|240&t,(15&t)<<4|15&t,1):8===n?Je(t>>24&255,t>>16&255,t>>8&255,(255&t)/255):4===n?Je(t>>12&15|t>>8&240,t>>8&15|t>>4&240,t>>4&15|240&t,((15&t)<<4|15&t)/255):null):(t=He.exec(e))?new tt(t[1],t[2],t[3],1):(t=Ve.exec(e))?new tt(255*t[1]/100,255*t[2]/100,255*t[3]/100,1):(t=$e.exec(e))?Je(t[1],t[2],t[3],t[4]):(t=We.exec(e))?Je(255*t[1]/100,255*t[2]/100,255*t[3]/100,t[4]):(t=Qe.exec(e))?ot(t[1],t[2]/100,t[3]/100,1):(t=qe.exec(e))?ot(t[1],t[2]/100,t[3]/100,t[4]):Be.hasOwnProperty(e)?Ze(Be[e]):"transparent"===e?new tt(NaN,NaN,NaN,0):null}function Ze(e){return new tt(e>>16&255,e>>8&255,255&e,1)}function Je(e,t,n,r){return r<=0&&(e=t=n=NaN),new tt(e,t,n,r)}function Ge(e){return e instanceof Le||(e=Ke(e)),e?new tt((e=e.rgb()).r,e.g,e.b,e.opacity):new tt}function et(e,t,n,r){return 1===arguments.length?Ge(e):new tt(e,t,n,null==r?1:r)}function tt(e,t,n,r){this.r=+e,this.g=+t,this.b=+n,this.opacity=+r}function nt(){return"#"+it(this.r)+it(this.g)+it(this.b)}function rt(){var e=this.opacity;return(1===(e=isNaN(e)?1:Math.max(0,Math.min(1,e)))?"rgb(":"rgba(")+Math.max(0,Math.min(255,Math.round(this.r)||0))+", "+Math.max(0,Math.min(255,Math.round(this.g)||0))+", "+Math.max(0,Math.min(255,Math.round(this.b)||0))+(1===e?")":", "+e+")")}function it(e){return((e=Math.max(0,Math.min(255,Math.round(e)||0)))<16?"0":"")+e.toString(16)}function ot(e,t,n,r){return r<=0?e=t=n=NaN:n<=0||n>=1?e=t=NaN:t<=0&&(e=NaN),new at(e,t,n,r)}function ut(e){if(e instanceof at)return new at(e.h,e.s,e.l,e.opacity);if(e instanceof Le||(e=Ke(e)),!e)return new at;if(e instanceof at)return e;var t=(e=e.rgb()).r/255,n=e.g/255,r=e.b/255,i=Math.min(t,n,r),o=Math.max(t,n,r),u=NaN,a=o-i,l=(o+i)/2;return a?(u=t===o?(n-r)/a+6*(n<r):n===o?(r-t)/a+2:(t-n)/a+4,a/=l<.5?o+i:2-o-i,u*=60):a=l>0&&l<1?0:u,new at(u,a,l,e.opacity)}function at(e,t,n,r){this.h=+e,this.s=+t,this.l=+n,this.opacity=+r}function lt(e,t,n){return 255*(e<60?t+(n-t)*e/60:e<180?n:e<240?t+(n-t)*(240-e)/60:t)}function ct(e,t,n,r,i){var o=e*e,u=o*e;return((1-3*e+3*o-u)*t+(4-6*o+3*u)*n+(1+3*e+3*o-3*u)*r+u*i)/6}Oe(Le,Ke,{copy:function(e){return Object.assign(new this.constructor,this,e)},displayable:function(){return this.rgb().displayable()},hex:Ye,formatHex:Ye,formatHsl:function(){return ut(this).formatHsl()},formatRgb:Xe,toString:Xe}),Oe(tt,et,ze(Le,{brighter:function(e){return e=null==e?1/.7:Math.pow(1/.7,e),new tt(this.r*e,this.g*e,this.b*e,this.opacity)},darker:function(e){return e=null==e?.7:Math.pow(.7,e),new tt(this.r*e,this.g*e,this.b*e,this.opacity)},rgb:function(){return this},displayable:function(){return-.5<=this.r&&this.r<255.5&&-.5<=this.g&&this.g<255.5&&-.5<=this.b&&this.b<255.5&&0<=this.opacity&&this.opacity<=1},hex:nt,formatHex:nt,formatRgb:rt,toString:rt})),Oe(at,(function(e,t,n,r){return 1===arguments.length?ut(e):new at(e,t,n,null==r?1:r)}),ze(Le,{brighter:function(e){return e=null==e?1/.7:Math.pow(1/.7,e),new at(this.h,this.s,this.l*e,this.opacity)},darker:function(e){return e=null==e?.7:Math.pow(.7,e),new at(this.h,this.s,this.l*e,this.opacity)},rgb:function(){var e=this.h%360+360*(this.h<0),t=isNaN(e)||isNaN(this.s)?0:this.s,n=this.l,r=n+(n<.5?n:1-n)*t,i=2*n-r;return new tt(lt(e>=240?e-240:e+120,i,r),lt(e,i,r),lt(e<120?e+240:e-120,i,r),this.opacity)},displayable:function(){return(0<=this.s&&this.s<=1||isNaN(this.s))&&0<=this.l&&this.l<=1&&0<=this.opacity&&this.opacity<=1},formatHsl:function(){var e=this.opacity;return(1===(e=isNaN(e)?1:Math.max(0,Math.min(1,e)))?"hsl(":"hsla(")+(this.h||0)+", "+100*(this.s||0)+"%, "+100*(this.l||0)+"%"+(1===e?")":", "+e+")")}}));var st=function(e){return function(){return e}};function ft(e,t){return function(n){return e+n*t}}function dt(e){return 1===(e=+e)?pt:function(t,n){return n-t?function(e,t,n){return e=Math.pow(e,n),t=Math.pow(t,n)-e,n=1/n,function(r){return Math.pow(e+r*t,n)}}(t,n,e):st(isNaN(t)?n:t)}}function pt(e,t){var n=t-e;return n?ft(e,n):st(isNaN(e)?t:e)}var ht=function e(t){var n=dt(t);function r(e,t){var r=n((e=et(e)).r,(t=et(t)).r),i=n(e.g,t.g),o=n(e.b,t.b),u=pt(e.opacity,t.opacity);return function(t){return e.r=r(t),e.g=i(t),e.b=o(t),e.opacity=u(t),e+""}}return r.gamma=e,r}(1);function mt(e){return function(t){var n,r,i=t.length,o=new Array(i),u=new Array(i),a=new Array(i);for(n=0;n<i;++n)r=et(t[n]),o[n]=r.r||0,u[n]=r.g||0,a[n]=r.b||0;return o=e(o),u=e(u),a=e(a),r.opacity=1,function(e){return r.r=o(e),r.g=u(e),r.b=a(e),r+""}}}mt((function(e){var t=e.length-1;return function(n){var r=n<=0?n=0:n>=1?(n=1,t-1):Math.floor(n*t),i=e[r],o=e[r+1],u=r>0?e[r-1]:2*i-o,a=r<t-1?e[r+2]:2*o-i;return ct((n-r/t)*t,u,i,o,a)}})),mt((function(e){var t=e.length;return function(n){var r=Math.floor(((n%=1)<0?++n:n)*t),i=e[(r+t-1)%t],o=e[r%t],u=e[(r+1)%t],a=e[(r+2)%t];return ct((n-r/t)*t,i,o,u,a)}}));var gt=function(e,t){t||(t=[]);var n,r=e?Math.min(t.length,e.length):0,i=t.slice();return function(o){for(n=0;n<r;++n)i[n]=e[n]*(1-o)+t[n]*o;return i}};function vt(e){return ArrayBuffer.isView(e)&&!(e instanceof DataView)}function yt(e,t){var n,r=t?t.length:0,i=e?Math.min(r,e.length):0,o=new Array(i),u=new Array(r);for(n=0;n<i;++n)o[n]=St(e[n],t[n]);for(;n<r;++n)u[n]=t[n];return function(e){for(n=0;n<i;++n)u[n]=o[n](e);return u}}var bt=function(e,t){var n=new Date;return e=+e,t=+t,function(r){return n.setTime(e*(1-r)+t*r),n}},wt=function(e,t){return e=+e,t=+t,function(n){return e*(1-n)+t*n}},xt=function(e,t){var n,r={},i={};for(n in null!==e&&"object"===typeof e||(e={}),null!==t&&"object"===typeof t||(t={}),t)n in e?r[n]=St(e[n],t[n]):i[n]=t[n];return function(e){for(n in r)i[n]=r[n](e);return i}},_t=/[-+]?(?:\d+\.?\d*|\.?\d+)(?:[eE][-+]?\d+)?/g,kt=new RegExp(_t.source,"g");var Tt,Et,Ct=function(e,t){var n,r,i,o=_t.lastIndex=kt.lastIndex=0,u=-1,a=[],l=[];for(e+="",t+="";(n=_t.exec(e))&&(r=kt.exec(t));)(i=r.index)>o&&(i=t.slice(o,i),a[u]?a[u]+=i:a[++u]=i),(n=n[0])===(r=r[0])?a[u]?a[u]+=r:a[++u]=r:(a[++u]=null,l.push({i:u,x:wt(n,r)})),o=kt.lastIndex;return o<t.length&&(i=t.slice(o),a[u]?a[u]+=i:a[++u]=i),a.length<2?l[0]?function(e){return function(t){return e(t)+""}}(l[0].x):function(e){return function(){return e}}(t):(t=l.length,function(e){for(var n,r=0;r<t;++r)a[(n=l[r]).i]=n.x(e);return a.join("")})},St=function(e,t){var n,r=typeof t;return null==t||"boolean"===r?st(t):("number"===r?wt:"string"===r?(n=Ke(t))?(t=n,ht):Ct:t instanceof Ke?ht:t instanceof Date?bt:vt(t)?gt:Array.isArray(t)?yt:"function"!==typeof t.valueOf&&"function"!==typeof t.toString||isNaN(t)?xt:wt)(e,t)},Mt=0,Nt=0,Pt=0,At=0,Rt=0,Dt=0,Ot="object"===typeof performance&&performance.now?performance:Date,zt="object"===typeof window&&window.requestAnimationFrame?window.requestAnimationFrame.bind(window):function(e){setTimeout(e,17)};function Lt(){return Rt||(zt(Ut),Rt=Ot.now()+Dt)}function Ut(){Rt=0}function Ft(){this._call=this._time=this._next=null}function It(e,t,n){var r=new Ft;return r.restart(e,t,n),r}function jt(){Rt=(At=Ot.now())+Dt,Mt=Nt=0;try{!function(){Lt(),++Mt;for(var e,t=Tt;t;)(e=Rt-t._time)>=0&&t._call.call(null,e),t=t._next;--Mt}()}finally{Mt=0,function(){var e,t,n=Tt,r=1/0;for(;n;)n._call?(r>n._time&&(r=n._time),e=n,n=n._next):(t=n._next,n._next=null,n=e?e._next=t:Tt=t);Et=e,Vt(r)}(),Rt=0}}function Ht(){var e=Ot.now(),t=e-At;t>1e3&&(Dt-=t,At=e)}function Vt(e){Mt||(Nt&&(Nt=clearTimeout(Nt)),e-Rt>24?(e<1/0&&(Nt=setTimeout(jt,e-Ot.now()-Dt)),Pt&&(Pt=clearInterval(Pt))):(Pt||(At=Ot.now(),Pt=setInterval(Ht,1e3)),Mt=1,zt(jt)))}Ft.prototype=It.prototype={constructor:Ft,restart:function(e,t,n){if("function"!==typeof e)throw new TypeError("callback is not a function");n=(null==n?Lt():+n)+(null==t?0:+t),this._next||Et===this||(Et?Et._next=this:Tt=this,Et=this),this._call=e,this._time=n,Vt()},stop:function(){this._call&&(this._call=null,this._time=1/0,Vt())}};var $t=function(e,t,n){var r=new Ft;return t=null==t?0:+t,r.restart((function(n){r.stop(),e(n+t)}),t,n),r},Wt=k("start","end","cancel","interrupt"),Qt=[],qt=function(e,t,n,r,i,o){var u=e.__transition;if(u){if(n in u)return}else e.__transition={};!function(e,t,n){var r,i=e.__transition;function o(l){var c,s,f,d;if(1!==n.state)return a();for(c in i)if((d=i[c]).name===n.name){if(3===d.state)return $t(o);4===d.state?(d.state=6,d.timer.stop(),d.on.call("interrupt",e,e.__data__,d.index,d.group),delete i[c]):+c<t&&(d.state=6,d.timer.stop(),d.on.call("cancel",e,e.__data__,d.index,d.group),delete i[c])}if($t((function(){3===n.state&&(n.state=4,n.timer.restart(u,n.delay,n.time),u(l))})),n.state=2,n.on.call("start",e,e.__data__,n.index,n.group),2===n.state){for(n.state=3,r=new Array(f=n.tween.length),c=0,s=-1;c<f;++c)(d=n.tween[c].value.call(e,e.__data__,n.index,n.group))&&(r[++s]=d);r.length=s+1}}function u(t){for(var i=t<n.duration?n.ease.call(null,t/n.duration):(n.timer.restart(a),n.state=5,1),o=-1,u=r.length;++o<u;)r[o].call(e,i);5===n.state&&(n.on.call("end",e,e.__data__,n.index,n.group),a())}function a(){for(var r in n.state=6,n.timer.stop(),delete i[t],i)return;delete e.__transition}i[t]=n,n.timer=It((function(e){n.state=1,n.timer.restart(o,n.delay,n.time),n.delay<=e&&o(e-n.delay)}),0,n.time)}(e,n,{name:t,index:r,group:i,on:Wt,tween:Qt,time:o.time,delay:o.delay,duration:o.duration,ease:o.ease,timer:null,state:0})};function Bt(e,t){var n=Xt(e,t);if(n.state>0)throw new Error("too late; already scheduled");return n}function Yt(e,t){var n=Xt(e,t);if(n.state>3)throw new Error("too late; already running");return n}function Xt(e,t){var n=e.__transition;if(!n||!(n=n[t]))throw new Error("transition not found");return n}var Kt,Zt,Jt,Gt,en=function(e,t){var n,r,i,o=e.__transition,u=!0;if(o){for(i in t=null==t?null:t+"",o)(n=o[i]).name===t?(r=n.state>2&&n.state<5,n.state=6,n.timer.stop(),n.on.call(r?"interrupt":"cancel",e,e.__data__,n.index,n.group),delete o[i]):u=!1;u&&delete e.__transition}},tn=180/Math.PI,nn={translateX:0,translateY:0,rotate:0,skewX:0,scaleX:1,scaleY:1},rn=function(e,t,n,r,i,o){var u,a,l;return(u=Math.sqrt(e*e+t*t))&&(e/=u,t/=u),(l=e*n+t*r)&&(n-=e*l,r-=t*l),(a=Math.sqrt(n*n+r*r))&&(n/=a,r/=a,l/=a),e*r<t*n&&(e=-e,t=-t,l=-l,u=-u),{translateX:i,translateY:o,rotate:Math.atan2(t,e)*tn,skewX:Math.atan(l)*tn,scaleX:u,scaleY:a}};function on(e,t,n,r){function i(e){return e.length?e.pop()+" ":""}return function(o,u){var a=[],l=[];return o=e(o),u=e(u),function(e,r,i,o,u,a){if(e!==i||r!==o){var l=u.push("translate(",null,t,null,n);a.push({i:l-4,x:wt(e,i)},{i:l-2,x:wt(r,o)})}else(i||o)&&u.push("translate("+i+t+o+n)}(o.translateX,o.translateY,u.translateX,u.translateY,a,l),function(e,t,n,o){e!==t?(e-t>180?t+=360:t-e>180&&(e+=360),o.push({i:n.push(i(n)+"rotate(",null,r)-2,x:wt(e,t)})):t&&n.push(i(n)+"rotate("+t+r)}(o.rotate,u.rotate,a,l),function(e,t,n,o){e!==t?o.push({i:n.push(i(n)+"skewX(",null,r)-2,x:wt(e,t)}):t&&n.push(i(n)+"skewX("+t+r)}(o.skewX,u.skewX,a,l),function(e,t,n,r,o,u){if(e!==n||t!==r){var a=o.push(i(o)+"scale(",null,",",null,")");u.push({i:a-4,x:wt(e,n)},{i:a-2,x:wt(t,r)})}else 1===n&&1===r||o.push(i(o)+"scale("+n+","+r+")")}(o.scaleX,o.scaleY,u.scaleX,u.scaleY,a,l),o=u=null,function(e){for(var t,n=-1,r=l.length;++n<r;)a[(t=l[n]).i]=t.x(e);return a.join("")}}}var un=on((function(e){return"none"===e?nn:(Kt||(Kt=document.createElement("DIV"),Zt=document.documentElement,Jt=document.defaultView),Kt.style.transform=e,e=Jt.getComputedStyle(Zt.appendChild(Kt),null).getPropertyValue("transform"),Zt.removeChild(Kt),e=e.slice(7,-1).split(","),rn(+e[0],+e[1],+e[2],+e[3],+e[4],+e[5]))}),"px, ","px)","deg)"),an=on((function(e){return null==e?nn:(Gt||(Gt=document.createElementNS("http://www.w3.org/2000/svg","g")),Gt.setAttribute("transform",e),(e=Gt.transform.baseVal.consolidate())?(e=e.matrix,rn(e.a,e.b,e.c,e.d,e.e,e.f)):nn)}),", ",")",")");function ln(e,t){var n,r;return function(){var i=Yt(this,e),o=i.tween;if(o!==n)for(var u=0,a=(r=n=o).length;u<a;++u)if(r[u].name===t){(r=r.slice()).splice(u,1);break}i.tween=r}}function cn(e,t,n){var r,i;if("function"!==typeof n)throw new Error;return function(){var o=Yt(this,e),u=o.tween;if(u!==r){i=(r=u).slice();for(var a={name:t,value:n},l=0,c=i.length;l<c;++l)if(i[l].name===t){i[l]=a;break}l===c&&i.push(a)}o.tween=i}}function sn(e,t,n){var r=e._id;return e.each((function(){var e=Yt(this,r);(e.value||(e.value={}))[t]=n.apply(this,arguments)})),function(e){return Xt(e,r).value[t]}}var fn=function(e,t){var n;return("number"===typeof t?wt:t instanceof Ke?ht:(n=Ke(t))?(t=n,ht):Ct)(e,t)};function dn(e){return function(){this.removeAttribute(e)}}function pn(e){return function(){this.removeAttributeNS(e.space,e.local)}}function hn(e,t,n){var r,i,o=n+"";return function(){var u=this.getAttribute(e);return u===o?null:u===r?i:i=t(r=u,n)}}function mn(e,t,n){var r,i,o=n+"";return function(){var u=this.getAttributeNS(e.space,e.local);return u===o?null:u===r?i:i=t(r=u,n)}}function gn(e,t,n){var r,i,o;return function(){var u,a,l=n(this);if(null!=l)return(u=this.getAttribute(e))===(a=l+"")?null:u===r&&a===i?o:(i=a,o=t(r=u,l));this.removeAttribute(e)}}function vn(e,t,n){var r,i,o;return function(){var u,a,l=n(this);if(null!=l)return(u=this.getAttributeNS(e.space,e.local))===(a=l+"")?null:u===r&&a===i?o:(i=a,o=t(r=u,l));this.removeAttributeNS(e.space,e.local)}}function yn(e,t){return function(n){this.setAttribute(e,t.call(this,n))}}function bn(e,t){return function(n){this.setAttributeNS(e.space,e.local,t.call(this,n))}}function wn(e,t){var n,r;function i(){var i=t.apply(this,arguments);return i!==r&&(n=(r=i)&&bn(e,i)),n}return i._value=t,i}function xn(e,t){var n,r;function i(){var i=t.apply(this,arguments);return i!==r&&(n=(r=i)&&yn(e,i)),n}return i._value=t,i}function _n(e,t){return function(){Bt(this,e).delay=+t.apply(this,arguments)}}function kn(e,t){return t=+t,function(){Bt(this,e).delay=t}}function Tn(e,t){return function(){Yt(this,e).duration=+t.apply(this,arguments)}}function En(e,t){return t=+t,function(){Yt(this,e).duration=t}}function Cn(e,t){if("function"!==typeof t)throw new Error;return function(){Yt(this,e).ease=t}}function Sn(e,t,n){var r,i,o=function(e){return(e+"").trim().split(/^|\s+/).every((function(e){var t=e.indexOf(".");return t>=0&&(e=e.slice(0,t)),!e||"start"===e}))}(t)?Bt:Yt;return function(){var u=o(this,e),a=u.on;a!==r&&(i=(r=a).copy()).on(t,n),u.on=i}}var Mn=De.prototype.constructor;function Nn(e){return function(){this.style.removeProperty(e)}}function Pn(e,t,n){return function(r){this.style.setProperty(e,t.call(this,r),n)}}function An(e,t,n){var r,i;function o(){var o=t.apply(this,arguments);return o!==i&&(r=(i=o)&&Pn(e,o,n)),r}return o._value=t,o}function Rn(e){return function(t){this.textContent=e.call(this,t)}}function Dn(e){var t,n;function r(){var r=e.apply(this,arguments);return r!==n&&(t=(n=r)&&Rn(r)),t}return r._value=e,r}var On=0;function zn(e,t,n,r){this._groups=e,this._parents=t,this._name=n,this._id=r}function Ln(){return++On}var Un=De.prototype;zn.prototype=function(e){return De().transition(e)}.prototype={constructor:zn,select:function(e){var t=this._name,n=this._id;"function"!==typeof e&&(e=E(e));for(var r=this._groups,i=r.length,o=new Array(i),u=0;u<i;++u)for(var a,l,c=r[u],s=c.length,f=o[u]=new Array(s),d=0;d<s;++d)(a=c[d])&&(l=e.call(a,a.__data__,d,c))&&("__data__"in a&&(l.__data__=a.__data__),f[d]=l,qt(f[d],t,n,d,f,Xt(a,n)));return new zn(o,this._parents,t,n)},selectAll:function(e){var t=this._name,n=this._id;"function"!==typeof e&&(e=S(e));for(var r=this._groups,i=r.length,o=[],u=[],a=0;a<i;++a)for(var l,c=r[a],s=c.length,f=0;f<s;++f)if(l=c[f]){for(var d,p=e.call(l,l.__data__,f,c),h=Xt(l,n),m=0,g=p.length;m<g;++m)(d=p[m])&&qt(d,t,n,m,p,h);o.push(p),u.push(l)}return new zn(o,u,t,n)},filter:function(e){"function"!==typeof e&&(e=M(e));for(var t=this._groups,n=t.length,r=new Array(n),i=0;i<n;++i)for(var o,u=t[i],a=u.length,l=r[i]=[],c=0;c<a;++c)(o=u[c])&&e.call(o,o.__data__,c,u)&&l.push(o);return new zn(r,this._parents,this._name,this._id)},merge:function(e){if(e._id!==this._id)throw new Error;for(var t=this._groups,n=e._groups,r=t.length,i=n.length,o=Math.min(r,i),u=new Array(r),a=0;a<o;++a)for(var l,c=t[a],s=n[a],f=c.length,d=u[a]=new Array(f),p=0;p<f;++p)(l=c[p]||s[p])&&(d[p]=l);for(;a<r;++a)u[a]=t[a];return new zn(u,this._parents,this._name,this._id)},selection:function(){return new Mn(this._groups,this._parents)},transition:function(){for(var e=this._name,t=this._id,n=Ln(),r=this._groups,i=r.length,o=0;o<i;++o)for(var u,a=r[o],l=a.length,c=0;c<l;++c)if(u=a[c]){var s=Xt(u,t);qt(u,e,n,c,a,{time:s.time+s.delay+s.duration,delay:0,duration:s.duration,ease:s.ease})}return new zn(r,this._parents,e,n)},call:Un.call,nodes:Un.nodes,node:Un.node,size:Un.size,empty:Un.empty,each:Un.each,on:function(e,t){var n=this._id;return arguments.length<2?Xt(this.node(),n).on.on(e):this.each(Sn(n,e,t))},attr:function(e,t){var n=L(e),r="transform"===n?an:fn;return this.attrTween(e,"function"===typeof t?(n.local?vn:gn)(n,r,sn(this,"attr."+e,t)):null==t?(n.local?pn:dn)(n):(n.local?mn:hn)(n,r,t))},attrTween:function(e,t){var n="attr."+e;if(arguments.length<2)return(n=this.tween(n))&&n._value;if(null==t)return this.tween(n,null);if("function"!==typeof t)throw new Error;var r=L(e);return this.tween(n,(r.local?wn:xn)(r,t))},style:function(e,t,n){var r="transform"===(e+="")?un:fn;return null==t?this.styleTween(e,function(e,t){var n,r,i;return function(){var o=B(this,e),u=(this.style.removeProperty(e),B(this,e));return o===u?null:o===n&&u===r?i:i=t(n=o,r=u)}}(e,r)).on("end.style."+e,Nn(e)):"function"===typeof t?this.styleTween(e,function(e,t,n){var r,i,o;return function(){var u=B(this,e),a=n(this),l=a+"";return null==a&&(this.style.removeProperty(e),l=a=B(this,e)),u===l?null:u===r&&l===i?o:(i=l,o=t(r=u,a))}}(e,r,sn(this,"style."+e,t))).each(function(e,t){var n,r,i,o,u="style."+t,a="end."+u;return function(){var l=Yt(this,e),c=l.on,s=null==l.value[u]?o||(o=Nn(t)):void 0;c===n&&i===s||(r=(n=c).copy()).on(a,i=s),l.on=r}}(this._id,e)):this.styleTween(e,function(e,t,n){var r,i,o=n+"";return function(){var u=B(this,e);return u===o?null:u===r?i:i=t(r=u,n)}}(e,r,t),n).on("end.style."+e,null)},styleTween:function(e,t,n){var r="style."+(e+="");if(arguments.length<2)return(r=this.tween(r))&&r._value;if(null==t)return this.tween(r,null);if("function"!==typeof t)throw new Error;return this.tween(r,An(e,t,null==n?"":n))},text:function(e){return this.tween("text","function"===typeof e?function(e){return function(){var t=e(this);this.textContent=null==t?"":t}}(sn(this,"text",e)):function(e){return function(){this.textContent=e}}(null==e?"":e+""))},textTween:function(e){var t="text";if(arguments.length<1)return(t=this.tween(t))&&t._value;if(null==e)return this.tween(t,null);if("function"!==typeof e)throw new Error;return this.tween(t,Dn(e))},remove:function(){return this.on("end.remove",(e=this._id,function(){var t=this.parentNode;for(var n in this.__transition)if(+n!==e)return;t&&t.removeChild(this)}));var e},tween:function(e,t){var n=this._id;if(e+="",arguments.length<2){for(var r,i=Xt(this.node(),n).tween,o=0,u=i.length;o<u;++o)if((r=i[o]).name===e)return r.value;return null}return this.each((null==t?ln:cn)(n,e,t))},delay:function(e){var t=this._id;return arguments.length?this.each(("function"===typeof e?_n:kn)(t,e)):Xt(this.node(),t).delay},duration:function(e){var t=this._id;return arguments.length?this.each(("function"===typeof e?Tn:En)(t,e)):Xt(this.node(),t).duration},ease:function(e){var t=this._id;return arguments.length?this.each(Cn(t,e)):Xt(this.node(),t).ease},end:function(){var e,t,n=this,r=n._id,i=n.size();return new Promise((function(o,u){var a={value:u},l={value:function(){0===--i&&o()}};n.each((function(){var n=Yt(this,r),i=n.on;i!==e&&((t=(e=i).copy())._.cancel.push(a),t._.interrupt.push(a),t._.end.push(l)),n.on=t}))}))}};var Fn={time:null,delay:0,duration:250,ease:function(e){return((e*=2)<=1?e*e*e:(e-=2)*e*e+2)/2}};function In(e,t){for(var n;!(n=e.__transition)||!(n=n[t]);)if(!(e=e.parentNode))return Fn.time=Lt(),Fn;return n}De.prototype.interrupt=function(e){return this.each((function(){en(this,e)}))},De.prototype.transition=function(e){var t,n;e instanceof zn?(t=e._id,e=e._name):(t=Ln(),(n=Fn).time=Lt(),e=null==e?null:e+"");for(var r=this._groups,i=r.length,o=0;o<i;++o)for(var u,a=r[o],l=a.length,c=0;c<l;++c)(u=a[c])&&qt(u,e,t,c,a,n||In(u,t));return new zn(r,this._parents,e,t)};function jn(e){return[+e[0],+e[1]]}function Hn(e){return[jn(e[0]),jn(e[1])]}["w","e"].map(Vn),["n","s"].map(Vn),["n","w","e","s","nw","ne","sw","se"].map(Vn);function Vn(e){return{type:e}}Math.cos,Math.sin,Math.PI,Math.max;Array.prototype.slice;var $n=Math.PI,Wn=2*$n,Qn=Wn-1e-6;function qn(){this._x0=this._y0=this._x1=this._y1=null,this._=""}function Bn(){return new qn}qn.prototype=Bn.prototype={constructor:qn,moveTo:function(e,t){this._+="M"+(this._x0=this._x1=+e)+","+(this._y0=this._y1=+t)},closePath:function(){null!==this._x1&&(this._x1=this._x0,this._y1=this._y0,this._+="Z")},lineTo:function(e,t){this._+="L"+(this._x1=+e)+","+(this._y1=+t)},quadraticCurveTo:function(e,t,n,r){this._+="Q"+ +e+","+ +t+","+(this._x1=+n)+","+(this._y1=+r)},bezierCurveTo:function(e,t,n,r,i,o){this._+="C"+ +e+","+ +t+","+ +n+","+ +r+","+(this._x1=+i)+","+(this._y1=+o)},arcTo:function(e,t,n,r,i){e=+e,t=+t,n=+n,r=+r,i=+i;var o=this._x1,u=this._y1,a=n-e,l=r-t,c=o-e,s=u-t,f=c*c+s*s;if(i<0)throw new Error("negative radius: "+i);if(null===this._x1)this._+="M"+(this._x1=e)+","+(this._y1=t);else if(f>1e-6)if(Math.abs(s*a-l*c)>1e-6&&i){var d=n-o,p=r-u,h=a*a+l*l,m=d*d+p*p,g=Math.sqrt(h),v=Math.sqrt(f),y=i*Math.tan(($n-Math.acos((h+f-m)/(2*g*v)))/2),b=y/v,w=y/g;Math.abs(b-1)>1e-6&&(this._+="L"+(e+b*c)+","+(t+b*s)),this._+="A"+i+","+i+",0,0,"+ +(s*d>c*p)+","+(this._x1=e+w*a)+","+(this._y1=t+w*l)}else this._+="L"+(this._x1=e)+","+(this._y1=t);else;},arc:function(e,t,n,r,i,o){e=+e,t=+t,o=!!o;var u=(n=+n)*Math.cos(r),a=n*Math.sin(r),l=e+u,c=t+a,s=1^o,f=o?r-i:i-r;if(n<0)throw new Error("negative radius: "+n);null===this._x1?this._+="M"+l+","+c:(Math.abs(this._x1-l)>1e-6||Math.abs(this._y1-c)>1e-6)&&(this._+="L"+l+","+c),n&&(f<0&&(f=f%Wn+Wn),f>Qn?this._+="A"+n+","+n+",0,1,"+s+","+(e-u)+","+(t-a)+"A"+n+","+n+",0,1,"+s+","+(this._x1=l)+","+(this._y1=c):f>1e-6&&(this._+="A"+n+","+n+",0,"+ +(f>=$n)+","+s+","+(this._x1=e+n*Math.cos(i))+","+(this._y1=t+n*Math.sin(i))))},rect:function(e,t,n,r){this._+="M"+(this._x0=this._x1=+e)+","+(this._y0=this._y1=+t)+"h"+ +n+"v"+ +r+"h"+-n+"Z"},toString:function(){return this._}};function Yn(){}function Xn(e,t){var n=new Yn;if(e instanceof Yn)e.each((function(e,t){n.set(t,e)}));else if(Array.isArray(e)){var r,i=-1,o=e.length;if(null==t)for(;++i<o;)n.set(i,e[i]);else for(;++i<o;)n.set(t(r=e[i],i,e),r)}else if(e)for(var u in e)n.set(u,e[u]);return n}Yn.prototype=Xn.prototype={constructor:Yn,has:function(e){return"$"+e in this},get:function(e){return this["$"+e]},set:function(e,t){return this["$"+e]=t,this},remove:function(e){var t="$"+e;return t in this&&delete this[t]},clear:function(){for(var e in this)"$"===e[0]&&delete this[e]},keys:function(){var e=[];for(var t in this)"$"===t[0]&&e.push(t.slice(1));return e},values:function(){var e=[];for(var t in this)"$"===t[0]&&e.push(this[t]);return e},entries:function(){var e=[];for(var t in this)"$"===t[0]&&e.push({key:t.slice(1),value:this[t]});return e},size:function(){var e=0;for(var t in this)"$"===t[0]&&++e;return e},empty:function(){for(var e in this)if("$"===e[0])return!1;return!0},each:function(e){for(var t in this)"$"===t[0]&&e(this[t],t.slice(1),this)}};var Kn=Xn,Zn=function(){var e,t,n,r=[],i=[];function o(n,i,u,a){if(i>=r.length)return null!=e&&n.sort(e),null!=t?t(n):n;for(var l,c,s,f=-1,d=n.length,p=r[i++],h=Kn(),m=u();++f<d;)(s=h.get(l=p(c=n[f])+""))?s.push(c):h.set(l,[c]);return h.each((function(e,t){a(m,t,o(e,i,u,a))})),m}return n={object:function(e){return o(e,0,Jn,Gn)},map:function(e){return o(e,0,er,tr)},entries:function(e){return function e(n,o){if(++o>r.length)return n;var u,a=i[o-1];return null!=t&&o>=r.length?u=n.entries():(u=[],n.each((function(t,n){u.push({key:n,values:e(t,o)})}))),null!=a?u.sort((function(e,t){return a(e.key,t.key)})):u}(o(e,0,er,tr),0)},key:function(e){return r.push(e),n},sortKeys:function(e){return i[r.length-1]=e,n},sortValues:function(t){return e=t,n},rollup:function(e){return t=e,n}}};function Jn(){return{}}function Gn(e,t,n){e[t]=n}function er(){return Kn()}function tr(e,t,n){e.set(t,n)}function nr(){}var rr=Kn.prototype;function ir(e,t){var n=new nr;if(e instanceof nr)e.each((function(e){n.add(e)}));else if(e){var r=-1,i=e.length;if(null==t)for(;++r<i;)n.add(e[r]);else for(;++r<i;)n.add(t(e[r],r,e))}return n}nr.prototype=ir.prototype={constructor:nr,has:rr.has,add:function(e){return this["$"+(e+="")]=e,this},remove:rr.remove,clear:rr.clear,values:rr.keys,size:rr.size,empty:rr.empty,each:rr.each};Array.prototype.slice;var or={},ur={};function ar(e){return new Function("d","return {"+e.map((function(e,t){return JSON.stringify(e)+": d["+t+'] || ""'})).join(",")+"}")}function lr(e){var t=Object.create(null),n=[];return e.forEach((function(e){for(var r in e)r in t||n.push(t[r]=r)})),n}function cr(e,t){var n=e+"",r=n.length;return r<t?new Array(t-r+1).join(0)+n:n}function sr(e){var t,n=e.getUTCHours(),r=e.getUTCMinutes(),i=e.getUTCSeconds(),o=e.getUTCMilliseconds();return isNaN(e)?"Invalid Date":((t=e.getUTCFullYear())<0?"-"+cr(-t,6):t>9999?"+"+cr(t,6):cr(t,4))+"-"+cr(e.getUTCMonth()+1,2)+"-"+cr(e.getUTCDate(),2)+(o?"T"+cr(n,2)+":"+cr(r,2)+":"+cr(i,2)+"."+cr(o,3)+"Z":i?"T"+cr(n,2)+":"+cr(r,2)+":"+cr(i,2)+"Z":r||n?"T"+cr(n,2)+":"+cr(r,2)+"Z":"")}var fr=function(e){var t=new RegExp('["'+e+"\n\r]"),n=e.charCodeAt(0);function r(e,t){var r,i=[],o=e.length,u=0,a=0,l=o<=0,c=!1;function s(){if(l)return ur;if(c)return c=!1,or;var t,r,i=u;if(34===e.charCodeAt(i)){for(;u++<o&&34!==e.charCodeAt(u)||34===e.charCodeAt(++u););return(t=u)>=o?l=!0:10===(r=e.charCodeAt(u++))?c=!0:13===r&&(c=!0,10===e.charCodeAt(u)&&++u),e.slice(i+1,t-1).replace(/""/g,'"')}for(;u<o;){if(10===(r=e.charCodeAt(t=u++)))c=!0;else if(13===r)c=!0,10===e.charCodeAt(u)&&++u;else if(r!==n)continue;return e.slice(i,t)}return l=!0,e.slice(i,o)}for(10===e.charCodeAt(o-1)&&--o,13===e.charCodeAt(o-1)&&--o;(r=s())!==ur;){for(var f=[];r!==or&&r!==ur;)f.push(r),r=s();t&&null==(f=t(f,a++))||i.push(f)}return i}function i(t,n){return t.map((function(t){return n.map((function(e){return u(t[e])})).join(e)}))}function o(t){return t.map(u).join(e)}function u(e){return null==e?"":e instanceof Date?sr(e):t.test(e+="")?'"'+e.replace(/"/g,'""')+'"':e}return{parse:function(e,t){var n,i,o=r(e,(function(e,r){if(n)return n(e,r-1);i=e,n=t?function(e,t){var n=ar(e);return function(r,i){return t(n(r),i,e)}}(e,t):ar(e)}));return o.columns=i||[],o},parseRows:r,format:function(t,n){return null==n&&(n=lr(t)),[n.map(u).join(e)].concat(i(t,n)).join("\n")},formatBody:function(e,t){return null==t&&(t=lr(e)),i(e,t).join("\n")},formatRows:function(e){return e.map(o).join("\n")},formatRow:o,formatValue:u}},dr=fr(","),pr=dr.parse,hr=(dr.parseRows,dr.format,dr.formatBody,dr.formatRows,dr.formatRow,dr.formatValue,fr("\t")),mr=hr.parse;hr.parseRows,hr.format,hr.formatBody,hr.formatRows,hr.formatRow,hr.formatValue;function gr(e){if(!e.ok)throw new Error(e.status+" "+e.statusText);return e.text()}var vr=function(e,t){return fetch(e,t).then(gr)};function yr(e){return function(t,n,r){return 2===arguments.length&&"function"===typeof n&&(r=n,n=void 0),vr(t,n).then((function(t){return e(t,r)}))}}var br=yr(pr);yr(mr);function wr(e){return function(t,n){return vr(t,n).then((function(t){return(new DOMParser).parseFromString(t,e)}))}}wr("application/xml"),wr("text/html"),wr("image/svg+xml");function xr(e,t,n,r){if(isNaN(t)||isNaN(n))return e;var i,o,u,a,l,c,s,f,d,p=e._root,h={data:r},m=e._x0,g=e._y0,v=e._x1,y=e._y1;if(!p)return e._root=h,e;for(;p.length;)if((c=t>=(o=(m+v)/2))?m=o:v=o,(s=n>=(u=(g+y)/2))?g=u:y=u,i=p,!(p=p[f=s<<1|c]))return i[f]=h,e;if(a=+e._x.call(null,p.data),l=+e._y.call(null,p.data),t===a&&n===l)return h.next=p,i?i[f]=h:e._root=h,e;do{i=i?i[f]=new Array(4):e._root=new Array(4),(c=t>=(o=(m+v)/2))?m=o:v=o,(s=n>=(u=(g+y)/2))?g=u:y=u}while((f=s<<1|c)===(d=(l>=u)<<1|a>=o));return i[d]=p,i[f]=h,e}var _r=function(e,t,n,r,i){this.node=e,this.x0=t,this.y0=n,this.x1=r,this.y1=i};function kr(e){return e[0]}function Tr(e){return e[1]}function Er(e,t,n){var r=new Cr(null==t?kr:t,null==n?Tr:n,NaN,NaN,NaN,NaN);return null==e?r:r.addAll(e)}function Cr(e,t,n,r,i,o){this._x=e,this._y=t,this._x0=n,this._y0=r,this._x1=i,this._y1=o,this._root=void 0}function Sr(e){for(var t={data:e.data},n=t;e=e.next;)n=n.next={data:e.data};return t}var Mr=Er.prototype=Cr.prototype;Mr.copy=function(){var e,t,n=new Cr(this._x,this._y,this._x0,this._y0,this._x1,this._y1),r=this._root;if(!r)return n;if(!r.length)return n._root=Sr(r),n;for(e=[{source:r,target:n._root=new Array(4)}];r=e.pop();)for(var i=0;i<4;++i)(t=r.source[i])&&(t.length?e.push({source:t,target:r.target[i]=new Array(4)}):r.target[i]=Sr(t));return n},Mr.add=function(e){var t=+this._x.call(null,e),n=+this._y.call(null,e);return xr(this.cover(t,n),t,n,e)},Mr.addAll=function(e){var t,n,r,i,o=e.length,u=new Array(o),a=new Array(o),l=1/0,c=1/0,s=-1/0,f=-1/0;for(n=0;n<o;++n)isNaN(r=+this._x.call(null,t=e[n]))||isNaN(i=+this._y.call(null,t))||(u[n]=r,a[n]=i,r<l&&(l=r),r>s&&(s=r),i<c&&(c=i),i>f&&(f=i));if(l>s||c>f)return this;for(this.cover(l,c).cover(s,f),n=0;n<o;++n)xr(this,u[n],a[n],e[n]);return this},Mr.cover=function(e,t){if(isNaN(e=+e)||isNaN(t=+t))return this;var n=this._x0,r=this._y0,i=this._x1,o=this._y1;if(isNaN(n))i=(n=Math.floor(e))+1,o=(r=Math.floor(t))+1;else{for(var u,a,l=i-n,c=this._root;n>e||e>=i||r>t||t>=o;)switch(a=(t<r)<<1|e<n,(u=new Array(4))[a]=c,c=u,l*=2,a){case 0:i=n+l,o=r+l;break;case 1:n=i-l,o=r+l;break;case 2:i=n+l,r=o-l;break;case 3:n=i-l,r=o-l}this._root&&this._root.length&&(this._root=c)}return this._x0=n,this._y0=r,this._x1=i,this._y1=o,this},Mr.data=function(){var e=[];return this.visit((function(t){if(!t.length)do{e.push(t.data)}while(t=t.next)})),e},Mr.extent=function(e){return arguments.length?this.cover(+e[0][0],+e[0][1]).cover(+e[1][0],+e[1][1]):isNaN(this._x0)?void 0:[[this._x0,this._y0],[this._x1,this._y1]]},Mr.find=function(e,t,n){var r,i,o,u,a,l,c,s=this._x0,f=this._y0,d=this._x1,p=this._y1,h=[],m=this._root;for(m&&h.push(new _r(m,s,f,d,p)),null==n?n=1/0:(s=e-n,f=t-n,d=e+n,p=t+n,n*=n);l=h.pop();)if(!(!(m=l.node)||(i=l.x0)>d||(o=l.y0)>p||(u=l.x1)<s||(a=l.y1)<f))if(m.length){var g=(i+u)/2,v=(o+a)/2;h.push(new _r(m[3],g,v,u,a),new _r(m[2],i,v,g,a),new _r(m[1],g,o,u,v),new _r(m[0],i,o,g,v)),(c=(t>=v)<<1|e>=g)&&(l=h[h.length-1],h[h.length-1]=h[h.length-1-c],h[h.length-1-c]=l)}else{var y=e-+this._x.call(null,m.data),b=t-+this._y.call(null,m.data),w=y*y+b*b;if(w<n){var x=Math.sqrt(n=w);s=e-x,f=t-x,d=e+x,p=t+x,r=m.data}}return r},Mr.remove=function(e){if(isNaN(o=+this._x.call(null,e))||isNaN(u=+this._y.call(null,e)))return this;var t,n,r,i,o,u,a,l,c,s,f,d,p=this._root,h=this._x0,m=this._y0,g=this._x1,v=this._y1;if(!p)return this;if(p.length)for(;;){if((c=o>=(a=(h+g)/2))?h=a:g=a,(s=u>=(l=(m+v)/2))?m=l:v=l,t=p,!(p=p[f=s<<1|c]))return this;if(!p.length)break;(t[f+1&3]||t[f+2&3]||t[f+3&3])&&(n=t,d=f)}for(;p.data!==e;)if(r=p,!(p=p.next))return this;return(i=p.next)&&delete p.next,r?(i?r.next=i:delete r.next,this):t?(i?t[f]=i:delete t[f],(p=t[0]||t[1]||t[2]||t[3])&&p===(t[3]||t[2]||t[1]||t[0])&&!p.length&&(n?n[d]=p:this._root=p),this):(this._root=i,this)},Mr.removeAll=function(e){for(var t=0,n=e.length;t<n;++t)this.remove(e[t]);return this},Mr.root=function(){return this._root},Mr.size=function(){var e=0;return this.visit((function(t){if(!t.length)do{++e}while(t=t.next)})),e},Mr.visit=function(e){var t,n,r,i,o,u,a=[],l=this._root;for(l&&a.push(new _r(l,this._x0,this._y0,this._x1,this._y1));t=a.pop();)if(!e(l=t.node,r=t.x0,i=t.y0,o=t.x1,u=t.y1)&&l.length){var c=(r+o)/2,s=(i+u)/2;(n=l[3])&&a.push(new _r(n,c,s,o,u)),(n=l[2])&&a.push(new _r(n,r,s,c,u)),(n=l[1])&&a.push(new _r(n,c,i,o,s)),(n=l[0])&&a.push(new _r(n,r,i,c,s))}return this},Mr.visitAfter=function(e){var t,n=[],r=[];for(this._root&&n.push(new _r(this._root,this._x0,this._y0,this._x1,this._y1));t=n.pop();){var i=t.node;if(i.length){var o,u=t.x0,a=t.y0,l=t.x1,c=t.y1,s=(u+l)/2,f=(a+c)/2;(o=i[0])&&n.push(new _r(o,u,a,s,f)),(o=i[1])&&n.push(new _r(o,s,a,l,f)),(o=i[2])&&n.push(new _r(o,u,f,s,c)),(o=i[3])&&n.push(new _r(o,s,f,l,c))}r.push(t)}for(;t=r.pop();)e(t.node,t.x0,t.y0,t.x1,t.y1);return this},Mr.x=function(e){return arguments.length?(this._x=e,this):this._x},Mr.y=function(e){return arguments.length?(this._y=e,this):this._y};Math.PI,Math.sqrt(5);var Nr=function(){return Math.random()},Pr=(function e(t){function n(e,n){return e=null==e?0:+e,n=null==n?1:+n,1===arguments.length?(n=e,e=0):n-=e,function(){return t()*n+e}}return n.source=e,n}(Nr),function e(t){function n(e,n){var r,i;return e=null==e?0:+e,n=null==n?1:+n,function(){var o;if(null!=r)o=r,r=null;else do{r=2*t()-1,o=2*t()-1,i=r*r+o*o}while(!i||i>1);return e+n*o*Math.sqrt(-2*Math.log(i)/i)}}return n.source=e,n}(Nr)),Ar=(function e(t){function n(){var e=Pr.source(t).apply(this,arguments);return function(){return Math.exp(e())}}return n.source=e,n}(Nr),function e(t){function n(e){return function(){for(var n=0,r=0;r<e;++r)n+=t();return n}}return n.source=e,n}(Nr));(function e(t){function n(e){var n=Ar.source(t)(e);return function(){return n()/e}}return n.source=e,n})(Nr),function e(t){function n(e){return function(){return-Math.log(1-t())/e}}return n.source=e,n}(Nr);function Rr(e,t){switch(arguments.length){case 0:break;case 1:this.range(e);break;default:this.range(t).domain(e)}return this}var Dr=Array.prototype,Or=Dr.map,zr=Dr.slice;var Lr=function(e,t){return e=+e,t=+t,function(n){return Math.round(e*(1-n)+t*n)}},Ur=function(e){return+e},Fr=[0,1];function Ir(e){return e}function jr(e,t){return(t-=e=+e)?function(n){return(n-e)/t}:(n=isNaN(t)?NaN:.5,function(){return n});var n}function Hr(e){var t,n=e[0],r=e[e.length-1];return n>r&&(t=n,n=r,r=t),function(e){return Math.max(n,Math.min(r,e))}}function Vr(e,t,n){var r=e[0],i=e[1],o=t[0],u=t[1];return i<r?(r=jr(i,r),o=n(u,o)):(r=jr(r,i),o=n(o,u)),function(e){return o(r(e))}}function $r(e,t,n){var r=Math.min(e.length,t.length)-1,i=new Array(r),o=new Array(r),u=-1;for(e[r]<e[0]&&(e=e.slice().reverse(),t=t.slice().reverse());++u<r;)i[u]=jr(e[u],e[u+1]),o[u]=n(t[u],t[u+1]);return function(t){var n=a(e,t,1,r)-1;return o[n](i[n](t))}}function Wr(e,t){return t.domain(e.domain()).range(e.range()).interpolate(e.interpolate()).clamp(e.clamp()).unknown(e.unknown())}function Qr(){var e,t,n,r,i,o,u=Fr,a=Fr,l=St,c=Ir;function s(){return r=Math.min(u.length,a.length)>2?$r:Vr,i=o=null,f}function f(t){return isNaN(t=+t)?n:(i||(i=r(u.map(e),a,l)))(e(c(t)))}return f.invert=function(n){return c(t((o||(o=r(a,u.map(e),wt)))(n)))},f.domain=function(e){return arguments.length?(u=Or.call(e,Ur),c===Ir||(c=Hr(u)),s()):u.slice()},f.range=function(e){return arguments.length?(a=zr.call(e),s()):a.slice()},f.rangeRound=function(e){return a=zr.call(e),l=Lr,s()},f.clamp=function(e){return arguments.length?(c=e?Hr(u):Ir,f):c!==Ir},f.interpolate=function(e){return arguments.length?(l=e,s()):l},f.unknown=function(e){return arguments.length?(n=e,f):n},function(n,r){return e=n,t=r,s()}}var qr=/^(?:(.)?([<>=^]))?([+\-( ])?([$#])?(0)?(\d+)?(,)?(\.\d+)?(~)?([a-z%])?$/i;function Br(e){if(!(t=qr.exec(e)))throw new Error("invalid format: "+e);var t;return new Yr({fill:t[1],align:t[2],sign:t[3],symbol:t[4],zero:t[5],width:t[6],comma:t[7],precision:t[8]&&t[8].slice(1),trim:t[9],type:t[10]})}function Yr(e){this.fill=void 0===e.fill?" ":e.fill+"",this.align=void 0===e.align?">":e.align+"",this.sign=void 0===e.sign?"-":e.sign+"",this.symbol=void 0===e.symbol?"":e.symbol+"",this.zero=!!e.zero,this.width=void 0===e.width?void 0:+e.width,this.comma=!!e.comma,this.precision=void 0===e.precision?void 0:+e.precision,this.trim=!!e.trim,this.type=void 0===e.type?"":e.type+""}Br.prototype=Yr.prototype,Yr.prototype.toString=function(){return this.fill+this.align+this.sign+this.symbol+(this.zero?"0":"")+(void 0===this.width?"":Math.max(1,0|this.width))+(this.comma?",":"")+(void 0===this.precision?"":"."+Math.max(0,0|this.precision))+(this.trim?"~":"")+this.type};var Xr,Kr,Zr,Jr,Gr=function(e,t){if((n=(e=t?e.toExponential(t-1):e.toExponential()).indexOf("e"))<0)return null;var n,r=e.slice(0,n);return[r.length>1?r[0]+r.slice(2):r,+e.slice(n+1)]},ei=function(e){return(e=Gr(Math.abs(e)))?e[1]:NaN},ti=function(e,t){var n=Gr(e,t);if(!n)return e+"";var r=n[0],i=n[1];return i<0?"0."+new Array(-i).join("0")+r:r.length>i+1?r.slice(0,i+1)+"."+r.slice(i+1):r+new Array(i-r.length+2).join("0")},ni={"%":function(e,t){return(100*e).toFixed(t)},b:function(e){return Math.round(e).toString(2)},c:function(e){return e+""},d:function(e){return Math.round(e).toString(10)},e:function(e,t){return e.toExponential(t)},f:function(e,t){return e.toFixed(t)},g:function(e,t){return e.toPrecision(t)},o:function(e){return Math.round(e).toString(8)},p:function(e,t){return ti(100*e,t)},r:ti,s:function(e,t){var n=Gr(e,t);if(!n)return e+"";var r=n[0],i=n[1],o=i-(Xr=3*Math.max(-8,Math.min(8,Math.floor(i/3))))+1,u=r.length;return o===u?r:o>u?r+new Array(o-u+1).join("0"):o>0?r.slice(0,o)+"."+r.slice(o):"0."+new Array(1-o).join("0")+Gr(e,Math.max(0,t+o-1))[0]},X:function(e){return Math.round(e).toString(16).toUpperCase()},x:function(e){return Math.round(e).toString(16)}},ri=function(e){return e},ii=Array.prototype.map,oi=["y","z","a","f","p","n","\xb5","m","","k","M","G","T","P","E","Z","Y"];Kr=function(e){var t,n,r=void 0===e.grouping||void 0===e.thousands?ri:(t=ii.call(e.grouping,Number),n=e.thousands+"",function(e,r){for(var i=e.length,o=[],u=0,a=t[0],l=0;i>0&&a>0&&(l+a+1>r&&(a=Math.max(1,r-l)),o.push(e.substring(i-=a,i+a)),!((l+=a+1)>r));)a=t[u=(u+1)%t.length];return o.reverse().join(n)}),i=void 0===e.currency?"":e.currency[0]+"",o=void 0===e.currency?"":e.currency[1]+"",u=void 0===e.decimal?".":e.decimal+"",a=void 0===e.numerals?ri:function(e){return function(t){return t.replace(/[0-9]/g,(function(t){return e[+t]}))}}(ii.call(e.numerals,String)),l=void 0===e.percent?"%":e.percent+"",c=void 0===e.minus?"-":e.minus+"",s=void 0===e.nan?"NaN":e.nan+"";function f(e){var t=(e=Br(e)).fill,n=e.align,f=e.sign,d=e.symbol,p=e.zero,h=e.width,m=e.comma,g=e.precision,v=e.trim,y=e.type;"n"===y?(m=!0,y="g"):ni[y]||(void 0===g&&(g=12),v=!0,y="g"),(p||"0"===t&&"="===n)&&(p=!0,t="0",n="=");var b="$"===d?i:"#"===d&&/[boxX]/.test(y)?"0"+y.toLowerCase():"",w="$"===d?o:/[%p]/.test(y)?l:"",x=ni[y],_=/[defgprs%]/.test(y);function k(e){var i,o,l,d=b,k=w;if("c"===y)k=x(e)+k,e="";else{var T=(e=+e)<0||1/e<0;if(e=isNaN(e)?s:x(Math.abs(e),g),v&&(e=function(e){e:for(var t,n=e.length,r=1,i=-1;r<n;++r)switch(e[r]){case".":i=t=r;break;case"0":0===i&&(i=r),t=r;break;default:if(!+e[r])break e;i>0&&(i=0)}return i>0?e.slice(0,i)+e.slice(t+1):e}(e)),T&&0===+e&&"+"!==f&&(T=!1),d=(T?"("===f?f:c:"-"===f||"("===f?"":f)+d,k=("s"===y?oi[8+Xr/3]:"")+k+(T&&"("===f?")":""),_)for(i=-1,o=e.length;++i<o;)if(48>(l=e.charCodeAt(i))||l>57){k=(46===l?u+e.slice(i+1):e.slice(i))+k,e=e.slice(0,i);break}}m&&!p&&(e=r(e,1/0));var E=d.length+e.length+k.length,C=E<h?new Array(h-E+1).join(t):"";switch(m&&p&&(e=r(C+e,C.length?h-k.length:1/0),C=""),n){case"<":e=d+e+k+C;break;case"=":e=d+C+e+k;break;case"^":e=C.slice(0,E=C.length>>1)+d+e+k+C.slice(E);break;default:e=C+d+e+k}return a(e)}return g=void 0===g?6:/[gprs]/.test(y)?Math.max(1,Math.min(21,g)):Math.max(0,Math.min(20,g)),k.toString=function(){return e+""},k}return{format:f,formatPrefix:function(e,t){var n=f(((e=Br(e)).type="f",e)),r=3*Math.max(-8,Math.min(8,Math.floor(ei(t)/3))),i=Math.pow(10,-r),o=oi[8+r/3];return function(e){return n(i*e)+o}}}}({decimal:".",thousands:",",grouping:[3],currency:["$",""],minus:"-"}),Zr=Kr.format,Jr=Kr.formatPrefix;var ui=function(e,t,n,r){var i,o=h(e,t,n);switch((r=Br(null==r?",f":r)).type){case"s":var u=Math.max(Math.abs(e),Math.abs(t));return null!=r.precision||isNaN(i=function(e,t){return Math.max(0,3*Math.max(-8,Math.min(8,Math.floor(ei(t)/3)))-ei(Math.abs(e)))}(o,u))||(r.precision=i),Jr(r,u);case"":case"e":case"g":case"p":case"r":null!=r.precision||isNaN(i=function(e,t){return e=Math.abs(e),t=Math.abs(t)-e,Math.max(0,ei(t)-ei(e))+1}(o,Math.max(Math.abs(e),Math.abs(t))))||(r.precision=i-("e"===r.type));break;case"f":case"%":null!=r.precision||isNaN(i=function(e){return Math.max(0,-ei(Math.abs(e)))}(o))||(r.precision=i-2*("%"===r.type))}return Zr(r)};function ai(e){var t=e.domain;return e.ticks=function(e){var n=t();return d(n[0],n[n.length-1],null==e?10:e)},e.tickFormat=function(e,n){var r=t();return ui(r[0],r[r.length-1],null==e?10:e,n)},e.nice=function(n){null==n&&(n=10);var r,i=t(),o=0,u=i.length-1,a=i[o],l=i[u];return l<a&&(r=a,a=l,l=r,r=o,o=u,u=r),(r=p(a,l,n))>0?r=p(a=Math.floor(a/r)*r,l=Math.ceil(l/r)*r,n):r<0&&(r=p(a=Math.ceil(a*r)/r,l=Math.floor(l*r)/r,n)),r>0?(i[o]=Math.floor(a/r)*r,i[u]=Math.ceil(l/r)*r,t(i)):r<0&&(i[o]=Math.ceil(a*r)/r,i[u]=Math.floor(l*r)/r,t(i)),e},e}function li(e){return function(t){return t<0?-Math.pow(-t,e):Math.pow(t,e)}}function ci(e){return e<0?-Math.sqrt(-e):Math.sqrt(e)}function si(e){return e<0?-e*e:e*e}function fi(e){var t=e(Ir,Ir),n=1;function r(){return 1===n?e(Ir,Ir):.5===n?e(ci,si):e(li(n),li(1/n))}return t.exponent=function(e){return arguments.length?(n=+e,r()):n},ai(t)}function di(){var e=fi(Qr());return e.copy=function(){return Wr(e,di()).exponent(e.exponent())},Rr.apply(e,arguments),e}function pi(){return di.apply(null,arguments).exponent(.5)}var hi=new Date,mi=new Date;function gi(e,t,n,r){function i(t){return e(t=0===arguments.length?new Date:new Date(+t)),t}return i.floor=function(t){return e(t=new Date(+t)),t},i.ceil=function(n){return e(n=new Date(n-1)),t(n,1),e(n),n},i.round=function(e){var t=i(e),n=i.ceil(e);return e-t<n-e?t:n},i.offset=function(e,n){return t(e=new Date(+e),null==n?1:Math.floor(n)),e},i.range=function(n,r,o){var u,a=[];if(n=i.ceil(n),o=null==o?1:Math.floor(o),!(n<r)||!(o>0))return a;do{a.push(u=new Date(+n)),t(n,o),e(n)}while(u<n&&n<r);return a},i.filter=function(n){return gi((function(t){if(t>=t)for(;e(t),!n(t);)t.setTime(t-1)}),(function(e,r){if(e>=e)if(r<0)for(;++r<=0;)for(;t(e,-1),!n(e););else for(;--r>=0;)for(;t(e,1),!n(e););}))},n&&(i.count=function(t,r){return hi.setTime(+t),mi.setTime(+r),e(hi),e(mi),Math.floor(n(hi,mi))},i.every=function(e){return e=Math.floor(e),isFinite(e)&&e>0?e>1?i.filter(r?function(t){return r(t)%e===0}:function(t){return i.count(0,t)%e===0}):i:null}),i}var vi=gi((function(e){e.setMonth(0,1),e.setHours(0,0,0,0)}),(function(e,t){e.setFullYear(e.getFullYear()+t)}),(function(e,t){return t.getFullYear()-e.getFullYear()}),(function(e){return e.getFullYear()}));vi.every=function(e){return isFinite(e=Math.floor(e))&&e>0?gi((function(t){t.setFullYear(Math.floor(t.getFullYear()/e)*e),t.setMonth(0,1),t.setHours(0,0,0,0)}),(function(t,n){t.setFullYear(t.getFullYear()+n*e)})):null};var yi=vi,bi=(vi.range,gi((function(e){e.setDate(1),e.setHours(0,0,0,0)}),(function(e,t){e.setMonth(e.getMonth()+t)}),(function(e,t){return t.getMonth()-e.getMonth()+12*(t.getFullYear()-e.getFullYear())}),(function(e){return e.getMonth()})));bi.range;function wi(e){return gi((function(t){t.setDate(t.getDate()-(t.getDay()+7-e)%7),t.setHours(0,0,0,0)}),(function(e,t){e.setDate(e.getDate()+7*t)}),(function(e,t){return(t-e-6e4*(t.getTimezoneOffset()-e.getTimezoneOffset()))/6048e5}))}var xi=wi(0),_i=wi(1),ki=wi(2),Ti=wi(3),Ei=wi(4),Ci=wi(5),Si=wi(6),Mi=(xi.range,_i.range,ki.range,Ti.range,Ei.range,Ci.range,Si.range,gi((function(e){e.setHours(0,0,0,0)}),(function(e,t){e.setDate(e.getDate()+t)}),(function(e,t){return(t-e-6e4*(t.getTimezoneOffset()-e.getTimezoneOffset()))/864e5}),(function(e){return e.getDate()-1}))),Ni=Mi,Pi=(Mi.range,gi((function(e){e.setTime(e-e.getMilliseconds()-1e3*e.getSeconds()-6e4*e.getMinutes())}),(function(e,t){e.setTime(+e+36e5*t)}),(function(e,t){return(t-e)/36e5}),(function(e){return e.getHours()}))),Ai=(Pi.range,gi((function(e){e.setTime(e-e.getMilliseconds()-1e3*e.getSeconds())}),(function(e,t){e.setTime(+e+6e4*t)}),(function(e,t){return(t-e)/6e4}),(function(e){return e.getMinutes()}))),Ri=(Ai.range,gi((function(e){e.setTime(e-e.getMilliseconds())}),(function(e,t){e.setTime(+e+1e3*t)}),(function(e,t){return(t-e)/1e3}),(function(e){return e.getUTCSeconds()}))),Di=(Ri.range,gi((function(){}),(function(e,t){e.setTime(+e+t)}),(function(e,t){return t-e})));Di.every=function(e){return e=Math.floor(e),isFinite(e)&&e>0?e>1?gi((function(t){t.setTime(Math.floor(t/e)*e)}),(function(t,n){t.setTime(+t+n*e)}),(function(t,n){return(n-t)/e})):Di:null};Di.range;function Oi(e){return gi((function(t){t.setUTCDate(t.getUTCDate()-(t.getUTCDay()+7-e)%7),t.setUTCHours(0,0,0,0)}),(function(e,t){e.setUTCDate(e.getUTCDate()+7*t)}),(function(e,t){return(t-e)/6048e5}))}var zi=Oi(0),Li=Oi(1),Ui=Oi(2),Fi=Oi(3),Ii=Oi(4),ji=Oi(5),Hi=Oi(6),Vi=(zi.range,Li.range,Ui.range,Fi.range,Ii.range,ji.range,Hi.range,gi((function(e){e.setUTCHours(0,0,0,0)}),(function(e,t){e.setUTCDate(e.getUTCDate()+t)}),(function(e,t){return(t-e)/864e5}),(function(e){return e.getUTCDate()-1}))),$i=Vi,Wi=(Vi.range,gi((function(e){e.setUTCMonth(0,1),e.setUTCHours(0,0,0,0)}),(function(e,t){e.setUTCFullYear(e.getUTCFullYear()+t)}),(function(e,t){return t.getUTCFullYear()-e.getUTCFullYear()}),(function(e){return e.getUTCFullYear()})));Wi.every=function(e){return isFinite(e=Math.floor(e))&&e>0?gi((function(t){t.setUTCFullYear(Math.floor(t.getUTCFullYear()/e)*e),t.setUTCMonth(0,1),t.setUTCHours(0,0,0,0)}),(function(t,n){t.setUTCFullYear(t.getUTCFullYear()+n*e)})):null};var Qi=Wi;Wi.range;function qi(e){if(0<=e.y&&e.y<100){var t=new Date(-1,e.m,e.d,e.H,e.M,e.S,e.L);return t.setFullYear(e.y),t}return new Date(e.y,e.m,e.d,e.H,e.M,e.S,e.L)}function Bi(e){if(0<=e.y&&e.y<100){var t=new Date(Date.UTC(-1,e.m,e.d,e.H,e.M,e.S,e.L));return t.setUTCFullYear(e.y),t}return new Date(Date.UTC(e.y,e.m,e.d,e.H,e.M,e.S,e.L))}function Yi(e,t,n){return{y:e,m:t,d:n,H:0,M:0,S:0,L:0}}var Xi,Ki={"-":"",_:" ",0:"0"},Zi=/^\s*\d+/,Ji=/^%/,Gi=/[\\^$*+?|[\]().{}]/g;function eo(e,t,n){var r=e<0?"-":"",i=(r?-e:e)+"",o=i.length;return r+(o<n?new Array(n-o+1).join(t)+i:i)}function to(e){return e.replace(Gi,"\\$&")}function no(e){return new RegExp("^(?:"+e.map(to).join("|")+")","i")}function ro(e){for(var t={},n=-1,r=e.length;++n<r;)t[e[n].toLowerCase()]=n;return t}function io(e,t,n){var r=Zi.exec(t.slice(n,n+1));return r?(e.w=+r[0],n+r[0].length):-1}function oo(e,t,n){var r=Zi.exec(t.slice(n,n+1));return r?(e.u=+r[0],n+r[0].length):-1}function uo(e,t,n){var r=Zi.exec(t.slice(n,n+2));return r?(e.U=+r[0],n+r[0].length):-1}function ao(e,t,n){var r=Zi.exec(t.slice(n,n+2));return r?(e.V=+r[0],n+r[0].length):-1}function lo(e,t,n){var r=Zi.exec(t.slice(n,n+2));return r?(e.W=+r[0],n+r[0].length):-1}function co(e,t,n){var r=Zi.exec(t.slice(n,n+4));return r?(e.y=+r[0],n+r[0].length):-1}function so(e,t,n){var r=Zi.exec(t.slice(n,n+2));return r?(e.y=+r[0]+(+r[0]>68?1900:2e3),n+r[0].length):-1}function fo(e,t,n){var r=/^(Z)|([+-]\d\d)(?::?(\d\d))?/.exec(t.slice(n,n+6));return r?(e.Z=r[1]?0:-(r[2]+(r[3]||"00")),n+r[0].length):-1}function po(e,t,n){var r=Zi.exec(t.slice(n,n+1));return r?(e.q=3*r[0]-3,n+r[0].length):-1}function ho(e,t,n){var r=Zi.exec(t.slice(n,n+2));return r?(e.m=r[0]-1,n+r[0].length):-1}function mo(e,t,n){var r=Zi.exec(t.slice(n,n+2));return r?(e.d=+r[0],n+r[0].length):-1}function go(e,t,n){var r=Zi.exec(t.slice(n,n+3));return r?(e.m=0,e.d=+r[0],n+r[0].length):-1}function vo(e,t,n){var r=Zi.exec(t.slice(n,n+2));return r?(e.H=+r[0],n+r[0].length):-1}function yo(e,t,n){var r=Zi.exec(t.slice(n,n+2));return r?(e.M=+r[0],n+r[0].length):-1}function bo(e,t,n){var r=Zi.exec(t.slice(n,n+2));return r?(e.S=+r[0],n+r[0].length):-1}function wo(e,t,n){var r=Zi.exec(t.slice(n,n+3));return r?(e.L=+r[0],n+r[0].length):-1}function xo(e,t,n){var r=Zi.exec(t.slice(n,n+6));return r?(e.L=Math.floor(r[0]/1e3),n+r[0].length):-1}function _o(e,t,n){var r=Ji.exec(t.slice(n,n+1));return r?n+r[0].length:-1}function ko(e,t,n){var r=Zi.exec(t.slice(n));return r?(e.Q=+r[0],n+r[0].length):-1}function To(e,t,n){var r=Zi.exec(t.slice(n));return r?(e.s=+r[0],n+r[0].length):-1}function Eo(e,t){return eo(e.getDate(),t,2)}function Co(e,t){return eo(e.getHours(),t,2)}function So(e,t){return eo(e.getHours()%12||12,t,2)}function Mo(e,t){return eo(1+Ni.count(yi(e),e),t,3)}function No(e,t){return eo(e.getMilliseconds(),t,3)}function Po(e,t){return No(e,t)+"000"}function Ao(e,t){return eo(e.getMonth()+1,t,2)}function Ro(e,t){return eo(e.getMinutes(),t,2)}function Do(e,t){return eo(e.getSeconds(),t,2)}function Oo(e){var t=e.getDay();return 0===t?7:t}function zo(e,t){return eo(xi.count(yi(e)-1,e),t,2)}function Lo(e,t){var n=e.getDay();return e=n>=4||0===n?Ei(e):Ei.ceil(e),eo(Ei.count(yi(e),e)+(4===yi(e).getDay()),t,2)}function Uo(e){return e.getDay()}function Fo(e,t){return eo(_i.count(yi(e)-1,e),t,2)}function Io(e,t){return eo(e.getFullYear()%100,t,2)}function jo(e,t){return eo(e.getFullYear()%1e4,t,4)}function Ho(e){var t=e.getTimezoneOffset();return(t>0?"-":(t*=-1,"+"))+eo(t/60|0,"0",2)+eo(t%60,"0",2)}function Vo(e,t){return eo(e.getUTCDate(),t,2)}function $o(e,t){return eo(e.getUTCHours(),t,2)}function Wo(e,t){return eo(e.getUTCHours()%12||12,t,2)}function Qo(e,t){return eo(1+$i.count(Qi(e),e),t,3)}function qo(e,t){return eo(e.getUTCMilliseconds(),t,3)}function Bo(e,t){return qo(e,t)+"000"}function Yo(e,t){return eo(e.getUTCMonth()+1,t,2)}function Xo(e,t){return eo(e.getUTCMinutes(),t,2)}function Ko(e,t){return eo(e.getUTCSeconds(),t,2)}function Zo(e){var t=e.getUTCDay();return 0===t?7:t}function Jo(e,t){return eo(zi.count(Qi(e)-1,e),t,2)}function Go(e,t){var n=e.getUTCDay();return e=n>=4||0===n?Ii(e):Ii.ceil(e),eo(Ii.count(Qi(e),e)+(4===Qi(e).getUTCDay()),t,2)}function eu(e){return e.getUTCDay()}function tu(e,t){return eo(Li.count(Qi(e)-1,e),t,2)}function nu(e,t){return eo(e.getUTCFullYear()%100,t,2)}function ru(e,t){return eo(e.getUTCFullYear()%1e4,t,4)}function iu(){return"+0000"}function ou(){return"%"}function uu(e){return+e}function au(e){return Math.floor(+e/1e3)}!function(e){Xi=function(e){var t=e.dateTime,n=e.date,r=e.time,i=e.periods,o=e.days,u=e.shortDays,a=e.months,l=e.shortMonths,c=no(i),s=ro(i),f=no(o),d=ro(o),p=no(u),h=ro(u),m=no(a),g=ro(a),v=no(l),y=ro(l),b={a:function(e){return u[e.getDay()]},A:function(e){return o[e.getDay()]},b:function(e){return l[e.getMonth()]},B:function(e){return a[e.getMonth()]},c:null,d:Eo,e:Eo,f:Po,H:Co,I:So,j:Mo,L:No,m:Ao,M:Ro,p:function(e){return i[+(e.getHours()>=12)]},q:function(e){return 1+~~(e.getMonth()/3)},Q:uu,s:au,S:Do,u:Oo,U:zo,V:Lo,w:Uo,W:Fo,x:null,X:null,y:Io,Y:jo,Z:Ho,"%":ou},w={a:function(e){return u[e.getUTCDay()]},A:function(e){return o[e.getUTCDay()]},b:function(e){return l[e.getUTCMonth()]},B:function(e){return a[e.getUTCMonth()]},c:null,d:Vo,e:Vo,f:Bo,H:$o,I:Wo,j:Qo,L:qo,m:Yo,M:Xo,p:function(e){return i[+(e.getUTCHours()>=12)]},q:function(e){return 1+~~(e.getUTCMonth()/3)},Q:uu,s:au,S:Ko,u:Zo,U:Jo,V:Go,w:eu,W:tu,x:null,X:null,y:nu,Y:ru,Z:iu,"%":ou},x={a:function(e,t,n){var r=p.exec(t.slice(n));return r?(e.w=h[r[0].toLowerCase()],n+r[0].length):-1},A:function(e,t,n){var r=f.exec(t.slice(n));return r?(e.w=d[r[0].toLowerCase()],n+r[0].length):-1},b:function(e,t,n){var r=v.exec(t.slice(n));return r?(e.m=y[r[0].toLowerCase()],n+r[0].length):-1},B:function(e,t,n){var r=m.exec(t.slice(n));return r?(e.m=g[r[0].toLowerCase()],n+r[0].length):-1},c:function(e,n,r){return T(e,t,n,r)},d:mo,e:mo,f:xo,H:vo,I:vo,j:go,L:wo,m:ho,M:yo,p:function(e,t,n){var r=c.exec(t.slice(n));return r?(e.p=s[r[0].toLowerCase()],n+r[0].length):-1},q:po,Q:ko,s:To,S:bo,u:oo,U:uo,V:ao,w:io,W:lo,x:function(e,t,r){return T(e,n,t,r)},X:function(e,t,n){return T(e,r,t,n)},y:so,Y:co,Z:fo,"%":_o};function _(e,t){return function(n){var r,i,o,u=[],a=-1,l=0,c=e.length;for(n instanceof Date||(n=new Date(+n));++a<c;)37===e.charCodeAt(a)&&(u.push(e.slice(l,a)),null!=(i=Ki[r=e.charAt(++a)])?r=e.charAt(++a):i="e"===r?" ":"0",(o=t[r])&&(r=o(n,i)),u.push(r),l=a+1);return u.push(e.slice(l,a)),u.join("")}}function k(e,t){return function(n){var r,i,o=Yi(1900,void 0,1);if(T(o,e,n+="",0)!=n.length)return null;if("Q"in o)return new Date(o.Q);if("s"in o)return new Date(1e3*o.s+("L"in o?o.L:0));if(t&&!("Z"in o)&&(o.Z=0),"p"in o&&(o.H=o.H%12+12*o.p),void 0===o.m&&(o.m="q"in o?o.q:0),"V"in o){if(o.V<1||o.V>53)return null;"w"in o||(o.w=1),"Z"in o?(i=(r=Bi(Yi(o.y,0,1))).getUTCDay(),r=i>4||0===i?Li.ceil(r):Li(r),r=$i.offset(r,7*(o.V-1)),o.y=r.getUTCFullYear(),o.m=r.getUTCMonth(),o.d=r.getUTCDate()+(o.w+6)%7):(i=(r=qi(Yi(o.y,0,1))).getDay(),r=i>4||0===i?_i.ceil(r):_i(r),r=Ni.offset(r,7*(o.V-1)),o.y=r.getFullYear(),o.m=r.getMonth(),o.d=r.getDate()+(o.w+6)%7)}else("W"in o||"U"in o)&&("w"in o||(o.w="u"in o?o.u%7:"W"in o?1:0),i="Z"in o?Bi(Yi(o.y,0,1)).getUTCDay():qi(Yi(o.y,0,1)).getDay(),o.m=0,o.d="W"in o?(o.w+6)%7+7*o.W-(i+5)%7:o.w+7*o.U-(i+6)%7);return"Z"in o?(o.H+=o.Z/100|0,o.M+=o.Z%100,Bi(o)):qi(o)}}function T(e,t,n,r){for(var i,o,u=0,a=t.length,l=n.length;u<a;){if(r>=l)return-1;if(37===(i=t.charCodeAt(u++))){if(i=t.charAt(u++),!(o=x[i in Ki?t.charAt(u++):i])||(r=o(e,n,r))<0)return-1}else if(i!=n.charCodeAt(r++))return-1}return r}return b.x=_(n,b),b.X=_(r,b),b.c=_(t,b),w.x=_(n,w),w.X=_(r,w),w.c=_(t,w),{format:function(e){var t=_(e+="",b);return t.toString=function(){return e},t},parse:function(e){var t=k(e+="",!1);return t.toString=function(){return e},t},utcFormat:function(e){var t=_(e+="",w);return t.toString=function(){return e},t},utcParse:function(e){var t=k(e+="",!0);return t.toString=function(){return e},t}}}(e),Xi.format,Xi.parse,Xi.utcFormat,Xi.utcParse}({dateTime:"%x, %X",date:"%-m/%-d/%Y",time:"%-I:%M:%S %p",periods:["AM","PM"],days:["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],shortDays:["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],months:["January","February","March","April","May","June","July","August","September","October","November","December"],shortMonths:["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"]});var lu=gi((function(e){e.setUTCDate(1),e.setUTCHours(0,0,0,0)}),(function(e,t){e.setUTCMonth(e.getUTCMonth()+t)}),(function(e,t){return t.getUTCMonth()-e.getUTCMonth()+12*(t.getUTCFullYear()-e.getUTCFullYear())}),(function(e){return e.getUTCMonth()})),cu=(lu.range,gi((function(e){e.setUTCMinutes(0,0,0)}),(function(e,t){e.setTime(+e+36e5*t)}),(function(e,t){return(t-e)/36e5}),(function(e){return e.getUTCHours()}))),su=(cu.range,gi((function(e){e.setUTCSeconds(0,0)}),(function(e,t){e.setTime(+e+6e4*t)}),(function(e,t){return(t-e)/6e4}),(function(e){return e.getUTCMinutes()})));su.range;function fu(){this._=null}function du(e){e.U=e.C=e.L=e.R=e.P=e.N=null}function pu(e,t){var n=t,r=t.R,i=n.U;i?i.L===n?i.L=r:i.R=r:e._=r,r.U=i,n.U=r,n.R=r.L,n.R&&(n.R.U=n),r.L=n}function hu(e,t){var n=t,r=t.L,i=n.U;i?i.L===n?i.L=r:i.R=r:e._=r,r.U=i,n.U=r,n.L=r.R,n.L&&(n.L.U=n),r.R=n}function mu(e){for(;e.L;)e=e.L;return e}fu.prototype={constructor:fu,insert:function(e,t){var n,r,i;if(e){if(t.P=e,t.N=e.N,e.N&&(e.N.P=t),e.N=t,e.R){for(e=e.R;e.L;)e=e.L;e.L=t}else e.R=t;n=e}else this._?(e=mu(this._),t.P=null,t.N=e,e.P=e.L=t,n=e):(t.P=t.N=null,this._=t,n=null);for(t.L=t.R=null,t.U=n,t.C=!0,e=t;n&&n.C;)n===(r=n.U).L?(i=r.R)&&i.C?(n.C=i.C=!1,r.C=!0,e=r):(e===n.R&&(pu(this,n),n=(e=n).U),n.C=!1,r.C=!0,hu(this,r)):(i=r.L)&&i.C?(n.C=i.C=!1,r.C=!0,e=r):(e===n.L&&(hu(this,n),n=(e=n).U),n.C=!1,r.C=!0,pu(this,r)),n=e.U;this._.C=!1},remove:function(e){e.N&&(e.N.P=e.P),e.P&&(e.P.N=e.N),e.N=e.P=null;var t,n,r,i=e.U,o=e.L,u=e.R;if(n=o?u?mu(u):o:u,i?i.L===e?i.L=n:i.R=n:this._=n,o&&u?(r=n.C,n.C=e.C,n.L=o,o.U=n,n!==u?(i=n.U,n.U=e.U,e=n.R,i.L=e,n.R=u,u.U=n):(n.U=i,i=n,e=n.R)):(r=e.C,e=n),e&&(e.U=i),!r)if(e&&e.C)e.C=!1;else{do{if(e===this._)break;if(e===i.L){if((t=i.R).C&&(t.C=!1,i.C=!0,pu(this,i),t=i.R),t.L&&t.L.C||t.R&&t.R.C){t.R&&t.R.C||(t.L.C=!1,t.C=!0,hu(this,t),t=i.R),t.C=i.C,i.C=t.R.C=!1,pu(this,i),e=this._;break}}else if((t=i.L).C&&(t.C=!1,i.C=!0,hu(this,i),t=i.L),t.L&&t.L.C||t.R&&t.R.C){t.L&&t.L.C||(t.R.C=!1,t.C=!0,pu(this,t),t=i.L),t.C=i.C,i.C=t.L.C=!1,hu(this,i),e=this._;break}t.C=!0,e=i,i=i.U}while(!e.C);e&&(e.C=!1)}}};var gu=fu;function vu(e,t,n,r){var i=[null,null],o=Hu.push(i)-1;return i.left=e,i.right=t,n&&bu(i,e,t,n),r&&bu(i,t,e,r),Iu[e.index].halfedges.push(o),Iu[t.index].halfedges.push(o),i}function yu(e,t,n){var r=[t,n];return r.left=e,r}function bu(e,t,n,r){e[0]||e[1]?e.left===n?e[1]=r:e[0]=r:(e[0]=r,e.left=t,e.right=n)}function wu(e,t,n,r,i){var o,u=e[0],a=e[1],l=u[0],c=u[1],s=0,f=1,d=a[0]-l,p=a[1]-c;if(o=t-l,d||!(o>0)){if(o/=d,d<0){if(o<s)return;o<f&&(f=o)}else if(d>0){if(o>f)return;o>s&&(s=o)}if(o=r-l,d||!(o<0)){if(o/=d,d<0){if(o>f)return;o>s&&(s=o)}else if(d>0){if(o<s)return;o<f&&(f=o)}if(o=n-c,p||!(o>0)){if(o/=p,p<0){if(o<s)return;o<f&&(f=o)}else if(p>0){if(o>f)return;o>s&&(s=o)}if(o=i-c,p||!(o<0)){if(o/=p,p<0){if(o>f)return;o>s&&(s=o)}else if(p>0){if(o<s)return;o<f&&(f=o)}return!(s>0||f<1)||(s>0&&(e[0]=[l+s*d,c+s*p]),f<1&&(e[1]=[l+f*d,c+f*p]),!0)}}}}}function xu(e,t,n,r,i){var o=e[1];if(o)return!0;var u,a,l=e[0],c=e.left,s=e.right,f=c[0],d=c[1],p=s[0],h=s[1],m=(f+p)/2,g=(d+h)/2;if(h===d){if(m<t||m>=r)return;if(f>p){if(l){if(l[1]>=i)return}else l=[m,n];o=[m,i]}else{if(l){if(l[1]<n)return}else l=[m,i];o=[m,n]}}else if(a=g-(u=(f-p)/(h-d))*m,u<-1||u>1)if(f>p){if(l){if(l[1]>=i)return}else l=[(n-a)/u,n];o=[(i-a)/u,i]}else{if(l){if(l[1]<n)return}else l=[(i-a)/u,i];o=[(n-a)/u,n]}else if(d<h){if(l){if(l[0]>=r)return}else l=[t,u*t+a];o=[r,u*r+a]}else{if(l){if(l[0]<t)return}else l=[r,u*r+a];o=[t,u*t+a]}return e[0]=l,e[1]=o,!0}function _u(e,t){var n=e.site,r=t.left,i=t.right;return n===i&&(i=r,r=n),i?Math.atan2(i[1]-r[1],i[0]-r[0]):(n===r?(r=t[1],i=t[0]):(r=t[0],i=t[1]),Math.atan2(r[0]-i[0],i[1]-r[1]))}function ku(e,t){return t[+(t.left!==e.site)]}function Tu(e,t){return t[+(t.left===e.site)]}var Eu,Cu=[];function Su(){du(this),this.x=this.y=this.arc=this.site=this.cy=null}function Mu(e){var t=e.P,n=e.N;if(t&&n){var r=t.site,i=e.site,o=n.site;if(r!==o){var u=i[0],a=i[1],l=r[0]-u,c=r[1]-a,s=o[0]-u,f=o[1]-a,d=2*(l*f-c*s);if(!(d>=-$u)){var p=l*l+c*c,h=s*s+f*f,m=(f*p-c*h)/d,g=(l*h-s*p)/d,v=Cu.pop()||new Su;v.arc=e,v.site=i,v.x=m+u,v.y=(v.cy=g+a)+Math.sqrt(m*m+g*g),e.circle=v;for(var y=null,b=ju._;b;)if(v.y<b.y||v.y===b.y&&v.x<=b.x){if(!b.L){y=b.P;break}b=b.L}else{if(!b.R){y=b;break}b=b.R}ju.insert(y,v),y||(Eu=v)}}}}function Nu(e){var t=e.circle;t&&(t.P||(Eu=t.N),ju.remove(t),Cu.push(t),du(t),e.circle=null)}var Pu=[];function Au(){du(this),this.edge=this.site=this.circle=null}function Ru(e){var t=Pu.pop()||new Au;return t.site=e,t}function Du(e){Nu(e),Fu.remove(e),Pu.push(e),du(e)}function Ou(e){var t=e.circle,n=t.x,r=t.cy,i=[n,r],o=e.P,u=e.N,a=[e];Du(e);for(var l=o;l.circle&&Math.abs(n-l.circle.x)<Vu&&Math.abs(r-l.circle.cy)<Vu;)o=l.P,a.unshift(l),Du(l),l=o;a.unshift(l),Nu(l);for(var c=u;c.circle&&Math.abs(n-c.circle.x)<Vu&&Math.abs(r-c.circle.cy)<Vu;)u=c.N,a.push(c),Du(c),c=u;a.push(c),Nu(c);var s,f=a.length;for(s=1;s<f;++s)c=a[s],l=a[s-1],bu(c.edge,l.site,c.site,i);l=a[0],(c=a[f-1]).edge=vu(l.site,c.site,null,i),Mu(l),Mu(c)}function zu(e){for(var t,n,r,i,o=e[0],u=e[1],a=Fu._;a;)if((r=Lu(a,u)-o)>Vu)a=a.L;else{if(!((i=o-Uu(a,u))>Vu)){r>-Vu?(t=a.P,n=a):i>-Vu?(t=a,n=a.N):t=n=a;break}if(!a.R){t=a;break}a=a.R}!function(e){Iu[e.index]={site:e,halfedges:[]}}(e);var l=Ru(e);if(Fu.insert(t,l),t||n){if(t===n)return Nu(t),n=Ru(t.site),Fu.insert(l,n),l.edge=n.edge=vu(t.site,l.site),Mu(t),void Mu(n);if(n){Nu(t),Nu(n);var c=t.site,s=c[0],f=c[1],d=e[0]-s,p=e[1]-f,h=n.site,m=h[0]-s,g=h[1]-f,v=2*(d*g-p*m),y=d*d+p*p,b=m*m+g*g,w=[(g*y-p*b)/v+s,(d*b-m*y)/v+f];bu(n.edge,c,h,w),l.edge=vu(c,e,null,w),n.edge=vu(e,h,null,w),Mu(t),Mu(n)}else l.edge=vu(t.site,l.site)}}function Lu(e,t){var n=e.site,r=n[0],i=n[1],o=i-t;if(!o)return r;var u=e.P;if(!u)return-1/0;var a=(n=u.site)[0],l=n[1],c=l-t;if(!c)return a;var s=a-r,f=1/o-1/c,d=s/c;return f?(-d+Math.sqrt(d*d-2*f*(s*s/(-2*c)-l+c/2+i-o/2)))/f+r:(r+a)/2}function Uu(e,t){var n=e.N;if(n)return Lu(n,t);var r=e.site;return r[1]===t?r[0]:1/0}var Fu,Iu,ju,Hu,Vu=1e-6,$u=1e-12;function Wu(e,t){return t[1]-e[1]||t[0]-e[0]}function Qu(e,t){var n,r,i,o=e.sort(Wu).pop();for(Hu=[],Iu=new Array(e.length),Fu=new gu,ju=new gu;;)if(i=Eu,o&&(!i||o[1]<i.y||o[1]===i.y&&o[0]<i.x))o[0]===n&&o[1]===r||(zu(o),n=o[0],r=o[1]),o=e.pop();else{if(!i)break;Ou(i.arc)}if(function(){for(var e,t,n,r,i=0,o=Iu.length;i<o;++i)if((e=Iu[i])&&(r=(t=e.halfedges).length)){var u=new Array(r),a=new Array(r);for(n=0;n<r;++n)u[n]=n,a[n]=_u(e,Hu[t[n]]);for(u.sort((function(e,t){return a[t]-a[e]})),n=0;n<r;++n)a[n]=t[u[n]];for(n=0;n<r;++n)t[n]=a[n]}}(),t){var u=+t[0][0],a=+t[0][1],l=+t[1][0],c=+t[1][1];!function(e,t,n,r){for(var i,o=Hu.length;o--;)xu(i=Hu[o],e,t,n,r)&&wu(i,e,t,n,r)&&(Math.abs(i[0][0]-i[1][0])>Vu||Math.abs(i[0][1]-i[1][1])>Vu)||delete Hu[o]}(u,a,l,c),function(e,t,n,r){var i,o,u,a,l,c,s,f,d,p,h,m,g=Iu.length,v=!0;for(i=0;i<g;++i)if(o=Iu[i]){for(u=o.site,a=(l=o.halfedges).length;a--;)Hu[l[a]]||l.splice(a,1);for(a=0,c=l.length;a<c;)h=(p=Tu(o,Hu[l[a]]))[0],m=p[1],f=(s=ku(o,Hu[l[++a%c]]))[0],d=s[1],(Math.abs(h-f)>Vu||Math.abs(m-d)>Vu)&&(l.splice(a,0,Hu.push(yu(u,p,Math.abs(h-e)<Vu&&r-m>Vu?[e,Math.abs(f-e)<Vu?d:r]:Math.abs(m-r)<Vu&&n-h>Vu?[Math.abs(d-r)<Vu?f:n,r]:Math.abs(h-n)<Vu&&m-t>Vu?[n,Math.abs(f-n)<Vu?d:t]:Math.abs(m-t)<Vu&&h-e>Vu?[Math.abs(d-t)<Vu?f:e,t]:null))-1),++c);c&&(v=!1)}if(v){var y,b,w,x=1/0;for(i=0,v=null;i<g;++i)(o=Iu[i])&&(w=(y=(u=o.site)[0]-e)*y+(b=u[1]-t)*b)<x&&(x=w,v=o);if(v){var _=[e,t],k=[e,r],T=[n,r],E=[n,t];v.halfedges.push(Hu.push(yu(u=v.site,_,k))-1,Hu.push(yu(u,k,T))-1,Hu.push(yu(u,T,E))-1,Hu.push(yu(u,E,_))-1)}}for(i=0;i<g;++i)(o=Iu[i])&&(o.halfedges.length||delete Iu[i])}(u,a,l,c)}this.edges=Hu,this.cells=Iu,Fu=ju=Hu=Iu=null}Qu.prototype={constructor:Qu,polygons:function(){var e=this.edges;return this.cells.map((function(t){var n=t.halfedges.map((function(n){return ku(t,e[n])}));return n.data=t.site.data,n}))},triangles:function(){var e=[],t=this.edges;return this.cells.forEach((function(n,r){if(o=(i=n.halfedges).length)for(var i,o,u,a,l,c,s=n.site,f=-1,d=t[i[o-1]],p=d.left===s?d.right:d.left;++f<o;)u=p,p=(d=t[i[f]]).left===s?d.right:d.left,u&&p&&r<u.index&&r<p.index&&(l=u,c=p,((a=s)[0]-c[0])*(l[1]-a[1])-(a[0]-l[0])*(c[1]-a[1])<0)&&e.push([s.data,u.data,p.data])})),e},links:function(){return this.edges.filter((function(e){return e.right})).map((function(e){return{source:e.left.data,target:e.right.data}}))},find:function(e,t,n){for(var r,i,o=this,u=o._found||0,a=o.cells.length;!(i=o.cells[u]);)if(++u>=a)return null;var l=e-i.site[0],c=t-i.site[1],s=l*l+c*c;do{i=o.cells[r=u],u=null,i.halfedges.forEach((function(n){var r=o.edges[n],a=r.left;if(a!==i.site&&a||(a=r.right)){var l=e-a[0],c=t-a[1],f=l*l+c*c;f<s&&(s=f,u=a.index)}}))}while(null!==u);return o._found=r,null==n||s<=n*n?i.site:null}};Math.SQRT2;function qu(e,t,n){this.k=e,this.x=t,this.y=n}qu.prototype={constructor:qu,scale:function(e){return 1===e?this:new qu(this.k*e,this.x,this.y)},translate:function(e,t){return 0===e&0===t?this:new qu(this.k,this.x+this.k*e,this.y+this.k*t)},apply:function(e){return[e[0]*this.k+this.x,e[1]*this.k+this.y]},applyX:function(e){return e*this.k+this.x},applyY:function(e){return e*this.k+this.y},invert:function(e){return[(e[0]-this.x)/this.k,(e[1]-this.y)/this.k]},invertX:function(e){return(e-this.x)/this.k},invertY:function(e){return(e-this.y)/this.k},rescaleX:function(e){return e.copy().domain(e.range().map(this.invertX,this).map(e.invert,e))},rescaleY:function(e){return e.copy().domain(e.range().map(this.invertY,this).map(e.invert,e))},toString:function(){return"translate("+this.x+","+this.y+") scale("+this.k+")"}};new qu(1,0,0);qu.prototype},,,,,,function(e,t,n){"use strict";function r(e,t){if(!(e instanceof t))throw new TypeError("Cannot call a class as a function")}n.d(t,"a",(function(){return r}))},function(e,t,n){"use strict";function r(e,t){for(var n=0;n<t.length;n++){var r=t[n];r.enumerable=r.enumerable||!1,r.configurable=!0,"value"in r&&(r.writable=!0),Object.defineProperty(e,r.key,r)}}function i(e,t,n){return t&&r(e.prototype,t),n&&r(e,n),e}n.d(t,"a",(function(){return i}))},function(e,t,n){"use strict";function r(e){if(void 0===e)throw new ReferenceError("this hasn't been initialised - super() hasn't been called");return e}n.d(t,"a",(function(){return r}))},function(e,t,n){"use strict";function r(e){return(r=Object.setPrototypeOf?Object.getPrototypeOf:function(e){return e.__proto__||Object.getPrototypeOf(e)})(e)}function i(){if("undefined"===typeof Reflect||!Reflect.construct)return!1;if(Reflect.construct.sham)return!1;if("function"===typeof Proxy)return!0;try{return Date.prototype.toString.call(Reflect.construct(Date,[],(function(){}))),!0}catch(e){return!1}}function o(e){return(o="function"===typeof Symbol&&"symbol"===typeof Symbol.iterator?function(e){return typeof e}:function(e){return e&&"function"===typeof Symbol&&e.constructor===Symbol&&e!==Symbol.prototype?"symbol":typeof e})(e)}n.d(t,"a",(function(){return l}));var u=n(10);function a(e,t){return!t||"object"!==o(t)&&"function"!==typeof t?Object(u.a)(e):t}function l(e){return function(){var t,n=r(e);if(i()){var o=r(this).constructor;t=Reflect.construct(n,arguments,o)}else t=n.apply(this,arguments);return a(this,t)}}},function(e,t,n){"use strict";function r(e,t){return(r=Object.setPrototypeOf||function(e,t){return e.__proto__=t,e})(e,t)}function i(e,t){if("function"!==typeof t&&null!==t)throw new TypeError("Super expression must either be null or a function");e.prototype=Object.create(t&&t.prototype,{constructor:{value:e,writable:!0,configurable:!0}}),t&&r(e,t)}n.d(t,"a",(function(){return i}))},,,,,,,,,function(e,t,n){"use strict";var r=Object.getOwnPropertySymbols,i=Object.prototype.hasOwnProperty,o=Object.prototype.propertyIsEnumerable;function u(e){if(null===e||void 0===e)throw new TypeError("Object.assign cannot be called with null or undefined");return Object(e)}e.exports=function(){try{if(!Object.assign)return!1;var e=new String("abc");if(e[5]="de","5"===Object.getOwnPropertyNames(e)[0])return!1;for(var t={},n=0;n<10;n++)t["_"+String.fromCharCode(n)]=n;if("0123456789"!==Object.getOwnPropertyNames(t).map((function(e){return t[e]})).join(""))return!1;var r={};return"abcdefghijklmnopqrst".split("").forEach((function(e){r[e]=e})),"abcdefghijklmnopqrst"===Object.keys(Object.assign({},r)).join("")}catch(i){return!1}}()?Object.assign:function(e,t){for(var n,a,l=u(e),c=1;c<arguments.length;c++){for(var s in n=Object(arguments[c]))i.call(n,s)&&(l[s]=n[s]);if(r){a=r(n);for(var f=0;f<a.length;f++)o.call(n,a[f])&&(l[a[f]]=n[a[f]])}}return l}},function(e,t,n){"use strict";!function e(){if("undefined"!==typeof __REACT_DEVTOOLS_GLOBAL_HOOK__&&"function"===typeof __REACT_DEVTOOLS_GLOBAL_HOOK__.checkDCE){0;try{__REACT_DEVTOOLS_GLOBAL_HOOK__.checkDCE(e)}catch(t){console.error(t)}}}(),e.exports=n(30)},,,,,,,function(e,t,n){"use strict";var r=n(21),i="function"===typeof Symbol&&Symbol.for,o=i?Symbol.for("react.element"):60103,u=i?Symbol.for("react.portal"):60106,a=i?Symbol.for("react.fragment"):60107,l=i?Symbol.for("react.strict_mode"):60108,c=i?Symbol.for("react.profiler"):60114,s=i?Symbol.for("react.provider"):60109,f=i?Symbol.for("react.context"):60110,d=i?Symbol.for("react.forward_ref"):60112,p=i?Symbol.for("react.suspense"):60113,h=i?Symbol.for("react.memo"):60115,m=i?Symbol.for("react.lazy"):60116,g="function"===typeof Symbol&&Symbol.iterator;function v(e){for(var t="https://reactjs.org/docs/error-decoder.html?invariant="+e,n=1;n<arguments.length;n++)t+="&args[]="+encodeURIComponent(arguments[n]);return"Minified React error #"+e+"; visit "+t+" for the full message or use the non-minified dev environment for full errors and additional helpful warnings."}var y={isMounted:function(){return!1},enqueueForceUpdate:function(){},enqueueReplaceState:function(){},enqueueSetState:function(){}},b={};function w(e,t,n){this.props=e,this.context=t,this.refs=b,this.updater=n||y}function x(){}function _(e,t,n){this.props=e,this.context=t,this.refs=b,this.updater=n||y}w.prototype.isReactComponent={},w.prototype.setState=function(e,t){if("object"!==typeof e&&"function"!==typeof e&&null!=e)throw Error(v(85));this.updater.enqueueSetState(this,e,t,"setState")},w.prototype.forceUpdate=function(e){this.updater.enqueueForceUpdate(this,e,"forceUpdate")},x.prototype=w.prototype;var k=_.prototype=new x;k.constructor=_,r(k,w.prototype),k.isPureReactComponent=!0;var T={current:null},E=Object.prototype.hasOwnProperty,C={key:!0,ref:!0,__self:!0,__source:!0};function S(e,t,n){var r,i={},u=null,a=null;if(null!=t)for(r in void 0!==t.ref&&(a=t.ref),void 0!==t.key&&(u=""+t.key),t)E.call(t,r)&&!C.hasOwnProperty(r)&&(i[r]=t[r]);var l=arguments.length-2;if(1===l)i.children=n;else if(1<l){for(var c=Array(l),s=0;s<l;s++)c[s]=arguments[s+2];i.children=c}if(e&&e.defaultProps)for(r in l=e.defaultProps)void 0===i[r]&&(i[r]=l[r]);return{$$typeof:o,type:e,key:u,ref:a,props:i,_owner:T.current}}function M(e){return"object"===typeof e&&null!==e&&e.$$typeof===o}var N=/\/+/g,P=[];function A(e,t,n,r){if(P.length){var i=P.pop();return i.result=e,i.keyPrefix=t,i.func=n,i.context=r,i.count=0,i}return{result:e,keyPrefix:t,func:n,context:r,count:0}}function R(e){e.result=null,e.keyPrefix=null,e.func=null,e.context=null,e.count=0,10>P.length&&P.push(e)}function D(e,t,n){return null==e?0:function e(t,n,r,i){var a=typeof t;"undefined"!==a&&"boolean"!==a||(t=null);var l=!1;if(null===t)l=!0;else switch(a){case"string":case"number":l=!0;break;case"object":switch(t.$$typeof){case o:case u:l=!0}}if(l)return r(i,t,""===n?"."+O(t,0):n),1;if(l=0,n=""===n?".":n+":",Array.isArray(t))for(var c=0;c<t.length;c++){var s=n+O(a=t[c],c);l+=e(a,s,r,i)}else if(null===t||"object"!==typeof t?s=null:s="function"===typeof(s=g&&t[g]||t["@@iterator"])?s:null,"function"===typeof s)for(t=s.call(t),c=0;!(a=t.next()).done;)l+=e(a=a.value,s=n+O(a,c++),r,i);else if("object"===a)throw r=""+t,Error(v(31,"[object Object]"===r?"object with keys {"+Object.keys(t).join(", ")+"}":r,""));return l}(e,"",t,n)}function O(e,t){return"object"===typeof e&&null!==e&&null!=e.key?function(e){var t={"=":"=0",":":"=2"};return"$"+(""+e).replace(/[=:]/g,(function(e){return t[e]}))}(e.key):t.toString(36)}function z(e,t){e.func.call(e.context,t,e.count++)}function L(e,t,n){var r=e.result,i=e.keyPrefix;e=e.func.call(e.context,t,e.count++),Array.isArray(e)?U(e,r,n,(function(e){return e})):null!=e&&(M(e)&&(e=function(e,t){return{$$typeof:o,type:e.type,key:t,ref:e.ref,props:e.props,_owner:e._owner}}(e,i+(!e.key||t&&t.key===e.key?"":(""+e.key).replace(N,"$&/")+"/")+n)),r.push(e))}function U(e,t,n,r,i){var o="";null!=n&&(o=(""+n).replace(N,"$&/")+"/"),D(e,L,t=A(t,o,r,i)),R(t)}var F={current:null};function I(){var e=F.current;if(null===e)throw Error(v(321));return e}var j={ReactCurrentDispatcher:F,ReactCurrentBatchConfig:{suspense:null},ReactCurrentOwner:T,IsSomeRendererActing:{current:!1},assign:r};t.Children={map:function(e,t,n){if(null==e)return e;var r=[];return U(e,r,null,t,n),r},forEach:function(e,t,n){if(null==e)return e;D(e,z,t=A(null,null,t,n)),R(t)},count:function(e){return D(e,(function(){return null}),null)},toArray:function(e){var t=[];return U(e,t,null,(function(e){return e})),t},only:function(e){if(!M(e))throw Error(v(143));return e}},t.Component=w,t.Fragment=a,t.Profiler=c,t.PureComponent=_,t.StrictMode=l,t.Suspense=p,t.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED=j,t.cloneElement=function(e,t,n){if(null===e||void 0===e)throw Error(v(267,e));var i=r({},e.props),u=e.key,a=e.ref,l=e._owner;if(null!=t){if(void 0!==t.ref&&(a=t.ref,l=T.current),void 0!==t.key&&(u=""+t.key),e.type&&e.type.defaultProps)var c=e.type.defaultProps;for(s in t)E.call(t,s)&&!C.hasOwnProperty(s)&&(i[s]=void 0===t[s]&&void 0!==c?c[s]:t[s])}var s=arguments.length-2;if(1===s)i.children=n;else if(1<s){c=Array(s);for(var f=0;f<s;f++)c[f]=arguments[f+2];i.children=c}return{$$typeof:o,type:e.type,key:u,ref:a,props:i,_owner:l}},t.createContext=function(e,t){return void 0===t&&(t=null),(e={$$typeof:f,_calculateChangedBits:t,_currentValue:e,_currentValue2:e,_threadCount:0,Provider:null,Consumer:null}).Provider={$$typeof:s,_context:e},e.Consumer=e},t.createElement=S,t.createFactory=function(e){var t=S.bind(null,e);return t.type=e,t},t.createRef=function(){return{current:null}},t.forwardRef=function(e){return{$$typeof:d,render:e}},t.isValidElement=M,t.lazy=function(e){return{$$typeof:m,_ctor:e,_status:-1,_result:null}},t.memo=function(e,t){return{$$typeof:h,type:e,compare:void 0===t?null:t}},t.useCallback=function(e,t){return I().useCallback(e,t)},t.useContext=function(e,t){return I().useContext(e,t)},t.useDebugValue=function(){},t.useEffect=function(e,t){return I().useEffect(e,t)},t.useImperativeHandle=function(e,t,n){return I().useImperativeHandle(e,t,n)},t.useLayoutEffect=function(e,t){return I().useLayoutEffect(e,t)},t.useMemo=function(e,t){return I().useMemo(e,t)},t.useReducer=function(e,t,n){return I().useReducer(e,t,n)},t.useRef=function(e){return I().useRef(e)},t.useState=function(e){return I().useState(e)},t.version="16.13.1"},function(e,t,n){"use strict";var r=n(0),i=n(21),o=n(31);function u(e){for(var t="https://reactjs.org/docs/error-decoder.html?invariant="+e,n=1;n<arguments.length;n++)t+="&args[]="+encodeURIComponent(arguments[n]);return"Minified React error #"+e+"; visit "+t+" for the full message or use the non-minified dev environment for full errors and additional helpful warnings."}if(!r)throw Error(u(227));function a(e,t,n,r,i,o,u,a,l){var c=Array.prototype.slice.call(arguments,3);try{t.apply(n,c)}catch(s){this.onError(s)}}var l=!1,c=null,s=!1,f=null,d={onError:function(e){l=!0,c=e}};function p(e,t,n,r,i,o,u,s,f){l=!1,c=null,a.apply(d,arguments)}var h=null,m=null,g=null;function v(e,t,n){var r=e.type||"unknown-event";e.currentTarget=g(n),function(e,t,n,r,i,o,a,d,h){if(p.apply(this,arguments),l){if(!l)throw Error(u(198));var m=c;l=!1,c=null,s||(s=!0,f=m)}}(r,t,void 0,e),e.currentTarget=null}var y=null,b={};function w(){if(y)for(var e in b){var t=b[e],n=y.indexOf(e);if(!(-1<n))throw Error(u(96,e));if(!_[n]){if(!t.extractEvents)throw Error(u(97,e));for(var r in _[n]=t,n=t.eventTypes){var i=void 0,o=n[r],a=t,l=r;if(k.hasOwnProperty(l))throw Error(u(99,l));k[l]=o;var c=o.phasedRegistrationNames;if(c){for(i in c)c.hasOwnProperty(i)&&x(c[i],a,l);i=!0}else o.registrationName?(x(o.registrationName,a,l),i=!0):i=!1;if(!i)throw Error(u(98,r,e))}}}}function x(e,t,n){if(T[e])throw Error(u(100,e));T[e]=t,E[e]=t.eventTypes[n].dependencies}var _=[],k={},T={},E={};function C(e){var t,n=!1;for(t in e)if(e.hasOwnProperty(t)){var r=e[t];if(!b.hasOwnProperty(t)||b[t]!==r){if(b[t])throw Error(u(102,t));b[t]=r,n=!0}}n&&w()}var S=!("undefined"===typeof window||"undefined"===typeof window.document||"undefined"===typeof window.document.createElement),M=null,N=null,P=null;function A(e){if(e=m(e)){if("function"!==typeof M)throw Error(u(280));var t=e.stateNode;t&&(t=h(t),M(e.stateNode,e.type,t))}}function R(e){N?P?P.push(e):P=[e]:N=e}function D(){if(N){var e=N,t=P;if(P=N=null,A(e),t)for(e=0;e<t.length;e++)A(t[e])}}function O(e,t){return e(t)}function z(e,t,n,r,i){return e(t,n,r,i)}function L(){}var U=O,F=!1,I=!1;function j(){null===N&&null===P||(L(),D())}function H(e,t,n){if(I)return e(t,n);I=!0;try{return U(e,t,n)}finally{I=!1,j()}}var V=/^[:A-Z_a-z\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u02FF\u0370-\u037D\u037F-\u1FFF\u200C-\u200D\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD][:A-Z_a-z\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u02FF\u0370-\u037D\u037F-\u1FFF\u200C-\u200D\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD\-.0-9\u00B7\u0300-\u036F\u203F-\u2040]*$/,$=Object.prototype.hasOwnProperty,W={},Q={};function q(e,t,n,r,i,o){this.acceptsBooleans=2===t||3===t||4===t,this.attributeName=r,this.attributeNamespace=i,this.mustUseProperty=n,this.propertyName=e,this.type=t,this.sanitizeURL=o}var B={};"children dangerouslySetInnerHTML defaultValue defaultChecked innerHTML suppressContentEditableWarning suppressHydrationWarning style".split(" ").forEach((function(e){B[e]=new q(e,0,!1,e,null,!1)})),[["acceptCharset","accept-charset"],["className","class"],["htmlFor","for"],["httpEquiv","http-equiv"]].forEach((function(e){var t=e[0];B[t]=new q(t,1,!1,e[1],null,!1)})),["contentEditable","draggable","spellCheck","value"].forEach((function(e){B[e]=new q(e,2,!1,e.toLowerCase(),null,!1)})),["autoReverse","externalResourcesRequired","focusable","preserveAlpha"].forEach((function(e){B[e]=new q(e,2,!1,e,null,!1)})),"allowFullScreen async autoFocus autoPlay controls default defer disabled disablePictureInPicture formNoValidate hidden loop noModule noValidate open playsInline readOnly required reversed scoped seamless itemScope".split(" ").forEach((function(e){B[e]=new q(e,3,!1,e.toLowerCase(),null,!1)})),["checked","multiple","muted","selected"].forEach((function(e){B[e]=new q(e,3,!0,e,null,!1)})),["capture","download"].forEach((function(e){B[e]=new q(e,4,!1,e,null,!1)})),["cols","rows","size","span"].forEach((function(e){B[e]=new q(e,6,!1,e,null,!1)})),["rowSpan","start"].forEach((function(e){B[e]=new q(e,5,!1,e.toLowerCase(),null,!1)}));var Y=/[\-:]([a-z])/g;function X(e){return e[1].toUpperCase()}"accent-height alignment-baseline arabic-form baseline-shift cap-height clip-path clip-rule color-interpolation color-interpolation-filters color-profile color-rendering dominant-baseline enable-background fill-opacity fill-rule flood-color flood-opacity font-family font-size font-size-adjust font-stretch font-style font-variant font-weight glyph-name glyph-orientation-horizontal glyph-orientation-vertical horiz-adv-x horiz-origin-x image-rendering letter-spacing lighting-color marker-end marker-mid marker-start overline-position overline-thickness paint-order panose-1 pointer-events rendering-intent shape-rendering stop-color stop-opacity strikethrough-position strikethrough-thickness stroke-dasharray stroke-dashoffset stroke-linecap stroke-linejoin stroke-miterlimit stroke-opacity stroke-width text-anchor text-decoration text-rendering underline-position underline-thickness unicode-bidi unicode-range units-per-em v-alphabetic v-hanging v-ideographic v-mathematical vector-effect vert-adv-y vert-origin-x vert-origin-y word-spacing writing-mode xmlns:xlink x-height".split(" ").forEach((function(e){var t=e.replace(Y,X);B[t]=new q(t,1,!1,e,null,!1)})),"xlink:actuate xlink:arcrole xlink:role xlink:show xlink:title xlink:type".split(" ").forEach((function(e){var t=e.replace(Y,X);B[t]=new q(t,1,!1,e,"http://www.w3.org/1999/xlink",!1)})),["xml:base","xml:lang","xml:space"].forEach((function(e){var t=e.replace(Y,X);B[t]=new q(t,1,!1,e,"http://www.w3.org/XML/1998/namespace",!1)})),["tabIndex","crossOrigin"].forEach((function(e){B[e]=new q(e,1,!1,e.toLowerCase(),null,!1)})),B.xlinkHref=new q("xlinkHref",1,!1,"xlink:href","http://www.w3.org/1999/xlink",!0),["src","href","action","formAction"].forEach((function(e){B[e]=new q(e,1,!1,e.toLowerCase(),null,!0)}));var K=r.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED;function Z(e,t,n,r){var i=B.hasOwnProperty(t)?B[t]:null;(null!==i?0===i.type:!r&&(2<t.length&&("o"===t[0]||"O"===t[0])&&("n"===t[1]||"N"===t[1])))||(function(e,t,n,r){if(null===t||"undefined"===typeof t||function(e,t,n,r){if(null!==n&&0===n.type)return!1;switch(typeof t){case"function":case"symbol":return!0;case"boolean":return!r&&(null!==n?!n.acceptsBooleans:"data-"!==(e=e.toLowerCase().slice(0,5))&&"aria-"!==e);default:return!1}}(e,t,n,r))return!0;if(r)return!1;if(null!==n)switch(n.type){case 3:return!t;case 4:return!1===t;case 5:return isNaN(t);case 6:return isNaN(t)||1>t}return!1}(t,n,i,r)&&(n=null),r||null===i?function(e){return!!$.call(Q,e)||!$.call(W,e)&&(V.test(e)?Q[e]=!0:(W[e]=!0,!1))}(t)&&(null===n?e.removeAttribute(t):e.setAttribute(t,""+n)):i.mustUseProperty?e[i.propertyName]=null===n?3!==i.type&&"":n:(t=i.attributeName,r=i.attributeNamespace,null===n?e.removeAttribute(t):(n=3===(i=i.type)||4===i&&!0===n?"":""+n,r?e.setAttributeNS(r,t,n):e.setAttribute(t,n))))}K.hasOwnProperty("ReactCurrentDispatcher")||(K.ReactCurrentDispatcher={current:null}),K.hasOwnProperty("ReactCurrentBatchConfig")||(K.ReactCurrentBatchConfig={suspense:null});var J=/^(.*)[\\\/]/,G="function"===typeof Symbol&&Symbol.for,ee=G?Symbol.for("react.element"):60103,te=G?Symbol.for("react.portal"):60106,ne=G?Symbol.for("react.fragment"):60107,re=G?Symbol.for("react.strict_mode"):60108,ie=G?Symbol.for("react.profiler"):60114,oe=G?Symbol.for("react.provider"):60109,ue=G?Symbol.for("react.context"):60110,ae=G?Symbol.for("react.concurrent_mode"):60111,le=G?Symbol.for("react.forward_ref"):60112,ce=G?Symbol.for("react.suspense"):60113,se=G?Symbol.for("react.suspense_list"):60120,fe=G?Symbol.for("react.memo"):60115,de=G?Symbol.for("react.lazy"):60116,pe=G?Symbol.for("react.block"):60121,he="function"===typeof Symbol&&Symbol.iterator;function me(e){return null===e||"object"!==typeof e?null:"function"===typeof(e=he&&e[he]||e["@@iterator"])?e:null}function ge(e){if(null==e)return null;if("function"===typeof e)return e.displayName||e.name||null;if("string"===typeof e)return e;switch(e){case ne:return"Fragment";case te:return"Portal";case ie:return"Profiler";case re:return"StrictMode";case ce:return"Suspense";case se:return"SuspenseList"}if("object"===typeof e)switch(e.$$typeof){case ue:return"Context.Consumer";case oe:return"Context.Provider";case le:var t=e.render;return t=t.displayName||t.name||"",e.displayName||(""!==t?"ForwardRef("+t+")":"ForwardRef");case fe:return ge(e.type);case pe:return ge(e.render);case de:if(e=1===e._status?e._result:null)return ge(e)}return null}function ve(e){var t="";do{e:switch(e.tag){case 3:case 4:case 6:case 7:case 10:case 9:var n="";break e;default:var r=e._debugOwner,i=e._debugSource,o=ge(e.type);n=null,r&&(n=ge(r.type)),r=o,o="",i?o=" (at "+i.fileName.replace(J,"")+":"+i.lineNumber+")":n&&(o=" (created by "+n+")"),n="\n    in "+(r||"Unknown")+o}t+=n,e=e.return}while(e);return t}function ye(e){switch(typeof e){case"boolean":case"number":case"object":case"string":case"undefined":return e;default:return""}}function be(e){var t=e.type;return(e=e.nodeName)&&"input"===e.toLowerCase()&&("checkbox"===t||"radio"===t)}function we(e){e._valueTracker||(e._valueTracker=function(e){var t=be(e)?"checked":"value",n=Object.getOwnPropertyDescriptor(e.constructor.prototype,t),r=""+e[t];if(!e.hasOwnProperty(t)&&"undefined"!==typeof n&&"function"===typeof n.get&&"function"===typeof n.set){var i=n.get,o=n.set;return Object.defineProperty(e,t,{configurable:!0,get:function(){return i.call(this)},set:function(e){r=""+e,o.call(this,e)}}),Object.defineProperty(e,t,{enumerable:n.enumerable}),{getValue:function(){return r},setValue:function(e){r=""+e},stopTracking:function(){e._valueTracker=null,delete e[t]}}}}(e))}function xe(e){if(!e)return!1;var t=e._valueTracker;if(!t)return!0;var n=t.getValue(),r="";return e&&(r=be(e)?e.checked?"true":"false":e.value),(e=r)!==n&&(t.setValue(e),!0)}function _e(e,t){var n=t.checked;return i({},t,{defaultChecked:void 0,defaultValue:void 0,value:void 0,checked:null!=n?n:e._wrapperState.initialChecked})}function ke(e,t){var n=null==t.defaultValue?"":t.defaultValue,r=null!=t.checked?t.checked:t.defaultChecked;n=ye(null!=t.value?t.value:n),e._wrapperState={initialChecked:r,initialValue:n,controlled:"checkbox"===t.type||"radio"===t.type?null!=t.checked:null!=t.value}}function Te(e,t){null!=(t=t.checked)&&Z(e,"checked",t,!1)}function Ee(e,t){Te(e,t);var n=ye(t.value),r=t.type;if(null!=n)"number"===r?(0===n&&""===e.value||e.value!=n)&&(e.value=""+n):e.value!==""+n&&(e.value=""+n);else if("submit"===r||"reset"===r)return void e.removeAttribute("value");t.hasOwnProperty("value")?Se(e,t.type,n):t.hasOwnProperty("defaultValue")&&Se(e,t.type,ye(t.defaultValue)),null==t.checked&&null!=t.defaultChecked&&(e.defaultChecked=!!t.defaultChecked)}function Ce(e,t,n){if(t.hasOwnProperty("value")||t.hasOwnProperty("defaultValue")){var r=t.type;if(!("submit"!==r&&"reset"!==r||void 0!==t.value&&null!==t.value))return;t=""+e._wrapperState.initialValue,n||t===e.value||(e.value=t),e.defaultValue=t}""!==(n=e.name)&&(e.name=""),e.defaultChecked=!!e._wrapperState.initialChecked,""!==n&&(e.name=n)}function Se(e,t,n){"number"===t&&e.ownerDocument.activeElement===e||(null==n?e.defaultValue=""+e._wrapperState.initialValue:e.defaultValue!==""+n&&(e.defaultValue=""+n))}function Me(e,t){return e=i({children:void 0},t),(t=function(e){var t="";return r.Children.forEach(e,(function(e){null!=e&&(t+=e)})),t}(t.children))&&(e.children=t),e}function Ne(e,t,n,r){if(e=e.options,t){t={};for(var i=0;i<n.length;i++)t["$"+n[i]]=!0;for(n=0;n<e.length;n++)i=t.hasOwnProperty("$"+e[n].value),e[n].selected!==i&&(e[n].selected=i),i&&r&&(e[n].defaultSelected=!0)}else{for(n=""+ye(n),t=null,i=0;i<e.length;i++){if(e[i].value===n)return e[i].selected=!0,void(r&&(e[i].defaultSelected=!0));null!==t||e[i].disabled||(t=e[i])}null!==t&&(t.selected=!0)}}function Pe(e,t){if(null!=t.dangerouslySetInnerHTML)throw Error(u(91));return i({},t,{value:void 0,defaultValue:void 0,children:""+e._wrapperState.initialValue})}function Ae(e,t){var n=t.value;if(null==n){if(n=t.children,t=t.defaultValue,null!=n){if(null!=t)throw Error(u(92));if(Array.isArray(n)){if(!(1>=n.length))throw Error(u(93));n=n[0]}t=n}null==t&&(t=""),n=t}e._wrapperState={initialValue:ye(n)}}function Re(e,t){var n=ye(t.value),r=ye(t.defaultValue);null!=n&&((n=""+n)!==e.value&&(e.value=n),null==t.defaultValue&&e.defaultValue!==n&&(e.defaultValue=n)),null!=r&&(e.defaultValue=""+r)}function De(e){var t=e.textContent;t===e._wrapperState.initialValue&&""!==t&&null!==t&&(e.value=t)}var Oe="http://www.w3.org/1999/xhtml",ze="http://www.w3.org/2000/svg";function Le(e){switch(e){case"svg":return"http://www.w3.org/2000/svg";case"math":return"http://www.w3.org/1998/Math/MathML";default:return"http://www.w3.org/1999/xhtml"}}function Ue(e,t){return null==e||"http://www.w3.org/1999/xhtml"===e?Le(t):"http://www.w3.org/2000/svg"===e&&"foreignObject"===t?"http://www.w3.org/1999/xhtml":e}var Fe,Ie=function(e){return"undefined"!==typeof MSApp&&MSApp.execUnsafeLocalFunction?function(t,n,r,i){MSApp.execUnsafeLocalFunction((function(){return e(t,n)}))}:e}((function(e,t){if(e.namespaceURI!==ze||"innerHTML"in e)e.innerHTML=t;else{for((Fe=Fe||document.createElement("div")).innerHTML="<svg>"+t.valueOf().toString()+"</svg>",t=Fe.firstChild;e.firstChild;)e.removeChild(e.firstChild);for(;t.firstChild;)e.appendChild(t.firstChild)}}));function je(e,t){if(t){var n=e.firstChild;if(n&&n===e.lastChild&&3===n.nodeType)return void(n.nodeValue=t)}e.textContent=t}function He(e,t){var n={};return n[e.toLowerCase()]=t.toLowerCase(),n["Webkit"+e]="webkit"+t,n["Moz"+e]="moz"+t,n}var Ve={animationend:He("Animation","AnimationEnd"),animationiteration:He("Animation","AnimationIteration"),animationstart:He("Animation","AnimationStart"),transitionend:He("Transition","TransitionEnd")},$e={},We={};function Qe(e){if($e[e])return $e[e];if(!Ve[e])return e;var t,n=Ve[e];for(t in n)if(n.hasOwnProperty(t)&&t in We)return $e[e]=n[t];return e}S&&(We=document.createElement("div").style,"AnimationEvent"in window||(delete Ve.animationend.animation,delete Ve.animationiteration.animation,delete Ve.animationstart.animation),"TransitionEvent"in window||delete Ve.transitionend.transition);var qe=Qe("animationend"),Be=Qe("animationiteration"),Ye=Qe("animationstart"),Xe=Qe("transitionend"),Ke="abort canplay canplaythrough durationchange emptied encrypted ended error loadeddata loadedmetadata loadstart pause play playing progress ratechange seeked seeking stalled suspend timeupdate volumechange waiting".split(" "),Ze=new("function"===typeof WeakMap?WeakMap:Map);function Je(e){var t=Ze.get(e);return void 0===t&&(t=new Map,Ze.set(e,t)),t}function Ge(e){var t=e,n=e;if(e.alternate)for(;t.return;)t=t.return;else{e=t;do{0!==(1026&(t=e).effectTag)&&(n=t.return),e=t.return}while(e)}return 3===t.tag?n:null}function et(e){if(13===e.tag){var t=e.memoizedState;if(null===t&&(null!==(e=e.alternate)&&(t=e.memoizedState)),null!==t)return t.dehydrated}return null}function tt(e){if(Ge(e)!==e)throw Error(u(188))}function nt(e){if(!(e=function(e){var t=e.alternate;if(!t){if(null===(t=Ge(e)))throw Error(u(188));return t!==e?null:e}for(var n=e,r=t;;){var i=n.return;if(null===i)break;var o=i.alternate;if(null===o){if(null!==(r=i.return)){n=r;continue}break}if(i.child===o.child){for(o=i.child;o;){if(o===n)return tt(i),e;if(o===r)return tt(i),t;o=o.sibling}throw Error(u(188))}if(n.return!==r.return)n=i,r=o;else{for(var a=!1,l=i.child;l;){if(l===n){a=!0,n=i,r=o;break}if(l===r){a=!0,r=i,n=o;break}l=l.sibling}if(!a){for(l=o.child;l;){if(l===n){a=!0,n=o,r=i;break}if(l===r){a=!0,r=o,n=i;break}l=l.sibling}if(!a)throw Error(u(189))}}if(n.alternate!==r)throw Error(u(190))}if(3!==n.tag)throw Error(u(188));return n.stateNode.current===n?e:t}(e)))return null;for(var t=e;;){if(5===t.tag||6===t.tag)return t;if(t.child)t.child.return=t,t=t.child;else{if(t===e)break;for(;!t.sibling;){if(!t.return||t.return===e)return null;t=t.return}t.sibling.return=t.return,t=t.sibling}}return null}function rt(e,t){if(null==t)throw Error(u(30));return null==e?t:Array.isArray(e)?Array.isArray(t)?(e.push.apply(e,t),e):(e.push(t),e):Array.isArray(t)?[e].concat(t):[e,t]}function it(e,t,n){Array.isArray(e)?e.forEach(t,n):e&&t.call(n,e)}var ot=null;function ut(e){if(e){var t=e._dispatchListeners,n=e._dispatchInstances;if(Array.isArray(t))for(var r=0;r<t.length&&!e.isPropagationStopped();r++)v(e,t[r],n[r]);else t&&v(e,t,n);e._dispatchListeners=null,e._dispatchInstances=null,e.isPersistent()||e.constructor.release(e)}}function at(e){if(null!==e&&(ot=rt(ot,e)),e=ot,ot=null,e){if(it(e,ut),ot)throw Error(u(95));if(s)throw e=f,s=!1,f=null,e}}function lt(e){return(e=e.target||e.srcElement||window).correspondingUseElement&&(e=e.correspondingUseElement),3===e.nodeType?e.parentNode:e}function ct(e){if(!S)return!1;var t=(e="on"+e)in document;return t||((t=document.createElement("div")).setAttribute(e,"return;"),t="function"===typeof t[e]),t}var st=[];function ft(e){e.topLevelType=null,e.nativeEvent=null,e.targetInst=null,e.ancestors.length=0,10>st.length&&st.push(e)}function dt(e,t,n,r){if(st.length){var i=st.pop();return i.topLevelType=e,i.eventSystemFlags=r,i.nativeEvent=t,i.targetInst=n,i}return{topLevelType:e,eventSystemFlags:r,nativeEvent:t,targetInst:n,ancestors:[]}}function pt(e){var t=e.targetInst,n=t;do{if(!n){e.ancestors.push(n);break}var r=n;if(3===r.tag)r=r.stateNode.containerInfo;else{for(;r.return;)r=r.return;r=3!==r.tag?null:r.stateNode.containerInfo}if(!r)break;5!==(t=n.tag)&&6!==t||e.ancestors.push(n),n=Sn(r)}while(n);for(n=0;n<e.ancestors.length;n++){t=e.ancestors[n];var i=lt(e.nativeEvent);r=e.topLevelType;var o=e.nativeEvent,u=e.eventSystemFlags;0===n&&(u|=64);for(var a=null,l=0;l<_.length;l++){var c=_[l];c&&(c=c.extractEvents(r,t,o,i,u))&&(a=rt(a,c))}at(a)}}function ht(e,t,n){if(!n.has(e)){switch(e){case"scroll":Yt(t,"scroll",!0);break;case"focus":case"blur":Yt(t,"focus",!0),Yt(t,"blur",!0),n.set("blur",null),n.set("focus",null);break;case"cancel":case"close":ct(e)&&Yt(t,e,!0);break;case"invalid":case"submit":case"reset":break;default:-1===Ke.indexOf(e)&&Bt(e,t)}n.set(e,null)}}var mt,gt,vt,yt=!1,bt=[],wt=null,xt=null,_t=null,kt=new Map,Tt=new Map,Et=[],Ct="mousedown mouseup touchcancel touchend touchstart auxclick dblclick pointercancel pointerdown pointerup dragend dragstart drop compositionend compositionstart keydown keypress keyup input textInput close cancel copy cut paste click change contextmenu reset submit".split(" "),St="focus blur dragenter dragleave mouseover mouseout pointerover pointerout gotpointercapture lostpointercapture".split(" ");function Mt(e,t,n,r,i){return{blockedOn:e,topLevelType:t,eventSystemFlags:32|n,nativeEvent:i,container:r}}function Nt(e,t){switch(e){case"focus":case"blur":wt=null;break;case"dragenter":case"dragleave":xt=null;break;case"mouseover":case"mouseout":_t=null;break;case"pointerover":case"pointerout":kt.delete(t.pointerId);break;case"gotpointercapture":case"lostpointercapture":Tt.delete(t.pointerId)}}function Pt(e,t,n,r,i,o){return null===e||e.nativeEvent!==o?(e=Mt(t,n,r,i,o),null!==t&&(null!==(t=Mn(t))&&gt(t)),e):(e.eventSystemFlags|=r,e)}function At(e){var t=Sn(e.target);if(null!==t){var n=Ge(t);if(null!==n)if(13===(t=n.tag)){if(null!==(t=et(n)))return e.blockedOn=t,void o.unstable_runWithPriority(e.priority,(function(){vt(n)}))}else if(3===t&&n.stateNode.hydrate)return void(e.blockedOn=3===n.tag?n.stateNode.containerInfo:null)}e.blockedOn=null}function Rt(e){if(null!==e.blockedOn)return!1;var t=Jt(e.topLevelType,e.eventSystemFlags,e.container,e.nativeEvent);if(null!==t){var n=Mn(t);return null!==n&&gt(n),e.blockedOn=t,!1}return!0}function Dt(e,t,n){Rt(e)&&n.delete(t)}function Ot(){for(yt=!1;0<bt.length;){var e=bt[0];if(null!==e.blockedOn){null!==(e=Mn(e.blockedOn))&&mt(e);break}var t=Jt(e.topLevelType,e.eventSystemFlags,e.container,e.nativeEvent);null!==t?e.blockedOn=t:bt.shift()}null!==wt&&Rt(wt)&&(wt=null),null!==xt&&Rt(xt)&&(xt=null),null!==_t&&Rt(_t)&&(_t=null),kt.forEach(Dt),Tt.forEach(Dt)}function zt(e,t){e.blockedOn===t&&(e.blockedOn=null,yt||(yt=!0,o.unstable_scheduleCallback(o.unstable_NormalPriority,Ot)))}function Lt(e){function t(t){return zt(t,e)}if(0<bt.length){zt(bt[0],e);for(var n=1;n<bt.length;n++){var r=bt[n];r.blockedOn===e&&(r.blockedOn=null)}}for(null!==wt&&zt(wt,e),null!==xt&&zt(xt,e),null!==_t&&zt(_t,e),kt.forEach(t),Tt.forEach(t),n=0;n<Et.length;n++)(r=Et[n]).blockedOn===e&&(r.blockedOn=null);for(;0<Et.length&&null===(n=Et[0]).blockedOn;)At(n),null===n.blockedOn&&Et.shift()}var Ut={},Ft=new Map,It=new Map,jt=["abort","abort",qe,"animationEnd",Be,"animationIteration",Ye,"animationStart","canplay","canPlay","canplaythrough","canPlayThrough","durationchange","durationChange","emptied","emptied","encrypted","encrypted","ended","ended","error","error","gotpointercapture","gotPointerCapture","load","load","loadeddata","loadedData","loadedmetadata","loadedMetadata","loadstart","loadStart","lostpointercapture","lostPointerCapture","playing","playing","progress","progress","seeking","seeking","stalled","stalled","suspend","suspend","timeupdate","timeUpdate",Xe,"transitionEnd","waiting","waiting"];function Ht(e,t){for(var n=0;n<e.length;n+=2){var r=e[n],i=e[n+1],o="on"+(i[0].toUpperCase()+i.slice(1));o={phasedRegistrationNames:{bubbled:o,captured:o+"Capture"},dependencies:[r],eventPriority:t},It.set(r,t),Ft.set(r,o),Ut[i]=o}}Ht("blur blur cancel cancel click click close close contextmenu contextMenu copy copy cut cut auxclick auxClick dblclick doubleClick dragend dragEnd dragstart dragStart drop drop focus focus input input invalid invalid keydown keyDown keypress keyPress keyup keyUp mousedown mouseDown mouseup mouseUp paste paste pause pause play play pointercancel pointerCancel pointerdown pointerDown pointerup pointerUp ratechange rateChange reset reset seeked seeked submit submit touchcancel touchCancel touchend touchEnd touchstart touchStart volumechange volumeChange".split(" "),0),Ht("drag drag dragenter dragEnter dragexit dragExit dragleave dragLeave dragover dragOver mousemove mouseMove mouseout mouseOut mouseover mouseOver pointermove pointerMove pointerout pointerOut pointerover pointerOver scroll scroll toggle toggle touchmove touchMove wheel wheel".split(" "),1),Ht(jt,2);for(var Vt="change selectionchange textInput compositionstart compositionend compositionupdate".split(" "),$t=0;$t<Vt.length;$t++)It.set(Vt[$t],0);var Wt=o.unstable_UserBlockingPriority,Qt=o.unstable_runWithPriority,qt=!0;function Bt(e,t){Yt(t,e,!1)}function Yt(e,t,n){var r=It.get(t);switch(void 0===r?2:r){case 0:r=Xt.bind(null,t,1,e);break;case 1:r=Kt.bind(null,t,1,e);break;default:r=Zt.bind(null,t,1,e)}n?e.addEventListener(t,r,!0):e.addEventListener(t,r,!1)}function Xt(e,t,n,r){F||L();var i=Zt,o=F;F=!0;try{z(i,e,t,n,r)}finally{(F=o)||j()}}function Kt(e,t,n,r){Qt(Wt,Zt.bind(null,e,t,n,r))}function Zt(e,t,n,r){if(qt)if(0<bt.length&&-1<Ct.indexOf(e))e=Mt(null,e,t,n,r),bt.push(e);else{var i=Jt(e,t,n,r);if(null===i)Nt(e,r);else if(-1<Ct.indexOf(e))e=Mt(i,e,t,n,r),bt.push(e);else if(!function(e,t,n,r,i){switch(t){case"focus":return wt=Pt(wt,e,t,n,r,i),!0;case"dragenter":return xt=Pt(xt,e,t,n,r,i),!0;case"mouseover":return _t=Pt(_t,e,t,n,r,i),!0;case"pointerover":var o=i.pointerId;return kt.set(o,Pt(kt.get(o)||null,e,t,n,r,i)),!0;case"gotpointercapture":return o=i.pointerId,Tt.set(o,Pt(Tt.get(o)||null,e,t,n,r,i)),!0}return!1}(i,e,t,n,r)){Nt(e,r),e=dt(e,r,null,t);try{H(pt,e)}finally{ft(e)}}}}function Jt(e,t,n,r){if(null!==(n=Sn(n=lt(r)))){var i=Ge(n);if(null===i)n=null;else{var o=i.tag;if(13===o){if(null!==(n=et(i)))return n;n=null}else if(3===o){if(i.stateNode.hydrate)return 3===i.tag?i.stateNode.containerInfo:null;n=null}else i!==n&&(n=null)}}e=dt(e,r,n,t);try{H(pt,e)}finally{ft(e)}return null}var Gt={animationIterationCount:!0,borderImageOutset:!0,borderImageSlice:!0,borderImageWidth:!0,boxFlex:!0,boxFlexGroup:!0,boxOrdinalGroup:!0,columnCount:!0,columns:!0,flex:!0,flexGrow:!0,flexPositive:!0,flexShrink:!0,flexNegative:!0,flexOrder:!0,gridArea:!0,gridRow:!0,gridRowEnd:!0,gridRowSpan:!0,gridRowStart:!0,gridColumn:!0,gridColumnEnd:!0,gridColumnSpan:!0,gridColumnStart:!0,fontWeight:!0,lineClamp:!0,lineHeight:!0,opacity:!0,order:!0,orphans:!0,tabSize:!0,widows:!0,zIndex:!0,zoom:!0,fillOpacity:!0,floodOpacity:!0,stopOpacity:!0,strokeDasharray:!0,strokeDashoffset:!0,strokeMiterlimit:!0,strokeOpacity:!0,strokeWidth:!0},en=["Webkit","ms","Moz","O"];function tn(e,t,n){return null==t||"boolean"===typeof t||""===t?"":n||"number"!==typeof t||0===t||Gt.hasOwnProperty(e)&&Gt[e]?(""+t).trim():t+"px"}function nn(e,t){for(var n in e=e.style,t)if(t.hasOwnProperty(n)){var r=0===n.indexOf("--"),i=tn(n,t[n],r);"float"===n&&(n="cssFloat"),r?e.setProperty(n,i):e[n]=i}}Object.keys(Gt).forEach((function(e){en.forEach((function(t){t=t+e.charAt(0).toUpperCase()+e.substring(1),Gt[t]=Gt[e]}))}));var rn=i({menuitem:!0},{area:!0,base:!0,br:!0,col:!0,embed:!0,hr:!0,img:!0,input:!0,keygen:!0,link:!0,meta:!0,param:!0,source:!0,track:!0,wbr:!0});function on(e,t){if(t){if(rn[e]&&(null!=t.children||null!=t.dangerouslySetInnerHTML))throw Error(u(137,e,""));if(null!=t.dangerouslySetInnerHTML){if(null!=t.children)throw Error(u(60));if("object"!==typeof t.dangerouslySetInnerHTML||!("__html"in t.dangerouslySetInnerHTML))throw Error(u(61))}if(null!=t.style&&"object"!==typeof t.style)throw Error(u(62,""))}}function un(e,t){if(-1===e.indexOf("-"))return"string"===typeof t.is;switch(e){case"annotation-xml":case"color-profile":case"font-face":case"font-face-src":case"font-face-uri":case"font-face-format":case"font-face-name":case"missing-glyph":return!1;default:return!0}}var an=Oe;function ln(e,t){var n=Je(e=9===e.nodeType||11===e.nodeType?e:e.ownerDocument);t=E[t];for(var r=0;r<t.length;r++)ht(t[r],e,n)}function cn(){}function sn(e){if("undefined"===typeof(e=e||("undefined"!==typeof document?document:void 0)))return null;try{return e.activeElement||e.body}catch(t){return e.body}}function fn(e){for(;e&&e.firstChild;)e=e.firstChild;return e}function dn(e,t){var n,r=fn(e);for(e=0;r;){if(3===r.nodeType){if(n=e+r.textContent.length,e<=t&&n>=t)return{node:r,offset:t-e};e=n}e:{for(;r;){if(r.nextSibling){r=r.nextSibling;break e}r=r.parentNode}r=void 0}r=fn(r)}}function pn(){for(var e=window,t=sn();t instanceof e.HTMLIFrameElement;){try{var n="string"===typeof t.contentWindow.location.href}catch(r){n=!1}if(!n)break;t=sn((e=t.contentWindow).document)}return t}function hn(e){var t=e&&e.nodeName&&e.nodeName.toLowerCase();return t&&("input"===t&&("text"===e.type||"search"===e.type||"tel"===e.type||"url"===e.type||"password"===e.type)||"textarea"===t||"true"===e.contentEditable)}var mn=null,gn=null;function vn(e,t){switch(e){case"button":case"input":case"select":case"textarea":return!!t.autoFocus}return!1}function yn(e,t){return"textarea"===e||"option"===e||"noscript"===e||"string"===typeof t.children||"number"===typeof t.children||"object"===typeof t.dangerouslySetInnerHTML&&null!==t.dangerouslySetInnerHTML&&null!=t.dangerouslySetInnerHTML.__html}var bn="function"===typeof setTimeout?setTimeout:void 0,wn="function"===typeof clearTimeout?clearTimeout:void 0;function xn(e){for(;null!=e;e=e.nextSibling){var t=e.nodeType;if(1===t||3===t)break}return e}function _n(e){e=e.previousSibling;for(var t=0;e;){if(8===e.nodeType){var n=e.data;if("$"===n||"$!"===n||"$?"===n){if(0===t)return e;t--}else"/$"===n&&t++}e=e.previousSibling}return null}var kn=Math.random().toString(36).slice(2),Tn="__reactInternalInstance$"+kn,En="__reactEventHandlers$"+kn,Cn="__reactContainere$"+kn;function Sn(e){var t=e[Tn];if(t)return t;for(var n=e.parentNode;n;){if(t=n[Cn]||n[Tn]){if(n=t.alternate,null!==t.child||null!==n&&null!==n.child)for(e=_n(e);null!==e;){if(n=e[Tn])return n;e=_n(e)}return t}n=(e=n).parentNode}return null}function Mn(e){return!(e=e[Tn]||e[Cn])||5!==e.tag&&6!==e.tag&&13!==e.tag&&3!==e.tag?null:e}function Nn(e){if(5===e.tag||6===e.tag)return e.stateNode;throw Error(u(33))}function Pn(e){return e[En]||null}function An(e){do{e=e.return}while(e&&5!==e.tag);return e||null}function Rn(e,t){var n=e.stateNode;if(!n)return null;var r=h(n);if(!r)return null;n=r[t];e:switch(t){case"onClick":case"onClickCapture":case"onDoubleClick":case"onDoubleClickCapture":case"onMouseDown":case"onMouseDownCapture":case"onMouseMove":case"onMouseMoveCapture":case"onMouseUp":case"onMouseUpCapture":case"onMouseEnter":(r=!r.disabled)||(r=!("button"===(e=e.type)||"input"===e||"select"===e||"textarea"===e)),e=!r;break e;default:e=!1}if(e)return null;if(n&&"function"!==typeof n)throw Error(u(231,t,typeof n));return n}function Dn(e,t,n){(t=Rn(e,n.dispatchConfig.phasedRegistrationNames[t]))&&(n._dispatchListeners=rt(n._dispatchListeners,t),n._dispatchInstances=rt(n._dispatchInstances,e))}function On(e){if(e&&e.dispatchConfig.phasedRegistrationNames){for(var t=e._targetInst,n=[];t;)n.push(t),t=An(t);for(t=n.length;0<t--;)Dn(n[t],"captured",e);for(t=0;t<n.length;t++)Dn(n[t],"bubbled",e)}}function zn(e,t,n){e&&n&&n.dispatchConfig.registrationName&&(t=Rn(e,n.dispatchConfig.registrationName))&&(n._dispatchListeners=rt(n._dispatchListeners,t),n._dispatchInstances=rt(n._dispatchInstances,e))}function Ln(e){e&&e.dispatchConfig.registrationName&&zn(e._targetInst,null,e)}function Un(e){it(e,On)}var Fn=null,In=null,jn=null;function Hn(){if(jn)return jn;var e,t,n=In,r=n.length,i="value"in Fn?Fn.value:Fn.textContent,o=i.length;for(e=0;e<r&&n[e]===i[e];e++);var u=r-e;for(t=1;t<=u&&n[r-t]===i[o-t];t++);return jn=i.slice(e,1<t?1-t:void 0)}function Vn(){return!0}function $n(){return!1}function Wn(e,t,n,r){for(var i in this.dispatchConfig=e,this._targetInst=t,this.nativeEvent=n,e=this.constructor.Interface)e.hasOwnProperty(i)&&((t=e[i])?this[i]=t(n):"target"===i?this.target=r:this[i]=n[i]);return this.isDefaultPrevented=(null!=n.defaultPrevented?n.defaultPrevented:!1===n.returnValue)?Vn:$n,this.isPropagationStopped=$n,this}function Qn(e,t,n,r){if(this.eventPool.length){var i=this.eventPool.pop();return this.call(i,e,t,n,r),i}return new this(e,t,n,r)}function qn(e){if(!(e instanceof this))throw Error(u(279));e.destructor(),10>this.eventPool.length&&this.eventPool.push(e)}function Bn(e){e.eventPool=[],e.getPooled=Qn,e.release=qn}i(Wn.prototype,{preventDefault:function(){this.defaultPrevented=!0;var e=this.nativeEvent;e&&(e.preventDefault?e.preventDefault():"unknown"!==typeof e.returnValue&&(e.returnValue=!1),this.isDefaultPrevented=Vn)},stopPropagation:function(){var e=this.nativeEvent;e&&(e.stopPropagation?e.stopPropagation():"unknown"!==typeof e.cancelBubble&&(e.cancelBubble=!0),this.isPropagationStopped=Vn)},persist:function(){this.isPersistent=Vn},isPersistent:$n,destructor:function(){var e,t=this.constructor.Interface;for(e in t)this[e]=null;this.nativeEvent=this._targetInst=this.dispatchConfig=null,this.isPropagationStopped=this.isDefaultPrevented=$n,this._dispatchInstances=this._dispatchListeners=null}}),Wn.Interface={type:null,target:null,currentTarget:function(){return null},eventPhase:null,bubbles:null,cancelable:null,timeStamp:function(e){return e.timeStamp||Date.now()},defaultPrevented:null,isTrusted:null},Wn.extend=function(e){function t(){}function n(){return r.apply(this,arguments)}var r=this;t.prototype=r.prototype;var o=new t;return i(o,n.prototype),n.prototype=o,n.prototype.constructor=n,n.Interface=i({},r.Interface,e),n.extend=r.extend,Bn(n),n},Bn(Wn);var Yn=Wn.extend({data:null}),Xn=Wn.extend({data:null}),Kn=[9,13,27,32],Zn=S&&"CompositionEvent"in window,Jn=null;S&&"documentMode"in document&&(Jn=document.documentMode);var Gn=S&&"TextEvent"in window&&!Jn,er=S&&(!Zn||Jn&&8<Jn&&11>=Jn),tr=String.fromCharCode(32),nr={beforeInput:{phasedRegistrationNames:{bubbled:"onBeforeInput",captured:"onBeforeInputCapture"},dependencies:["compositionend","keypress","textInput","paste"]},compositionEnd:{phasedRegistrationNames:{bubbled:"onCompositionEnd",captured:"onCompositionEndCapture"},dependencies:"blur compositionend keydown keypress keyup mousedown".split(" ")},compositionStart:{phasedRegistrationNames:{bubbled:"onCompositionStart",captured:"onCompositionStartCapture"},dependencies:"blur compositionstart keydown keypress keyup mousedown".split(" ")},compositionUpdate:{phasedRegistrationNames:{bubbled:"onCompositionUpdate",captured:"onCompositionUpdateCapture"},dependencies:"blur compositionupdate keydown keypress keyup mousedown".split(" ")}},rr=!1;function ir(e,t){switch(e){case"keyup":return-1!==Kn.indexOf(t.keyCode);case"keydown":return 229!==t.keyCode;case"keypress":case"mousedown":case"blur":return!0;default:return!1}}function or(e){return"object"===typeof(e=e.detail)&&"data"in e?e.data:null}var ur=!1;var ar={eventTypes:nr,extractEvents:function(e,t,n,r){var i;if(Zn)e:{switch(e){case"compositionstart":var o=nr.compositionStart;break e;case"compositionend":o=nr.compositionEnd;break e;case"compositionupdate":o=nr.compositionUpdate;break e}o=void 0}else ur?ir(e,n)&&(o=nr.compositionEnd):"keydown"===e&&229===n.keyCode&&(o=nr.compositionStart);return o?(er&&"ko"!==n.locale&&(ur||o!==nr.compositionStart?o===nr.compositionEnd&&ur&&(i=Hn()):(In="value"in(Fn=r)?Fn.value:Fn.textContent,ur=!0)),o=Yn.getPooled(o,t,n,r),i?o.data=i:null!==(i=or(n))&&(o.data=i),Un(o),i=o):i=null,(e=Gn?function(e,t){switch(e){case"compositionend":return or(t);case"keypress":return 32!==t.which?null:(rr=!0,tr);case"textInput":return(e=t.data)===tr&&rr?null:e;default:return null}}(e,n):function(e,t){if(ur)return"compositionend"===e||!Zn&&ir(e,t)?(e=Hn(),jn=In=Fn=null,ur=!1,e):null;switch(e){case"paste":return null;case"keypress":if(!(t.ctrlKey||t.altKey||t.metaKey)||t.ctrlKey&&t.altKey){if(t.char&&1<t.char.length)return t.char;if(t.which)return String.fromCharCode(t.which)}return null;case"compositionend":return er&&"ko"!==t.locale?null:t.data;default:return null}}(e,n))?((t=Xn.getPooled(nr.beforeInput,t,n,r)).data=e,Un(t)):t=null,null===i?t:null===t?i:[i,t]}},lr={color:!0,date:!0,datetime:!0,"datetime-local":!0,email:!0,month:!0,number:!0,password:!0,range:!0,search:!0,tel:!0,text:!0,time:!0,url:!0,week:!0};function cr(e){var t=e&&e.nodeName&&e.nodeName.toLowerCase();return"input"===t?!!lr[e.type]:"textarea"===t}var sr={change:{phasedRegistrationNames:{bubbled:"onChange",captured:"onChangeCapture"},dependencies:"blur change click focus input keydown keyup selectionchange".split(" ")}};function fr(e,t,n){return(e=Wn.getPooled(sr.change,e,t,n)).type="change",R(n),Un(e),e}var dr=null,pr=null;function hr(e){at(e)}function mr(e){if(xe(Nn(e)))return e}function gr(e,t){if("change"===e)return t}var vr=!1;function yr(){dr&&(dr.detachEvent("onpropertychange",br),pr=dr=null)}function br(e){if("value"===e.propertyName&&mr(pr))if(e=fr(pr,e,lt(e)),F)at(e);else{F=!0;try{O(hr,e)}finally{F=!1,j()}}}function wr(e,t,n){"focus"===e?(yr(),pr=n,(dr=t).attachEvent("onpropertychange",br)):"blur"===e&&yr()}function xr(e){if("selectionchange"===e||"keyup"===e||"keydown"===e)return mr(pr)}function _r(e,t){if("click"===e)return mr(t)}function kr(e,t){if("input"===e||"change"===e)return mr(t)}S&&(vr=ct("input")&&(!document.documentMode||9<document.documentMode));var Tr={eventTypes:sr,_isInputEventSupported:vr,extractEvents:function(e,t,n,r){var i=t?Nn(t):window,o=i.nodeName&&i.nodeName.toLowerCase();if("select"===o||"input"===o&&"file"===i.type)var u=gr;else if(cr(i))if(vr)u=kr;else{u=xr;var a=wr}else(o=i.nodeName)&&"input"===o.toLowerCase()&&("checkbox"===i.type||"radio"===i.type)&&(u=_r);if(u&&(u=u(e,t)))return fr(u,n,r);a&&a(e,i,t),"blur"===e&&(e=i._wrapperState)&&e.controlled&&"number"===i.type&&Se(i,"number",i.value)}},Er=Wn.extend({view:null,detail:null}),Cr={Alt:"altKey",Control:"ctrlKey",Meta:"metaKey",Shift:"shiftKey"};function Sr(e){var t=this.nativeEvent;return t.getModifierState?t.getModifierState(e):!!(e=Cr[e])&&!!t[e]}function Mr(){return Sr}var Nr=0,Pr=0,Ar=!1,Rr=!1,Dr=Er.extend({screenX:null,screenY:null,clientX:null,clientY:null,pageX:null,pageY:null,ctrlKey:null,shiftKey:null,altKey:null,metaKey:null,getModifierState:Mr,button:null,buttons:null,relatedTarget:function(e){return e.relatedTarget||(e.fromElement===e.srcElement?e.toElement:e.fromElement)},movementX:function(e){if("movementX"in e)return e.movementX;var t=Nr;return Nr=e.screenX,Ar?"mousemove"===e.type?e.screenX-t:0:(Ar=!0,0)},movementY:function(e){if("movementY"in e)return e.movementY;var t=Pr;return Pr=e.screenY,Rr?"mousemove"===e.type?e.screenY-t:0:(Rr=!0,0)}}),Or=Dr.extend({pointerId:null,width:null,height:null,pressure:null,tangentialPressure:null,tiltX:null,tiltY:null,twist:null,pointerType:null,isPrimary:null}),zr={mouseEnter:{registrationName:"onMouseEnter",dependencies:["mouseout","mouseover"]},mouseLeave:{registrationName:"onMouseLeave",dependencies:["mouseout","mouseover"]},pointerEnter:{registrationName:"onPointerEnter",dependencies:["pointerout","pointerover"]},pointerLeave:{registrationName:"onPointerLeave",dependencies:["pointerout","pointerover"]}},Lr={eventTypes:zr,extractEvents:function(e,t,n,r,i){var o="mouseover"===e||"pointerover"===e,u="mouseout"===e||"pointerout"===e;if(o&&0===(32&i)&&(n.relatedTarget||n.fromElement)||!u&&!o)return null;(o=r.window===r?r:(o=r.ownerDocument)?o.defaultView||o.parentWindow:window,u)?(u=t,null!==(t=(t=n.relatedTarget||n.toElement)?Sn(t):null)&&(t!==Ge(t)||5!==t.tag&&6!==t.tag)&&(t=null)):u=null;if(u===t)return null;if("mouseout"===e||"mouseover"===e)var a=Dr,l=zr.mouseLeave,c=zr.mouseEnter,s="mouse";else"pointerout"!==e&&"pointerover"!==e||(a=Or,l=zr.pointerLeave,c=zr.pointerEnter,s="pointer");if(e=null==u?o:Nn(u),o=null==t?o:Nn(t),(l=a.getPooled(l,u,n,r)).type=s+"leave",l.target=e,l.relatedTarget=o,(n=a.getPooled(c,t,n,r)).type=s+"enter",n.target=o,n.relatedTarget=e,s=t,(r=u)&&s)e:{for(c=s,u=0,e=a=r;e;e=An(e))u++;for(e=0,t=c;t;t=An(t))e++;for(;0<u-e;)a=An(a),u--;for(;0<e-u;)c=An(c),e--;for(;u--;){if(a===c||a===c.alternate)break e;a=An(a),c=An(c)}a=null}else a=null;for(c=a,a=[];r&&r!==c&&(null===(u=r.alternate)||u!==c);)a.push(r),r=An(r);for(r=[];s&&s!==c&&(null===(u=s.alternate)||u!==c);)r.push(s),s=An(s);for(s=0;s<a.length;s++)zn(a[s],"bubbled",l);for(s=r.length;0<s--;)zn(r[s],"captured",n);return 0===(64&i)?[l]:[l,n]}};var Ur="function"===typeof Object.is?Object.is:function(e,t){return e===t&&(0!==e||1/e===1/t)||e!==e&&t!==t},Fr=Object.prototype.hasOwnProperty;function Ir(e,t){if(Ur(e,t))return!0;if("object"!==typeof e||null===e||"object"!==typeof t||null===t)return!1;var n=Object.keys(e),r=Object.keys(t);if(n.length!==r.length)return!1;for(r=0;r<n.length;r++)if(!Fr.call(t,n[r])||!Ur(e[n[r]],t[n[r]]))return!1;return!0}var jr=S&&"documentMode"in document&&11>=document.documentMode,Hr={select:{phasedRegistrationNames:{bubbled:"onSelect",captured:"onSelectCapture"},dependencies:"blur contextmenu dragend focus keydown keyup mousedown mouseup selectionchange".split(" ")}},Vr=null,$r=null,Wr=null,Qr=!1;function qr(e,t){var n=t.window===t?t.document:9===t.nodeType?t:t.ownerDocument;return Qr||null==Vr||Vr!==sn(n)?null:("selectionStart"in(n=Vr)&&hn(n)?n={start:n.selectionStart,end:n.selectionEnd}:n={anchorNode:(n=(n.ownerDocument&&n.ownerDocument.defaultView||window).getSelection()).anchorNode,anchorOffset:n.anchorOffset,focusNode:n.focusNode,focusOffset:n.focusOffset},Wr&&Ir(Wr,n)?null:(Wr=n,(e=Wn.getPooled(Hr.select,$r,e,t)).type="select",e.target=Vr,Un(e),e))}var Br={eventTypes:Hr,extractEvents:function(e,t,n,r,i,o){if(!(o=!(i=o||(r.window===r?r.document:9===r.nodeType?r:r.ownerDocument)))){e:{i=Je(i),o=E.onSelect;for(var u=0;u<o.length;u++)if(!i.has(o[u])){i=!1;break e}i=!0}o=!i}if(o)return null;switch(i=t?Nn(t):window,e){case"focus":(cr(i)||"true"===i.contentEditable)&&(Vr=i,$r=t,Wr=null);break;case"blur":Wr=$r=Vr=null;break;case"mousedown":Qr=!0;break;case"contextmenu":case"mouseup":case"dragend":return Qr=!1,qr(n,r);case"selectionchange":if(jr)break;case"keydown":case"keyup":return qr(n,r)}return null}},Yr=Wn.extend({animationName:null,elapsedTime:null,pseudoElement:null}),Xr=Wn.extend({clipboardData:function(e){return"clipboardData"in e?e.clipboardData:window.clipboardData}}),Kr=Er.extend({relatedTarget:null});function Zr(e){var t=e.keyCode;return"charCode"in e?0===(e=e.charCode)&&13===t&&(e=13):e=t,10===e&&(e=13),32<=e||13===e?e:0}var Jr={Esc:"Escape",Spacebar:" ",Left:"ArrowLeft",Up:"ArrowUp",Right:"ArrowRight",Down:"ArrowDown",Del:"Delete",Win:"OS",Menu:"ContextMenu",Apps:"ContextMenu",Scroll:"ScrollLock",MozPrintableKey:"Unidentified"},Gr={8:"Backspace",9:"Tab",12:"Clear",13:"Enter",16:"Shift",17:"Control",18:"Alt",19:"Pause",20:"CapsLock",27:"Escape",32:" ",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"ArrowLeft",38:"ArrowUp",39:"ArrowRight",40:"ArrowDown",45:"Insert",46:"Delete",112:"F1",113:"F2",114:"F3",115:"F4",116:"F5",117:"F6",118:"F7",119:"F8",120:"F9",121:"F10",122:"F11",123:"F12",144:"NumLock",145:"ScrollLock",224:"Meta"},ei=Er.extend({key:function(e){if(e.key){var t=Jr[e.key]||e.key;if("Unidentified"!==t)return t}return"keypress"===e.type?13===(e=Zr(e))?"Enter":String.fromCharCode(e):"keydown"===e.type||"keyup"===e.type?Gr[e.keyCode]||"Unidentified":""},location:null,ctrlKey:null,shiftKey:null,altKey:null,metaKey:null,repeat:null,locale:null,getModifierState:Mr,charCode:function(e){return"keypress"===e.type?Zr(e):0},keyCode:function(e){return"keydown"===e.type||"keyup"===e.type?e.keyCode:0},which:function(e){return"keypress"===e.type?Zr(e):"keydown"===e.type||"keyup"===e.type?e.keyCode:0}}),ti=Dr.extend({dataTransfer:null}),ni=Er.extend({touches:null,targetTouches:null,changedTouches:null,altKey:null,metaKey:null,ctrlKey:null,shiftKey:null,getModifierState:Mr}),ri=Wn.extend({propertyName:null,elapsedTime:null,pseudoElement:null}),ii=Dr.extend({deltaX:function(e){return"deltaX"in e?e.deltaX:"wheelDeltaX"in e?-e.wheelDeltaX:0},deltaY:function(e){return"deltaY"in e?e.deltaY:"wheelDeltaY"in e?-e.wheelDeltaY:"wheelDelta"in e?-e.wheelDelta:0},deltaZ:null,deltaMode:null}),oi={eventTypes:Ut,extractEvents:function(e,t,n,r){var i=Ft.get(e);if(!i)return null;switch(e){case"keypress":if(0===Zr(n))return null;case"keydown":case"keyup":e=ei;break;case"blur":case"focus":e=Kr;break;case"click":if(2===n.button)return null;case"auxclick":case"dblclick":case"mousedown":case"mousemove":case"mouseup":case"mouseout":case"mouseover":case"contextmenu":e=Dr;break;case"drag":case"dragend":case"dragenter":case"dragexit":case"dragleave":case"dragover":case"dragstart":case"drop":e=ti;break;case"touchcancel":case"touchend":case"touchmove":case"touchstart":e=ni;break;case qe:case Be:case Ye:e=Yr;break;case Xe:e=ri;break;case"scroll":e=Er;break;case"wheel":e=ii;break;case"copy":case"cut":case"paste":e=Xr;break;case"gotpointercapture":case"lostpointercapture":case"pointercancel":case"pointerdown":case"pointermove":case"pointerout":case"pointerover":case"pointerup":e=Or;break;default:e=Wn}return Un(t=e.getPooled(i,t,n,r)),t}};if(y)throw Error(u(101));y=Array.prototype.slice.call("ResponderEventPlugin SimpleEventPlugin EnterLeaveEventPlugin ChangeEventPlugin SelectEventPlugin BeforeInputEventPlugin".split(" ")),w(),h=Pn,m=Mn,g=Nn,C({SimpleEventPlugin:oi,EnterLeaveEventPlugin:Lr,ChangeEventPlugin:Tr,SelectEventPlugin:Br,BeforeInputEventPlugin:ar});var ui=[],ai=-1;function li(e){0>ai||(e.current=ui[ai],ui[ai]=null,ai--)}function ci(e,t){ai++,ui[ai]=e.current,e.current=t}var si={},fi={current:si},di={current:!1},pi=si;function hi(e,t){var n=e.type.contextTypes;if(!n)return si;var r=e.stateNode;if(r&&r.__reactInternalMemoizedUnmaskedChildContext===t)return r.__reactInternalMemoizedMaskedChildContext;var i,o={};for(i in n)o[i]=t[i];return r&&((e=e.stateNode).__reactInternalMemoizedUnmaskedChildContext=t,e.__reactInternalMemoizedMaskedChildContext=o),o}function mi(e){return null!==(e=e.childContextTypes)&&void 0!==e}function gi(){li(di),li(fi)}function vi(e,t,n){if(fi.current!==si)throw Error(u(168));ci(fi,t),ci(di,n)}function yi(e,t,n){var r=e.stateNode;if(e=t.childContextTypes,"function"!==typeof r.getChildContext)return n;for(var o in r=r.getChildContext())if(!(o in e))throw Error(u(108,ge(t)||"Unknown",o));return i({},n,{},r)}function bi(e){return e=(e=e.stateNode)&&e.__reactInternalMemoizedMergedChildContext||si,pi=fi.current,ci(fi,e),ci(di,di.current),!0}function wi(e,t,n){var r=e.stateNode;if(!r)throw Error(u(169));n?(e=yi(e,t,pi),r.__reactInternalMemoizedMergedChildContext=e,li(di),li(fi),ci(fi,e)):li(di),ci(di,n)}var xi=o.unstable_runWithPriority,_i=o.unstable_scheduleCallback,ki=o.unstable_cancelCallback,Ti=o.unstable_requestPaint,Ei=o.unstable_now,Ci=o.unstable_getCurrentPriorityLevel,Si=o.unstable_ImmediatePriority,Mi=o.unstable_UserBlockingPriority,Ni=o.unstable_NormalPriority,Pi=o.unstable_LowPriority,Ai=o.unstable_IdlePriority,Ri={},Di=o.unstable_shouldYield,Oi=void 0!==Ti?Ti:function(){},zi=null,Li=null,Ui=!1,Fi=Ei(),Ii=1e4>Fi?Ei:function(){return Ei()-Fi};function ji(){switch(Ci()){case Si:return 99;case Mi:return 98;case Ni:return 97;case Pi:return 96;case Ai:return 95;default:throw Error(u(332))}}function Hi(e){switch(e){case 99:return Si;case 98:return Mi;case 97:return Ni;case 96:return Pi;case 95:return Ai;default:throw Error(u(332))}}function Vi(e,t){return e=Hi(e),xi(e,t)}function $i(e,t,n){return e=Hi(e),_i(e,t,n)}function Wi(e){return null===zi?(zi=[e],Li=_i(Si,qi)):zi.push(e),Ri}function Qi(){if(null!==Li){var e=Li;Li=null,ki(e)}qi()}function qi(){if(!Ui&&null!==zi){Ui=!0;var e=0;try{var t=zi;Vi(99,(function(){for(;e<t.length;e++){var n=t[e];do{n=n(!0)}while(null!==n)}})),zi=null}catch(n){throw null!==zi&&(zi=zi.slice(e+1)),_i(Si,Qi),n}finally{Ui=!1}}}function Bi(e,t,n){return 1073741821-(1+((1073741821-e+t/10)/(n/=10)|0))*n}function Yi(e,t){if(e&&e.defaultProps)for(var n in t=i({},t),e=e.defaultProps)void 0===t[n]&&(t[n]=e[n]);return t}var Xi={current:null},Ki=null,Zi=null,Ji=null;function Gi(){Ji=Zi=Ki=null}function eo(e){var t=Xi.current;li(Xi),e.type._context._currentValue=t}function to(e,t){for(;null!==e;){var n=e.alternate;if(e.childExpirationTime<t)e.childExpirationTime=t,null!==n&&n.childExpirationTime<t&&(n.childExpirationTime=t);else{if(!(null!==n&&n.childExpirationTime<t))break;n.childExpirationTime=t}e=e.return}}function no(e,t){Ki=e,Ji=Zi=null,null!==(e=e.dependencies)&&null!==e.firstContext&&(e.expirationTime>=t&&(Pu=!0),e.firstContext=null)}function ro(e,t){if(Ji!==e&&!1!==t&&0!==t)if("number"===typeof t&&1073741823!==t||(Ji=e,t=1073741823),t={context:e,observedBits:t,next:null},null===Zi){if(null===Ki)throw Error(u(308));Zi=t,Ki.dependencies={expirationTime:0,firstContext:t,responders:null}}else Zi=Zi.next=t;return e._currentValue}var io=!1;function oo(e){e.updateQueue={baseState:e.memoizedState,baseQueue:null,shared:{pending:null},effects:null}}function uo(e,t){e=e.updateQueue,t.updateQueue===e&&(t.updateQueue={baseState:e.baseState,baseQueue:e.baseQueue,shared:e.shared,effects:e.effects})}function ao(e,t){return(e={expirationTime:e,suspenseConfig:t,tag:0,payload:null,callback:null,next:null}).next=e}function lo(e,t){if(null!==(e=e.updateQueue)){var n=(e=e.shared).pending;null===n?t.next=t:(t.next=n.next,n.next=t),e.pending=t}}function co(e,t){var n=e.alternate;null!==n&&uo(n,e),null===(n=(e=e.updateQueue).baseQueue)?(e.baseQueue=t.next=t,t.next=t):(t.next=n.next,n.next=t)}function so(e,t,n,r){var o=e.updateQueue;io=!1;var u=o.baseQueue,a=o.shared.pending;if(null!==a){if(null!==u){var l=u.next;u.next=a.next,a.next=l}u=a,o.shared.pending=null,null!==(l=e.alternate)&&(null!==(l=l.updateQueue)&&(l.baseQueue=a))}if(null!==u){l=u.next;var c=o.baseState,s=0,f=null,d=null,p=null;if(null!==l)for(var h=l;;){if((a=h.expirationTime)<r){var m={expirationTime:h.expirationTime,suspenseConfig:h.suspenseConfig,tag:h.tag,payload:h.payload,callback:h.callback,next:null};null===p?(d=p=m,f=c):p=p.next=m,a>s&&(s=a)}else{null!==p&&(p=p.next={expirationTime:1073741823,suspenseConfig:h.suspenseConfig,tag:h.tag,payload:h.payload,callback:h.callback,next:null}),ol(a,h.suspenseConfig);e:{var g=e,v=h;switch(a=t,m=n,v.tag){case 1:if("function"===typeof(g=v.payload)){c=g.call(m,c,a);break e}c=g;break e;case 3:g.effectTag=-4097&g.effectTag|64;case 0:if(null===(a="function"===typeof(g=v.payload)?g.call(m,c,a):g)||void 0===a)break e;c=i({},c,a);break e;case 2:io=!0}}null!==h.callback&&(e.effectTag|=32,null===(a=o.effects)?o.effects=[h]:a.push(h))}if(null===(h=h.next)||h===l){if(null===(a=o.shared.pending))break;h=u.next=a.next,a.next=l,o.baseQueue=u=a,o.shared.pending=null}}null===p?f=c:p.next=d,o.baseState=f,o.baseQueue=p,ul(s),e.expirationTime=s,e.memoizedState=c}}function fo(e,t,n){if(e=t.effects,t.effects=null,null!==e)for(t=0;t<e.length;t++){var r=e[t],i=r.callback;if(null!==i){if(r.callback=null,r=i,i=n,"function"!==typeof r)throw Error(u(191,r));r.call(i)}}}var po=K.ReactCurrentBatchConfig,ho=(new r.Component).refs;function mo(e,t,n,r){n=null===(n=n(r,t=e.memoizedState))||void 0===n?t:i({},t,n),e.memoizedState=n,0===e.expirationTime&&(e.updateQueue.baseState=n)}var go={isMounted:function(e){return!!(e=e._reactInternalFiber)&&Ge(e)===e},enqueueSetState:function(e,t,n){e=e._reactInternalFiber;var r=qa(),i=po.suspense;(i=ao(r=Ba(r,e,i),i)).payload=t,void 0!==n&&null!==n&&(i.callback=n),lo(e,i),Ya(e,r)},enqueueReplaceState:function(e,t,n){e=e._reactInternalFiber;var r=qa(),i=po.suspense;(i=ao(r=Ba(r,e,i),i)).tag=1,i.payload=t,void 0!==n&&null!==n&&(i.callback=n),lo(e,i),Ya(e,r)},enqueueForceUpdate:function(e,t){e=e._reactInternalFiber;var n=qa(),r=po.suspense;(r=ao(n=Ba(n,e,r),r)).tag=2,void 0!==t&&null!==t&&(r.callback=t),lo(e,r),Ya(e,n)}};function vo(e,t,n,r,i,o,u){return"function"===typeof(e=e.stateNode).shouldComponentUpdate?e.shouldComponentUpdate(r,o,u):!t.prototype||!t.prototype.isPureReactComponent||(!Ir(n,r)||!Ir(i,o))}function yo(e,t,n){var r=!1,i=si,o=t.contextType;return"object"===typeof o&&null!==o?o=ro(o):(i=mi(t)?pi:fi.current,o=(r=null!==(r=t.contextTypes)&&void 0!==r)?hi(e,i):si),t=new t(n,o),e.memoizedState=null!==t.state&&void 0!==t.state?t.state:null,t.updater=go,e.stateNode=t,t._reactInternalFiber=e,r&&((e=e.stateNode).__reactInternalMemoizedUnmaskedChildContext=i,e.__reactInternalMemoizedMaskedChildContext=o),t}function bo(e,t,n,r){e=t.state,"function"===typeof t.componentWillReceiveProps&&t.componentWillReceiveProps(n,r),"function"===typeof t.UNSAFE_componentWillReceiveProps&&t.UNSAFE_componentWillReceiveProps(n,r),t.state!==e&&go.enqueueReplaceState(t,t.state,null)}function wo(e,t,n,r){var i=e.stateNode;i.props=n,i.state=e.memoizedState,i.refs=ho,oo(e);var o=t.contextType;"object"===typeof o&&null!==o?i.context=ro(o):(o=mi(t)?pi:fi.current,i.context=hi(e,o)),so(e,n,i,r),i.state=e.memoizedState,"function"===typeof(o=t.getDerivedStateFromProps)&&(mo(e,t,o,n),i.state=e.memoizedState),"function"===typeof t.getDerivedStateFromProps||"function"===typeof i.getSnapshotBeforeUpdate||"function"!==typeof i.UNSAFE_componentWillMount&&"function"!==typeof i.componentWillMount||(t=i.state,"function"===typeof i.componentWillMount&&i.componentWillMount(),"function"===typeof i.UNSAFE_componentWillMount&&i.UNSAFE_componentWillMount(),t!==i.state&&go.enqueueReplaceState(i,i.state,null),so(e,n,i,r),i.state=e.memoizedState),"function"===typeof i.componentDidMount&&(e.effectTag|=4)}var xo=Array.isArray;function _o(e,t,n){if(null!==(e=n.ref)&&"function"!==typeof e&&"object"!==typeof e){if(n._owner){if(n=n._owner){if(1!==n.tag)throw Error(u(309));var r=n.stateNode}if(!r)throw Error(u(147,e));var i=""+e;return null!==t&&null!==t.ref&&"function"===typeof t.ref&&t.ref._stringRef===i?t.ref:((t=function(e){var t=r.refs;t===ho&&(t=r.refs={}),null===e?delete t[i]:t[i]=e})._stringRef=i,t)}if("string"!==typeof e)throw Error(u(284));if(!n._owner)throw Error(u(290,e))}return e}function ko(e,t){if("textarea"!==e.type)throw Error(u(31,"[object Object]"===Object.prototype.toString.call(t)?"object with keys {"+Object.keys(t).join(", ")+"}":t,""))}function To(e){function t(t,n){if(e){var r=t.lastEffect;null!==r?(r.nextEffect=n,t.lastEffect=n):t.firstEffect=t.lastEffect=n,n.nextEffect=null,n.effectTag=8}}function n(n,r){if(!e)return null;for(;null!==r;)t(n,r),r=r.sibling;return null}function r(e,t){for(e=new Map;null!==t;)null!==t.key?e.set(t.key,t):e.set(t.index,t),t=t.sibling;return e}function i(e,t){return(e=Cl(e,t)).index=0,e.sibling=null,e}function o(t,n,r){return t.index=r,e?null!==(r=t.alternate)?(r=r.index)<n?(t.effectTag=2,n):r:(t.effectTag=2,n):n}function a(t){return e&&null===t.alternate&&(t.effectTag=2),t}function l(e,t,n,r){return null===t||6!==t.tag?((t=Nl(n,e.mode,r)).return=e,t):((t=i(t,n)).return=e,t)}function c(e,t,n,r){return null!==t&&t.elementType===n.type?((r=i(t,n.props)).ref=_o(e,t,n),r.return=e,r):((r=Sl(n.type,n.key,n.props,null,e.mode,r)).ref=_o(e,t,n),r.return=e,r)}function s(e,t,n,r){return null===t||4!==t.tag||t.stateNode.containerInfo!==n.containerInfo||t.stateNode.implementation!==n.implementation?((t=Pl(n,e.mode,r)).return=e,t):((t=i(t,n.children||[])).return=e,t)}function f(e,t,n,r,o){return null===t||7!==t.tag?((t=Ml(n,e.mode,r,o)).return=e,t):((t=i(t,n)).return=e,t)}function d(e,t,n){if("string"===typeof t||"number"===typeof t)return(t=Nl(""+t,e.mode,n)).return=e,t;if("object"===typeof t&&null!==t){switch(t.$$typeof){case ee:return(n=Sl(t.type,t.key,t.props,null,e.mode,n)).ref=_o(e,null,t),n.return=e,n;case te:return(t=Pl(t,e.mode,n)).return=e,t}if(xo(t)||me(t))return(t=Ml(t,e.mode,n,null)).return=e,t;ko(e,t)}return null}function p(e,t,n,r){var i=null!==t?t.key:null;if("string"===typeof n||"number"===typeof n)return null!==i?null:l(e,t,""+n,r);if("object"===typeof n&&null!==n){switch(n.$$typeof){case ee:return n.key===i?n.type===ne?f(e,t,n.props.children,r,i):c(e,t,n,r):null;case te:return n.key===i?s(e,t,n,r):null}if(xo(n)||me(n))return null!==i?null:f(e,t,n,r,null);ko(e,n)}return null}function h(e,t,n,r,i){if("string"===typeof r||"number"===typeof r)return l(t,e=e.get(n)||null,""+r,i);if("object"===typeof r&&null!==r){switch(r.$$typeof){case ee:return e=e.get(null===r.key?n:r.key)||null,r.type===ne?f(t,e,r.props.children,i,r.key):c(t,e,r,i);case te:return s(t,e=e.get(null===r.key?n:r.key)||null,r,i)}if(xo(r)||me(r))return f(t,e=e.get(n)||null,r,i,null);ko(t,r)}return null}function m(i,u,a,l){for(var c=null,s=null,f=u,m=u=0,g=null;null!==f&&m<a.length;m++){f.index>m?(g=f,f=null):g=f.sibling;var v=p(i,f,a[m],l);if(null===v){null===f&&(f=g);break}e&&f&&null===v.alternate&&t(i,f),u=o(v,u,m),null===s?c=v:s.sibling=v,s=v,f=g}if(m===a.length)return n(i,f),c;if(null===f){for(;m<a.length;m++)null!==(f=d(i,a[m],l))&&(u=o(f,u,m),null===s?c=f:s.sibling=f,s=f);return c}for(f=r(i,f);m<a.length;m++)null!==(g=h(f,i,m,a[m],l))&&(e&&null!==g.alternate&&f.delete(null===g.key?m:g.key),u=o(g,u,m),null===s?c=g:s.sibling=g,s=g);return e&&f.forEach((function(e){return t(i,e)})),c}function g(i,a,l,c){var s=me(l);if("function"!==typeof s)throw Error(u(150));if(null==(l=s.call(l)))throw Error(u(151));for(var f=s=null,m=a,g=a=0,v=null,y=l.next();null!==m&&!y.done;g++,y=l.next()){m.index>g?(v=m,m=null):v=m.sibling;var b=p(i,m,y.value,c);if(null===b){null===m&&(m=v);break}e&&m&&null===b.alternate&&t(i,m),a=o(b,a,g),null===f?s=b:f.sibling=b,f=b,m=v}if(y.done)return n(i,m),s;if(null===m){for(;!y.done;g++,y=l.next())null!==(y=d(i,y.value,c))&&(a=o(y,a,g),null===f?s=y:f.sibling=y,f=y);return s}for(m=r(i,m);!y.done;g++,y=l.next())null!==(y=h(m,i,g,y.value,c))&&(e&&null!==y.alternate&&m.delete(null===y.key?g:y.key),a=o(y,a,g),null===f?s=y:f.sibling=y,f=y);return e&&m.forEach((function(e){return t(i,e)})),s}return function(e,r,o,l){var c="object"===typeof o&&null!==o&&o.type===ne&&null===o.key;c&&(o=o.props.children);var s="object"===typeof o&&null!==o;if(s)switch(o.$$typeof){case ee:e:{for(s=o.key,c=r;null!==c;){if(c.key===s){switch(c.tag){case 7:if(o.type===ne){n(e,c.sibling),(r=i(c,o.props.children)).return=e,e=r;break e}break;default:if(c.elementType===o.type){n(e,c.sibling),(r=i(c,o.props)).ref=_o(e,c,o),r.return=e,e=r;break e}}n(e,c);break}t(e,c),c=c.sibling}o.type===ne?((r=Ml(o.props.children,e.mode,l,o.key)).return=e,e=r):((l=Sl(o.type,o.key,o.props,null,e.mode,l)).ref=_o(e,r,o),l.return=e,e=l)}return a(e);case te:e:{for(c=o.key;null!==r;){if(r.key===c){if(4===r.tag&&r.stateNode.containerInfo===o.containerInfo&&r.stateNode.implementation===o.implementation){n(e,r.sibling),(r=i(r,o.children||[])).return=e,e=r;break e}n(e,r);break}t(e,r),r=r.sibling}(r=Pl(o,e.mode,l)).return=e,e=r}return a(e)}if("string"===typeof o||"number"===typeof o)return o=""+o,null!==r&&6===r.tag?(n(e,r.sibling),(r=i(r,o)).return=e,e=r):(n(e,r),(r=Nl(o,e.mode,l)).return=e,e=r),a(e);if(xo(o))return m(e,r,o,l);if(me(o))return g(e,r,o,l);if(s&&ko(e,o),"undefined"===typeof o&&!c)switch(e.tag){case 1:case 0:throw e=e.type,Error(u(152,e.displayName||e.name||"Component"))}return n(e,r)}}var Eo=To(!0),Co=To(!1),So={},Mo={current:So},No={current:So},Po={current:So};function Ao(e){if(e===So)throw Error(u(174));return e}function Ro(e,t){switch(ci(Po,t),ci(No,e),ci(Mo,So),e=t.nodeType){case 9:case 11:t=(t=t.documentElement)?t.namespaceURI:Ue(null,"");break;default:t=Ue(t=(e=8===e?t.parentNode:t).namespaceURI||null,e=e.tagName)}li(Mo),ci(Mo,t)}function Do(){li(Mo),li(No),li(Po)}function Oo(e){Ao(Po.current);var t=Ao(Mo.current),n=Ue(t,e.type);t!==n&&(ci(No,e),ci(Mo,n))}function zo(e){No.current===e&&(li(Mo),li(No))}var Lo={current:0};function Uo(e){for(var t=e;null!==t;){if(13===t.tag){var n=t.memoizedState;if(null!==n&&(null===(n=n.dehydrated)||"$?"===n.data||"$!"===n.data))return t}else if(19===t.tag&&void 0!==t.memoizedProps.revealOrder){if(0!==(64&t.effectTag))return t}else if(null!==t.child){t.child.return=t,t=t.child;continue}if(t===e)break;for(;null===t.sibling;){if(null===t.return||t.return===e)return null;t=t.return}t.sibling.return=t.return,t=t.sibling}return null}function Fo(e,t){return{responder:e,props:t}}var Io=K.ReactCurrentDispatcher,jo=K.ReactCurrentBatchConfig,Ho=0,Vo=null,$o=null,Wo=null,Qo=!1;function qo(){throw Error(u(321))}function Bo(e,t){if(null===t)return!1;for(var n=0;n<t.length&&n<e.length;n++)if(!Ur(e[n],t[n]))return!1;return!0}function Yo(e,t,n,r,i,o){if(Ho=o,Vo=t,t.memoizedState=null,t.updateQueue=null,t.expirationTime=0,Io.current=null===e||null===e.memoizedState?vu:yu,e=n(r,i),t.expirationTime===Ho){o=0;do{if(t.expirationTime=0,!(25>o))throw Error(u(301));o+=1,Wo=$o=null,t.updateQueue=null,Io.current=bu,e=n(r,i)}while(t.expirationTime===Ho)}if(Io.current=gu,t=null!==$o&&null!==$o.next,Ho=0,Wo=$o=Vo=null,Qo=!1,t)throw Error(u(300));return e}function Xo(){var e={memoizedState:null,baseState:null,baseQueue:null,queue:null,next:null};return null===Wo?Vo.memoizedState=Wo=e:Wo=Wo.next=e,Wo}function Ko(){if(null===$o){var e=Vo.alternate;e=null!==e?e.memoizedState:null}else e=$o.next;var t=null===Wo?Vo.memoizedState:Wo.next;if(null!==t)Wo=t,$o=e;else{if(null===e)throw Error(u(310));e={memoizedState:($o=e).memoizedState,baseState:$o.baseState,baseQueue:$o.baseQueue,queue:$o.queue,next:null},null===Wo?Vo.memoizedState=Wo=e:Wo=Wo.next=e}return Wo}function Zo(e,t){return"function"===typeof t?t(e):t}function Jo(e){var t=Ko(),n=t.queue;if(null===n)throw Error(u(311));n.lastRenderedReducer=e;var r=$o,i=r.baseQueue,o=n.pending;if(null!==o){if(null!==i){var a=i.next;i.next=o.next,o.next=a}r.baseQueue=i=o,n.pending=null}if(null!==i){i=i.next,r=r.baseState;var l=a=o=null,c=i;do{var s=c.expirationTime;if(s<Ho){var f={expirationTime:c.expirationTime,suspenseConfig:c.suspenseConfig,action:c.action,eagerReducer:c.eagerReducer,eagerState:c.eagerState,next:null};null===l?(a=l=f,o=r):l=l.next=f,s>Vo.expirationTime&&(Vo.expirationTime=s,ul(s))}else null!==l&&(l=l.next={expirationTime:1073741823,suspenseConfig:c.suspenseConfig,action:c.action,eagerReducer:c.eagerReducer,eagerState:c.eagerState,next:null}),ol(s,c.suspenseConfig),r=c.eagerReducer===e?c.eagerState:e(r,c.action);c=c.next}while(null!==c&&c!==i);null===l?o=r:l.next=a,Ur(r,t.memoizedState)||(Pu=!0),t.memoizedState=r,t.baseState=o,t.baseQueue=l,n.lastRenderedState=r}return[t.memoizedState,n.dispatch]}function Go(e){var t=Ko(),n=t.queue;if(null===n)throw Error(u(311));n.lastRenderedReducer=e;var r=n.dispatch,i=n.pending,o=t.memoizedState;if(null!==i){n.pending=null;var a=i=i.next;do{o=e(o,a.action),a=a.next}while(a!==i);Ur(o,t.memoizedState)||(Pu=!0),t.memoizedState=o,null===t.baseQueue&&(t.baseState=o),n.lastRenderedState=o}return[o,r]}function eu(e){var t=Xo();return"function"===typeof e&&(e=e()),t.memoizedState=t.baseState=e,e=(e=t.queue={pending:null,dispatch:null,lastRenderedReducer:Zo,lastRenderedState:e}).dispatch=mu.bind(null,Vo,e),[t.memoizedState,e]}function tu(e,t,n,r){return e={tag:e,create:t,destroy:n,deps:r,next:null},null===(t=Vo.updateQueue)?(t={lastEffect:null},Vo.updateQueue=t,t.lastEffect=e.next=e):null===(n=t.lastEffect)?t.lastEffect=e.next=e:(r=n.next,n.next=e,e.next=r,t.lastEffect=e),e}function nu(){return Ko().memoizedState}function ru(e,t,n,r){var i=Xo();Vo.effectTag|=e,i.memoizedState=tu(1|t,n,void 0,void 0===r?null:r)}function iu(e,t,n,r){var i=Ko();r=void 0===r?null:r;var o=void 0;if(null!==$o){var u=$o.memoizedState;if(o=u.destroy,null!==r&&Bo(r,u.deps))return void tu(t,n,o,r)}Vo.effectTag|=e,i.memoizedState=tu(1|t,n,o,r)}function ou(e,t){return ru(516,4,e,t)}function uu(e,t){return iu(516,4,e,t)}function au(e,t){return iu(4,2,e,t)}function lu(e,t){return"function"===typeof t?(e=e(),t(e),function(){t(null)}):null!==t&&void 0!==t?(e=e(),t.current=e,function(){t.current=null}):void 0}function cu(e,t,n){return n=null!==n&&void 0!==n?n.concat([e]):null,iu(4,2,lu.bind(null,t,e),n)}function su(){}function fu(e,t){return Xo().memoizedState=[e,void 0===t?null:t],e}function du(e,t){var n=Ko();t=void 0===t?null:t;var r=n.memoizedState;return null!==r&&null!==t&&Bo(t,r[1])?r[0]:(n.memoizedState=[e,t],e)}function pu(e,t){var n=Ko();t=void 0===t?null:t;var r=n.memoizedState;return null!==r&&null!==t&&Bo(t,r[1])?r[0]:(e=e(),n.memoizedState=[e,t],e)}function hu(e,t,n){var r=ji();Vi(98>r?98:r,(function(){e(!0)})),Vi(97<r?97:r,(function(){var r=jo.suspense;jo.suspense=void 0===t?null:t;try{e(!1),n()}finally{jo.suspense=r}}))}function mu(e,t,n){var r=qa(),i=po.suspense;i={expirationTime:r=Ba(r,e,i),suspenseConfig:i,action:n,eagerReducer:null,eagerState:null,next:null};var o=t.pending;if(null===o?i.next=i:(i.next=o.next,o.next=i),t.pending=i,o=e.alternate,e===Vo||null!==o&&o===Vo)Qo=!0,i.expirationTime=Ho,Vo.expirationTime=Ho;else{if(0===e.expirationTime&&(null===o||0===o.expirationTime)&&null!==(o=t.lastRenderedReducer))try{var u=t.lastRenderedState,a=o(u,n);if(i.eagerReducer=o,i.eagerState=a,Ur(a,u))return}catch(l){}Ya(e,r)}}var gu={readContext:ro,useCallback:qo,useContext:qo,useEffect:qo,useImperativeHandle:qo,useLayoutEffect:qo,useMemo:qo,useReducer:qo,useRef:qo,useState:qo,useDebugValue:qo,useResponder:qo,useDeferredValue:qo,useTransition:qo},vu={readContext:ro,useCallback:fu,useContext:ro,useEffect:ou,useImperativeHandle:function(e,t,n){return n=null!==n&&void 0!==n?n.concat([e]):null,ru(4,2,lu.bind(null,t,e),n)},useLayoutEffect:function(e,t){return ru(4,2,e,t)},useMemo:function(e,t){var n=Xo();return t=void 0===t?null:t,e=e(),n.memoizedState=[e,t],e},useReducer:function(e,t,n){var r=Xo();return t=void 0!==n?n(t):t,r.memoizedState=r.baseState=t,e=(e=r.queue={pending:null,dispatch:null,lastRenderedReducer:e,lastRenderedState:t}).dispatch=mu.bind(null,Vo,e),[r.memoizedState,e]},useRef:function(e){return e={current:e},Xo().memoizedState=e},useState:eu,useDebugValue:su,useResponder:Fo,useDeferredValue:function(e,t){var n=eu(e),r=n[0],i=n[1];return ou((function(){var n=jo.suspense;jo.suspense=void 0===t?null:t;try{i(e)}finally{jo.suspense=n}}),[e,t]),r},useTransition:function(e){var t=eu(!1),n=t[0];return t=t[1],[fu(hu.bind(null,t,e),[t,e]),n]}},yu={readContext:ro,useCallback:du,useContext:ro,useEffect:uu,useImperativeHandle:cu,useLayoutEffect:au,useMemo:pu,useReducer:Jo,useRef:nu,useState:function(){return Jo(Zo)},useDebugValue:su,useResponder:Fo,useDeferredValue:function(e,t){var n=Jo(Zo),r=n[0],i=n[1];return uu((function(){var n=jo.suspense;jo.suspense=void 0===t?null:t;try{i(e)}finally{jo.suspense=n}}),[e,t]),r},useTransition:function(e){var t=Jo(Zo),n=t[0];return t=t[1],[du(hu.bind(null,t,e),[t,e]),n]}},bu={readContext:ro,useCallback:du,useContext:ro,useEffect:uu,useImperativeHandle:cu,useLayoutEffect:au,useMemo:pu,useReducer:Go,useRef:nu,useState:function(){return Go(Zo)},useDebugValue:su,useResponder:Fo,useDeferredValue:function(e,t){var n=Go(Zo),r=n[0],i=n[1];return uu((function(){var n=jo.suspense;jo.suspense=void 0===t?null:t;try{i(e)}finally{jo.suspense=n}}),[e,t]),r},useTransition:function(e){var t=Go(Zo),n=t[0];return t=t[1],[du(hu.bind(null,t,e),[t,e]),n]}},wu=null,xu=null,_u=!1;function ku(e,t){var n=Tl(5,null,null,0);n.elementType="DELETED",n.type="DELETED",n.stateNode=t,n.return=e,n.effectTag=8,null!==e.lastEffect?(e.lastEffect.nextEffect=n,e.lastEffect=n):e.firstEffect=e.lastEffect=n}function Tu(e,t){switch(e.tag){case 5:var n=e.type;return null!==(t=1!==t.nodeType||n.toLowerCase()!==t.nodeName.toLowerCase()?null:t)&&(e.stateNode=t,!0);case 6:return null!==(t=""===e.pendingProps||3!==t.nodeType?null:t)&&(e.stateNode=t,!0);case 13:default:return!1}}function Eu(e){if(_u){var t=xu;if(t){var n=t;if(!Tu(e,t)){if(!(t=xn(n.nextSibling))||!Tu(e,t))return e.effectTag=-1025&e.effectTag|2,_u=!1,void(wu=e);ku(wu,n)}wu=e,xu=xn(t.firstChild)}else e.effectTag=-1025&e.effectTag|2,_u=!1,wu=e}}function Cu(e){for(e=e.return;null!==e&&5!==e.tag&&3!==e.tag&&13!==e.tag;)e=e.return;wu=e}function Su(e){if(e!==wu)return!1;if(!_u)return Cu(e),_u=!0,!1;var t=e.type;if(5!==e.tag||"head"!==t&&"body"!==t&&!yn(t,e.memoizedProps))for(t=xu;t;)ku(e,t),t=xn(t.nextSibling);if(Cu(e),13===e.tag){if(!(e=null!==(e=e.memoizedState)?e.dehydrated:null))throw Error(u(317));e:{for(e=e.nextSibling,t=0;e;){if(8===e.nodeType){var n=e.data;if("/$"===n){if(0===t){xu=xn(e.nextSibling);break e}t--}else"$"!==n&&"$!"!==n&&"$?"!==n||t++}e=e.nextSibling}xu=null}}else xu=wu?xn(e.stateNode.nextSibling):null;return!0}function Mu(){xu=wu=null,_u=!1}var Nu=K.ReactCurrentOwner,Pu=!1;function Au(e,t,n,r){t.child=null===e?Co(t,null,n,r):Eo(t,e.child,n,r)}function Ru(e,t,n,r,i){n=n.render;var o=t.ref;return no(t,i),r=Yo(e,t,n,r,o,i),null===e||Pu?(t.effectTag|=1,Au(e,t,r,i),t.child):(t.updateQueue=e.updateQueue,t.effectTag&=-517,e.expirationTime<=i&&(e.expirationTime=0),Yu(e,t,i))}function Du(e,t,n,r,i,o){if(null===e){var u=n.type;return"function"!==typeof u||El(u)||void 0!==u.defaultProps||null!==n.compare||void 0!==n.defaultProps?((e=Sl(n.type,null,r,null,t.mode,o)).ref=t.ref,e.return=t,t.child=e):(t.tag=15,t.type=u,Ou(e,t,u,r,i,o))}return u=e.child,i<o&&(i=u.memoizedProps,(n=null!==(n=n.compare)?n:Ir)(i,r)&&e.ref===t.ref)?Yu(e,t,o):(t.effectTag|=1,(e=Cl(u,r)).ref=t.ref,e.return=t,t.child=e)}function Ou(e,t,n,r,i,o){return null!==e&&Ir(e.memoizedProps,r)&&e.ref===t.ref&&(Pu=!1,i<o)?(t.expirationTime=e.expirationTime,Yu(e,t,o)):Lu(e,t,n,r,o)}function zu(e,t){var n=t.ref;(null===e&&null!==n||null!==e&&e.ref!==n)&&(t.effectTag|=128)}function Lu(e,t,n,r,i){var o=mi(n)?pi:fi.current;return o=hi(t,o),no(t,i),n=Yo(e,t,n,r,o,i),null===e||Pu?(t.effectTag|=1,Au(e,t,n,i),t.child):(t.updateQueue=e.updateQueue,t.effectTag&=-517,e.expirationTime<=i&&(e.expirationTime=0),Yu(e,t,i))}function Uu(e,t,n,r,i){if(mi(n)){var o=!0;bi(t)}else o=!1;if(no(t,i),null===t.stateNode)null!==e&&(e.alternate=null,t.alternate=null,t.effectTag|=2),yo(t,n,r),wo(t,n,r,i),r=!0;else if(null===e){var u=t.stateNode,a=t.memoizedProps;u.props=a;var l=u.context,c=n.contextType;"object"===typeof c&&null!==c?c=ro(c):c=hi(t,c=mi(n)?pi:fi.current);var s=n.getDerivedStateFromProps,f="function"===typeof s||"function"===typeof u.getSnapshotBeforeUpdate;f||"function"!==typeof u.UNSAFE_componentWillReceiveProps&&"function"!==typeof u.componentWillReceiveProps||(a!==r||l!==c)&&bo(t,u,r,c),io=!1;var d=t.memoizedState;u.state=d,so(t,r,u,i),l=t.memoizedState,a!==r||d!==l||di.current||io?("function"===typeof s&&(mo(t,n,s,r),l=t.memoizedState),(a=io||vo(t,n,a,r,d,l,c))?(f||"function"!==typeof u.UNSAFE_componentWillMount&&"function"!==typeof u.componentWillMount||("function"===typeof u.componentWillMount&&u.componentWillMount(),"function"===typeof u.UNSAFE_componentWillMount&&u.UNSAFE_componentWillMount()),"function"===typeof u.componentDidMount&&(t.effectTag|=4)):("function"===typeof u.componentDidMount&&(t.effectTag|=4),t.memoizedProps=r,t.memoizedState=l),u.props=r,u.state=l,u.context=c,r=a):("function"===typeof u.componentDidMount&&(t.effectTag|=4),r=!1)}else u=t.stateNode,uo(e,t),a=t.memoizedProps,u.props=t.type===t.elementType?a:Yi(t.type,a),l=u.context,"object"===typeof(c=n.contextType)&&null!==c?c=ro(c):c=hi(t,c=mi(n)?pi:fi.current),(f="function"===typeof(s=n.getDerivedStateFromProps)||"function"===typeof u.getSnapshotBeforeUpdate)||"function"!==typeof u.UNSAFE_componentWillReceiveProps&&"function"!==typeof u.componentWillReceiveProps||(a!==r||l!==c)&&bo(t,u,r,c),io=!1,l=t.memoizedState,u.state=l,so(t,r,u,i),d=t.memoizedState,a!==r||l!==d||di.current||io?("function"===typeof s&&(mo(t,n,s,r),d=t.memoizedState),(s=io||vo(t,n,a,r,l,d,c))?(f||"function"!==typeof u.UNSAFE_componentWillUpdate&&"function"!==typeof u.componentWillUpdate||("function"===typeof u.componentWillUpdate&&u.componentWillUpdate(r,d,c),"function"===typeof u.UNSAFE_componentWillUpdate&&u.UNSAFE_componentWillUpdate(r,d,c)),"function"===typeof u.componentDidUpdate&&(t.effectTag|=4),"function"===typeof u.getSnapshotBeforeUpdate&&(t.effectTag|=256)):("function"!==typeof u.componentDidUpdate||a===e.memoizedProps&&l===e.memoizedState||(t.effectTag|=4),"function"!==typeof u.getSnapshotBeforeUpdate||a===e.memoizedProps&&l===e.memoizedState||(t.effectTag|=256),t.memoizedProps=r,t.memoizedState=d),u.props=r,u.state=d,u.context=c,r=s):("function"!==typeof u.componentDidUpdate||a===e.memoizedProps&&l===e.memoizedState||(t.effectTag|=4),"function"!==typeof u.getSnapshotBeforeUpdate||a===e.memoizedProps&&l===e.memoizedState||(t.effectTag|=256),r=!1);return Fu(e,t,n,r,o,i)}function Fu(e,t,n,r,i,o){zu(e,t);var u=0!==(64&t.effectTag);if(!r&&!u)return i&&wi(t,n,!1),Yu(e,t,o);r=t.stateNode,Nu.current=t;var a=u&&"function"!==typeof n.getDerivedStateFromError?null:r.render();return t.effectTag|=1,null!==e&&u?(t.child=Eo(t,e.child,null,o),t.child=Eo(t,null,a,o)):Au(e,t,a,o),t.memoizedState=r.state,i&&wi(t,n,!0),t.child}function Iu(e){var t=e.stateNode;t.pendingContext?vi(0,t.pendingContext,t.pendingContext!==t.context):t.context&&vi(0,t.context,!1),Ro(e,t.containerInfo)}var ju,Hu,Vu,$u={dehydrated:null,retryTime:0};function Wu(e,t,n){var r,i=t.mode,o=t.pendingProps,u=Lo.current,a=!1;if((r=0!==(64&t.effectTag))||(r=0!==(2&u)&&(null===e||null!==e.memoizedState)),r?(a=!0,t.effectTag&=-65):null!==e&&null===e.memoizedState||void 0===o.fallback||!0===o.unstable_avoidThisFallback||(u|=1),ci(Lo,1&u),null===e){if(void 0!==o.fallback&&Eu(t),a){if(a=o.fallback,(o=Ml(null,i,0,null)).return=t,0===(2&t.mode))for(e=null!==t.memoizedState?t.child.child:t.child,o.child=e;null!==e;)e.return=o,e=e.sibling;return(n=Ml(a,i,n,null)).return=t,o.sibling=n,t.memoizedState=$u,t.child=o,n}return i=o.children,t.memoizedState=null,t.child=Co(t,null,i,n)}if(null!==e.memoizedState){if(i=(e=e.child).sibling,a){if(o=o.fallback,(n=Cl(e,e.pendingProps)).return=t,0===(2&t.mode)&&(a=null!==t.memoizedState?t.child.child:t.child)!==e.child)for(n.child=a;null!==a;)a.return=n,a=a.sibling;return(i=Cl(i,o)).return=t,n.sibling=i,n.childExpirationTime=0,t.memoizedState=$u,t.child=n,i}return n=Eo(t,e.child,o.children,n),t.memoizedState=null,t.child=n}if(e=e.child,a){if(a=o.fallback,(o=Ml(null,i,0,null)).return=t,o.child=e,null!==e&&(e.return=o),0===(2&t.mode))for(e=null!==t.memoizedState?t.child.child:t.child,o.child=e;null!==e;)e.return=o,e=e.sibling;return(n=Ml(a,i,n,null)).return=t,o.sibling=n,n.effectTag|=2,o.childExpirationTime=0,t.memoizedState=$u,t.child=o,n}return t.memoizedState=null,t.child=Eo(t,e,o.children,n)}function Qu(e,t){e.expirationTime<t&&(e.expirationTime=t);var n=e.alternate;null!==n&&n.expirationTime<t&&(n.expirationTime=t),to(e.return,t)}function qu(e,t,n,r,i,o){var u=e.memoizedState;null===u?e.memoizedState={isBackwards:t,rendering:null,renderingStartTime:0,last:r,tail:n,tailExpiration:0,tailMode:i,lastEffect:o}:(u.isBackwards=t,u.rendering=null,u.renderingStartTime=0,u.last=r,u.tail=n,u.tailExpiration=0,u.tailMode=i,u.lastEffect=o)}function Bu(e,t,n){var r=t.pendingProps,i=r.revealOrder,o=r.tail;if(Au(e,t,r.children,n),0!==(2&(r=Lo.current)))r=1&r|2,t.effectTag|=64;else{if(null!==e&&0!==(64&e.effectTag))e:for(e=t.child;null!==e;){if(13===e.tag)null!==e.memoizedState&&Qu(e,n);else if(19===e.tag)Qu(e,n);else if(null!==e.child){e.child.return=e,e=e.child;continue}if(e===t)break e;for(;null===e.sibling;){if(null===e.return||e.return===t)break e;e=e.return}e.sibling.return=e.return,e=e.sibling}r&=1}if(ci(Lo,r),0===(2&t.mode))t.memoizedState=null;else switch(i){case"forwards":for(n=t.child,i=null;null!==n;)null!==(e=n.alternate)&&null===Uo(e)&&(i=n),n=n.sibling;null===(n=i)?(i=t.child,t.child=null):(i=n.sibling,n.sibling=null),qu(t,!1,i,n,o,t.lastEffect);break;case"backwards":for(n=null,i=t.child,t.child=null;null!==i;){if(null!==(e=i.alternate)&&null===Uo(e)){t.child=i;break}e=i.sibling,i.sibling=n,n=i,i=e}qu(t,!0,n,null,o,t.lastEffect);break;case"together":qu(t,!1,null,null,void 0,t.lastEffect);break;default:t.memoizedState=null}return t.child}function Yu(e,t,n){null!==e&&(t.dependencies=e.dependencies);var r=t.expirationTime;if(0!==r&&ul(r),t.childExpirationTime<n)return null;if(null!==e&&t.child!==e.child)throw Error(u(153));if(null!==t.child){for(n=Cl(e=t.child,e.pendingProps),t.child=n,n.return=t;null!==e.sibling;)e=e.sibling,(n=n.sibling=Cl(e,e.pendingProps)).return=t;n.sibling=null}return t.child}function Xu(e,t){switch(e.tailMode){case"hidden":t=e.tail;for(var n=null;null!==t;)null!==t.alternate&&(n=t),t=t.sibling;null===n?e.tail=null:n.sibling=null;break;case"collapsed":n=e.tail;for(var r=null;null!==n;)null!==n.alternate&&(r=n),n=n.sibling;null===r?t||null===e.tail?e.tail=null:e.tail.sibling=null:r.sibling=null}}function Ku(e,t,n){var r=t.pendingProps;switch(t.tag){case 2:case 16:case 15:case 0:case 11:case 7:case 8:case 12:case 9:case 14:return null;case 1:return mi(t.type)&&gi(),null;case 3:return Do(),li(di),li(fi),(n=t.stateNode).pendingContext&&(n.context=n.pendingContext,n.pendingContext=null),null!==e&&null!==e.child||!Su(t)||(t.effectTag|=4),null;case 5:zo(t),n=Ao(Po.current);var o=t.type;if(null!==e&&null!=t.stateNode)Hu(e,t,o,r,n),e.ref!==t.ref&&(t.effectTag|=128);else{if(!r){if(null===t.stateNode)throw Error(u(166));return null}if(e=Ao(Mo.current),Su(t)){r=t.stateNode,o=t.type;var a=t.memoizedProps;switch(r[Tn]=t,r[En]=a,o){case"iframe":case"object":case"embed":Bt("load",r);break;case"video":case"audio":for(e=0;e<Ke.length;e++)Bt(Ke[e],r);break;case"source":Bt("error",r);break;case"img":case"image":case"link":Bt("error",r),Bt("load",r);break;case"form":Bt("reset",r),Bt("submit",r);break;case"details":Bt("toggle",r);break;case"input":ke(r,a),Bt("invalid",r),ln(n,"onChange");break;case"select":r._wrapperState={wasMultiple:!!a.multiple},Bt("invalid",r),ln(n,"onChange");break;case"textarea":Ae(r,a),Bt("invalid",r),ln(n,"onChange")}for(var l in on(o,a),e=null,a)if(a.hasOwnProperty(l)){var c=a[l];"children"===l?"string"===typeof c?r.textContent!==c&&(e=["children",c]):"number"===typeof c&&r.textContent!==""+c&&(e=["children",""+c]):T.hasOwnProperty(l)&&null!=c&&ln(n,l)}switch(o){case"input":we(r),Ce(r,a,!0);break;case"textarea":we(r),De(r);break;case"select":case"option":break;default:"function"===typeof a.onClick&&(r.onclick=cn)}n=e,t.updateQueue=n,null!==n&&(t.effectTag|=4)}else{switch(l=9===n.nodeType?n:n.ownerDocument,e===an&&(e=Le(o)),e===an?"script"===o?((e=l.createElement("div")).innerHTML="<script><\/script>",e=e.removeChild(e.firstChild)):"string"===typeof r.is?e=l.createElement(o,{is:r.is}):(e=l.createElement(o),"select"===o&&(l=e,r.multiple?l.multiple=!0:r.size&&(l.size=r.size))):e=l.createElementNS(e,o),e[Tn]=t,e[En]=r,ju(e,t),t.stateNode=e,l=un(o,r),o){case"iframe":case"object":case"embed":Bt("load",e),c=r;break;case"video":case"audio":for(c=0;c<Ke.length;c++)Bt(Ke[c],e);c=r;break;case"source":Bt("error",e),c=r;break;case"img":case"image":case"link":Bt("error",e),Bt("load",e),c=r;break;case"form":Bt("reset",e),Bt("submit",e),c=r;break;case"details":Bt("toggle",e),c=r;break;case"input":ke(e,r),c=_e(e,r),Bt("invalid",e),ln(n,"onChange");break;case"option":c=Me(e,r);break;case"select":e._wrapperState={wasMultiple:!!r.multiple},c=i({},r,{value:void 0}),Bt("invalid",e),ln(n,"onChange");break;case"textarea":Ae(e,r),c=Pe(e,r),Bt("invalid",e),ln(n,"onChange");break;default:c=r}on(o,c);var s=c;for(a in s)if(s.hasOwnProperty(a)){var f=s[a];"style"===a?nn(e,f):"dangerouslySetInnerHTML"===a?null!=(f=f?f.__html:void 0)&&Ie(e,f):"children"===a?"string"===typeof f?("textarea"!==o||""!==f)&&je(e,f):"number"===typeof f&&je(e,""+f):"suppressContentEditableWarning"!==a&&"suppressHydrationWarning"!==a&&"autoFocus"!==a&&(T.hasOwnProperty(a)?null!=f&&ln(n,a):null!=f&&Z(e,a,f,l))}switch(o){case"input":we(e),Ce(e,r,!1);break;case"textarea":we(e),De(e);break;case"option":null!=r.value&&e.setAttribute("value",""+ye(r.value));break;case"select":e.multiple=!!r.multiple,null!=(n=r.value)?Ne(e,!!r.multiple,n,!1):null!=r.defaultValue&&Ne(e,!!r.multiple,r.defaultValue,!0);break;default:"function"===typeof c.onClick&&(e.onclick=cn)}vn(o,r)&&(t.effectTag|=4)}null!==t.ref&&(t.effectTag|=128)}return null;case 6:if(e&&null!=t.stateNode)Vu(0,t,e.memoizedProps,r);else{if("string"!==typeof r&&null===t.stateNode)throw Error(u(166));n=Ao(Po.current),Ao(Mo.current),Su(t)?(n=t.stateNode,r=t.memoizedProps,n[Tn]=t,n.nodeValue!==r&&(t.effectTag|=4)):((n=(9===n.nodeType?n:n.ownerDocument).createTextNode(r))[Tn]=t,t.stateNode=n)}return null;case 13:return li(Lo),r=t.memoizedState,0!==(64&t.effectTag)?(t.expirationTime=n,t):(n=null!==r,r=!1,null===e?void 0!==t.memoizedProps.fallback&&Su(t):(r=null!==(o=e.memoizedState),n||null===o||null!==(o=e.child.sibling)&&(null!==(a=t.firstEffect)?(t.firstEffect=o,o.nextEffect=a):(t.firstEffect=t.lastEffect=o,o.nextEffect=null),o.effectTag=8)),n&&!r&&0!==(2&t.mode)&&(null===e&&!0!==t.memoizedProps.unstable_avoidThisFallback||0!==(1&Lo.current)?Sa===wa&&(Sa=xa):(Sa!==wa&&Sa!==xa||(Sa=_a),0!==Ra&&null!==Ta&&(Dl(Ta,Ca),Ol(Ta,Ra)))),(n||r)&&(t.effectTag|=4),null);case 4:return Do(),null;case 10:return eo(t),null;case 17:return mi(t.type)&&gi(),null;case 19:if(li(Lo),null===(r=t.memoizedState))return null;if(o=0!==(64&t.effectTag),null===(a=r.rendering)){if(o)Xu(r,!1);else if(Sa!==wa||null!==e&&0!==(64&e.effectTag))for(a=t.child;null!==a;){if(null!==(e=Uo(a))){for(t.effectTag|=64,Xu(r,!1),null!==(o=e.updateQueue)&&(t.updateQueue=o,t.effectTag|=4),null===r.lastEffect&&(t.firstEffect=null),t.lastEffect=r.lastEffect,r=t.child;null!==r;)a=n,(o=r).effectTag&=2,o.nextEffect=null,o.firstEffect=null,o.lastEffect=null,null===(e=o.alternate)?(o.childExpirationTime=0,o.expirationTime=a,o.child=null,o.memoizedProps=null,o.memoizedState=null,o.updateQueue=null,o.dependencies=null):(o.childExpirationTime=e.childExpirationTime,o.expirationTime=e.expirationTime,o.child=e.child,o.memoizedProps=e.memoizedProps,o.memoizedState=e.memoizedState,o.updateQueue=e.updateQueue,a=e.dependencies,o.dependencies=null===a?null:{expirationTime:a.expirationTime,firstContext:a.firstContext,responders:a.responders}),r=r.sibling;return ci(Lo,1&Lo.current|2),t.child}a=a.sibling}}else{if(!o)if(null!==(e=Uo(a))){if(t.effectTag|=64,o=!0,null!==(n=e.updateQueue)&&(t.updateQueue=n,t.effectTag|=4),Xu(r,!0),null===r.tail&&"hidden"===r.tailMode&&!a.alternate)return null!==(t=t.lastEffect=r.lastEffect)&&(t.nextEffect=null),null}else 2*Ii()-r.renderingStartTime>r.tailExpiration&&1<n&&(t.effectTag|=64,o=!0,Xu(r,!1),t.expirationTime=t.childExpirationTime=n-1);r.isBackwards?(a.sibling=t.child,t.child=a):(null!==(n=r.last)?n.sibling=a:t.child=a,r.last=a)}return null!==r.tail?(0===r.tailExpiration&&(r.tailExpiration=Ii()+500),n=r.tail,r.rendering=n,r.tail=n.sibling,r.lastEffect=t.lastEffect,r.renderingStartTime=Ii(),n.sibling=null,t=Lo.current,ci(Lo,o?1&t|2:1&t),n):null}throw Error(u(156,t.tag))}function Zu(e){switch(e.tag){case 1:mi(e.type)&&gi();var t=e.effectTag;return 4096&t?(e.effectTag=-4097&t|64,e):null;case 3:if(Do(),li(di),li(fi),0!==(64&(t=e.effectTag)))throw Error(u(285));return e.effectTag=-4097&t|64,e;case 5:return zo(e),null;case 13:return li(Lo),4096&(t=e.effectTag)?(e.effectTag=-4097&t|64,e):null;case 19:return li(Lo),null;case 4:return Do(),null;case 10:return eo(e),null;default:return null}}function Ju(e,t){return{value:e,source:t,stack:ve(t)}}ju=function(e,t){for(var n=t.child;null!==n;){if(5===n.tag||6===n.tag)e.appendChild(n.stateNode);else if(4!==n.tag&&null!==n.child){n.child.return=n,n=n.child;continue}if(n===t)break;for(;null===n.sibling;){if(null===n.return||n.return===t)return;n=n.return}n.sibling.return=n.return,n=n.sibling}},Hu=function(e,t,n,r,o){var u=e.memoizedProps;if(u!==r){var a,l,c=t.stateNode;switch(Ao(Mo.current),e=null,n){case"input":u=_e(c,u),r=_e(c,r),e=[];break;case"option":u=Me(c,u),r=Me(c,r),e=[];break;case"select":u=i({},u,{value:void 0}),r=i({},r,{value:void 0}),e=[];break;case"textarea":u=Pe(c,u),r=Pe(c,r),e=[];break;default:"function"!==typeof u.onClick&&"function"===typeof r.onClick&&(c.onclick=cn)}for(a in on(n,r),n=null,u)if(!r.hasOwnProperty(a)&&u.hasOwnProperty(a)&&null!=u[a])if("style"===a)for(l in c=u[a])c.hasOwnProperty(l)&&(n||(n={}),n[l]="");else"dangerouslySetInnerHTML"!==a&&"children"!==a&&"suppressContentEditableWarning"!==a&&"suppressHydrationWarning"!==a&&"autoFocus"!==a&&(T.hasOwnProperty(a)?e||(e=[]):(e=e||[]).push(a,null));for(a in r){var s=r[a];if(c=null!=u?u[a]:void 0,r.hasOwnProperty(a)&&s!==c&&(null!=s||null!=c))if("style"===a)if(c){for(l in c)!c.hasOwnProperty(l)||s&&s.hasOwnProperty(l)||(n||(n={}),n[l]="");for(l in s)s.hasOwnProperty(l)&&c[l]!==s[l]&&(n||(n={}),n[l]=s[l])}else n||(e||(e=[]),e.push(a,n)),n=s;else"dangerouslySetInnerHTML"===a?(s=s?s.__html:void 0,c=c?c.__html:void 0,null!=s&&c!==s&&(e=e||[]).push(a,s)):"children"===a?c===s||"string"!==typeof s&&"number"!==typeof s||(e=e||[]).push(a,""+s):"suppressContentEditableWarning"!==a&&"suppressHydrationWarning"!==a&&(T.hasOwnProperty(a)?(null!=s&&ln(o,a),e||c===s||(e=[])):(e=e||[]).push(a,s))}n&&(e=e||[]).push("style",n),o=e,(t.updateQueue=o)&&(t.effectTag|=4)}},Vu=function(e,t,n,r){n!==r&&(t.effectTag|=4)};var Gu="function"===typeof WeakSet?WeakSet:Set;function ea(e,t){var n=t.source,r=t.stack;null===r&&null!==n&&(r=ve(n)),null!==n&&ge(n.type),t=t.value,null!==e&&1===e.tag&&ge(e.type);try{console.error(t)}catch(i){setTimeout((function(){throw i}))}}function ta(e){var t=e.ref;if(null!==t)if("function"===typeof t)try{t(null)}catch(n){yl(e,n)}else t.current=null}function na(e,t){switch(t.tag){case 0:case 11:case 15:case 22:return;case 1:if(256&t.effectTag&&null!==e){var n=e.memoizedProps,r=e.memoizedState;t=(e=t.stateNode).getSnapshotBeforeUpdate(t.elementType===t.type?n:Yi(t.type,n),r),e.__reactInternalSnapshotBeforeUpdate=t}return;case 3:case 5:case 6:case 4:case 17:return}throw Error(u(163))}function ra(e,t){if(null!==(t=null!==(t=t.updateQueue)?t.lastEffect:null)){var n=t=t.next;do{if((n.tag&e)===e){var r=n.destroy;n.destroy=void 0,void 0!==r&&r()}n=n.next}while(n!==t)}}function ia(e,t){if(null!==(t=null!==(t=t.updateQueue)?t.lastEffect:null)){var n=t=t.next;do{if((n.tag&e)===e){var r=n.create;n.destroy=r()}n=n.next}while(n!==t)}}function oa(e,t,n){switch(n.tag){case 0:case 11:case 15:case 22:return void ia(3,n);case 1:if(e=n.stateNode,4&n.effectTag)if(null===t)e.componentDidMount();else{var r=n.elementType===n.type?t.memoizedProps:Yi(n.type,t.memoizedProps);e.componentDidUpdate(r,t.memoizedState,e.__reactInternalSnapshotBeforeUpdate)}return void(null!==(t=n.updateQueue)&&fo(n,t,e));case 3:if(null!==(t=n.updateQueue)){if(e=null,null!==n.child)switch(n.child.tag){case 5:e=n.child.stateNode;break;case 1:e=n.child.stateNode}fo(n,t,e)}return;case 5:return e=n.stateNode,void(null===t&&4&n.effectTag&&vn(n.type,n.memoizedProps)&&e.focus());case 6:case 4:case 12:return;case 13:return void(null===n.memoizedState&&(n=n.alternate,null!==n&&(n=n.memoizedState,null!==n&&(n=n.dehydrated,null!==n&&Lt(n)))));case 19:case 17:case 20:case 21:return}throw Error(u(163))}function ua(e,t,n){switch("function"===typeof _l&&_l(t),t.tag){case 0:case 11:case 14:case 15:case 22:if(null!==(e=t.updateQueue)&&null!==(e=e.lastEffect)){var r=e.next;Vi(97<n?97:n,(function(){var e=r;do{var n=e.destroy;if(void 0!==n){var i=t;try{n()}catch(o){yl(i,o)}}e=e.next}while(e!==r)}))}break;case 1:ta(t),"function"===typeof(n=t.stateNode).componentWillUnmount&&function(e,t){try{t.props=e.memoizedProps,t.state=e.memoizedState,t.componentWillUnmount()}catch(n){yl(e,n)}}(t,n);break;case 5:ta(t);break;case 4:sa(e,t,n)}}function aa(e){var t=e.alternate;e.return=null,e.child=null,e.memoizedState=null,e.updateQueue=null,e.dependencies=null,e.alternate=null,e.firstEffect=null,e.lastEffect=null,e.pendingProps=null,e.memoizedProps=null,e.stateNode=null,null!==t&&aa(t)}function la(e){return 5===e.tag||3===e.tag||4===e.tag}function ca(e){e:{for(var t=e.return;null!==t;){if(la(t)){var n=t;break e}t=t.return}throw Error(u(160))}switch(t=n.stateNode,n.tag){case 5:var r=!1;break;case 3:case 4:t=t.containerInfo,r=!0;break;default:throw Error(u(161))}16&n.effectTag&&(je(t,""),n.effectTag&=-17);e:t:for(n=e;;){for(;null===n.sibling;){if(null===n.return||la(n.return)){n=null;break e}n=n.return}for(n.sibling.return=n.return,n=n.sibling;5!==n.tag&&6!==n.tag&&18!==n.tag;){if(2&n.effectTag)continue t;if(null===n.child||4===n.tag)continue t;n.child.return=n,n=n.child}if(!(2&n.effectTag)){n=n.stateNode;break e}}r?function e(t,n,r){var i=t.tag,o=5===i||6===i;if(o)t=o?t.stateNode:t.stateNode.instance,n?8===r.nodeType?r.parentNode.insertBefore(t,n):r.insertBefore(t,n):(8===r.nodeType?(n=r.parentNode).insertBefore(t,r):(n=r).appendChild(t),null!==(r=r._reactRootContainer)&&void 0!==r||null!==n.onclick||(n.onclick=cn));else if(4!==i&&null!==(t=t.child))for(e(t,n,r),t=t.sibling;null!==t;)e(t,n,r),t=t.sibling}(e,n,t):function e(t,n,r){var i=t.tag,o=5===i||6===i;if(o)t=o?t.stateNode:t.stateNode.instance,n?r.insertBefore(t,n):r.appendChild(t);else if(4!==i&&null!==(t=t.child))for(e(t,n,r),t=t.sibling;null!==t;)e(t,n,r),t=t.sibling}(e,n,t)}function sa(e,t,n){for(var r,i,o=t,a=!1;;){if(!a){a=o.return;e:for(;;){if(null===a)throw Error(u(160));switch(r=a.stateNode,a.tag){case 5:i=!1;break e;case 3:case 4:r=r.containerInfo,i=!0;break e}a=a.return}a=!0}if(5===o.tag||6===o.tag){e:for(var l=e,c=o,s=n,f=c;;)if(ua(l,f,s),null!==f.child&&4!==f.tag)f.child.return=f,f=f.child;else{if(f===c)break e;for(;null===f.sibling;){if(null===f.return||f.return===c)break e;f=f.return}f.sibling.return=f.return,f=f.sibling}i?(l=r,c=o.stateNode,8===l.nodeType?l.parentNode.removeChild(c):l.removeChild(c)):r.removeChild(o.stateNode)}else if(4===o.tag){if(null!==o.child){r=o.stateNode.containerInfo,i=!0,o.child.return=o,o=o.child;continue}}else if(ua(e,o,n),null!==o.child){o.child.return=o,o=o.child;continue}if(o===t)break;for(;null===o.sibling;){if(null===o.return||o.return===t)return;4===(o=o.return).tag&&(a=!1)}o.sibling.return=o.return,o=o.sibling}}function fa(e,t){switch(t.tag){case 0:case 11:case 14:case 15:case 22:return void ra(3,t);case 1:return;case 5:var n=t.stateNode;if(null!=n){var r=t.memoizedProps,i=null!==e?e.memoizedProps:r;e=t.type;var o=t.updateQueue;if(t.updateQueue=null,null!==o){for(n[En]=r,"input"===e&&"radio"===r.type&&null!=r.name&&Te(n,r),un(e,i),t=un(e,r),i=0;i<o.length;i+=2){var a=o[i],l=o[i+1];"style"===a?nn(n,l):"dangerouslySetInnerHTML"===a?Ie(n,l):"children"===a?je(n,l):Z(n,a,l,t)}switch(e){case"input":Ee(n,r);break;case"textarea":Re(n,r);break;case"select":t=n._wrapperState.wasMultiple,n._wrapperState.wasMultiple=!!r.multiple,null!=(e=r.value)?Ne(n,!!r.multiple,e,!1):t!==!!r.multiple&&(null!=r.defaultValue?Ne(n,!!r.multiple,r.defaultValue,!0):Ne(n,!!r.multiple,r.multiple?[]:"",!1))}}}return;case 6:if(null===t.stateNode)throw Error(u(162));return void(t.stateNode.nodeValue=t.memoizedProps);case 3:return void((t=t.stateNode).hydrate&&(t.hydrate=!1,Lt(t.containerInfo)));case 12:return;case 13:if(n=t,null===t.memoizedState?r=!1:(r=!0,n=t.child,Oa=Ii()),null!==n)e:for(e=n;;){if(5===e.tag)o=e.stateNode,r?"function"===typeof(o=o.style).setProperty?o.setProperty("display","none","important"):o.display="none":(o=e.stateNode,i=void 0!==(i=e.memoizedProps.style)&&null!==i&&i.hasOwnProperty("display")?i.display:null,o.style.display=tn("display",i));else if(6===e.tag)e.stateNode.nodeValue=r?"":e.memoizedProps;else{if(13===e.tag&&null!==e.memoizedState&&null===e.memoizedState.dehydrated){(o=e.child.sibling).return=e,e=o;continue}if(null!==e.child){e.child.return=e,e=e.child;continue}}if(e===n)break;for(;null===e.sibling;){if(null===e.return||e.return===n)break e;e=e.return}e.sibling.return=e.return,e=e.sibling}return void da(t);case 19:return void da(t);case 17:return}throw Error(u(163))}function da(e){var t=e.updateQueue;if(null!==t){e.updateQueue=null;var n=e.stateNode;null===n&&(n=e.stateNode=new Gu),t.forEach((function(t){var r=wl.bind(null,e,t);n.has(t)||(n.add(t),t.then(r,r))}))}}var pa="function"===typeof WeakMap?WeakMap:Map;function ha(e,t,n){(n=ao(n,null)).tag=3,n.payload={element:null};var r=t.value;return n.callback=function(){La||(La=!0,Ua=r),ea(e,t)},n}function ma(e,t,n){(n=ao(n,null)).tag=3;var r=e.type.getDerivedStateFromError;if("function"===typeof r){var i=t.value;n.payload=function(){return ea(e,t),r(i)}}var o=e.stateNode;return null!==o&&"function"===typeof o.componentDidCatch&&(n.callback=function(){"function"!==typeof r&&(null===Fa?Fa=new Set([this]):Fa.add(this),ea(e,t));var n=t.stack;this.componentDidCatch(t.value,{componentStack:null!==n?n:""})}),n}var ga,va=Math.ceil,ya=K.ReactCurrentDispatcher,ba=K.ReactCurrentOwner,wa=0,xa=3,_a=4,ka=0,Ta=null,Ea=null,Ca=0,Sa=wa,Ma=null,Na=1073741823,Pa=1073741823,Aa=null,Ra=0,Da=!1,Oa=0,za=null,La=!1,Ua=null,Fa=null,Ia=!1,ja=null,Ha=90,Va=null,$a=0,Wa=null,Qa=0;function qa(){return 0!==(48&ka)?1073741821-(Ii()/10|0):0!==Qa?Qa:Qa=1073741821-(Ii()/10|0)}function Ba(e,t,n){if(0===(2&(t=t.mode)))return 1073741823;var r=ji();if(0===(4&t))return 99===r?1073741823:1073741822;if(0!==(16&ka))return Ca;if(null!==n)e=Bi(e,0|n.timeoutMs||5e3,250);else switch(r){case 99:e=1073741823;break;case 98:e=Bi(e,150,100);break;case 97:case 96:e=Bi(e,5e3,250);break;case 95:e=2;break;default:throw Error(u(326))}return null!==Ta&&e===Ca&&--e,e}function Ya(e,t){if(50<$a)throw $a=0,Wa=null,Error(u(185));if(null!==(e=Xa(e,t))){var n=ji();1073741823===t?0!==(8&ka)&&0===(48&ka)?Ga(e):(Za(e),0===ka&&Qi()):Za(e),0===(4&ka)||98!==n&&99!==n||(null===Va?Va=new Map([[e,t]]):(void 0===(n=Va.get(e))||n>t)&&Va.set(e,t))}}function Xa(e,t){e.expirationTime<t&&(e.expirationTime=t);var n=e.alternate;null!==n&&n.expirationTime<t&&(n.expirationTime=t);var r=e.return,i=null;if(null===r&&3===e.tag)i=e.stateNode;else for(;null!==r;){if(n=r.alternate,r.childExpirationTime<t&&(r.childExpirationTime=t),null!==n&&n.childExpirationTime<t&&(n.childExpirationTime=t),null===r.return&&3===r.tag){i=r.stateNode;break}r=r.return}return null!==i&&(Ta===i&&(ul(t),Sa===_a&&Dl(i,Ca)),Ol(i,t)),i}function Ka(e){var t=e.lastExpiredTime;if(0!==t)return t;if(!Rl(e,t=e.firstPendingTime))return t;var n=e.lastPingedTime;return 2>=(e=n>(e=e.nextKnownPendingLevel)?n:e)&&t!==e?0:e}function Za(e){if(0!==e.lastExpiredTime)e.callbackExpirationTime=1073741823,e.callbackPriority=99,e.callbackNode=Wi(Ga.bind(null,e));else{var t=Ka(e),n=e.callbackNode;if(0===t)null!==n&&(e.callbackNode=null,e.callbackExpirationTime=0,e.callbackPriority=90);else{var r=qa();if(1073741823===t?r=99:1===t||2===t?r=95:r=0>=(r=10*(1073741821-t)-10*(1073741821-r))?99:250>=r?98:5250>=r?97:95,null!==n){var i=e.callbackPriority;if(e.callbackExpirationTime===t&&i>=r)return;n!==Ri&&ki(n)}e.callbackExpirationTime=t,e.callbackPriority=r,t=1073741823===t?Wi(Ga.bind(null,e)):$i(r,Ja.bind(null,e),{timeout:10*(1073741821-t)-Ii()}),e.callbackNode=t}}}function Ja(e,t){if(Qa=0,t)return zl(e,t=qa()),Za(e),null;var n=Ka(e);if(0!==n){if(t=e.callbackNode,0!==(48&ka))throw Error(u(327));if(ml(),e===Ta&&n===Ca||nl(e,n),null!==Ea){var r=ka;ka|=16;for(var i=il();;)try{ll();break}catch(l){rl(e,l)}if(Gi(),ka=r,ya.current=i,1===Sa)throw t=Ma,nl(e,n),Dl(e,n),Za(e),t;if(null===Ea)switch(i=e.finishedWork=e.current.alternate,e.finishedExpirationTime=n,r=Sa,Ta=null,r){case wa:case 1:throw Error(u(345));case 2:zl(e,2<n?2:n);break;case xa:if(Dl(e,n),n===(r=e.lastSuspendedTime)&&(e.nextKnownPendingLevel=fl(i)),1073741823===Na&&10<(i=Oa+500-Ii())){if(Da){var o=e.lastPingedTime;if(0===o||o>=n){e.lastPingedTime=n,nl(e,n);break}}if(0!==(o=Ka(e))&&o!==n)break;if(0!==r&&r!==n){e.lastPingedTime=r;break}e.timeoutHandle=bn(dl.bind(null,e),i);break}dl(e);break;case _a:if(Dl(e,n),n===(r=e.lastSuspendedTime)&&(e.nextKnownPendingLevel=fl(i)),Da&&(0===(i=e.lastPingedTime)||i>=n)){e.lastPingedTime=n,nl(e,n);break}if(0!==(i=Ka(e))&&i!==n)break;if(0!==r&&r!==n){e.lastPingedTime=r;break}if(1073741823!==Pa?r=10*(1073741821-Pa)-Ii():1073741823===Na?r=0:(r=10*(1073741821-Na)-5e3,0>(r=(i=Ii())-r)&&(r=0),(n=10*(1073741821-n)-i)<(r=(120>r?120:480>r?480:1080>r?1080:1920>r?1920:3e3>r?3e3:4320>r?4320:1960*va(r/1960))-r)&&(r=n)),10<r){e.timeoutHandle=bn(dl.bind(null,e),r);break}dl(e);break;case 5:if(1073741823!==Na&&null!==Aa){o=Na;var a=Aa;if(0>=(r=0|a.busyMinDurationMs)?r=0:(i=0|a.busyDelayMs,r=(o=Ii()-(10*(1073741821-o)-(0|a.timeoutMs||5e3)))<=i?0:i+r-o),10<r){Dl(e,n),e.timeoutHandle=bn(dl.bind(null,e),r);break}}dl(e);break;default:throw Error(u(329))}if(Za(e),e.callbackNode===t)return Ja.bind(null,e)}}return null}function Ga(e){var t=e.lastExpiredTime;if(t=0!==t?t:1073741823,0!==(48&ka))throw Error(u(327));if(ml(),e===Ta&&t===Ca||nl(e,t),null!==Ea){var n=ka;ka|=16;for(var r=il();;)try{al();break}catch(i){rl(e,i)}if(Gi(),ka=n,ya.current=r,1===Sa)throw n=Ma,nl(e,t),Dl(e,t),Za(e),n;if(null!==Ea)throw Error(u(261));e.finishedWork=e.current.alternate,e.finishedExpirationTime=t,Ta=null,dl(e),Za(e)}return null}function el(e,t){var n=ka;ka|=1;try{return e(t)}finally{0===(ka=n)&&Qi()}}function tl(e,t){var n=ka;ka&=-2,ka|=8;try{return e(t)}finally{0===(ka=n)&&Qi()}}function nl(e,t){e.finishedWork=null,e.finishedExpirationTime=0;var n=e.timeoutHandle;if(-1!==n&&(e.timeoutHandle=-1,wn(n)),null!==Ea)for(n=Ea.return;null!==n;){var r=n;switch(r.tag){case 1:null!==(r=r.type.childContextTypes)&&void 0!==r&&gi();break;case 3:Do(),li(di),li(fi);break;case 5:zo(r);break;case 4:Do();break;case 13:case 19:li(Lo);break;case 10:eo(r)}n=n.return}Ta=e,Ea=Cl(e.current,null),Ca=t,Sa=wa,Ma=null,Pa=Na=1073741823,Aa=null,Ra=0,Da=!1}function rl(e,t){for(;;){try{if(Gi(),Io.current=gu,Qo)for(var n=Vo.memoizedState;null!==n;){var r=n.queue;null!==r&&(r.pending=null),n=n.next}if(Ho=0,Wo=$o=Vo=null,Qo=!1,null===Ea||null===Ea.return)return Sa=1,Ma=t,Ea=null;e:{var i=e,o=Ea.return,u=Ea,a=t;if(t=Ca,u.effectTag|=2048,u.firstEffect=u.lastEffect=null,null!==a&&"object"===typeof a&&"function"===typeof a.then){var l=a;if(0===(2&u.mode)){var c=u.alternate;c?(u.updateQueue=c.updateQueue,u.memoizedState=c.memoizedState,u.expirationTime=c.expirationTime):(u.updateQueue=null,u.memoizedState=null)}var s=0!==(1&Lo.current),f=o;do{var d;if(d=13===f.tag){var p=f.memoizedState;if(null!==p)d=null!==p.dehydrated;else{var h=f.memoizedProps;d=void 0!==h.fallback&&(!0!==h.unstable_avoidThisFallback||!s)}}if(d){var m=f.updateQueue;if(null===m){var g=new Set;g.add(l),f.updateQueue=g}else m.add(l);if(0===(2&f.mode)){if(f.effectTag|=64,u.effectTag&=-2981,1===u.tag)if(null===u.alternate)u.tag=17;else{var v=ao(1073741823,null);v.tag=2,lo(u,v)}u.expirationTime=1073741823;break e}a=void 0,u=t;var y=i.pingCache;if(null===y?(y=i.pingCache=new pa,a=new Set,y.set(l,a)):void 0===(a=y.get(l))&&(a=new Set,y.set(l,a)),!a.has(u)){a.add(u);var b=bl.bind(null,i,l,u);l.then(b,b)}f.effectTag|=4096,f.expirationTime=t;break e}f=f.return}while(null!==f);a=Error((ge(u.type)||"A React component")+" suspended while rendering, but no fallback UI was specified.\n\nAdd a <Suspense fallback=...> component higher in the tree to provide a loading indicator or placeholder to display."+ve(u))}5!==Sa&&(Sa=2),a=Ju(a,u),f=o;do{switch(f.tag){case 3:l=a,f.effectTag|=4096,f.expirationTime=t,co(f,ha(f,l,t));break e;case 1:l=a;var w=f.type,x=f.stateNode;if(0===(64&f.effectTag)&&("function"===typeof w.getDerivedStateFromError||null!==x&&"function"===typeof x.componentDidCatch&&(null===Fa||!Fa.has(x)))){f.effectTag|=4096,f.expirationTime=t,co(f,ma(f,l,t));break e}}f=f.return}while(null!==f)}Ea=sl(Ea)}catch(_){t=_;continue}break}}function il(){var e=ya.current;return ya.current=gu,null===e?gu:e}function ol(e,t){e<Na&&2<e&&(Na=e),null!==t&&e<Pa&&2<e&&(Pa=e,Aa=t)}function ul(e){e>Ra&&(Ra=e)}function al(){for(;null!==Ea;)Ea=cl(Ea)}function ll(){for(;null!==Ea&&!Di();)Ea=cl(Ea)}function cl(e){var t=ga(e.alternate,e,Ca);return e.memoizedProps=e.pendingProps,null===t&&(t=sl(e)),ba.current=null,t}function sl(e){Ea=e;do{var t=Ea.alternate;if(e=Ea.return,0===(2048&Ea.effectTag)){if(t=Ku(t,Ea,Ca),1===Ca||1!==Ea.childExpirationTime){for(var n=0,r=Ea.child;null!==r;){var i=r.expirationTime,o=r.childExpirationTime;i>n&&(n=i),o>n&&(n=o),r=r.sibling}Ea.childExpirationTime=n}if(null!==t)return t;null!==e&&0===(2048&e.effectTag)&&(null===e.firstEffect&&(e.firstEffect=Ea.firstEffect),null!==Ea.lastEffect&&(null!==e.lastEffect&&(e.lastEffect.nextEffect=Ea.firstEffect),e.lastEffect=Ea.lastEffect),1<Ea.effectTag&&(null!==e.lastEffect?e.lastEffect.nextEffect=Ea:e.firstEffect=Ea,e.lastEffect=Ea))}else{if(null!==(t=Zu(Ea)))return t.effectTag&=2047,t;null!==e&&(e.firstEffect=e.lastEffect=null,e.effectTag|=2048)}if(null!==(t=Ea.sibling))return t;Ea=e}while(null!==Ea);return Sa===wa&&(Sa=5),null}function fl(e){var t=e.expirationTime;return t>(e=e.childExpirationTime)?t:e}function dl(e){var t=ji();return Vi(99,pl.bind(null,e,t)),null}function pl(e,t){do{ml()}while(null!==ja);if(0!==(48&ka))throw Error(u(327));var n=e.finishedWork,r=e.finishedExpirationTime;if(null===n)return null;if(e.finishedWork=null,e.finishedExpirationTime=0,n===e.current)throw Error(u(177));e.callbackNode=null,e.callbackExpirationTime=0,e.callbackPriority=90,e.nextKnownPendingLevel=0;var i=fl(n);if(e.firstPendingTime=i,r<=e.lastSuspendedTime?e.firstSuspendedTime=e.lastSuspendedTime=e.nextKnownPendingLevel=0:r<=e.firstSuspendedTime&&(e.firstSuspendedTime=r-1),r<=e.lastPingedTime&&(e.lastPingedTime=0),r<=e.lastExpiredTime&&(e.lastExpiredTime=0),e===Ta&&(Ea=Ta=null,Ca=0),1<n.effectTag?null!==n.lastEffect?(n.lastEffect.nextEffect=n,i=n.firstEffect):i=n:i=n.firstEffect,null!==i){var o=ka;ka|=32,ba.current=null,mn=qt;var a=pn();if(hn(a)){if("selectionStart"in a)var l={start:a.selectionStart,end:a.selectionEnd};else e:{var c=(l=(l=a.ownerDocument)&&l.defaultView||window).getSelection&&l.getSelection();if(c&&0!==c.rangeCount){l=c.anchorNode;var s=c.anchorOffset,f=c.focusNode;c=c.focusOffset;try{l.nodeType,f.nodeType}catch(C){l=null;break e}var d=0,p=-1,h=-1,m=0,g=0,v=a,y=null;t:for(;;){for(var b;v!==l||0!==s&&3!==v.nodeType||(p=d+s),v!==f||0!==c&&3!==v.nodeType||(h=d+c),3===v.nodeType&&(d+=v.nodeValue.length),null!==(b=v.firstChild);)y=v,v=b;for(;;){if(v===a)break t;if(y===l&&++m===s&&(p=d),y===f&&++g===c&&(h=d),null!==(b=v.nextSibling))break;y=(v=y).parentNode}v=b}l=-1===p||-1===h?null:{start:p,end:h}}else l=null}l=l||{start:0,end:0}}else l=null;gn={activeElementDetached:null,focusedElem:a,selectionRange:l},qt=!1,za=i;do{try{hl()}catch(C){if(null===za)throw Error(u(330));yl(za,C),za=za.nextEffect}}while(null!==za);za=i;do{try{for(a=e,l=t;null!==za;){var w=za.effectTag;if(16&w&&je(za.stateNode,""),128&w){var x=za.alternate;if(null!==x){var _=x.ref;null!==_&&("function"===typeof _?_(null):_.current=null)}}switch(1038&w){case 2:ca(za),za.effectTag&=-3;break;case 6:ca(za),za.effectTag&=-3,fa(za.alternate,za);break;case 1024:za.effectTag&=-1025;break;case 1028:za.effectTag&=-1025,fa(za.alternate,za);break;case 4:fa(za.alternate,za);break;case 8:sa(a,s=za,l),aa(s)}za=za.nextEffect}}catch(C){if(null===za)throw Error(u(330));yl(za,C),za=za.nextEffect}}while(null!==za);if(_=gn,x=pn(),w=_.focusedElem,l=_.selectionRange,x!==w&&w&&w.ownerDocument&&function e(t,n){return!(!t||!n)&&(t===n||(!t||3!==t.nodeType)&&(n&&3===n.nodeType?e(t,n.parentNode):"contains"in t?t.contains(n):!!t.compareDocumentPosition&&!!(16&t.compareDocumentPosition(n))))}(w.ownerDocument.documentElement,w)){null!==l&&hn(w)&&(x=l.start,void 0===(_=l.end)&&(_=x),"selectionStart"in w?(w.selectionStart=x,w.selectionEnd=Math.min(_,w.value.length)):(_=(x=w.ownerDocument||document)&&x.defaultView||window).getSelection&&(_=_.getSelection(),s=w.textContent.length,a=Math.min(l.start,s),l=void 0===l.end?a:Math.min(l.end,s),!_.extend&&a>l&&(s=l,l=a,a=s),s=dn(w,a),f=dn(w,l),s&&f&&(1!==_.rangeCount||_.anchorNode!==s.node||_.anchorOffset!==s.offset||_.focusNode!==f.node||_.focusOffset!==f.offset)&&((x=x.createRange()).setStart(s.node,s.offset),_.removeAllRanges(),a>l?(_.addRange(x),_.extend(f.node,f.offset)):(x.setEnd(f.node,f.offset),_.addRange(x))))),x=[];for(_=w;_=_.parentNode;)1===_.nodeType&&x.push({element:_,left:_.scrollLeft,top:_.scrollTop});for("function"===typeof w.focus&&w.focus(),w=0;w<x.length;w++)(_=x[w]).element.scrollLeft=_.left,_.element.scrollTop=_.top}qt=!!mn,gn=mn=null,e.current=n,za=i;do{try{for(w=e;null!==za;){var k=za.effectTag;if(36&k&&oa(w,za.alternate,za),128&k){x=void 0;var T=za.ref;if(null!==T){var E=za.stateNode;switch(za.tag){case 5:x=E;break;default:x=E}"function"===typeof T?T(x):T.current=x}}za=za.nextEffect}}catch(C){if(null===za)throw Error(u(330));yl(za,C),za=za.nextEffect}}while(null!==za);za=null,Oi(),ka=o}else e.current=n;if(Ia)Ia=!1,ja=e,Ha=t;else for(za=i;null!==za;)t=za.nextEffect,za.nextEffect=null,za=t;if(0===(t=e.firstPendingTime)&&(Fa=null),1073741823===t?e===Wa?$a++:($a=0,Wa=e):$a=0,"function"===typeof xl&&xl(n.stateNode,r),Za(e),La)throw La=!1,e=Ua,Ua=null,e;return 0!==(8&ka)||Qi(),null}function hl(){for(;null!==za;){var e=za.effectTag;0!==(256&e)&&na(za.alternate,za),0===(512&e)||Ia||(Ia=!0,$i(97,(function(){return ml(),null}))),za=za.nextEffect}}function ml(){if(90!==Ha){var e=97<Ha?97:Ha;return Ha=90,Vi(e,gl)}}function gl(){if(null===ja)return!1;var e=ja;if(ja=null,0!==(48&ka))throw Error(u(331));var t=ka;for(ka|=32,e=e.current.firstEffect;null!==e;){try{var n=e;if(0!==(512&n.effectTag))switch(n.tag){case 0:case 11:case 15:case 22:ra(5,n),ia(5,n)}}catch(r){if(null===e)throw Error(u(330));yl(e,r)}n=e.nextEffect,e.nextEffect=null,e=n}return ka=t,Qi(),!0}function vl(e,t,n){lo(e,t=ha(e,t=Ju(n,t),1073741823)),null!==(e=Xa(e,1073741823))&&Za(e)}function yl(e,t){if(3===e.tag)vl(e,e,t);else for(var n=e.return;null!==n;){if(3===n.tag){vl(n,e,t);break}if(1===n.tag){var r=n.stateNode;if("function"===typeof n.type.getDerivedStateFromError||"function"===typeof r.componentDidCatch&&(null===Fa||!Fa.has(r))){lo(n,e=ma(n,e=Ju(t,e),1073741823)),null!==(n=Xa(n,1073741823))&&Za(n);break}}n=n.return}}function bl(e,t,n){var r=e.pingCache;null!==r&&r.delete(t),Ta===e&&Ca===n?Sa===_a||Sa===xa&&1073741823===Na&&Ii()-Oa<500?nl(e,Ca):Da=!0:Rl(e,n)&&(0!==(t=e.lastPingedTime)&&t<n||(e.lastPingedTime=n,Za(e)))}function wl(e,t){var n=e.stateNode;null!==n&&n.delete(t),0===(t=0)&&(t=Ba(t=qa(),e,null)),null!==(e=Xa(e,t))&&Za(e)}ga=function(e,t,n){var r=t.expirationTime;if(null!==e){var i=t.pendingProps;if(e.memoizedProps!==i||di.current)Pu=!0;else{if(r<n){switch(Pu=!1,t.tag){case 3:Iu(t),Mu();break;case 5:if(Oo(t),4&t.mode&&1!==n&&i.hidden)return t.expirationTime=t.childExpirationTime=1,null;break;case 1:mi(t.type)&&bi(t);break;case 4:Ro(t,t.stateNode.containerInfo);break;case 10:r=t.memoizedProps.value,i=t.type._context,ci(Xi,i._currentValue),i._currentValue=r;break;case 13:if(null!==t.memoizedState)return 0!==(r=t.child.childExpirationTime)&&r>=n?Wu(e,t,n):(ci(Lo,1&Lo.current),null!==(t=Yu(e,t,n))?t.sibling:null);ci(Lo,1&Lo.current);break;case 19:if(r=t.childExpirationTime>=n,0!==(64&e.effectTag)){if(r)return Bu(e,t,n);t.effectTag|=64}if(null!==(i=t.memoizedState)&&(i.rendering=null,i.tail=null),ci(Lo,Lo.current),!r)return null}return Yu(e,t,n)}Pu=!1}}else Pu=!1;switch(t.expirationTime=0,t.tag){case 2:if(r=t.type,null!==e&&(e.alternate=null,t.alternate=null,t.effectTag|=2),e=t.pendingProps,i=hi(t,fi.current),no(t,n),i=Yo(null,t,r,e,i,n),t.effectTag|=1,"object"===typeof i&&null!==i&&"function"===typeof i.render&&void 0===i.$$typeof){if(t.tag=1,t.memoizedState=null,t.updateQueue=null,mi(r)){var o=!0;bi(t)}else o=!1;t.memoizedState=null!==i.state&&void 0!==i.state?i.state:null,oo(t);var a=r.getDerivedStateFromProps;"function"===typeof a&&mo(t,r,a,e),i.updater=go,t.stateNode=i,i._reactInternalFiber=t,wo(t,r,e,n),t=Fu(null,t,r,!0,o,n)}else t.tag=0,Au(null,t,i,n),t=t.child;return t;case 16:e:{if(i=t.elementType,null!==e&&(e.alternate=null,t.alternate=null,t.effectTag|=2),e=t.pendingProps,function(e){if(-1===e._status){e._status=0;var t=e._ctor;t=t(),e._result=t,t.then((function(t){0===e._status&&(t=t.default,e._status=1,e._result=t)}),(function(t){0===e._status&&(e._status=2,e._result=t)}))}}(i),1!==i._status)throw i._result;switch(i=i._result,t.type=i,o=t.tag=function(e){if("function"===typeof e)return El(e)?1:0;if(void 0!==e&&null!==e){if((e=e.$$typeof)===le)return 11;if(e===fe)return 14}return 2}(i),e=Yi(i,e),o){case 0:t=Lu(null,t,i,e,n);break e;case 1:t=Uu(null,t,i,e,n);break e;case 11:t=Ru(null,t,i,e,n);break e;case 14:t=Du(null,t,i,Yi(i.type,e),r,n);break e}throw Error(u(306,i,""))}return t;case 0:return r=t.type,i=t.pendingProps,Lu(e,t,r,i=t.elementType===r?i:Yi(r,i),n);case 1:return r=t.type,i=t.pendingProps,Uu(e,t,r,i=t.elementType===r?i:Yi(r,i),n);case 3:if(Iu(t),r=t.updateQueue,null===e||null===r)throw Error(u(282));if(r=t.pendingProps,i=null!==(i=t.memoizedState)?i.element:null,uo(e,t),so(t,r,null,n),(r=t.memoizedState.element)===i)Mu(),t=Yu(e,t,n);else{if((i=t.stateNode.hydrate)&&(xu=xn(t.stateNode.containerInfo.firstChild),wu=t,i=_u=!0),i)for(n=Co(t,null,r,n),t.child=n;n;)n.effectTag=-3&n.effectTag|1024,n=n.sibling;else Au(e,t,r,n),Mu();t=t.child}return t;case 5:return Oo(t),null===e&&Eu(t),r=t.type,i=t.pendingProps,o=null!==e?e.memoizedProps:null,a=i.children,yn(r,i)?a=null:null!==o&&yn(r,o)&&(t.effectTag|=16),zu(e,t),4&t.mode&&1!==n&&i.hidden?(t.expirationTime=t.childExpirationTime=1,t=null):(Au(e,t,a,n),t=t.child),t;case 6:return null===e&&Eu(t),null;case 13:return Wu(e,t,n);case 4:return Ro(t,t.stateNode.containerInfo),r=t.pendingProps,null===e?t.child=Eo(t,null,r,n):Au(e,t,r,n),t.child;case 11:return r=t.type,i=t.pendingProps,Ru(e,t,r,i=t.elementType===r?i:Yi(r,i),n);case 7:return Au(e,t,t.pendingProps,n),t.child;case 8:case 12:return Au(e,t,t.pendingProps.children,n),t.child;case 10:e:{r=t.type._context,i=t.pendingProps,a=t.memoizedProps,o=i.value;var l=t.type._context;if(ci(Xi,l._currentValue),l._currentValue=o,null!==a)if(l=a.value,0===(o=Ur(l,o)?0:0|("function"===typeof r._calculateChangedBits?r._calculateChangedBits(l,o):1073741823))){if(a.children===i.children&&!di.current){t=Yu(e,t,n);break e}}else for(null!==(l=t.child)&&(l.return=t);null!==l;){var c=l.dependencies;if(null!==c){a=l.child;for(var s=c.firstContext;null!==s;){if(s.context===r&&0!==(s.observedBits&o)){1===l.tag&&((s=ao(n,null)).tag=2,lo(l,s)),l.expirationTime<n&&(l.expirationTime=n),null!==(s=l.alternate)&&s.expirationTime<n&&(s.expirationTime=n),to(l.return,n),c.expirationTime<n&&(c.expirationTime=n);break}s=s.next}}else a=10===l.tag&&l.type===t.type?null:l.child;if(null!==a)a.return=l;else for(a=l;null!==a;){if(a===t){a=null;break}if(null!==(l=a.sibling)){l.return=a.return,a=l;break}a=a.return}l=a}Au(e,t,i.children,n),t=t.child}return t;case 9:return i=t.type,r=(o=t.pendingProps).children,no(t,n),r=r(i=ro(i,o.unstable_observedBits)),t.effectTag|=1,Au(e,t,r,n),t.child;case 14:return o=Yi(i=t.type,t.pendingProps),Du(e,t,i,o=Yi(i.type,o),r,n);case 15:return Ou(e,t,t.type,t.pendingProps,r,n);case 17:return r=t.type,i=t.pendingProps,i=t.elementType===r?i:Yi(r,i),null!==e&&(e.alternate=null,t.alternate=null,t.effectTag|=2),t.tag=1,mi(r)?(e=!0,bi(t)):e=!1,no(t,n),yo(t,r,i),wo(t,r,i,n),Fu(null,t,r,!0,e,n);case 19:return Bu(e,t,n)}throw Error(u(156,t.tag))};var xl=null,_l=null;function kl(e,t,n,r){this.tag=e,this.key=n,this.sibling=this.child=this.return=this.stateNode=this.type=this.elementType=null,this.index=0,this.ref=null,this.pendingProps=t,this.dependencies=this.memoizedState=this.updateQueue=this.memoizedProps=null,this.mode=r,this.effectTag=0,this.lastEffect=this.firstEffect=this.nextEffect=null,this.childExpirationTime=this.expirationTime=0,this.alternate=null}function Tl(e,t,n,r){return new kl(e,t,n,r)}function El(e){return!(!(e=e.prototype)||!e.isReactComponent)}function Cl(e,t){var n=e.alternate;return null===n?((n=Tl(e.tag,t,e.key,e.mode)).elementType=e.elementType,n.type=e.type,n.stateNode=e.stateNode,n.alternate=e,e.alternate=n):(n.pendingProps=t,n.effectTag=0,n.nextEffect=null,n.firstEffect=null,n.lastEffect=null),n.childExpirationTime=e.childExpirationTime,n.expirationTime=e.expirationTime,n.child=e.child,n.memoizedProps=e.memoizedProps,n.memoizedState=e.memoizedState,n.updateQueue=e.updateQueue,t=e.dependencies,n.dependencies=null===t?null:{expirationTime:t.expirationTime,firstContext:t.firstContext,responders:t.responders},n.sibling=e.sibling,n.index=e.index,n.ref=e.ref,n}function Sl(e,t,n,r,i,o){var a=2;if(r=e,"function"===typeof e)El(e)&&(a=1);else if("string"===typeof e)a=5;else e:switch(e){case ne:return Ml(n.children,i,o,t);case ae:a=8,i|=7;break;case re:a=8,i|=1;break;case ie:return(e=Tl(12,n,t,8|i)).elementType=ie,e.type=ie,e.expirationTime=o,e;case ce:return(e=Tl(13,n,t,i)).type=ce,e.elementType=ce,e.expirationTime=o,e;case se:return(e=Tl(19,n,t,i)).elementType=se,e.expirationTime=o,e;default:if("object"===typeof e&&null!==e)switch(e.$$typeof){case oe:a=10;break e;case ue:a=9;break e;case le:a=11;break e;case fe:a=14;break e;case de:a=16,r=null;break e;case pe:a=22;break e}throw Error(u(130,null==e?e:typeof e,""))}return(t=Tl(a,n,t,i)).elementType=e,t.type=r,t.expirationTime=o,t}function Ml(e,t,n,r){return(e=Tl(7,e,r,t)).expirationTime=n,e}function Nl(e,t,n){return(e=Tl(6,e,null,t)).expirationTime=n,e}function Pl(e,t,n){return(t=Tl(4,null!==e.children?e.children:[],e.key,t)).expirationTime=n,t.stateNode={containerInfo:e.containerInfo,pendingChildren:null,implementation:e.implementation},t}function Al(e,t,n){this.tag=t,this.current=null,this.containerInfo=e,this.pingCache=this.pendingChildren=null,this.finishedExpirationTime=0,this.finishedWork=null,this.timeoutHandle=-1,this.pendingContext=this.context=null,this.hydrate=n,this.callbackNode=null,this.callbackPriority=90,this.lastExpiredTime=this.lastPingedTime=this.nextKnownPendingLevel=this.lastSuspendedTime=this.firstSuspendedTime=this.firstPendingTime=0}function Rl(e,t){var n=e.firstSuspendedTime;return e=e.lastSuspendedTime,0!==n&&n>=t&&e<=t}function Dl(e,t){var n=e.firstSuspendedTime,r=e.lastSuspendedTime;n<t&&(e.firstSuspendedTime=t),(r>t||0===n)&&(e.lastSuspendedTime=t),t<=e.lastPingedTime&&(e.lastPingedTime=0),t<=e.lastExpiredTime&&(e.lastExpiredTime=0)}function Ol(e,t){t>e.firstPendingTime&&(e.firstPendingTime=t);var n=e.firstSuspendedTime;0!==n&&(t>=n?e.firstSuspendedTime=e.lastSuspendedTime=e.nextKnownPendingLevel=0:t>=e.lastSuspendedTime&&(e.lastSuspendedTime=t+1),t>e.nextKnownPendingLevel&&(e.nextKnownPendingLevel=t))}function zl(e,t){var n=e.lastExpiredTime;(0===n||n>t)&&(e.lastExpiredTime=t)}function Ll(e,t,n,r){var i=t.current,o=qa(),a=po.suspense;o=Ba(o,i,a);e:if(n){t:{if(Ge(n=n._reactInternalFiber)!==n||1!==n.tag)throw Error(u(170));var l=n;do{switch(l.tag){case 3:l=l.stateNode.context;break t;case 1:if(mi(l.type)){l=l.stateNode.__reactInternalMemoizedMergedChildContext;break t}}l=l.return}while(null!==l);throw Error(u(171))}if(1===n.tag){var c=n.type;if(mi(c)){n=yi(n,c,l);break e}}n=l}else n=si;return null===t.context?t.context=n:t.pendingContext=n,(t=ao(o,a)).payload={element:e},null!==(r=void 0===r?null:r)&&(t.callback=r),lo(i,t),Ya(i,o),o}function Ul(e){if(!(e=e.current).child)return null;switch(e.child.tag){case 5:default:return e.child.stateNode}}function Fl(e,t){null!==(e=e.memoizedState)&&null!==e.dehydrated&&e.retryTime<t&&(e.retryTime=t)}function Il(e,t){Fl(e,t),(e=e.alternate)&&Fl(e,t)}function jl(e,t,n){var r=new Al(e,t,n=null!=n&&!0===n.hydrate),i=Tl(3,null,null,2===t?7:1===t?3:0);r.current=i,i.stateNode=r,oo(i),e[Cn]=r.current,n&&0!==t&&function(e,t){var n=Je(t);Ct.forEach((function(e){ht(e,t,n)})),St.forEach((function(e){ht(e,t,n)}))}(0,9===e.nodeType?e:e.ownerDocument),this._internalRoot=r}function Hl(e){return!(!e||1!==e.nodeType&&9!==e.nodeType&&11!==e.nodeType&&(8!==e.nodeType||" react-mount-point-unstable "!==e.nodeValue))}function Vl(e,t,n,r,i){var o=n._reactRootContainer;if(o){var u=o._internalRoot;if("function"===typeof i){var a=i;i=function(){var e=Ul(u);a.call(e)}}Ll(t,u,e,i)}else{if(o=n._reactRootContainer=function(e,t){if(t||(t=!(!(t=e?9===e.nodeType?e.documentElement:e.firstChild:null)||1!==t.nodeType||!t.hasAttribute("data-reactroot"))),!t)for(var n;n=e.lastChild;)e.removeChild(n);return new jl(e,0,t?{hydrate:!0}:void 0)}(n,r),u=o._internalRoot,"function"===typeof i){var l=i;i=function(){var e=Ul(u);l.call(e)}}tl((function(){Ll(t,u,e,i)}))}return Ul(u)}function $l(e,t,n){var r=3<arguments.length&&void 0!==arguments[3]?arguments[3]:null;return{$$typeof:te,key:null==r?null:""+r,children:e,containerInfo:t,implementation:n}}function Wl(e,t){var n=2<arguments.length&&void 0!==arguments[2]?arguments[2]:null;if(!Hl(t))throw Error(u(200));return $l(e,t,null,n)}jl.prototype.render=function(e){Ll(e,this._internalRoot,null,null)},jl.prototype.unmount=function(){var e=this._internalRoot,t=e.containerInfo;Ll(null,e,null,(function(){t[Cn]=null}))},mt=function(e){if(13===e.tag){var t=Bi(qa(),150,100);Ya(e,t),Il(e,t)}},gt=function(e){13===e.tag&&(Ya(e,3),Il(e,3))},vt=function(e){if(13===e.tag){var t=qa();Ya(e,t=Ba(t,e,null)),Il(e,t)}},M=function(e,t,n){switch(t){case"input":if(Ee(e,n),t=n.name,"radio"===n.type&&null!=t){for(n=e;n.parentNode;)n=n.parentNode;for(n=n.querySelectorAll("input[name="+JSON.stringify(""+t)+'][type="radio"]'),t=0;t<n.length;t++){var r=n[t];if(r!==e&&r.form===e.form){var i=Pn(r);if(!i)throw Error(u(90));xe(r),Ee(r,i)}}}break;case"textarea":Re(e,n);break;case"select":null!=(t=n.value)&&Ne(e,!!n.multiple,t,!1)}},O=el,z=function(e,t,n,r,i){var o=ka;ka|=4;try{return Vi(98,e.bind(null,t,n,r,i))}finally{0===(ka=o)&&Qi()}},L=function(){0===(49&ka)&&(function(){if(null!==Va){var e=Va;Va=null,e.forEach((function(e,t){zl(t,e),Za(t)})),Qi()}}(),ml())},U=function(e,t){var n=ka;ka|=2;try{return e(t)}finally{0===(ka=n)&&Qi()}};var Ql={Events:[Mn,Nn,Pn,C,k,Un,function(e){it(e,Ln)},R,D,Zt,at,ml,{current:!1}]};!function(e){var t=e.findFiberByHostInstance;(function(e){if("undefined"===typeof __REACT_DEVTOOLS_GLOBAL_HOOK__)return!1;var t=__REACT_DEVTOOLS_GLOBAL_HOOK__;if(t.isDisabled||!t.supportsFiber)return!0;try{var n=t.inject(e);xl=function(e){try{t.onCommitFiberRoot(n,e,void 0,64===(64&e.current.effectTag))}catch(r){}},_l=function(e){try{t.onCommitFiberUnmount(n,e)}catch(r){}}}catch(r){}})(i({},e,{overrideHookState:null,overrideProps:null,setSuspenseHandler:null,scheduleUpdate:null,currentDispatcherRef:K.ReactCurrentDispatcher,findHostInstanceByFiber:function(e){return null===(e=nt(e))?null:e.stateNode},findFiberByHostInstance:function(e){return t?t(e):null},findHostInstancesForRefresh:null,scheduleRefresh:null,scheduleRoot:null,setRefreshHandler:null,getCurrentFiber:null}))}({findFiberByHostInstance:Sn,bundleType:0,version:"16.13.1",rendererPackageName:"react-dom"}),t.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED=Ql,t.createPortal=Wl,t.findDOMNode=function(e){if(null==e)return null;if(1===e.nodeType)return e;var t=e._reactInternalFiber;if(void 0===t){if("function"===typeof e.render)throw Error(u(188));throw Error(u(268,Object.keys(e)))}return e=null===(e=nt(t))?null:e.stateNode},t.flushSync=function(e,t){if(0!==(48&ka))throw Error(u(187));var n=ka;ka|=1;try{return Vi(99,e.bind(null,t))}finally{ka=n,Qi()}},t.hydrate=function(e,t,n){if(!Hl(t))throw Error(u(200));return Vl(null,e,t,!0,n)},t.render=function(e,t,n){if(!Hl(t))throw Error(u(200));return Vl(null,e,t,!1,n)},t.unmountComponentAtNode=function(e){if(!Hl(e))throw Error(u(40));return!!e._reactRootContainer&&(tl((function(){Vl(null,null,e,!1,(function(){e._reactRootContainer=null,e[Cn]=null}))})),!0)},t.unstable_batchedUpdates=el,t.unstable_createPortal=function(e,t){return Wl(e,t,2<arguments.length&&void 0!==arguments[2]?arguments[2]:null)},t.unstable_renderSubtreeIntoContainer=function(e,t,n,r){if(!Hl(n))throw Error(u(200));if(null==e||void 0===e._reactInternalFiber)throw Error(u(38));return Vl(e,t,n,!1,r)},t.version="16.13.1"},function(e,t,n){"use strict";e.exports=n(32)},function(e,t,n){"use strict";var r,i,o,u,a;if("undefined"===typeof window||"function"!==typeof MessageChannel){var l=null,c=null,s=function e(){if(null!==l)try{var n=t.unstable_now();l(!0,n),l=null}catch(r){throw setTimeout(e,0),r}},f=Date.now();t.unstable_now=function(){return Date.now()-f},r=function(e){null!==l?setTimeout(r,0,e):(l=e,setTimeout(s,0))},i=function(e,t){c=setTimeout(e,t)},o=function(){clearTimeout(c)},u=function(){return!1},a=t.unstable_forceFrameRate=function(){}}else{var d=window.performance,p=window.Date,h=window.setTimeout,m=window.clearTimeout;if("undefined"!==typeof console){var g=window.cancelAnimationFrame;"function"!==typeof window.requestAnimationFrame&&console.error("This browser doesn't support requestAnimationFrame. Make sure that you load a polyfill in older browsers. https://fb.me/react-polyfills"),"function"!==typeof g&&console.error("This browser doesn't support cancelAnimationFrame. Make sure that you load a polyfill in older browsers. https://fb.me/react-polyfills")}if("object"===typeof d&&"function"===typeof d.now)t.unstable_now=function(){return d.now()};else{var v=p.now();t.unstable_now=function(){return p.now()-v}}var y=!1,b=null,w=-1,x=5,_=0;u=function(){return t.unstable_now()>=_},a=function(){},t.unstable_forceFrameRate=function(e){0>e||125<e?console.error("forceFrameRate takes a positive int between 0 and 125, forcing framerates higher than 125 fps is not unsupported"):x=0<e?Math.floor(1e3/e):5};var k=new MessageChannel,T=k.port2;k.port1.onmessage=function(){if(null!==b){var e=t.unstable_now();_=e+x;try{b(!0,e)?T.postMessage(null):(y=!1,b=null)}catch(n){throw T.postMessage(null),n}}else y=!1},r=function(e){b=e,y||(y=!0,T.postMessage(null))},i=function(e,n){w=h((function(){e(t.unstable_now())}),n)},o=function(){m(w),w=-1}}function E(e,t){var n=e.length;e.push(t);e:for(;;){var r=n-1>>>1,i=e[r];if(!(void 0!==i&&0<M(i,t)))break e;e[r]=t,e[n]=i,n=r}}function C(e){return void 0===(e=e[0])?null:e}function S(e){var t=e[0];if(void 0!==t){var n=e.pop();if(n!==t){e[0]=n;e:for(var r=0,i=e.length;r<i;){var o=2*(r+1)-1,u=e[o],a=o+1,l=e[a];if(void 0!==u&&0>M(u,n))void 0!==l&&0>M(l,u)?(e[r]=l,e[a]=n,r=a):(e[r]=u,e[o]=n,r=o);else{if(!(void 0!==l&&0>M(l,n)))break e;e[r]=l,e[a]=n,r=a}}}return t}return null}function M(e,t){var n=e.sortIndex-t.sortIndex;return 0!==n?n:e.id-t.id}var N=[],P=[],A=1,R=null,D=3,O=!1,z=!1,L=!1;function U(e){for(var t=C(P);null!==t;){if(null===t.callback)S(P);else{if(!(t.startTime<=e))break;S(P),t.sortIndex=t.expirationTime,E(N,t)}t=C(P)}}function F(e){if(L=!1,U(e),!z)if(null!==C(N))z=!0,r(I);else{var t=C(P);null!==t&&i(F,t.startTime-e)}}function I(e,n){z=!1,L&&(L=!1,o()),O=!0;var r=D;try{for(U(n),R=C(N);null!==R&&(!(R.expirationTime>n)||e&&!u());){var a=R.callback;if(null!==a){R.callback=null,D=R.priorityLevel;var l=a(R.expirationTime<=n);n=t.unstable_now(),"function"===typeof l?R.callback=l:R===C(N)&&S(N),U(n)}else S(N);R=C(N)}if(null!==R)var c=!0;else{var s=C(P);null!==s&&i(F,s.startTime-n),c=!1}return c}finally{R=null,D=r,O=!1}}function j(e){switch(e){case 1:return-1;case 2:return 250;case 5:return 1073741823;case 4:return 1e4;default:return 5e3}}var H=a;t.unstable_IdlePriority=5,t.unstable_ImmediatePriority=1,t.unstable_LowPriority=4,t.unstable_NormalPriority=3,t.unstable_Profiling=null,t.unstable_UserBlockingPriority=2,t.unstable_cancelCallback=function(e){e.callback=null},t.unstable_continueExecution=function(){z||O||(z=!0,r(I))},t.unstable_getCurrentPriorityLevel=function(){return D},t.unstable_getFirstCallbackNode=function(){return C(N)},t.unstable_next=function(e){switch(D){case 1:case 2:case 3:var t=3;break;default:t=D}var n=D;D=t;try{return e()}finally{D=n}},t.unstable_pauseExecution=function(){},t.unstable_requestPaint=H,t.unstable_runWithPriority=function(e,t){switch(e){case 1:case 2:case 3:case 4:case 5:break;default:e=3}var n=D;D=e;try{return t()}finally{D=n}},t.unstable_scheduleCallback=function(e,n,u){var a=t.unstable_now();if("object"===typeof u&&null!==u){var l=u.delay;l="number"===typeof l&&0<l?a+l:a,u="number"===typeof u.timeout?u.timeout:j(e)}else u=j(e),l=a;return e={id:A++,callback:n,priorityLevel:e,startTime:l,expirationTime:u=l+u,sortIndex:-1},l>a?(e.sortIndex=l,E(P,e),null===C(N)&&e===C(P)&&(L?o():L=!0,i(F,l-a))):(e.sortIndex=u,E(N,e),z||O||(z=!0,r(I))),e},t.unstable_shouldYield=function(){var e=t.unstable_now();U(e);var n=C(N);return n!==R&&null!==R&&null!==n&&null!==n.callback&&n.startTime<=e&&n.expirationTime<R.expirationTime||u()},t.unstable_wrapCallback=function(e){var t=D;return function(){var n=D;D=t;try{return e.apply(this,arguments)}finally{D=n}}}}]]);
//# sourceMappingURL=2.64b0803b.chunk.js.map

# kiara\TopicModelling-\vis-files\tm_1\jscode-test-2.js
!function(e){function t(t){for(var n,i,l=t[0],a=t[1],f=t[2],c=0,s=[];c<l.length;c++)i=l[c],Object.prototype.hasOwnProperty.call(o,i)&&o[i]&&s.push(o[i][0]),o[i]=0;for(n in a)Object.prototype.hasOwnProperty.call(a,n)&&(e[n]=a[n]);for(p&&p(t);s.length;)s.shift()();return u.push.apply(u,f||[]),r()}function r(){for(var e,t=0;t<u.length;t++){for(var r=u[t],n=!0,l=1;l<r.length;l++){var a=r[l];0!==o[a]&&(n=!1)}n&&(u.splice(t--,1),e=i(i.s=r[0]))}return e}var n={},o={1:0},u=[];function i(t){if(n[t])return n[t].exports;var r=n[t]={i:t,l:!1,exports:{}};return e[t].call(r.exports,r,r.exports,i),r.l=!0,r.exports}i.m=e,i.c=n,i.d=function(e,t,r){i.o(e,t)||Object.defineProperty(e,t,{enumerable:!0,get:r})},i.r=function(e){"undefined"!=typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(e,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(e,"__esModule",{value:!0})},i.t=function(e,t){if(1&t&&(e=i(e)),8&t)return e;if(4&t&&"object"==typeof e&&e&&e.__esModule)return e;var r=Object.create(null);if(i.r(r),Object.defineProperty(r,"default",{enumerable:!0,value:e}),2&t&&"string"!=typeof e)for(var n in e)i.d(r,n,function(t){return e[t]}.bind(null,n));return r},i.n=function(e){var t=e&&e.__esModule?function(){return e.default}:function(){return e};return i.d(t,"a",t),t},i.o=function(e,t){return Object.prototype.hasOwnProperty.call(e,t)},i.p="/";var l=this["webpackJsonpvisualisation-d3"]=this["webpackJsonpvisualisation-d3"]||[],a=l.push.bind(l);l.push=t,l=l.slice();for(var f=0;f<l.length;f++)t(l[f]);var p=a;r()}([]);

(this["webpackJsonpvisualisation-d3"]=this["webpackJsonpvisualisation-d3"]||[]).push([[2],[function(e,t,n){"use strict";e.exports=n(29)},,function(e,t,n){"use strict";n.d(t,"b",(function(){return m})),n.d(t,"c",(function(){return g})),n.d(t,"d",(function(){return Zn})),n.d(t,"a",(function(){return br})),n.d(t,"e",(function(){return pi}));var r=function(e,t){return e<t?-1:e>t?1:e>=t?0:NaN},i=function(e){var t;return 1===e.length&&(t=e,e=function(e,n){return r(t(e),n)}),{left:function(t,n,r,i){for(null==r&&(r=0),null==i&&(i=t.length);r<i;){var o=r+i>>>1;e(t[o],n)<0?r=o+1:i=o}return r},right:function(t,n,r,i){for(null==r&&(r=0),null==i&&(i=t.length);r<i;){var o=r+i>>>1;e(t[o],n)>0?i=o:r=o+1}return r}}};var o=i(r),u=o.right,a=(o.left,u);var l=Array.prototype,c=(l.slice,l.map,Math.sqrt(50)),s=Math.sqrt(10),f=Math.sqrt(2),d=function(e,t,n){var r,i,o,u,a=-1;if(n=+n,(e=+e)===(t=+t)&&n>0)return[e];if((r=t<e)&&(i=e,e=t,t=i),0===(u=p(e,t,n))||!isFinite(u))return[];if(u>0)for(e=Math.ceil(e/u),t=Math.floor(t/u),o=new Array(i=Math.ceil(t-e+1));++a<i;)o[a]=(e+a)*u;else for(e=Math.floor(e*u),t=Math.ceil(t*u),o=new Array(i=Math.ceil(e-t+1));++a<i;)o[a]=(e-a)/u;return r&&o.reverse(),o};function p(e,t,n){var r=(t-e)/Math.max(0,n),i=Math.floor(Math.log(r)/Math.LN10),o=r/Math.pow(10,i);return i>=0?(o>=c?10:o>=s?5:o>=f?2:1)*Math.pow(10,i):-Math.pow(10,-i)/(o>=c?10:o>=s?5:o>=f?2:1)}function h(e,t,n){var r=Math.abs(t-e)/Math.max(0,n),i=Math.pow(10,Math.floor(Math.log(r)/Math.LN10)),o=r/i;return o>=c?i*=10:o>=s?i*=5:o>=f&&(i*=2),t<e?-i:i}var m=function(e,t){var n,r,i=e.length,o=-1;if(null==t){for(;++o<i;)if(null!=(n=e[o])&&n>=n)for(r=n;++o<i;)null!=(n=e[o])&&n>r&&(r=n)}else for(;++o<i;)if(null!=(n=t(e[o],o,e))&&n>=n)for(r=n;++o<i;)null!=(n=t(e[o],o,e))&&n>r&&(r=n);return r},g=function(e,t){var n,r,i=e.length,o=-1;if(null==t){for(;++o<i;)if(null!=(n=e[o])&&n>=n)for(r=n;++o<i;)null!=(n=e[o])&&r>n&&(r=n)}else for(;++o<i;)if(null!=(n=t(e[o],o,e))&&n>=n)for(r=n;++o<i;)null!=(n=t(e[o],o,e))&&r>n&&(r=n);return r};Array.prototype.slice;var v={value:function(){}};function y(){for(var e,t=0,n=arguments.length,r={};t<n;++t){if(!(e=arguments[t]+"")||e in r||/[\s.]/.test(e))throw new Error("illegal type: "+e);r[e]=[]}return new b(r)}function b(e){this._=e}function w(e,t){return e.trim().split(/^|\s+/).map((function(e){var n="",r=e.indexOf(".");if(r>=0&&(n=e.slice(r+1),e=e.slice(0,r)),e&&!t.hasOwnProperty(e))throw new Error("unknown type: "+e);return{type:e,name:n}}))}function x(e,t){for(var n,r=0,i=e.length;r<i;++r)if((n=e[r]).name===t)return n.value}function _(e,t,n){for(var r=0,i=e.length;r<i;++r)if(e[r].name===t){e[r]=v,e=e.slice(0,r).concat(e.slice(r+1));break}return null!=n&&e.push({name:t,value:n}),e}b.prototype=y.prototype={constructor:b,on:function(e,t){var n,r=this._,i=w(e+"",r),o=-1,u=i.length;if(!(arguments.length<2)){if(null!=t&&"function"!==typeof t)throw new Error("invalid callback: "+t);for(;++o<u;)if(n=(e=i[o]).type)r[n]=_(r[n],e.name,t);else if(null==t)for(n in r)r[n]=_(r[n],e.name,null);return this}for(;++o<u;)if((n=(e=i[o]).type)&&(n=x(r[n],e.name)))return n},copy:function(){var e={},t=this._;for(var n in t)e[n]=t[n].slice();return new b(e)},call:function(e,t){if((n=arguments.length-2)>0)for(var n,r,i=new Array(n),o=0;o<n;++o)i[o]=arguments[o+2];if(!this._.hasOwnProperty(e))throw new Error("unknown type: "+e);for(o=0,n=(r=this._[e]).length;o<n;++o)r[o].value.apply(t,i)},apply:function(e,t,n){if(!this._.hasOwnProperty(e))throw new Error("unknown type: "+e);for(var r=this._[e],i=0,o=r.length;i<o;++i)r[i].value.apply(t,n)}};var k=y;function T(){}var E=function(e){return null==e?T:function(){return this.querySelector(e)}};function C(){return[]}var S=function(e){return null==e?C:function(){return this.querySelectorAll(e)}},M=function(e){return function(){return this.matches(e)}},N=function(e){return new Array(e.length)};function P(e,t){this.ownerDocument=e.ownerDocument,this.namespaceURI=e.namespaceURI,this._next=null,this._parent=e,this.__data__=t}P.prototype={constructor:P,appendChild:function(e){return this._parent.insertBefore(e,this._next)},insertBefore:function(e,t){return this._parent.insertBefore(e,t)},querySelector:function(e){return this._parent.querySelector(e)},querySelectorAll:function(e){return this._parent.querySelectorAll(e)}};function A(e,t,n,r,i,o){for(var u,a=0,l=t.length,c=o.length;a<c;++a)(u=t[a])?(u.__data__=o[a],r[a]=u):n[a]=new P(e,o[a]);for(;a<l;++a)(u=t[a])&&(i[a]=u)}function R(e,t,n,r,i,o,u){var a,l,c,s={},f=t.length,d=o.length,p=new Array(f);for(a=0;a<f;++a)(l=t[a])&&(p[a]=c="$"+u.call(l,l.__data__,a,t),c in s?i[a]=l:s[c]=l);for(a=0;a<d;++a)(l=s[c="$"+u.call(e,o[a],a,o)])?(r[a]=l,l.__data__=o[a],s[c]=null):n[a]=new P(e,o[a]);for(a=0;a<f;++a)(l=t[a])&&s[p[a]]===l&&(i[a]=l)}function D(e,t){return e<t?-1:e>t?1:e>=t?0:NaN}var O="http://www.w3.org/1999/xhtml",z={svg:"http://www.w3.org/2000/svg",xhtml:O,xlink:"http://www.w3.org/1999/xlink",xml:"http://www.w3.org/XML/1998/namespace",xmlns:"http://www.w3.org/2000/xmlns/"},L=function(e){var t=e+="",n=t.indexOf(":");return n>=0&&"xmlns"!==(t=e.slice(0,n))&&(e=e.slice(n+1)),z.hasOwnProperty(t)?{space:z[t],local:e}:e};function U(e){return function(){this.removeAttribute(e)}}function F(e){return function(){this.removeAttributeNS(e.space,e.local)}}function I(e,t){return function(){this.setAttribute(e,t)}}function j(e,t){return function(){this.setAttributeNS(e.space,e.local,t)}}function H(e,t){return function(){var n=t.apply(this,arguments);null==n?this.removeAttribute(e):this.setAttribute(e,n)}}function V(e,t){return function(){var n=t.apply(this,arguments);null==n?this.removeAttributeNS(e.space,e.local):this.setAttributeNS(e.space,e.local,n)}}var $=function(e){return e.ownerDocument&&e.ownerDocument.defaultView||e.document&&e||e.defaultView};function W(e){return function(){this.style.removeProperty(e)}}function Q(e,t,n){return function(){this.style.setProperty(e,t,n)}}function q(e,t,n){return function(){var r=t.apply(this,arguments);null==r?this.style.removeProperty(e):this.style.setProperty(e,r,n)}}function B(e,t){return e.style.getPropertyValue(t)||$(e).getComputedStyle(e,null).getPropertyValue(t)}function Y(e){return function(){delete this[e]}}function X(e,t){return function(){this[e]=t}}function K(e,t){return function(){var n=t.apply(this,arguments);null==n?delete this[e]:this[e]=n}}function Z(e){return e.trim().split(/^|\s+/)}function J(e){return e.classList||new G(e)}function G(e){this._node=e,this._names=Z(e.getAttribute("class")||"")}function ee(e,t){for(var n=J(e),r=-1,i=t.length;++r<i;)n.add(t[r])}function te(e,t){for(var n=J(e),r=-1,i=t.length;++r<i;)n.remove(t[r])}function ne(e){return function(){ee(this,e)}}function re(e){return function(){te(this,e)}}function ie(e,t){return function(){(t.apply(this,arguments)?ee:te)(this,e)}}G.prototype={add:function(e){this._names.indexOf(e)<0&&(this._names.push(e),this._node.setAttribute("class",this._names.join(" ")))},remove:function(e){var t=this._names.indexOf(e);t>=0&&(this._names.splice(t,1),this._node.setAttribute("class",this._names.join(" ")))},contains:function(e){return this._names.indexOf(e)>=0}};function oe(){this.textContent=""}function ue(e){return function(){this.textContent=e}}function ae(e){return function(){var t=e.apply(this,arguments);this.textContent=null==t?"":t}}function le(){this.innerHTML=""}function ce(e){return function(){this.innerHTML=e}}function se(e){return function(){var t=e.apply(this,arguments);this.innerHTML=null==t?"":t}}function fe(){this.nextSibling&&this.parentNode.appendChild(this)}function de(){this.previousSibling&&this.parentNode.insertBefore(this,this.parentNode.firstChild)}function pe(e){return function(){var t=this.ownerDocument,n=this.namespaceURI;return n===O&&t.documentElement.namespaceURI===O?t.createElement(e):t.createElementNS(n,e)}}function he(e){return function(){return this.ownerDocument.createElementNS(e.space,e.local)}}var me=function(e){var t=L(e);return(t.local?he:pe)(t)};function ge(){return null}function ve(){var e=this.parentNode;e&&e.removeChild(this)}function ye(){var e=this.cloneNode(!1),t=this.parentNode;return t?t.insertBefore(e,this.nextSibling):e}function be(){var e=this.cloneNode(!0),t=this.parentNode;return t?t.insertBefore(e,this.nextSibling):e}var we={},xe=null;"undefined"!==typeof document&&("onmouseenter"in document.documentElement||(we={mouseenter:"mouseover",mouseleave:"mouseout"}));function _e(e,t,n){return e=ke(e,t,n),function(t){var n=t.relatedTarget;n&&(n===this||8&n.compareDocumentPosition(this))||e.call(this,t)}}function ke(e,t,n){return function(r){var i=xe;xe=r;try{e.call(this,this.__data__,t,n)}finally{xe=i}}}function Te(e){return e.trim().split(/^|\s+/).map((function(e){var t="",n=e.indexOf(".");return n>=0&&(t=e.slice(n+1),e=e.slice(0,n)),{type:e,name:t}}))}function Ee(e){return function(){var t=this.__on;if(t){for(var n,r=0,i=-1,o=t.length;r<o;++r)n=t[r],e.type&&n.type!==e.type||n.name!==e.name?t[++i]=n:this.removeEventListener(n.type,n.listener,n.capture);++i?t.length=i:delete this.__on}}}function Ce(e,t,n){var r=we.hasOwnProperty(e.type)?_e:ke;return function(i,o,u){var a,l=this.__on,c=r(t,o,u);if(l)for(var s=0,f=l.length;s<f;++s)if((a=l[s]).type===e.type&&a.name===e.name)return this.removeEventListener(a.type,a.listener,a.capture),this.addEventListener(a.type,a.listener=c,a.capture=n),void(a.value=t);this.addEventListener(e.type,c,n),a={type:e.type,name:e.name,value:t,listener:c,capture:n},l?l.push(a):this.__on=[a]}}function Se(e,t,n){var r=$(e),i=r.CustomEvent;"function"===typeof i?i=new i(t,n):(i=r.document.createEvent("Event"),n?(i.initEvent(t,n.bubbles,n.cancelable),i.detail=n.detail):i.initEvent(t,!1,!1)),e.dispatchEvent(i)}function Me(e,t){return function(){return Se(this,e,t)}}function Ne(e,t){return function(){return Se(this,e,t.apply(this,arguments))}}var Pe=[null];function Ae(e,t){this._groups=e,this._parents=t}function Re(){return new Ae([[document.documentElement]],Pe)}Ae.prototype=Re.prototype={constructor:Ae,select:function(e){"function"!==typeof e&&(e=E(e));for(var t=this._groups,n=t.length,r=new Array(n),i=0;i<n;++i)for(var o,u,a=t[i],l=a.length,c=r[i]=new Array(l),s=0;s<l;++s)(o=a[s])&&(u=e.call(o,o.__data__,s,a))&&("__data__"in o&&(u.__data__=o.__data__),c[s]=u);return new Ae(r,this._parents)},selectAll:function(e){"function"!==typeof e&&(e=S(e));for(var t=this._groups,n=t.length,r=[],i=[],o=0;o<n;++o)for(var u,a=t[o],l=a.length,c=0;c<l;++c)(u=a[c])&&(r.push(e.call(u,u.__data__,c,a)),i.push(u));return new Ae(r,i)},filter:function(e){"function"!==typeof e&&(e=M(e));for(var t=this._groups,n=t.length,r=new Array(n),i=0;i<n;++i)for(var o,u=t[i],a=u.length,l=r[i]=[],c=0;c<a;++c)(o=u[c])&&e.call(o,o.__data__,c,u)&&l.push(o);return new Ae(r,this._parents)},data:function(e,t){if(!e)return h=new Array(this.size()),s=-1,this.each((function(e){h[++s]=e})),h;var n,r=t?R:A,i=this._parents,o=this._groups;"function"!==typeof e&&(n=e,e=function(){return n});for(var u=o.length,a=new Array(u),l=new Array(u),c=new Array(u),s=0;s<u;++s){var f=i[s],d=o[s],p=d.length,h=e.call(f,f&&f.__data__,s,i),m=h.length,g=l[s]=new Array(m),v=a[s]=new Array(m);r(f,d,g,v,c[s]=new Array(p),h,t);for(var y,b,w=0,x=0;w<m;++w)if(y=g[w]){for(w>=x&&(x=w+1);!(b=v[x])&&++x<m;);y._next=b||null}}return(a=new Ae(a,i))._enter=l,a._exit=c,a},enter:function(){return new Ae(this._enter||this._groups.map(N),this._parents)},exit:function(){return new Ae(this._exit||this._groups.map(N),this._parents)},join:function(e,t,n){var r=this.enter(),i=this,o=this.exit();return r="function"===typeof e?e(r):r.append(e+""),null!=t&&(i=t(i)),null==n?o.remove():n(o),r&&i?r.merge(i).order():i},merge:function(e){for(var t=this._groups,n=e._groups,r=t.length,i=n.length,o=Math.min(r,i),u=new Array(r),a=0;a<o;++a)for(var l,c=t[a],s=n[a],f=c.length,d=u[a]=new Array(f),p=0;p<f;++p)(l=c[p]||s[p])&&(d[p]=l);for(;a<r;++a)u[a]=t[a];return new Ae(u,this._parents)},order:function(){for(var e=this._groups,t=-1,n=e.length;++t<n;)for(var r,i=e[t],o=i.length-1,u=i[o];--o>=0;)(r=i[o])&&(u&&4^r.compareDocumentPosition(u)&&u.parentNode.insertBefore(r,u),u=r);return this},sort:function(e){function t(t,n){return t&&n?e(t.__data__,n.__data__):!t-!n}e||(e=D);for(var n=this._groups,r=n.length,i=new Array(r),o=0;o<r;++o){for(var u,a=n[o],l=a.length,c=i[o]=new Array(l),s=0;s<l;++s)(u=a[s])&&(c[s]=u);c.sort(t)}return new Ae(i,this._parents).order()},call:function(){var e=arguments[0];return arguments[0]=this,e.apply(null,arguments),this},nodes:function(){var e=new Array(this.size()),t=-1;return this.each((function(){e[++t]=this})),e},node:function(){for(var e=this._groups,t=0,n=e.length;t<n;++t)for(var r=e[t],i=0,o=r.length;i<o;++i){var u=r[i];if(u)return u}return null},size:function(){var e=0;return this.each((function(){++e})),e},empty:function(){return!this.node()},each:function(e){for(var t=this._groups,n=0,r=t.length;n<r;++n)for(var i,o=t[n],u=0,a=o.length;u<a;++u)(i=o[u])&&e.call(i,i.__data__,u,o);return this},attr:function(e,t){var n=L(e);if(arguments.length<2){var r=this.node();return n.local?r.getAttributeNS(n.space,n.local):r.getAttribute(n)}return this.each((null==t?n.local?F:U:"function"===typeof t?n.local?V:H:n.local?j:I)(n,t))},style:function(e,t,n){return arguments.length>1?this.each((null==t?W:"function"===typeof t?q:Q)(e,t,null==n?"":n)):B(this.node(),e)},property:function(e,t){return arguments.length>1?this.each((null==t?Y:"function"===typeof t?K:X)(e,t)):this.node()[e]},classed:function(e,t){var n=Z(e+"");if(arguments.length<2){for(var r=J(this.node()),i=-1,o=n.length;++i<o;)if(!r.contains(n[i]))return!1;return!0}return this.each(("function"===typeof t?ie:t?ne:re)(n,t))},text:function(e){return arguments.length?this.each(null==e?oe:("function"===typeof e?ae:ue)(e)):this.node().textContent},html:function(e){return arguments.length?this.each(null==e?le:("function"===typeof e?se:ce)(e)):this.node().innerHTML},raise:function(){return this.each(fe)},lower:function(){return this.each(de)},append:function(e){var t="function"===typeof e?e:me(e);return this.select((function(){return this.appendChild(t.apply(this,arguments))}))},insert:function(e,t){var n="function"===typeof e?e:me(e),r=null==t?ge:"function"===typeof t?t:E(t);return this.select((function(){return this.insertBefore(n.apply(this,arguments),r.apply(this,arguments)||null)}))},remove:function(){return this.each(ve)},clone:function(e){return this.select(e?be:ye)},datum:function(e){return arguments.length?this.property("__data__",e):this.node().__data__},on:function(e,t,n){var r,i,o=Te(e+""),u=o.length;if(!(arguments.length<2)){for(a=t?Ce:Ee,null==n&&(n=!1),r=0;r<u;++r)this.each(a(o[r],t,n));return this}var a=this.node().__on;if(a)for(var l,c=0,s=a.length;c<s;++c)for(r=0,l=a[c];r<u;++r)if((i=o[r]).type===l.type&&i.name===l.name)return l.value},dispatch:function(e,t){return this.each(("function"===typeof t?Ne:Me)(e,t))}};var De=Re;var Oe=function(e,t,n){e.prototype=t.prototype=n,n.constructor=e};function ze(e,t){var n=Object.create(e.prototype);for(var r in t)n[r]=t[r];return n}function Le(){}var Ue="\\s*([+-]?\\d+)\\s*",Fe="\\s*([+-]?\\d*\\.?\\d+(?:[eE][+-]?\\d+)?)\\s*",Ie="\\s*([+-]?\\d*\\.?\\d+(?:[eE][+-]?\\d+)?)%\\s*",je=/^#([0-9a-f]{3,8})$/,He=new RegExp("^rgb\\("+[Ue,Ue,Ue]+"\\)$"),Ve=new RegExp("^rgb\\("+[Ie,Ie,Ie]+"\\)$"),$e=new RegExp("^rgba\\("+[Ue,Ue,Ue,Fe]+"\\)$"),We=new RegExp("^rgba\\("+[Ie,Ie,Ie,Fe]+"\\)$"),Qe=new RegExp("^hsl\\("+[Fe,Ie,Ie]+"\\)$"),qe=new RegExp("^hsla\\("+[Fe,Ie,Ie,Fe]+"\\)$"),Be={aliceblue:15792383,antiquewhite:16444375,aqua:65535,aquamarine:8388564,azure:15794175,beige:16119260,bisque:16770244,black:0,blanchedalmond:16772045,blue:255,blueviolet:9055202,brown:10824234,burlywood:14596231,cadetblue:6266528,chartreuse:8388352,chocolate:13789470,coral:16744272,cornflowerblue:6591981,cornsilk:16775388,crimson:14423100,cyan:65535,darkblue:139,darkcyan:35723,darkgoldenrod:12092939,darkgray:11119017,darkgreen:25600,darkgrey:11119017,darkkhaki:12433259,darkmagenta:9109643,darkolivegreen:5597999,darkorange:16747520,darkorchid:10040012,darkred:9109504,darksalmon:15308410,darkseagreen:9419919,darkslateblue:4734347,darkslategray:3100495,darkslategrey:3100495,darkturquoise:52945,darkviolet:9699539,deeppink:16716947,deepskyblue:49151,dimgray:6908265,dimgrey:6908265,dodgerblue:2003199,firebrick:11674146,floralwhite:16775920,forestgreen:2263842,fuchsia:16711935,gainsboro:14474460,ghostwhite:16316671,gold:16766720,goldenrod:14329120,gray:8421504,green:32768,greenyellow:11403055,grey:8421504,honeydew:15794160,hotpink:16738740,indianred:13458524,indigo:4915330,ivory:16777200,khaki:15787660,lavender:15132410,lavenderblush:16773365,lawngreen:8190976,lemonchiffon:16775885,lightblue:11393254,lightcoral:15761536,lightcyan:14745599,lightgoldenrodyellow:16448210,lightgray:13882323,lightgreen:9498256,lightgrey:13882323,lightpink:16758465,lightsalmon:16752762,lightseagreen:2142890,lightskyblue:8900346,lightslategray:7833753,lightslategrey:7833753,lightsteelblue:11584734,lightyellow:16777184,lime:65280,limegreen:3329330,linen:16445670,magenta:16711935,maroon:8388608,mediumaquamarine:6737322,mediumblue:205,mediumorchid:12211667,mediumpurple:9662683,mediumseagreen:3978097,mediumslateblue:8087790,mediumspringgreen:64154,mediumturquoise:4772300,mediumvioletred:13047173,midnightblue:1644912,mintcream:16121850,mistyrose:16770273,moccasin:16770229,navajowhite:16768685,navy:128,oldlace:16643558,olive:8421376,olivedrab:7048739,orange:16753920,orangered:16729344,orchid:14315734,palegoldenrod:15657130,palegreen:10025880,paleturquoise:11529966,palevioletred:14381203,papayawhip:16773077,peachpuff:16767673,peru:13468991,pink:16761035,plum:14524637,powderblue:11591910,purple:8388736,rebeccapurple:6697881,red:16711680,rosybrown:12357519,royalblue:4286945,saddlebrown:9127187,salmon:16416882,sandybrown:16032864,seagreen:3050327,seashell:16774638,sienna:10506797,silver:12632256,skyblue:8900331,slateblue:6970061,slategray:7372944,slategrey:7372944,snow:16775930,springgreen:65407,steelblue:4620980,tan:13808780,teal:32896,thistle:14204888,tomato:16737095,turquoise:4251856,violet:15631086,wheat:16113331,white:16777215,whitesmoke:16119285,yellow:16776960,yellowgreen:10145074};function Ye(){return this.rgb().formatHex()}function Xe(){return this.rgb().formatRgb()}function Ke(e){var t,n;return e=(e+"").trim().toLowerCase(),(t=je.exec(e))?(n=t[1].length,t=parseInt(t[1],16),6===n?Ze(t):3===n?new tt(t>>8&15|t>>4&240,t>>4&15|240&t,(15&t)<<4|15&t,1):8===n?Je(t>>24&255,t>>16&255,t>>8&255,(255&t)/255):4===n?Je(t>>12&15|t>>8&240,t>>8&15|t>>4&240,t>>4&15|240&t,((15&t)<<4|15&t)/255):null):(t=He.exec(e))?new tt(t[1],t[2],t[3],1):(t=Ve.exec(e))?new tt(255*t[1]/100,255*t[2]/100,255*t[3]/100,1):(t=$e.exec(e))?Je(t[1],t[2],t[3],t[4]):(t=We.exec(e))?Je(255*t[1]/100,255*t[2]/100,255*t[3]/100,t[4]):(t=Qe.exec(e))?ot(t[1],t[2]/100,t[3]/100,1):(t=qe.exec(e))?ot(t[1],t[2]/100,t[3]/100,t[4]):Be.hasOwnProperty(e)?Ze(Be[e]):"transparent"===e?new tt(NaN,NaN,NaN,0):null}function Ze(e){return new tt(e>>16&255,e>>8&255,255&e,1)}function Je(e,t,n,r){return r<=0&&(e=t=n=NaN),new tt(e,t,n,r)}function Ge(e){return e instanceof Le||(e=Ke(e)),e?new tt((e=e.rgb()).r,e.g,e.b,e.opacity):new tt}function et(e,t,n,r){return 1===arguments.length?Ge(e):new tt(e,t,n,null==r?1:r)}function tt(e,t,n,r){this.r=+e,this.g=+t,this.b=+n,this.opacity=+r}function nt(){return"#"+it(this.r)+it(this.g)+it(this.b)}function rt(){var e=this.opacity;return(1===(e=isNaN(e)?1:Math.max(0,Math.min(1,e)))?"rgb(":"rgba(")+Math.max(0,Math.min(255,Math.round(this.r)||0))+", "+Math.max(0,Math.min(255,Math.round(this.g)||0))+", "+Math.max(0,Math.min(255,Math.round(this.b)||0))+(1===e?")":", "+e+")")}function it(e){return((e=Math.max(0,Math.min(255,Math.round(e)||0)))<16?"0":"")+e.toString(16)}function ot(e,t,n,r){return r<=0?e=t=n=NaN:n<=0||n>=1?e=t=NaN:t<=0&&(e=NaN),new at(e,t,n,r)}function ut(e){if(e instanceof at)return new at(e.h,e.s,e.l,e.opacity);if(e instanceof Le||(e=Ke(e)),!e)return new at;if(e instanceof at)return e;var t=(e=e.rgb()).r/255,n=e.g/255,r=e.b/255,i=Math.min(t,n,r),o=Math.max(t,n,r),u=NaN,a=o-i,l=(o+i)/2;return a?(u=t===o?(n-r)/a+6*(n<r):n===o?(r-t)/a+2:(t-n)/a+4,a/=l<.5?o+i:2-o-i,u*=60):a=l>0&&l<1?0:u,new at(u,a,l,e.opacity)}function at(e,t,n,r){this.h=+e,this.s=+t,this.l=+n,this.opacity=+r}function lt(e,t,n){return 255*(e<60?t+(n-t)*e/60:e<180?n:e<240?t+(n-t)*(240-e)/60:t)}function ct(e,t,n,r,i){var o=e*e,u=o*e;return((1-3*e+3*o-u)*t+(4-6*o+3*u)*n+(1+3*e+3*o-3*u)*r+u*i)/6}Oe(Le,Ke,{copy:function(e){return Object.assign(new this.constructor,this,e)},displayable:function(){return this.rgb().displayable()},hex:Ye,formatHex:Ye,formatHsl:function(){return ut(this).formatHsl()},formatRgb:Xe,toString:Xe}),Oe(tt,et,ze(Le,{brighter:function(e){return e=null==e?1/.7:Math.pow(1/.7,e),new tt(this.r*e,this.g*e,this.b*e,this.opacity)},darker:function(e){return e=null==e?.7:Math.pow(.7,e),new tt(this.r*e,this.g*e,this.b*e,this.opacity)},rgb:function(){return this},displayable:function(){return-.5<=this.r&&this.r<255.5&&-.5<=this.g&&this.g<255.5&&-.5<=this.b&&this.b<255.5&&0<=this.opacity&&this.opacity<=1},hex:nt,formatHex:nt,formatRgb:rt,toString:rt})),Oe(at,(function(e,t,n,r){return 1===arguments.length?ut(e):new at(e,t,n,null==r?1:r)}),ze(Le,{brighter:function(e){return e=null==e?1/.7:Math.pow(1/.7,e),new at(this.h,this.s,this.l*e,this.opacity)},darker:function(e){return e=null==e?.7:Math.pow(.7,e),new at(this.h,this.s,this.l*e,this.opacity)},rgb:function(){var e=this.h%360+360*(this.h<0),t=isNaN(e)||isNaN(this.s)?0:this.s,n=this.l,r=n+(n<.5?n:1-n)*t,i=2*n-r;return new tt(lt(e>=240?e-240:e+120,i,r),lt(e,i,r),lt(e<120?e+240:e-120,i,r),this.opacity)},displayable:function(){return(0<=this.s&&this.s<=1||isNaN(this.s))&&0<=this.l&&this.l<=1&&0<=this.opacity&&this.opacity<=1},formatHsl:function(){var e=this.opacity;return(1===(e=isNaN(e)?1:Math.max(0,Math.min(1,e)))?"hsl(":"hsla(")+(this.h||0)+", "+100*(this.s||0)+"%, "+100*(this.l||0)+"%"+(1===e?")":", "+e+")")}}));var st=function(e){return function(){return e}};function ft(e,t){return function(n){return e+n*t}}function dt(e){return 1===(e=+e)?pt:function(t,n){return n-t?function(e,t,n){return e=Math.pow(e,n),t=Math.pow(t,n)-e,n=1/n,function(r){return Math.pow(e+r*t,n)}}(t,n,e):st(isNaN(t)?n:t)}}function pt(e,t){var n=t-e;return n?ft(e,n):st(isNaN(e)?t:e)}var ht=function e(t){var n=dt(t);function r(e,t){var r=n((e=et(e)).r,(t=et(t)).r),i=n(e.g,t.g),o=n(e.b,t.b),u=pt(e.opacity,t.opacity);return function(t){return e.r=r(t),e.g=i(t),e.b=o(t),e.opacity=u(t),e+""}}return r.gamma=e,r}(1);function mt(e){return function(t){var n,r,i=t.length,o=new Array(i),u=new Array(i),a=new Array(i);for(n=0;n<i;++n)r=et(t[n]),o[n]=r.r||0,u[n]=r.g||0,a[n]=r.b||0;return o=e(o),u=e(u),a=e(a),r.opacity=1,function(e){return r.r=o(e),r.g=u(e),r.b=a(e),r+""}}}mt((function(e){var t=e.length-1;return function(n){var r=n<=0?n=0:n>=1?(n=1,t-1):Math.floor(n*t),i=e[r],o=e[r+1],u=r>0?e[r-1]:2*i-o,a=r<t-1?e[r+2]:2*o-i;return ct((n-r/t)*t,u,i,o,a)}})),mt((function(e){var t=e.length;return function(n){var r=Math.floor(((n%=1)<0?++n:n)*t),i=e[(r+t-1)%t],o=e[r%t],u=e[(r+1)%t],a=e[(r+2)%t];return ct((n-r/t)*t,i,o,u,a)}}));var gt=function(e,t){t||(t=[]);var n,r=e?Math.min(t.length,e.length):0,i=t.slice();return function(o){for(n=0;n<r;++n)i[n]=e[n]*(1-o)+t[n]*o;return i}};function vt(e){return ArrayBuffer.isView(e)&&!(e instanceof DataView)}function yt(e,t){var n,r=t?t.length:0,i=e?Math.min(r,e.length):0,o=new Array(i),u=new Array(r);for(n=0;n<i;++n)o[n]=St(e[n],t[n]);for(;n<r;++n)u[n]=t[n];return function(e){for(n=0;n<i;++n)u[n]=o[n](e);return u}}var bt=function(e,t){var n=new Date;return e=+e,t=+t,function(r){return n.setTime(e*(1-r)+t*r),n}},wt=function(e,t){return e=+e,t=+t,function(n){return e*(1-n)+t*n}},xt=function(e,t){var n,r={},i={};for(n in null!==e&&"object"===typeof e||(e={}),null!==t&&"object"===typeof t||(t={}),t)n in e?r[n]=St(e[n],t[n]):i[n]=t[n];return function(e){for(n in r)i[n]=r[n](e);return i}},_t=/[-+]?(?:\d+\.?\d*|\.?\d+)(?:[eE][-+]?\d+)?/g,kt=new RegExp(_t.source,"g");var Tt,Et,Ct=function(e,t){var n,r,i,o=_t.lastIndex=kt.lastIndex=0,u=-1,a=[],l=[];for(e+="",t+="";(n=_t.exec(e))&&(r=kt.exec(t));)(i=r.index)>o&&(i=t.slice(o,i),a[u]?a[u]+=i:a[++u]=i),(n=n[0])===(r=r[0])?a[u]?a[u]+=r:a[++u]=r:(a[++u]=null,l.push({i:u,x:wt(n,r)})),o=kt.lastIndex;return o<t.length&&(i=t.slice(o),a[u]?a[u]+=i:a[++u]=i),a.length<2?l[0]?function(e){return function(t){return e(t)+""}}(l[0].x):function(e){return function(){return e}}(t):(t=l.length,function(e){for(var n,r=0;r<t;++r)a[(n=l[r]).i]=n.x(e);return a.join("")})},St=function(e,t){var n,r=typeof t;return null==t||"boolean"===r?st(t):("number"===r?wt:"string"===r?(n=Ke(t))?(t=n,ht):Ct:t instanceof Ke?ht:t instanceof Date?bt:vt(t)?gt:Array.isArray(t)?yt:"function"!==typeof t.valueOf&&"function"!==typeof t.toString||isNaN(t)?xt:wt)(e,t)},Mt=0,Nt=0,Pt=0,At=0,Rt=0,Dt=0,Ot="object"===typeof performance&&performance.now?performance:Date,zt="object"===typeof window&&window.requestAnimationFrame?window.requestAnimationFrame.bind(window):function(e){setTimeout(e,17)};function Lt(){return Rt||(zt(Ut),Rt=Ot.now()+Dt)}function Ut(){Rt=0}function Ft(){this._call=this._time=this._next=null}function It(e,t,n){var r=new Ft;return r.restart(e,t,n),r}function jt(){Rt=(At=Ot.now())+Dt,Mt=Nt=0;try{!function(){Lt(),++Mt;for(var e,t=Tt;t;)(e=Rt-t._time)>=0&&t._call.call(null,e),t=t._next;--Mt}()}finally{Mt=0,function(){var e,t,n=Tt,r=1/0;for(;n;)n._call?(r>n._time&&(r=n._time),e=n,n=n._next):(t=n._next,n._next=null,n=e?e._next=t:Tt=t);Et=e,Vt(r)}(),Rt=0}}function Ht(){var e=Ot.now(),t=e-At;t>1e3&&(Dt-=t,At=e)}function Vt(e){Mt||(Nt&&(Nt=clearTimeout(Nt)),e-Rt>24?(e<1/0&&(Nt=setTimeout(jt,e-Ot.now()-Dt)),Pt&&(Pt=clearInterval(Pt))):(Pt||(At=Ot.now(),Pt=setInterval(Ht,1e3)),Mt=1,zt(jt)))}Ft.prototype=It.prototype={constructor:Ft,restart:function(e,t,n){if("function"!==typeof e)throw new TypeError("callback is not a function");n=(null==n?Lt():+n)+(null==t?0:+t),this._next||Et===this||(Et?Et._next=this:Tt=this,Et=this),this._call=e,this._time=n,Vt()},stop:function(){this._call&&(this._call=null,this._time=1/0,Vt())}};var $t=function(e,t,n){var r=new Ft;return t=null==t?0:+t,r.restart((function(n){r.stop(),e(n+t)}),t,n),r},Wt=k("start","end","cancel","interrupt"),Qt=[],qt=function(e,t,n,r,i,o){var u=e.__transition;if(u){if(n in u)return}else e.__transition={};!function(e,t,n){var r,i=e.__transition;function o(l){var c,s,f,d;if(1!==n.state)return a();for(c in i)if((d=i[c]).name===n.name){if(3===d.state)return $t(o);4===d.state?(d.state=6,d.timer.stop(),d.on.call("interrupt",e,e.__data__,d.index,d.group),delete i[c]):+c<t&&(d.state=6,d.timer.stop(),d.on.call("cancel",e,e.__data__,d.index,d.group),delete i[c])}if($t((function(){3===n.state&&(n.state=4,n.timer.restart(u,n.delay,n.time),u(l))})),n.state=2,n.on.call("start",e,e.__data__,n.index,n.group),2===n.state){for(n.state=3,r=new Array(f=n.tween.length),c=0,s=-1;c<f;++c)(d=n.tween[c].value.call(e,e.__data__,n.index,n.group))&&(r[++s]=d);r.length=s+1}}function u(t){for(var i=t<n.duration?n.ease.call(null,t/n.duration):(n.timer.restart(a),n.state=5,1),o=-1,u=r.length;++o<u;)r[o].call(e,i);5===n.state&&(n.on.call("end",e,e.__data__,n.index,n.group),a())}function a(){for(var r in n.state=6,n.timer.stop(),delete i[t],i)return;delete e.__transition}i[t]=n,n.timer=It((function(e){n.state=1,n.timer.restart(o,n.delay,n.time),n.delay<=e&&o(e-n.delay)}),0,n.time)}(e,n,{name:t,index:r,group:i,on:Wt,tween:Qt,time:o.time,delay:o.delay,duration:o.duration,ease:o.ease,timer:null,state:0})};function Bt(e,t){var n=Xt(e,t);if(n.state>0)throw new Error("too late; already scheduled");return n}function Yt(e,t){var n=Xt(e,t);if(n.state>3)throw new Error("too late; already running");return n}function Xt(e,t){var n=e.__transition;if(!n||!(n=n[t]))throw new Error("transition not found");return n}var Kt,Zt,Jt,Gt,en=function(e,t){var n,r,i,o=e.__transition,u=!0;if(o){for(i in t=null==t?null:t+"",o)(n=o[i]).name===t?(r=n.state>2&&n.state<5,n.state=6,n.timer.stop(),n.on.call(r?"interrupt":"cancel",e,e.__data__,n.index,n.group),delete o[i]):u=!1;u&&delete e.__transition}},tn=180/Math.PI,nn={translateX:0,translateY:0,rotate:0,skewX:0,scaleX:1,scaleY:1},rn=function(e,t,n,r,i,o){var u,a,l;return(u=Math.sqrt(e*e+t*t))&&(e/=u,t/=u),(l=e*n+t*r)&&(n-=e*l,r-=t*l),(a=Math.sqrt(n*n+r*r))&&(n/=a,r/=a,l/=a),e*r<t*n&&(e=-e,t=-t,l=-l,u=-u),{translateX:i,translateY:o,rotate:Math.atan2(t,e)*tn,skewX:Math.atan(l)*tn,scaleX:u,scaleY:a}};function on(e,t,n,r){function i(e){return e.length?e.pop()+" ":""}return function(o,u){var a=[],l=[];return o=e(o),u=e(u),function(e,r,i,o,u,a){if(e!==i||r!==o){var l=u.push("translate(",null,t,null,n);a.push({i:l-4,x:wt(e,i)},{i:l-2,x:wt(r,o)})}else(i||o)&&u.push("translate("+i+t+o+n)}(o.translateX,o.translateY,u.translateX,u.translateY,a,l),function(e,t,n,o){e!==t?(e-t>180?t+=360:t-e>180&&(e+=360),o.push({i:n.push(i(n)+"rotate(",null,r)-2,x:wt(e,t)})):t&&n.push(i(n)+"rotate("+t+r)}(o.rotate,u.rotate,a,l),function(e,t,n,o){e!==t?o.push({i:n.push(i(n)+"skewX(",null,r)-2,x:wt(e,t)}):t&&n.push(i(n)+"skewX("+t+r)}(o.skewX,u.skewX,a,l),function(e,t,n,r,o,u){if(e!==n||t!==r){var a=o.push(i(o)+"scale(",null,",",null,")");u.push({i:a-4,x:wt(e,n)},{i:a-2,x:wt(t,r)})}else 1===n&&1===r||o.push(i(o)+"scale("+n+","+r+")")}(o.scaleX,o.scaleY,u.scaleX,u.scaleY,a,l),o=u=null,function(e){for(var t,n=-1,r=l.length;++n<r;)a[(t=l[n]).i]=t.x(e);return a.join("")}}}var un=on((function(e){return"none"===e?nn:(Kt||(Kt=document.createElement("DIV"),Zt=document.documentElement,Jt=document.defaultView),Kt.style.transform=e,e=Jt.getComputedStyle(Zt.appendChild(Kt),null).getPropertyValue("transform"),Zt.removeChild(Kt),e=e.slice(7,-1).split(","),rn(+e[0],+e[1],+e[2],+e[3],+e[4],+e[5]))}),"px, ","px)","deg)"),an=on((function(e){return null==e?nn:(Gt||(Gt=document.createElementNS("http://www.w3.org/2000/svg","g")),Gt.setAttribute("transform",e),(e=Gt.transform.baseVal.consolidate())?(e=e.matrix,rn(e.a,e.b,e.c,e.d,e.e,e.f)):nn)}),", ",")",")");function ln(e,t){var n,r;return function(){var i=Yt(this,e),o=i.tween;if(o!==n)for(var u=0,a=(r=n=o).length;u<a;++u)if(r[u].name===t){(r=r.slice()).splice(u,1);break}i.tween=r}}function cn(e,t,n){var r,i;if("function"!==typeof n)throw new Error;return function(){var o=Yt(this,e),u=o.tween;if(u!==r){i=(r=u).slice();for(var a={name:t,value:n},l=0,c=i.length;l<c;++l)if(i[l].name===t){i[l]=a;break}l===c&&i.push(a)}o.tween=i}}function sn(e,t,n){var r=e._id;return e.each((function(){var e=Yt(this,r);(e.value||(e.value={}))[t]=n.apply(this,arguments)})),function(e){return Xt(e,r).value[t]}}var fn=function(e,t){var n;return("number"===typeof t?wt:t instanceof Ke?ht:(n=Ke(t))?(t=n,ht):Ct)(e,t)};function dn(e){return function(){this.removeAttribute(e)}}function pn(e){return function(){this.removeAttributeNS(e.space,e.local)}}function hn(e,t,n){var r,i,o=n+"";return function(){var u=this.getAttribute(e);return u===o?null:u===r?i:i=t(r=u,n)}}function mn(e,t,n){var r,i,o=n+"";return function(){var u=this.getAttributeNS(e.space,e.local);return u===o?null:u===r?i:i=t(r=u,n)}}function gn(e,t,n){var r,i,o;return function(){var u,a,l=n(this);if(null!=l)return(u=this.getAttribute(e))===(a=l+"")?null:u===r&&a===i?o:(i=a,o=t(r=u,l));this.removeAttribute(e)}}function vn(e,t,n){var r,i,o;return function(){var u,a,l=n(this);if(null!=l)return(u=this.getAttributeNS(e.space,e.local))===(a=l+"")?null:u===r&&a===i?o:(i=a,o=t(r=u,l));this.removeAttributeNS(e.space,e.local)}}function yn(e,t){return function(n){this.setAttribute(e,t.call(this,n))}}function bn(e,t){return function(n){this.setAttributeNS(e.space,e.local,t.call(this,n))}}function wn(e,t){var n,r;function i(){var i=t.apply(this,arguments);return i!==r&&(n=(r=i)&&bn(e,i)),n}return i._value=t,i}function xn(e,t){var n,r;function i(){var i=t.apply(this,arguments);return i!==r&&(n=(r=i)&&yn(e,i)),n}return i._value=t,i}function _n(e,t){return function(){Bt(this,e).delay=+t.apply(this,arguments)}}function kn(e,t){return t=+t,function(){Bt(this,e).delay=t}}function Tn(e,t){return function(){Yt(this,e).duration=+t.apply(this,arguments)}}function En(e,t){return t=+t,function(){Yt(this,e).duration=t}}function Cn(e,t){if("function"!==typeof t)throw new Error;return function(){Yt(this,e).ease=t}}function Sn(e,t,n){var r,i,o=function(e){return(e+"").trim().split(/^|\s+/).every((function(e){var t=e.indexOf(".");return t>=0&&(e=e.slice(0,t)),!e||"start"===e}))}(t)?Bt:Yt;return function(){var u=o(this,e),a=u.on;a!==r&&(i=(r=a).copy()).on(t,n),u.on=i}}var Mn=De.prototype.constructor;function Nn(e){return function(){this.style.removeProperty(e)}}function Pn(e,t,n){return function(r){this.style.setProperty(e,t.call(this,r),n)}}function An(e,t,n){var r,i;function o(){var o=t.apply(this,arguments);return o!==i&&(r=(i=o)&&Pn(e,o,n)),r}return o._value=t,o}function Rn(e){return function(t){this.textContent=e.call(this,t)}}function Dn(e){var t,n;function r(){var r=e.apply(this,arguments);return r!==n&&(t=(n=r)&&Rn(r)),t}return r._value=e,r}var On=0;function zn(e,t,n,r){this._groups=e,this._parents=t,this._name=n,this._id=r}function Ln(){return++On}var Un=De.prototype;zn.prototype=function(e){return De().transition(e)}.prototype={constructor:zn,select:function(e){var t=this._name,n=this._id;"function"!==typeof e&&(e=E(e));for(var r=this._groups,i=r.length,o=new Array(i),u=0;u<i;++u)for(var a,l,c=r[u],s=c.length,f=o[u]=new Array(s),d=0;d<s;++d)(a=c[d])&&(l=e.call(a,a.__data__,d,c))&&("__data__"in a&&(l.__data__=a.__data__),f[d]=l,qt(f[d],t,n,d,f,Xt(a,n)));return new zn(o,this._parents,t,n)},selectAll:function(e){var t=this._name,n=this._id;"function"!==typeof e&&(e=S(e));for(var r=this._groups,i=r.length,o=[],u=[],a=0;a<i;++a)for(var l,c=r[a],s=c.length,f=0;f<s;++f)if(l=c[f]){for(var d,p=e.call(l,l.__data__,f,c),h=Xt(l,n),m=0,g=p.length;m<g;++m)(d=p[m])&&qt(d,t,n,m,p,h);o.push(p),u.push(l)}return new zn(o,u,t,n)},filter:function(e){"function"!==typeof e&&(e=M(e));for(var t=this._groups,n=t.length,r=new Array(n),i=0;i<n;++i)for(var o,u=t[i],a=u.length,l=r[i]=[],c=0;c<a;++c)(o=u[c])&&e.call(o,o.__data__,c,u)&&l.push(o);return new zn(r,this._parents,this._name,this._id)},merge:function(e){if(e._id!==this._id)throw new Error;for(var t=this._groups,n=e._groups,r=t.length,i=n.length,o=Math.min(r,i),u=new Array(r),a=0;a<o;++a)for(var l,c=t[a],s=n[a],f=c.length,d=u[a]=new Array(f),p=0;p<f;++p)(l=c[p]||s[p])&&(d[p]=l);for(;a<r;++a)u[a]=t[a];return new zn(u,this._parents,this._name,this._id)},selection:function(){return new Mn(this._groups,this._parents)},transition:function(){for(var e=this._name,t=this._id,n=Ln(),r=this._groups,i=r.length,o=0;o<i;++o)for(var u,a=r[o],l=a.length,c=0;c<l;++c)if(u=a[c]){var s=Xt(u,t);qt(u,e,n,c,a,{time:s.time+s.delay+s.duration,delay:0,duration:s.duration,ease:s.ease})}return new zn(r,this._parents,e,n)},call:Un.call,nodes:Un.nodes,node:Un.node,size:Un.size,empty:Un.empty,each:Un.each,on:function(e,t){var n=this._id;return arguments.length<2?Xt(this.node(),n).on.on(e):this.each(Sn(n,e,t))},attr:function(e,t){var n=L(e),r="transform"===n?an:fn;return this.attrTween(e,"function"===typeof t?(n.local?vn:gn)(n,r,sn(this,"attr."+e,t)):null==t?(n.local?pn:dn)(n):(n.local?mn:hn)(n,r,t))},attrTween:function(e,t){var n="attr."+e;if(arguments.length<2)return(n=this.tween(n))&&n._value;if(null==t)return this.tween(n,null);if("function"!==typeof t)throw new Error;var r=L(e);return this.tween(n,(r.local?wn:xn)(r,t))},style:function(e,t,n){var r="transform"===(e+="")?un:fn;return null==t?this.styleTween(e,function(e,t){var n,r,i;return function(){var o=B(this,e),u=(this.style.removeProperty(e),B(this,e));return o===u?null:o===n&&u===r?i:i=t(n=o,r=u)}}(e,r)).on("end.style."+e,Nn(e)):"function"===typeof t?this.styleTween(e,function(e,t,n){var r,i,o;return function(){var u=B(this,e),a=n(this),l=a+"";return null==a&&(this.style.removeProperty(e),l=a=B(this,e)),u===l?null:u===r&&l===i?o:(i=l,o=t(r=u,a))}}(e,r,sn(this,"style."+e,t))).each(function(e,t){var n,r,i,o,u="style."+t,a="end."+u;return function(){var l=Yt(this,e),c=l.on,s=null==l.value[u]?o||(o=Nn(t)):void 0;c===n&&i===s||(r=(n=c).copy()).on(a,i=s),l.on=r}}(this._id,e)):this.styleTween(e,function(e,t,n){var r,i,o=n+"";return function(){var u=B(this,e);return u===o?null:u===r?i:i=t(r=u,n)}}(e,r,t),n).on("end.style."+e,null)},styleTween:function(e,t,n){var r="style."+(e+="");if(arguments.length<2)return(r=this.tween(r))&&r._value;if(null==t)return this.tween(r,null);if("function"!==typeof t)throw new Error;return this.tween(r,An(e,t,null==n?"":n))},text:function(e){return this.tween("text","function"===typeof e?function(e){return function(){var t=e(this);this.textContent=null==t?"":t}}(sn(this,"text",e)):function(e){return function(){this.textContent=e}}(null==e?"":e+""))},textTween:function(e){var t="text";if(arguments.length<1)return(t=this.tween(t))&&t._value;if(null==e)return this.tween(t,null);if("function"!==typeof e)throw new Error;return this.tween(t,Dn(e))},remove:function(){return this.on("end.remove",(e=this._id,function(){var t=this.parentNode;for(var n in this.__transition)if(+n!==e)return;t&&t.removeChild(this)}));var e},tween:function(e,t){var n=this._id;if(e+="",arguments.length<2){for(var r,i=Xt(this.node(),n).tween,o=0,u=i.length;o<u;++o)if((r=i[o]).name===e)return r.value;return null}return this.each((null==t?ln:cn)(n,e,t))},delay:function(e){var t=this._id;return arguments.length?this.each(("function"===typeof e?_n:kn)(t,e)):Xt(this.node(),t).delay},duration:function(e){var t=this._id;return arguments.length?this.each(("function"===typeof e?Tn:En)(t,e)):Xt(this.node(),t).duration},ease:function(e){var t=this._id;return arguments.length?this.each(Cn(t,e)):Xt(this.node(),t).ease},end:function(){var e,t,n=this,r=n._id,i=n.size();return new Promise((function(o,u){var a={value:u},l={value:function(){0===--i&&o()}};n.each((function(){var n=Yt(this,r),i=n.on;i!==e&&((t=(e=i).copy())._.cancel.push(a),t._.interrupt.push(a),t._.end.push(l)),n.on=t}))}))}};var Fn={time:null,delay:0,duration:250,ease:function(e){return((e*=2)<=1?e*e*e:(e-=2)*e*e+2)/2}};function In(e,t){for(var n;!(n=e.__transition)||!(n=n[t]);)if(!(e=e.parentNode))return Fn.time=Lt(),Fn;return n}De.prototype.interrupt=function(e){return this.each((function(){en(this,e)}))},De.prototype.transition=function(e){var t,n;e instanceof zn?(t=e._id,e=e._name):(t=Ln(),(n=Fn).time=Lt(),e=null==e?null:e+"");for(var r=this._groups,i=r.length,o=0;o<i;++o)for(var u,a=r[o],l=a.length,c=0;c<l;++c)(u=a[c])&&qt(u,e,t,c,a,n||In(u,t));return new zn(r,this._parents,e,t)};function jn(e){return[+e[0],+e[1]]}function Hn(e){return[jn(e[0]),jn(e[1])]}["w","e"].map(Vn),["n","s"].map(Vn),["n","w","e","s","nw","ne","sw","se"].map(Vn);function Vn(e){return{type:e}}Math.cos,Math.sin,Math.PI,Math.max;Array.prototype.slice;var $n=Math.PI,Wn=2*$n,Qn=Wn-1e-6;function qn(){this._x0=this._y0=this._x1=this._y1=null,this._=""}function Bn(){return new qn}qn.prototype=Bn.prototype={constructor:qn,moveTo:function(e,t){this._+="M"+(this._x0=this._x1=+e)+","+(this._y0=this._y1=+t)},closePath:function(){null!==this._x1&&(this._x1=this._x0,this._y1=this._y0,this._+="Z")},lineTo:function(e,t){this._+="L"+(this._x1=+e)+","+(this._y1=+t)},quadraticCurveTo:function(e,t,n,r){this._+="Q"+ +e+","+ +t+","+(this._x1=+n)+","+(this._y1=+r)},bezierCurveTo:function(e,t,n,r,i,o){this._+="C"+ +e+","+ +t+","+ +n+","+ +r+","+(this._x1=+i)+","+(this._y1=+o)},arcTo:function(e,t,n,r,i){e=+e,t=+t,n=+n,r=+r,i=+i;var o=this._x1,u=this._y1,a=n-e,l=r-t,c=o-e,s=u-t,f=c*c+s*s;if(i<0)throw new Error("negative radius: "+i);if(null===this._x1)this._+="M"+(this._x1=e)+","+(this._y1=t);else if(f>1e-6)if(Math.abs(s*a-l*c)>1e-6&&i){var d=n-o,p=r-u,h=a*a+l*l,m=d*d+p*p,g=Math.sqrt(h),v=Math.sqrt(f),y=i*Math.tan(($n-Math.acos((h+f-m)/(2*g*v)))/2),b=y/v,w=y/g;Math.abs(b-1)>1e-6&&(this._+="L"+(e+b*c)+","+(t+b*s)),this._+="A"+i+","+i+",0,0,"+ +(s*d>c*p)+","+(this._x1=e+w*a)+","+(this._y1=t+w*l)}else this._+="L"+(this._x1=e)+","+(this._y1=t);else;},arc:function(e,t,n,r,i,o){e=+e,t=+t,o=!!o;var u=(n=+n)*Math.cos(r),a=n*Math.sin(r),l=e+u,c=t+a,s=1^o,f=o?r-i:i-r;if(n<0)throw new Error("negative radius: "+n);null===this._x1?this._+="M"+l+","+c:(Math.abs(this._x1-l)>1e-6||Math.abs(this._y1-c)>1e-6)&&(this._+="L"+l+","+c),n&&(f<0&&(f=f%Wn+Wn),f>Qn?this._+="A"+n+","+n+",0,1,"+s+","+(e-u)+","+(t-a)+"A"+n+","+n+",0,1,"+s+","+(this._x1=l)+","+(this._y1=c):f>1e-6&&(this._+="A"+n+","+n+",0,"+ +(f>=$n)+","+s+","+(this._x1=e+n*Math.cos(i))+","+(this._y1=t+n*Math.sin(i))))},rect:function(e,t,n,r){this._+="M"+(this._x0=this._x1=+e)+","+(this._y0=this._y1=+t)+"h"+ +n+"v"+ +r+"h"+-n+"Z"},toString:function(){return this._}};function Yn(){}function Xn(e,t){var n=new Yn;if(e instanceof Yn)e.each((function(e,t){n.set(t,e)}));else if(Array.isArray(e)){var r,i=-1,o=e.length;if(null==t)for(;++i<o;)n.set(i,e[i]);else for(;++i<o;)n.set(t(r=e[i],i,e),r)}else if(e)for(var u in e)n.set(u,e[u]);return n}Yn.prototype=Xn.prototype={constructor:Yn,has:function(e){return"$"+e in this},get:function(e){return this["$"+e]},set:function(e,t){return this["$"+e]=t,this},remove:function(e){var t="$"+e;return t in this&&delete this[t]},clear:function(){for(var e in this)"$"===e[0]&&delete this[e]},keys:function(){var e=[];for(var t in this)"$"===t[0]&&e.push(t.slice(1));return e},values:function(){var e=[];for(var t in this)"$"===t[0]&&e.push(this[t]);return e},entries:function(){var e=[];for(var t in this)"$"===t[0]&&e.push({key:t.slice(1),value:this[t]});return e},size:function(){var e=0;for(var t in this)"$"===t[0]&&++e;return e},empty:function(){for(var e in this)if("$"===e[0])return!1;return!0},each:function(e){for(var t in this)"$"===t[0]&&e(this[t],t.slice(1),this)}};var Kn=Xn,Zn=function(){var e,t,n,r=[],i=[];function o(n,i,u,a){if(i>=r.length)return null!=e&&n.sort(e),null!=t?t(n):n;for(var l,c,s,f=-1,d=n.length,p=r[i++],h=Kn(),m=u();++f<d;)(s=h.get(l=p(c=n[f])+""))?s.push(c):h.set(l,[c]);return h.each((function(e,t){a(m,t,o(e,i,u,a))})),m}return n={object:function(e){return o(e,0,Jn,Gn)},map:function(e){return o(e,0,er,tr)},entries:function(e){return function e(n,o){if(++o>r.length)return n;var u,a=i[o-1];return null!=t&&o>=r.length?u=n.entries():(u=[],n.each((function(t,n){u.push({key:n,values:e(t,o)})}))),null!=a?u.sort((function(e,t){return a(e.key,t.key)})):u}(o(e,0,er,tr),0)},key:function(e){return r.push(e),n},sortKeys:function(e){return i[r.length-1]=e,n},sortValues:function(t){return e=t,n},rollup:function(e){return t=e,n}}};function Jn(){return{}}function Gn(e,t,n){e[t]=n}function er(){return Kn()}function tr(e,t,n){e.set(t,n)}function nr(){}var rr=Kn.prototype;function ir(e,t){var n=new nr;if(e instanceof nr)e.each((function(e){n.add(e)}));else if(e){var r=-1,i=e.length;if(null==t)for(;++r<i;)n.add(e[r]);else for(;++r<i;)n.add(t(e[r],r,e))}return n}nr.prototype=ir.prototype={constructor:nr,has:rr.has,add:function(e){return this["$"+(e+="")]=e,this},remove:rr.remove,clear:rr.clear,values:rr.keys,size:rr.size,empty:rr.empty,each:rr.each};Array.prototype.slice;var or={},ur={};function ar(e){return new Function("d","return {"+e.map((function(e,t){return JSON.stringify(e)+": d["+t+'] || ""'})).join(",")+"}")}function lr(e){var t=Object.create(null),n=[];return e.forEach((function(e){for(var r in e)r in t||n.push(t[r]=r)})),n}function cr(e,t){var n=e+"",r=n.length;return r<t?new Array(t-r+1).join(0)+n:n}function sr(e){var t,n=e.getUTCHours(),r=e.getUTCMinutes(),i=e.getUTCSeconds(),o=e.getUTCMilliseconds();return isNaN(e)?"Invalid Date":((t=e.getUTCFullYear())<0?"-"+cr(-t,6):t>9999?"+"+cr(t,6):cr(t,4))+"-"+cr(e.getUTCMonth()+1,2)+"-"+cr(e.getUTCDate(),2)+(o?"T"+cr(n,2)+":"+cr(r,2)+":"+cr(i,2)+"."+cr(o,3)+"Z":i?"T"+cr(n,2)+":"+cr(r,2)+":"+cr(i,2)+"Z":r||n?"T"+cr(n,2)+":"+cr(r,2)+"Z":"")}var fr=function(e){var t=new RegExp('["'+e+"\n\r]"),n=e.charCodeAt(0);function r(e,t){var r,i=[],o=e.length,u=0,a=0,l=o<=0,c=!1;function s(){if(l)return ur;if(c)return c=!1,or;var t,r,i=u;if(34===e.charCodeAt(i)){for(;u++<o&&34!==e.charCodeAt(u)||34===e.charCodeAt(++u););return(t=u)>=o?l=!0:10===(r=e.charCodeAt(u++))?c=!0:13===r&&(c=!0,10===e.charCodeAt(u)&&++u),e.slice(i+1,t-1).replace(/""/g,'"')}for(;u<o;){if(10===(r=e.charCodeAt(t=u++)))c=!0;else if(13===r)c=!0,10===e.charCodeAt(u)&&++u;else if(r!==n)continue;return e.slice(i,t)}return l=!0,e.slice(i,o)}for(10===e.charCodeAt(o-1)&&--o,13===e.charCodeAt(o-1)&&--o;(r=s())!==ur;){for(var f=[];r!==or&&r!==ur;)f.push(r),r=s();t&&null==(f=t(f,a++))||i.push(f)}return i}function i(t,n){return t.map((function(t){return n.map((function(e){return u(t[e])})).join(e)}))}function o(t){return t.map(u).join(e)}function u(e){return null==e?"":e instanceof Date?sr(e):t.test(e+="")?'"'+e.replace(/"/g,'""')+'"':e}return{parse:function(e,t){var n,i,o=r(e,(function(e,r){if(n)return n(e,r-1);i=e,n=t?function(e,t){var n=ar(e);return function(r,i){return t(n(r),i,e)}}(e,t):ar(e)}));return o.columns=i||[],o},parseRows:r,format:function(t,n){return null==n&&(n=lr(t)),[n.map(u).join(e)].concat(i(t,n)).join("\n")},formatBody:function(e,t){return null==t&&(t=lr(e)),i(e,t).join("\n")},formatRows:function(e){return e.map(o).join("\n")},formatRow:o,formatValue:u}},dr=fr(","),pr=dr.parse,hr=(dr.parseRows,dr.format,dr.formatBody,dr.formatRows,dr.formatRow,dr.formatValue,fr("\t")),mr=hr.parse;hr.parseRows,hr.format,hr.formatBody,hr.formatRows,hr.formatRow,hr.formatValue;function gr(e){if(!e.ok)throw new Error(e.status+" "+e.statusText);return e.text()}var vr=function(e,t){return fetch(e,t).then(gr)};function yr(e){return function(t,n,r){return 2===arguments.length&&"function"===typeof n&&(r=n,n=void 0),vr(t,n).then((function(t){return e(t,r)}))}}var br=yr(pr);yr(mr);function wr(e){return function(t,n){return vr(t,n).then((function(t){return(new DOMParser).parseFromString(t,e)}))}}wr("application/xml"),wr("text/html"),wr("image/svg+xml");function xr(e,t,n,r){if(isNaN(t)||isNaN(n))return e;var i,o,u,a,l,c,s,f,d,p=e._root,h={data:r},m=e._x0,g=e._y0,v=e._x1,y=e._y1;if(!p)return e._root=h,e;for(;p.length;)if((c=t>=(o=(m+v)/2))?m=o:v=o,(s=n>=(u=(g+y)/2))?g=u:y=u,i=p,!(p=p[f=s<<1|c]))return i[f]=h,e;if(a=+e._x.call(null,p.data),l=+e._y.call(null,p.data),t===a&&n===l)return h.next=p,i?i[f]=h:e._root=h,e;do{i=i?i[f]=new Array(4):e._root=new Array(4),(c=t>=(o=(m+v)/2))?m=o:v=o,(s=n>=(u=(g+y)/2))?g=u:y=u}while((f=s<<1|c)===(d=(l>=u)<<1|a>=o));return i[d]=p,i[f]=h,e}var _r=function(e,t,n,r,i){this.node=e,this.x0=t,this.y0=n,this.x1=r,this.y1=i};function kr(e){return e[0]}function Tr(e){return e[1]}function Er(e,t,n){var r=new Cr(null==t?kr:t,null==n?Tr:n,NaN,NaN,NaN,NaN);return null==e?r:r.addAll(e)}function Cr(e,t,n,r,i,o){this._x=e,this._y=t,this._x0=n,this._y0=r,this._x1=i,this._y1=o,this._root=void 0}function Sr(e){for(var t={data:e.data},n=t;e=e.next;)n=n.next={data:e.data};return t}var Mr=Er.prototype=Cr.prototype;Mr.copy=function(){var e,t,n=new Cr(this._x,this._y,this._x0,this._y0,this._x1,this._y1),r=this._root;if(!r)return n;if(!r.length)return n._root=Sr(r),n;for(e=[{source:r,target:n._root=new Array(4)}];r=e.pop();)for(var i=0;i<4;++i)(t=r.source[i])&&(t.length?e.push({source:t,target:r.target[i]=new Array(4)}):r.target[i]=Sr(t));return n},Mr.add=function(e){var t=+this._x.call(null,e),n=+this._y.call(null,e);return xr(this.cover(t,n),t,n,e)},Mr.addAll=function(e){var t,n,r,i,o=e.length,u=new Array(o),a=new Array(o),l=1/0,c=1/0,s=-1/0,f=-1/0;for(n=0;n<o;++n)isNaN(r=+this._x.call(null,t=e[n]))||isNaN(i=+this._y.call(null,t))||(u[n]=r,a[n]=i,r<l&&(l=r),r>s&&(s=r),i<c&&(c=i),i>f&&(f=i));if(l>s||c>f)return this;for(this.cover(l,c).cover(s,f),n=0;n<o;++n)xr(this,u[n],a[n],e[n]);return this},Mr.cover=function(e,t){if(isNaN(e=+e)||isNaN(t=+t))return this;var n=this._x0,r=this._y0,i=this._x1,o=this._y1;if(isNaN(n))i=(n=Math.floor(e))+1,o=(r=Math.floor(t))+1;else{for(var u,a,l=i-n,c=this._root;n>e||e>=i||r>t||t>=o;)switch(a=(t<r)<<1|e<n,(u=new Array(4))[a]=c,c=u,l*=2,a){case 0:i=n+l,o=r+l;break;case 1:n=i-l,o=r+l;break;case 2:i=n+l,r=o-l;break;case 3:n=i-l,r=o-l}this._root&&this._root.length&&(this._root=c)}return this._x0=n,this._y0=r,this._x1=i,this._y1=o,this},Mr.data=function(){var e=[];return this.visit((function(t){if(!t.length)do{e.push(t.data)}while(t=t.next)})),e},Mr.extent=function(e){return arguments.length?this.cover(+e[0][0],+e[0][1]).cover(+e[1][0],+e[1][1]):isNaN(this._x0)?void 0:[[this._x0,this._y0],[this._x1,this._y1]]},Mr.find=function(e,t,n){var r,i,o,u,a,l,c,s=this._x0,f=this._y0,d=this._x1,p=this._y1,h=[],m=this._root;for(m&&h.push(new _r(m,s,f,d,p)),null==n?n=1/0:(s=e-n,f=t-n,d=e+n,p=t+n,n*=n);l=h.pop();)if(!(!(m=l.node)||(i=l.x0)>d||(o=l.y0)>p||(u=l.x1)<s||(a=l.y1)<f))if(m.length){var g=(i+u)/2,v=(o+a)/2;h.push(new _r(m[3],g,v,u,a),new _r(m[2],i,v,g,a),new _r(m[1],g,o,u,v),new _r(m[0],i,o,g,v)),(c=(t>=v)<<1|e>=g)&&(l=h[h.length-1],h[h.length-1]=h[h.length-1-c],h[h.length-1-c]=l)}else{var y=e-+this._x.call(null,m.data),b=t-+this._y.call(null,m.data),w=y*y+b*b;if(w<n){var x=Math.sqrt(n=w);s=e-x,f=t-x,d=e+x,p=t+x,r=m.data}}return r},Mr.remove=function(e){if(isNaN(o=+this._x.call(null,e))||isNaN(u=+this._y.call(null,e)))return this;var t,n,r,i,o,u,a,l,c,s,f,d,p=this._root,h=this._x0,m=this._y0,g=this._x1,v=this._y1;if(!p)return this;if(p.length)for(;;){if((c=o>=(a=(h+g)/2))?h=a:g=a,(s=u>=(l=(m+v)/2))?m=l:v=l,t=p,!(p=p[f=s<<1|c]))return this;if(!p.length)break;(t[f+1&3]||t[f+2&3]||t[f+3&3])&&(n=t,d=f)}for(;p.data!==e;)if(r=p,!(p=p.next))return this;return(i=p.next)&&delete p.next,r?(i?r.next=i:delete r.next,this):t?(i?t[f]=i:delete t[f],(p=t[0]||t[1]||t[2]||t[3])&&p===(t[3]||t[2]||t[1]||t[0])&&!p.length&&(n?n[d]=p:this._root=p),this):(this._root=i,this)},Mr.removeAll=function(e){for(var t=0,n=e.length;t<n;++t)this.remove(e[t]);return this},Mr.root=function(){return this._root},Mr.size=function(){var e=0;return this.visit((function(t){if(!t.length)do{++e}while(t=t.next)})),e},Mr.visit=function(e){var t,n,r,i,o,u,a=[],l=this._root;for(l&&a.push(new _r(l,this._x0,this._y0,this._x1,this._y1));t=a.pop();)if(!e(l=t.node,r=t.x0,i=t.y0,o=t.x1,u=t.y1)&&l.length){var c=(r+o)/2,s=(i+u)/2;(n=l[3])&&a.push(new _r(n,c,s,o,u)),(n=l[2])&&a.push(new _r(n,r,s,c,u)),(n=l[1])&&a.push(new _r(n,c,i,o,s)),(n=l[0])&&a.push(new _r(n,r,i,c,s))}return this},Mr.visitAfter=function(e){var t,n=[],r=[];for(this._root&&n.push(new _r(this._root,this._x0,this._y0,this._x1,this._y1));t=n.pop();){var i=t.node;if(i.length){var o,u=t.x0,a=t.y0,l=t.x1,c=t.y1,s=(u+l)/2,f=(a+c)/2;(o=i[0])&&n.push(new _r(o,u,a,s,f)),(o=i[1])&&n.push(new _r(o,s,a,l,f)),(o=i[2])&&n.push(new _r(o,u,f,s,c)),(o=i[3])&&n.push(new _r(o,s,f,l,c))}r.push(t)}for(;t=r.pop();)e(t.node,t.x0,t.y0,t.x1,t.y1);return this},Mr.x=function(e){return arguments.length?(this._x=e,this):this._x},Mr.y=function(e){return arguments.length?(this._y=e,this):this._y};Math.PI,Math.sqrt(5);var Nr=function(){return Math.random()},Pr=(function e(t){function n(e,n){return e=null==e?0:+e,n=null==n?1:+n,1===arguments.length?(n=e,e=0):n-=e,function(){return t()*n+e}}return n.source=e,n}(Nr),function e(t){function n(e,n){var r,i;return e=null==e?0:+e,n=null==n?1:+n,function(){var o;if(null!=r)o=r,r=null;else do{r=2*t()-1,o=2*t()-1,i=r*r+o*o}while(!i||i>1);return e+n*o*Math.sqrt(-2*Math.log(i)/i)}}return n.source=e,n}(Nr)),Ar=(function e(t){function n(){var e=Pr.source(t).apply(this,arguments);return function(){return Math.exp(e())}}return n.source=e,n}(Nr),function e(t){function n(e){return function(){for(var n=0,r=0;r<e;++r)n+=t();return n}}return n.source=e,n}(Nr));(function e(t){function n(e){var n=Ar.source(t)(e);return function(){return n()/e}}return n.source=e,n})(Nr),function e(t){function n(e){return function(){return-Math.log(1-t())/e}}return n.source=e,n}(Nr);function Rr(e,t){switch(arguments.length){case 0:break;case 1:this.range(e);break;default:this.range(t).domain(e)}return this}var Dr=Array.prototype,Or=Dr.map,zr=Dr.slice;var Lr=function(e,t){return e=+e,t=+t,function(n){return Math.round(e*(1-n)+t*n)}},Ur=function(e){return+e},Fr=[0,1];function Ir(e){return e}function jr(e,t){return(t-=e=+e)?function(n){return(n-e)/t}:(n=isNaN(t)?NaN:.5,function(){return n});var n}function Hr(e){var t,n=e[0],r=e[e.length-1];return n>r&&(t=n,n=r,r=t),function(e){return Math.max(n,Math.min(r,e))}}function Vr(e,t,n){var r=e[0],i=e[1],o=t[0],u=t[1];return i<r?(r=jr(i,r),o=n(u,o)):(r=jr(r,i),o=n(o,u)),function(e){return o(r(e))}}function $r(e,t,n){var r=Math.min(e.length,t.length)-1,i=new Array(r),o=new Array(r),u=-1;for(e[r]<e[0]&&(e=e.slice().reverse(),t=t.slice().reverse());++u<r;)i[u]=jr(e[u],e[u+1]),o[u]=n(t[u],t[u+1]);return function(t){var n=a(e,t,1,r)-1;return o[n](i[n](t))}}function Wr(e,t){return t.domain(e.domain()).range(e.range()).interpolate(e.interpolate()).clamp(e.clamp()).unknown(e.unknown())}function Qr(){var e,t,n,r,i,o,u=Fr,a=Fr,l=St,c=Ir;function s(){return r=Math.min(u.length,a.length)>2?$r:Vr,i=o=null,f}function f(t){return isNaN(t=+t)?n:(i||(i=r(u.map(e),a,l)))(e(c(t)))}return f.invert=function(n){return c(t((o||(o=r(a,u.map(e),wt)))(n)))},f.domain=function(e){return arguments.length?(u=Or.call(e,Ur),c===Ir||(c=Hr(u)),s()):u.slice()},f.range=function(e){return arguments.length?(a=zr.call(e),s()):a.slice()},f.rangeRound=function(e){return a=zr.call(e),l=Lr,s()},f.clamp=function(e){return arguments.length?(c=e?Hr(u):Ir,f):c!==Ir},f.interpolate=function(e){return arguments.length?(l=e,s()):l},f.unknown=function(e){return arguments.length?(n=e,f):n},function(n,r){return e=n,t=r,s()}}var qr=/^(?:(.)?([<>=^]))?([+\-( ])?([$#])?(0)?(\d+)?(,)?(\.\d+)?(~)?([a-z%])?$/i;function Br(e){if(!(t=qr.exec(e)))throw new Error("invalid format: "+e);var t;return new Yr({fill:t[1],align:t[2],sign:t[3],symbol:t[4],zero:t[5],width:t[6],comma:t[7],precision:t[8]&&t[8].slice(1),trim:t[9],type:t[10]})}function Yr(e){this.fill=void 0===e.fill?" ":e.fill+"",this.align=void 0===e.align?">":e.align+"",this.sign=void 0===e.sign?"-":e.sign+"",this.symbol=void 0===e.symbol?"":e.symbol+"",this.zero=!!e.zero,this.width=void 0===e.width?void 0:+e.width,this.comma=!!e.comma,this.precision=void 0===e.precision?void 0:+e.precision,this.trim=!!e.trim,this.type=void 0===e.type?"":e.type+""}Br.prototype=Yr.prototype,Yr.prototype.toString=function(){return this.fill+this.align+this.sign+this.symbol+(this.zero?"0":"")+(void 0===this.width?"":Math.max(1,0|this.width))+(this.comma?",":"")+(void 0===this.precision?"":"."+Math.max(0,0|this.precision))+(this.trim?"~":"")+this.type};var Xr,Kr,Zr,Jr,Gr=function(e,t){if((n=(e=t?e.toExponential(t-1):e.toExponential()).indexOf("e"))<0)return null;var n,r=e.slice(0,n);return[r.length>1?r[0]+r.slice(2):r,+e.slice(n+1)]},ei=function(e){return(e=Gr(Math.abs(e)))?e[1]:NaN},ti=function(e,t){var n=Gr(e,t);if(!n)return e+"";var r=n[0],i=n[1];return i<0?"0."+new Array(-i).join("0")+r:r.length>i+1?r.slice(0,i+1)+"."+r.slice(i+1):r+new Array(i-r.length+2).join("0")},ni={"%":function(e,t){return(100*e).toFixed(t)},b:function(e){return Math.round(e).toString(2)},c:function(e){return e+""},d:function(e){return Math.round(e).toString(10)},e:function(e,t){return e.toExponential(t)},f:function(e,t){return e.toFixed(t)},g:function(e,t){return e.toPrecision(t)},o:function(e){return Math.round(e).toString(8)},p:function(e,t){return ti(100*e,t)},r:ti,s:function(e,t){var n=Gr(e,t);if(!n)return e+"";var r=n[0],i=n[1],o=i-(Xr=3*Math.max(-8,Math.min(8,Math.floor(i/3))))+1,u=r.length;return o===u?r:o>u?r+new Array(o-u+1).join("0"):o>0?r.slice(0,o)+"."+r.slice(o):"0."+new Array(1-o).join("0")+Gr(e,Math.max(0,t+o-1))[0]},X:function(e){return Math.round(e).toString(16).toUpperCase()},x:function(e){return Math.round(e).toString(16)}},ri=function(e){return e},ii=Array.prototype.map,oi=["y","z","a","f","p","n","\xb5","m","","k","M","G","T","P","E","Z","Y"];Kr=function(e){var t,n,r=void 0===e.grouping||void 0===e.thousands?ri:(t=ii.call(e.grouping,Number),n=e.thousands+"",function(e,r){for(var i=e.length,o=[],u=0,a=t[0],l=0;i>0&&a>0&&(l+a+1>r&&(a=Math.max(1,r-l)),o.push(e.substring(i-=a,i+a)),!((l+=a+1)>r));)a=t[u=(u+1)%t.length];return o.reverse().join(n)}),i=void 0===e.currency?"":e.currency[0]+"",o=void 0===e.currency?"":e.currency[1]+"",u=void 0===e.decimal?".":e.decimal+"",a=void 0===e.numerals?ri:function(e){return function(t){return t.replace(/[0-9]/g,(function(t){return e[+t]}))}}(ii.call(e.numerals,String)),l=void 0===e.percent?"%":e.percent+"",c=void 0===e.minus?"-":e.minus+"",s=void 0===e.nan?"NaN":e.nan+"";function f(e){var t=(e=Br(e)).fill,n=e.align,f=e.sign,d=e.symbol,p=e.zero,h=e.width,m=e.comma,g=e.precision,v=e.trim,y=e.type;"n"===y?(m=!0,y="g"):ni[y]||(void 0===g&&(g=12),v=!0,y="g"),(p||"0"===t&&"="===n)&&(p=!0,t="0",n="=");var b="$"===d?i:"#"===d&&/[boxX]/.test(y)?"0"+y.toLowerCase():"",w="$"===d?o:/[%p]/.test(y)?l:"",x=ni[y],_=/[defgprs%]/.test(y);function k(e){var i,o,l,d=b,k=w;if("c"===y)k=x(e)+k,e="";else{var T=(e=+e)<0||1/e<0;if(e=isNaN(e)?s:x(Math.abs(e),g),v&&(e=function(e){e:for(var t,n=e.length,r=1,i=-1;r<n;++r)switch(e[r]){case".":i=t=r;break;case"0":0===i&&(i=r),t=r;break;default:if(!+e[r])break e;i>0&&(i=0)}return i>0?e.slice(0,i)+e.slice(t+1):e}(e)),T&&0===+e&&"+"!==f&&(T=!1),d=(T?"("===f?f:c:"-"===f||"("===f?"":f)+d,k=("s"===y?oi[8+Xr/3]:"")+k+(T&&"("===f?")":""),_)for(i=-1,o=e.length;++i<o;)if(48>(l=e.charCodeAt(i))||l>57){k=(46===l?u+e.slice(i+1):e.slice(i))+k,e=e.slice(0,i);break}}m&&!p&&(e=r(e,1/0));var E=d.length+e.length+k.length,C=E<h?new Array(h-E+1).join(t):"";switch(m&&p&&(e=r(C+e,C.length?h-k.length:1/0),C=""),n){case"<":e=d+e+k+C;break;case"=":e=d+C+e+k;break;case"^":e=C.slice(0,E=C.length>>1)+d+e+k+C.slice(E);break;default:e=C+d+e+k}return a(e)}return g=void 0===g?6:/[gprs]/.test(y)?Math.max(1,Math.min(21,g)):Math.max(0,Math.min(20,g)),k.toString=function(){return e+""},k}return{format:f,formatPrefix:function(e,t){var n=f(((e=Br(e)).type="f",e)),r=3*Math.max(-8,Math.min(8,Math.floor(ei(t)/3))),i=Math.pow(10,-r),o=oi[8+r/3];return function(e){return n(i*e)+o}}}}({decimal:".",thousands:",",grouping:[3],currency:["$",""],minus:"-"}),Zr=Kr.format,Jr=Kr.formatPrefix;var ui=function(e,t,n,r){var i,o=h(e,t,n);switch((r=Br(null==r?",f":r)).type){case"s":var u=Math.max(Math.abs(e),Math.abs(t));return null!=r.precision||isNaN(i=function(e,t){return Math.max(0,3*Math.max(-8,Math.min(8,Math.floor(ei(t)/3)))-ei(Math.abs(e)))}(o,u))||(r.precision=i),Jr(r,u);case"":case"e":case"g":case"p":case"r":null!=r.precision||isNaN(i=function(e,t){return e=Math.abs(e),t=Math.abs(t)-e,Math.max(0,ei(t)-ei(e))+1}(o,Math.max(Math.abs(e),Math.abs(t))))||(r.precision=i-("e"===r.type));break;case"f":case"%":null!=r.precision||isNaN(i=function(e){return Math.max(0,-ei(Math.abs(e)))}(o))||(r.precision=i-2*("%"===r.type))}return Zr(r)};function ai(e){var t=e.domain;return e.ticks=function(e){var n=t();return d(n[0],n[n.length-1],null==e?10:e)},e.tickFormat=function(e,n){var r=t();return ui(r[0],r[r.length-1],null==e?10:e,n)},e.nice=function(n){null==n&&(n=10);var r,i=t(),o=0,u=i.length-1,a=i[o],l=i[u];return l<a&&(r=a,a=l,l=r,r=o,o=u,u=r),(r=p(a,l,n))>0?r=p(a=Math.floor(a/r)*r,l=Math.ceil(l/r)*r,n):r<0&&(r=p(a=Math.ceil(a*r)/r,l=Math.floor(l*r)/r,n)),r>0?(i[o]=Math.floor(a/r)*r,i[u]=Math.ceil(l/r)*r,t(i)):r<0&&(i[o]=Math.ceil(a*r)/r,i[u]=Math.floor(l*r)/r,t(i)),e},e}function li(e){return function(t){return t<0?-Math.pow(-t,e):Math.pow(t,e)}}function ci(e){return e<0?-Math.sqrt(-e):Math.sqrt(e)}function si(e){return e<0?-e*e:e*e}function fi(e){var t=e(Ir,Ir),n=1;function r(){return 1===n?e(Ir,Ir):.5===n?e(ci,si):e(li(n),li(1/n))}return t.exponent=function(e){return arguments.length?(n=+e,r()):n},ai(t)}function di(){var e=fi(Qr());return e.copy=function(){return Wr(e,di()).exponent(e.exponent())},Rr.apply(e,arguments),e}function pi(){return di.apply(null,arguments).exponent(.5)}var hi=new Date,mi=new Date;function gi(e,t,n,r){function i(t){return e(t=0===arguments.length?new Date:new Date(+t)),t}return i.floor=function(t){return e(t=new Date(+t)),t},i.ceil=function(n){return e(n=new Date(n-1)),t(n,1),e(n),n},i.round=function(e){var t=i(e),n=i.ceil(e);return e-t<n-e?t:n},i.offset=function(e,n){return t(e=new Date(+e),null==n?1:Math.floor(n)),e},i.range=function(n,r,o){var u,a=[];if(n=i.ceil(n),o=null==o?1:Math.floor(o),!(n<r)||!(o>0))return a;do{a.push(u=new Date(+n)),t(n,o),e(n)}while(u<n&&n<r);return a},i.filter=function(n){return gi((function(t){if(t>=t)for(;e(t),!n(t);)t.setTime(t-1)}),(function(e,r){if(e>=e)if(r<0)for(;++r<=0;)for(;t(e,-1),!n(e););else for(;--r>=0;)for(;t(e,1),!n(e););}))},n&&(i.count=function(t,r){return hi.setTime(+t),mi.setTime(+r),e(hi),e(mi),Math.floor(n(hi,mi))},i.every=function(e){return e=Math.floor(e),isFinite(e)&&e>0?e>1?i.filter(r?function(t){return r(t)%e===0}:function(t){return i.count(0,t)%e===0}):i:null}),i}var vi=gi((function(e){e.setMonth(0,1),e.setHours(0,0,0,0)}),(function(e,t){e.setFullYear(e.getFullYear()+t)}),(function(e,t){return t.getFullYear()-e.getFullYear()}),(function(e){return e.getFullYear()}));vi.every=function(e){return isFinite(e=Math.floor(e))&&e>0?gi((function(t){t.setFullYear(Math.floor(t.getFullYear()/e)*e),t.setMonth(0,1),t.setHours(0,0,0,0)}),(function(t,n){t.setFullYear(t.getFullYear()+n*e)})):null};var yi=vi,bi=(vi.range,gi((function(e){e.setDate(1),e.setHours(0,0,0,0)}),(function(e,t){e.setMonth(e.getMonth()+t)}),(function(e,t){return t.getMonth()-e.getMonth()+12*(t.getFullYear()-e.getFullYear())}),(function(e){return e.getMonth()})));bi.range;function wi(e){return gi((function(t){t.setDate(t.getDate()-(t.getDay()+7-e)%7),t.setHours(0,0,0,0)}),(function(e,t){e.setDate(e.getDate()+7*t)}),(function(e,t){return(t-e-6e4*(t.getTimezoneOffset()-e.getTimezoneOffset()))/6048e5}))}var xi=wi(0),_i=wi(1),ki=wi(2),Ti=wi(3),Ei=wi(4),Ci=wi(5),Si=wi(6),Mi=(xi.range,_i.range,ki.range,Ti.range,Ei.range,Ci.range,Si.range,gi((function(e){e.setHours(0,0,0,0)}),(function(e,t){e.setDate(e.getDate()+t)}),(function(e,t){return(t-e-6e4*(t.getTimezoneOffset()-e.getTimezoneOffset()))/864e5}),(function(e){return e.getDate()-1}))),Ni=Mi,Pi=(Mi.range,gi((function(e){e.setTime(e-e.getMilliseconds()-1e3*e.getSeconds()-6e4*e.getMinutes())}),(function(e,t){e.setTime(+e+36e5*t)}),(function(e,t){return(t-e)/36e5}),(function(e){return e.getHours()}))),Ai=(Pi.range,gi((function(e){e.setTime(e-e.getMilliseconds()-1e3*e.getSeconds())}),(function(e,t){e.setTime(+e+6e4*t)}),(function(e,t){return(t-e)/6e4}),(function(e){return e.getMinutes()}))),Ri=(Ai.range,gi((function(e){e.setTime(e-e.getMilliseconds())}),(function(e,t){e.setTime(+e+1e3*t)}),(function(e,t){return(t-e)/1e3}),(function(e){return e.getUTCSeconds()}))),Di=(Ri.range,gi((function(){}),(function(e,t){e.setTime(+e+t)}),(function(e,t){return t-e})));Di.every=function(e){return e=Math.floor(e),isFinite(e)&&e>0?e>1?gi((function(t){t.setTime(Math.floor(t/e)*e)}),(function(t,n){t.setTime(+t+n*e)}),(function(t,n){return(n-t)/e})):Di:null};Di.range;function Oi(e){return gi((function(t){t.setUTCDate(t.getUTCDate()-(t.getUTCDay()+7-e)%7),t.setUTCHours(0,0,0,0)}),(function(e,t){e.setUTCDate(e.getUTCDate()+7*t)}),(function(e,t){return(t-e)/6048e5}))}var zi=Oi(0),Li=Oi(1),Ui=Oi(2),Fi=Oi(3),Ii=Oi(4),ji=Oi(5),Hi=Oi(6),Vi=(zi.range,Li.range,Ui.range,Fi.range,Ii.range,ji.range,Hi.range,gi((function(e){e.setUTCHours(0,0,0,0)}),(function(e,t){e.setUTCDate(e.getUTCDate()+t)}),(function(e,t){return(t-e)/864e5}),(function(e){return e.getUTCDate()-1}))),$i=Vi,Wi=(Vi.range,gi((function(e){e.setUTCMonth(0,1),e.setUTCHours(0,0,0,0)}),(function(e,t){e.setUTCFullYear(e.getUTCFullYear()+t)}),(function(e,t){return t.getUTCFullYear()-e.getUTCFullYear()}),(function(e){return e.getUTCFullYear()})));Wi.every=function(e){return isFinite(e=Math.floor(e))&&e>0?gi((function(t){t.setUTCFullYear(Math.floor(t.getUTCFullYear()/e)*e),t.setUTCMonth(0,1),t.setUTCHours(0,0,0,0)}),(function(t,n){t.setUTCFullYear(t.getUTCFullYear()+n*e)})):null};var Qi=Wi;Wi.range;function qi(e){if(0<=e.y&&e.y<100){var t=new Date(-1,e.m,e.d,e.H,e.M,e.S,e.L);return t.setFullYear(e.y),t}return new Date(e.y,e.m,e.d,e.H,e.M,e.S,e.L)}function Bi(e){if(0<=e.y&&e.y<100){var t=new Date(Date.UTC(-1,e.m,e.d,e.H,e.M,e.S,e.L));return t.setUTCFullYear(e.y),t}return new Date(Date.UTC(e.y,e.m,e.d,e.H,e.M,e.S,e.L))}function Yi(e,t,n){return{y:e,m:t,d:n,H:0,M:0,S:0,L:0}}var Xi,Ki={"-":"",_:" ",0:"0"},Zi=/^\s*\d+/,Ji=/^%/,Gi=/[\\^$*+?|[\]().{}]/g;function eo(e,t,n){var r=e<0?"-":"",i=(r?-e:e)+"",o=i.length;return r+(o<n?new Array(n-o+1).join(t)+i:i)}function to(e){return e.replace(Gi,"\\$&")}function no(e){return new RegExp("^(?:"+e.map(to).join("|")+")","i")}function ro(e){for(var t={},n=-1,r=e.length;++n<r;)t[e[n].toLowerCase()]=n;return t}function io(e,t,n){var r=Zi.exec(t.slice(n,n+1));return r?(e.w=+r[0],n+r[0].length):-1}function oo(e,t,n){var r=Zi.exec(t.slice(n,n+1));return r?(e.u=+r[0],n+r[0].length):-1}function uo(e,t,n){var r=Zi.exec(t.slice(n,n+2));return r?(e.U=+r[0],n+r[0].length):-1}function ao(e,t,n){var r=Zi.exec(t.slice(n,n+2));return r?(e.V=+r[0],n+r[0].length):-1}function lo(e,t,n){var r=Zi.exec(t.slice(n,n+2));return r?(e.W=+r[0],n+r[0].length):-1}function co(e,t,n){var r=Zi.exec(t.slice(n,n+4));return r?(e.y=+r[0],n+r[0].length):-1}function so(e,t,n){var r=Zi.exec(t.slice(n,n+2));return r?(e.y=+r[0]+(+r[0]>68?1900:2e3),n+r[0].length):-1}function fo(e,t,n){var r=/^(Z)|([+-]\d\d)(?::?(\d\d))?/.exec(t.slice(n,n+6));return r?(e.Z=r[1]?0:-(r[2]+(r[3]||"00")),n+r[0].length):-1}function po(e,t,n){var r=Zi.exec(t.slice(n,n+1));return r?(e.q=3*r[0]-3,n+r[0].length):-1}function ho(e,t,n){var r=Zi.exec(t.slice(n,n+2));return r?(e.m=r[0]-1,n+r[0].length):-1}function mo(e,t,n){var r=Zi.exec(t.slice(n,n+2));return r?(e.d=+r[0],n+r[0].length):-1}function go(e,t,n){var r=Zi.exec(t.slice(n,n+3));return r?(e.m=0,e.d=+r[0],n+r[0].length):-1}function vo(e,t,n){var r=Zi.exec(t.slice(n,n+2));return r?(e.H=+r[0],n+r[0].length):-1}function yo(e,t,n){var r=Zi.exec(t.slice(n,n+2));return r?(e.M=+r[0],n+r[0].length):-1}function bo(e,t,n){var r=Zi.exec(t.slice(n,n+2));return r?(e.S=+r[0],n+r[0].length):-1}function wo(e,t,n){var r=Zi.exec(t.slice(n,n+3));return r?(e.L=+r[0],n+r[0].length):-1}function xo(e,t,n){var r=Zi.exec(t.slice(n,n+6));return r?(e.L=Math.floor(r[0]/1e3),n+r[0].length):-1}function _o(e,t,n){var r=Ji.exec(t.slice(n,n+1));return r?n+r[0].length:-1}function ko(e,t,n){var r=Zi.exec(t.slice(n));return r?(e.Q=+r[0],n+r[0].length):-1}function To(e,t,n){var r=Zi.exec(t.slice(n));return r?(e.s=+r[0],n+r[0].length):-1}function Eo(e,t){return eo(e.getDate(),t,2)}function Co(e,t){return eo(e.getHours(),t,2)}function So(e,t){return eo(e.getHours()%12||12,t,2)}function Mo(e,t){return eo(1+Ni.count(yi(e),e),t,3)}function No(e,t){return eo(e.getMilliseconds(),t,3)}function Po(e,t){return No(e,t)+"000"}function Ao(e,t){return eo(e.getMonth()+1,t,2)}function Ro(e,t){return eo(e.getMinutes(),t,2)}function Do(e,t){return eo(e.getSeconds(),t,2)}function Oo(e){var t=e.getDay();return 0===t?7:t}function zo(e,t){return eo(xi.count(yi(e)-1,e),t,2)}function Lo(e,t){var n=e.getDay();return e=n>=4||0===n?Ei(e):Ei.ceil(e),eo(Ei.count(yi(e),e)+(4===yi(e).getDay()),t,2)}function Uo(e){return e.getDay()}function Fo(e,t){return eo(_i.count(yi(e)-1,e),t,2)}function Io(e,t){return eo(e.getFullYear()%100,t,2)}function jo(e,t){return eo(e.getFullYear()%1e4,t,4)}function Ho(e){var t=e.getTimezoneOffset();return(t>0?"-":(t*=-1,"+"))+eo(t/60|0,"0",2)+eo(t%60,"0",2)}function Vo(e,t){return eo(e.getUTCDate(),t,2)}function $o(e,t){return eo(e.getUTCHours(),t,2)}function Wo(e,t){return eo(e.getUTCHours()%12||12,t,2)}function Qo(e,t){return eo(1+$i.count(Qi(e),e),t,3)}function qo(e,t){return eo(e.getUTCMilliseconds(),t,3)}function Bo(e,t){return qo(e,t)+"000"}function Yo(e,t){return eo(e.getUTCMonth()+1,t,2)}function Xo(e,t){return eo(e.getUTCMinutes(),t,2)}function Ko(e,t){return eo(e.getUTCSeconds(),t,2)}function Zo(e){var t=e.getUTCDay();return 0===t?7:t}function Jo(e,t){return eo(zi.count(Qi(e)-1,e),t,2)}function Go(e,t){var n=e.getUTCDay();return e=n>=4||0===n?Ii(e):Ii.ceil(e),eo(Ii.count(Qi(e),e)+(4===Qi(e).getUTCDay()),t,2)}function eu(e){return e.getUTCDay()}function tu(e,t){return eo(Li.count(Qi(e)-1,e),t,2)}function nu(e,t){return eo(e.getUTCFullYear()%100,t,2)}function ru(e,t){return eo(e.getUTCFullYear()%1e4,t,4)}function iu(){return"+0000"}function ou(){return"%"}function uu(e){return+e}function au(e){return Math.floor(+e/1e3)}!function(e){Xi=function(e){var t=e.dateTime,n=e.date,r=e.time,i=e.periods,o=e.days,u=e.shortDays,a=e.months,l=e.shortMonths,c=no(i),s=ro(i),f=no(o),d=ro(o),p=no(u),h=ro(u),m=no(a),g=ro(a),v=no(l),y=ro(l),b={a:function(e){return u[e.getDay()]},A:function(e){return o[e.getDay()]},b:function(e){return l[e.getMonth()]},B:function(e){return a[e.getMonth()]},c:null,d:Eo,e:Eo,f:Po,H:Co,I:So,j:Mo,L:No,m:Ao,M:Ro,p:function(e){return i[+(e.getHours()>=12)]},q:function(e){return 1+~~(e.getMonth()/3)},Q:uu,s:au,S:Do,u:Oo,U:zo,V:Lo,w:Uo,W:Fo,x:null,X:null,y:Io,Y:jo,Z:Ho,"%":ou},w={a:function(e){return u[e.getUTCDay()]},A:function(e){return o[e.getUTCDay()]},b:function(e){return l[e.getUTCMonth()]},B:function(e){return a[e.getUTCMonth()]},c:null,d:Vo,e:Vo,f:Bo,H:$o,I:Wo,j:Qo,L:qo,m:Yo,M:Xo,p:function(e){return i[+(e.getUTCHours()>=12)]},q:function(e){return 1+~~(e.getUTCMonth()/3)},Q:uu,s:au,S:Ko,u:Zo,U:Jo,V:Go,w:eu,W:tu,x:null,X:null,y:nu,Y:ru,Z:iu,"%":ou},x={a:function(e,t,n){var r=p.exec(t.slice(n));return r?(e.w=h[r[0].toLowerCase()],n+r[0].length):-1},A:function(e,t,n){var r=f.exec(t.slice(n));return r?(e.w=d[r[0].toLowerCase()],n+r[0].length):-1},b:function(e,t,n){var r=v.exec(t.slice(n));return r?(e.m=y[r[0].toLowerCase()],n+r[0].length):-1},B:function(e,t,n){var r=m.exec(t.slice(n));return r?(e.m=g[r[0].toLowerCase()],n+r[0].length):-1},c:function(e,n,r){return T(e,t,n,r)},d:mo,e:mo,f:xo,H:vo,I:vo,j:go,L:wo,m:ho,M:yo,p:function(e,t,n){var r=c.exec(t.slice(n));return r?(e.p=s[r[0].toLowerCase()],n+r[0].length):-1},q:po,Q:ko,s:To,S:bo,u:oo,U:uo,V:ao,w:io,W:lo,x:function(e,t,r){return T(e,n,t,r)},X:function(e,t,n){return T(e,r,t,n)},y:so,Y:co,Z:fo,"%":_o};function _(e,t){return function(n){var r,i,o,u=[],a=-1,l=0,c=e.length;for(n instanceof Date||(n=new Date(+n));++a<c;)37===e.charCodeAt(a)&&(u.push(e.slice(l,a)),null!=(i=Ki[r=e.charAt(++a)])?r=e.charAt(++a):i="e"===r?" ":"0",(o=t[r])&&(r=o(n,i)),u.push(r),l=a+1);return u.push(e.slice(l,a)),u.join("")}}function k(e,t){return function(n){var r,i,o=Yi(1900,void 0,1);if(T(o,e,n+="",0)!=n.length)return null;if("Q"in o)return new Date(o.Q);if("s"in o)return new Date(1e3*o.s+("L"in o?o.L:0));if(t&&!("Z"in o)&&(o.Z=0),"p"in o&&(o.H=o.H%12+12*o.p),void 0===o.m&&(o.m="q"in o?o.q:0),"V"in o){if(o.V<1||o.V>53)return null;"w"in o||(o.w=1),"Z"in o?(i=(r=Bi(Yi(o.y,0,1))).getUTCDay(),r=i>4||0===i?Li.ceil(r):Li(r),r=$i.offset(r,7*(o.V-1)),o.y=r.getUTCFullYear(),o.m=r.getUTCMonth(),o.d=r.getUTCDate()+(o.w+6)%7):(i=(r=qi(Yi(o.y,0,1))).getDay(),r=i>4||0===i?_i.ceil(r):_i(r),r=Ni.offset(r,7*(o.V-1)),o.y=r.getFullYear(),o.m=r.getMonth(),o.d=r.getDate()+(o.w+6)%7)}else("W"in o||"U"in o)&&("w"in o||(o.w="u"in o?o.u%7:"W"in o?1:0),i="Z"in o?Bi(Yi(o.y,0,1)).getUTCDay():qi(Yi(o.y,0,1)).getDay(),o.m=0,o.d="W"in o?(o.w+6)%7+7*o.W-(i+5)%7:o.w+7*o.U-(i+6)%7);return"Z"in o?(o.H+=o.Z/100|0,o.M+=o.Z%100,Bi(o)):qi(o)}}function T(e,t,n,r){for(var i,o,u=0,a=t.length,l=n.length;u<a;){if(r>=l)return-1;if(37===(i=t.charCodeAt(u++))){if(i=t.charAt(u++),!(o=x[i in Ki?t.charAt(u++):i])||(r=o(e,n,r))<0)return-1}else if(i!=n.charCodeAt(r++))return-1}return r}return b.x=_(n,b),b.X=_(r,b),b.c=_(t,b),w.x=_(n,w),w.X=_(r,w),w.c=_(t,w),{format:function(e){var t=_(e+="",b);return t.toString=function(){return e},t},parse:function(e){var t=k(e+="",!1);return t.toString=function(){return e},t},utcFormat:function(e){var t=_(e+="",w);return t.toString=function(){return e},t},utcParse:function(e){var t=k(e+="",!0);return t.toString=function(){return e},t}}}(e),Xi.format,Xi.parse,Xi.utcFormat,Xi.utcParse}({dateTime:"%x, %X",date:"%-m/%-d/%Y",time:"%-I:%M:%S %p",periods:["AM","PM"],days:["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],shortDays:["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],months:["January","February","March","April","May","June","July","August","September","October","November","December"],shortMonths:["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"]});var lu=gi((function(e){e.setUTCDate(1),e.setUTCHours(0,0,0,0)}),(function(e,t){e.setUTCMonth(e.getUTCMonth()+t)}),(function(e,t){return t.getUTCMonth()-e.getUTCMonth()+12*(t.getUTCFullYear()-e.getUTCFullYear())}),(function(e){return e.getUTCMonth()})),cu=(lu.range,gi((function(e){e.setUTCMinutes(0,0,0)}),(function(e,t){e.setTime(+e+36e5*t)}),(function(e,t){return(t-e)/36e5}),(function(e){return e.getUTCHours()}))),su=(cu.range,gi((function(e){e.setUTCSeconds(0,0)}),(function(e,t){e.setTime(+e+6e4*t)}),(function(e,t){return(t-e)/6e4}),(function(e){return e.getUTCMinutes()})));su.range;function fu(){this._=null}function du(e){e.U=e.C=e.L=e.R=e.P=e.N=null}function pu(e,t){var n=t,r=t.R,i=n.U;i?i.L===n?i.L=r:i.R=r:e._=r,r.U=i,n.U=r,n.R=r.L,n.R&&(n.R.U=n),r.L=n}function hu(e,t){var n=t,r=t.L,i=n.U;i?i.L===n?i.L=r:i.R=r:e._=r,r.U=i,n.U=r,n.L=r.R,n.L&&(n.L.U=n),r.R=n}function mu(e){for(;e.L;)e=e.L;return e}fu.prototype={constructor:fu,insert:function(e,t){var n,r,i;if(e){if(t.P=e,t.N=e.N,e.N&&(e.N.P=t),e.N=t,e.R){for(e=e.R;e.L;)e=e.L;e.L=t}else e.R=t;n=e}else this._?(e=mu(this._),t.P=null,t.N=e,e.P=e.L=t,n=e):(t.P=t.N=null,this._=t,n=null);for(t.L=t.R=null,t.U=n,t.C=!0,e=t;n&&n.C;)n===(r=n.U).L?(i=r.R)&&i.C?(n.C=i.C=!1,r.C=!0,e=r):(e===n.R&&(pu(this,n),n=(e=n).U),n.C=!1,r.C=!0,hu(this,r)):(i=r.L)&&i.C?(n.C=i.C=!1,r.C=!0,e=r):(e===n.L&&(hu(this,n),n=(e=n).U),n.C=!1,r.C=!0,pu(this,r)),n=e.U;this._.C=!1},remove:function(e){e.N&&(e.N.P=e.P),e.P&&(e.P.N=e.N),e.N=e.P=null;var t,n,r,i=e.U,o=e.L,u=e.R;if(n=o?u?mu(u):o:u,i?i.L===e?i.L=n:i.R=n:this._=n,o&&u?(r=n.C,n.C=e.C,n.L=o,o.U=n,n!==u?(i=n.U,n.U=e.U,e=n.R,i.L=e,n.R=u,u.U=n):(n.U=i,i=n,e=n.R)):(r=e.C,e=n),e&&(e.U=i),!r)if(e&&e.C)e.C=!1;else{do{if(e===this._)break;if(e===i.L){if((t=i.R).C&&(t.C=!1,i.C=!0,pu(this,i),t=i.R),t.L&&t.L.C||t.R&&t.R.C){t.R&&t.R.C||(t.L.C=!1,t.C=!0,hu(this,t),t=i.R),t.C=i.C,i.C=t.R.C=!1,pu(this,i),e=this._;break}}else if((t=i.L).C&&(t.C=!1,i.C=!0,hu(this,i),t=i.L),t.L&&t.L.C||t.R&&t.R.C){t.L&&t.L.C||(t.R.C=!1,t.C=!0,pu(this,t),t=i.L),t.C=i.C,i.C=t.L.C=!1,hu(this,i),e=this._;break}t.C=!0,e=i,i=i.U}while(!e.C);e&&(e.C=!1)}}};var gu=fu;function vu(e,t,n,r){var i=[null,null],o=Hu.push(i)-1;return i.left=e,i.right=t,n&&bu(i,e,t,n),r&&bu(i,t,e,r),Iu[e.index].halfedges.push(o),Iu[t.index].halfedges.push(o),i}function yu(e,t,n){var r=[t,n];return r.left=e,r}function bu(e,t,n,r){e[0]||e[1]?e.left===n?e[1]=r:e[0]=r:(e[0]=r,e.left=t,e.right=n)}function wu(e,t,n,r,i){var o,u=e[0],a=e[1],l=u[0],c=u[1],s=0,f=1,d=a[0]-l,p=a[1]-c;if(o=t-l,d||!(o>0)){if(o/=d,d<0){if(o<s)return;o<f&&(f=o)}else if(d>0){if(o>f)return;o>s&&(s=o)}if(o=r-l,d||!(o<0)){if(o/=d,d<0){if(o>f)return;o>s&&(s=o)}else if(d>0){if(o<s)return;o<f&&(f=o)}if(o=n-c,p||!(o>0)){if(o/=p,p<0){if(o<s)return;o<f&&(f=o)}else if(p>0){if(o>f)return;o>s&&(s=o)}if(o=i-c,p||!(o<0)){if(o/=p,p<0){if(o>f)return;o>s&&(s=o)}else if(p>0){if(o<s)return;o<f&&(f=o)}return!(s>0||f<1)||(s>0&&(e[0]=[l+s*d,c+s*p]),f<1&&(e[1]=[l+f*d,c+f*p]),!0)}}}}}function xu(e,t,n,r,i){var o=e[1];if(o)return!0;var u,a,l=e[0],c=e.left,s=e.right,f=c[0],d=c[1],p=s[0],h=s[1],m=(f+p)/2,g=(d+h)/2;if(h===d){if(m<t||m>=r)return;if(f>p){if(l){if(l[1]>=i)return}else l=[m,n];o=[m,i]}else{if(l){if(l[1]<n)return}else l=[m,i];o=[m,n]}}else if(a=g-(u=(f-p)/(h-d))*m,u<-1||u>1)if(f>p){if(l){if(l[1]>=i)return}else l=[(n-a)/u,n];o=[(i-a)/u,i]}else{if(l){if(l[1]<n)return}else l=[(i-a)/u,i];o=[(n-a)/u,n]}else if(d<h){if(l){if(l[0]>=r)return}else l=[t,u*t+a];o=[r,u*r+a]}else{if(l){if(l[0]<t)return}else l=[r,u*r+a];o=[t,u*t+a]}return e[0]=l,e[1]=o,!0}function _u(e,t){var n=e.site,r=t.left,i=t.right;return n===i&&(i=r,r=n),i?Math.atan2(i[1]-r[1],i[0]-r[0]):(n===r?(r=t[1],i=t[0]):(r=t[0],i=t[1]),Math.atan2(r[0]-i[0],i[1]-r[1]))}function ku(e,t){return t[+(t.left!==e.site)]}function Tu(e,t){return t[+(t.left===e.site)]}var Eu,Cu=[];function Su(){du(this),this.x=this.y=this.arc=this.site=this.cy=null}function Mu(e){var t=e.P,n=e.N;if(t&&n){var r=t.site,i=e.site,o=n.site;if(r!==o){var u=i[0],a=i[1],l=r[0]-u,c=r[1]-a,s=o[0]-u,f=o[1]-a,d=2*(l*f-c*s);if(!(d>=-$u)){var p=l*l+c*c,h=s*s+f*f,m=(f*p-c*h)/d,g=(l*h-s*p)/d,v=Cu.pop()||new Su;v.arc=e,v.site=i,v.x=m+u,v.y=(v.cy=g+a)+Math.sqrt(m*m+g*g),e.circle=v;for(var y=null,b=ju._;b;)if(v.y<b.y||v.y===b.y&&v.x<=b.x){if(!b.L){y=b.P;break}b=b.L}else{if(!b.R){y=b;break}b=b.R}ju.insert(y,v),y||(Eu=v)}}}}function Nu(e){var t=e.circle;t&&(t.P||(Eu=t.N),ju.remove(t),Cu.push(t),du(t),e.circle=null)}var Pu=[];function Au(){du(this),this.edge=this.site=this.circle=null}function Ru(e){var t=Pu.pop()||new Au;return t.site=e,t}function Du(e){Nu(e),Fu.remove(e),Pu.push(e),du(e)}function Ou(e){var t=e.circle,n=t.x,r=t.cy,i=[n,r],o=e.P,u=e.N,a=[e];Du(e);for(var l=o;l.circle&&Math.abs(n-l.circle.x)<Vu&&Math.abs(r-l.circle.cy)<Vu;)o=l.P,a.unshift(l),Du(l),l=o;a.unshift(l),Nu(l);for(var c=u;c.circle&&Math.abs(n-c.circle.x)<Vu&&Math.abs(r-c.circle.cy)<Vu;)u=c.N,a.push(c),Du(c),c=u;a.push(c),Nu(c);var s,f=a.length;for(s=1;s<f;++s)c=a[s],l=a[s-1],bu(c.edge,l.site,c.site,i);l=a[0],(c=a[f-1]).edge=vu(l.site,c.site,null,i),Mu(l),Mu(c)}function zu(e){for(var t,n,r,i,o=e[0],u=e[1],a=Fu._;a;)if((r=Lu(a,u)-o)>Vu)a=a.L;else{if(!((i=o-Uu(a,u))>Vu)){r>-Vu?(t=a.P,n=a):i>-Vu?(t=a,n=a.N):t=n=a;break}if(!a.R){t=a;break}a=a.R}!function(e){Iu[e.index]={site:e,halfedges:[]}}(e);var l=Ru(e);if(Fu.insert(t,l),t||n){if(t===n)return Nu(t),n=Ru(t.site),Fu.insert(l,n),l.edge=n.edge=vu(t.site,l.site),Mu(t),void Mu(n);if(n){Nu(t),Nu(n);var c=t.site,s=c[0],f=c[1],d=e[0]-s,p=e[1]-f,h=n.site,m=h[0]-s,g=h[1]-f,v=2*(d*g-p*m),y=d*d+p*p,b=m*m+g*g,w=[(g*y-p*b)/v+s,(d*b-m*y)/v+f];bu(n.edge,c,h,w),l.edge=vu(c,e,null,w),n.edge=vu(e,h,null,w),Mu(t),Mu(n)}else l.edge=vu(t.site,l.site)}}function Lu(e,t){var n=e.site,r=n[0],i=n[1],o=i-t;if(!o)return r;var u=e.P;if(!u)return-1/0;var a=(n=u.site)[0],l=n[1],c=l-t;if(!c)return a;var s=a-r,f=1/o-1/c,d=s/c;return f?(-d+Math.sqrt(d*d-2*f*(s*s/(-2*c)-l+c/2+i-o/2)))/f+r:(r+a)/2}function Uu(e,t){var n=e.N;if(n)return Lu(n,t);var r=e.site;return r[1]===t?r[0]:1/0}var Fu,Iu,ju,Hu,Vu=1e-6,$u=1e-12;function Wu(e,t){return t[1]-e[1]||t[0]-e[0]}function Qu(e,t){var n,r,i,o=e.sort(Wu).pop();for(Hu=[],Iu=new Array(e.length),Fu=new gu,ju=new gu;;)if(i=Eu,o&&(!i||o[1]<i.y||o[1]===i.y&&o[0]<i.x))o[0]===n&&o[1]===r||(zu(o),n=o[0],r=o[1]),o=e.pop();else{if(!i)break;Ou(i.arc)}if(function(){for(var e,t,n,r,i=0,o=Iu.length;i<o;++i)if((e=Iu[i])&&(r=(t=e.halfedges).length)){var u=new Array(r),a=new Array(r);for(n=0;n<r;++n)u[n]=n,a[n]=_u(e,Hu[t[n]]);for(u.sort((function(e,t){return a[t]-a[e]})),n=0;n<r;++n)a[n]=t[u[n]];for(n=0;n<r;++n)t[n]=a[n]}}(),t){var u=+t[0][0],a=+t[0][1],l=+t[1][0],c=+t[1][1];!function(e,t,n,r){for(var i,o=Hu.length;o--;)xu(i=Hu[o],e,t,n,r)&&wu(i,e,t,n,r)&&(Math.abs(i[0][0]-i[1][0])>Vu||Math.abs(i[0][1]-i[1][1])>Vu)||delete Hu[o]}(u,a,l,c),function(e,t,n,r){var i,o,u,a,l,c,s,f,d,p,h,m,g=Iu.length,v=!0;for(i=0;i<g;++i)if(o=Iu[i]){for(u=o.site,a=(l=o.halfedges).length;a--;)Hu[l[a]]||l.splice(a,1);for(a=0,c=l.length;a<c;)h=(p=Tu(o,Hu[l[a]]))[0],m=p[1],f=(s=ku(o,Hu[l[++a%c]]))[0],d=s[1],(Math.abs(h-f)>Vu||Math.abs(m-d)>Vu)&&(l.splice(a,0,Hu.push(yu(u,p,Math.abs(h-e)<Vu&&r-m>Vu?[e,Math.abs(f-e)<Vu?d:r]:Math.abs(m-r)<Vu&&n-h>Vu?[Math.abs(d-r)<Vu?f:n,r]:Math.abs(h-n)<Vu&&m-t>Vu?[n,Math.abs(f-n)<Vu?d:t]:Math.abs(m-t)<Vu&&h-e>Vu?[Math.abs(d-t)<Vu?f:e,t]:null))-1),++c);c&&(v=!1)}if(v){var y,b,w,x=1/0;for(i=0,v=null;i<g;++i)(o=Iu[i])&&(w=(y=(u=o.site)[0]-e)*y+(b=u[1]-t)*b)<x&&(x=w,v=o);if(v){var _=[e,t],k=[e,r],T=[n,r],E=[n,t];v.halfedges.push(Hu.push(yu(u=v.site,_,k))-1,Hu.push(yu(u,k,T))-1,Hu.push(yu(u,T,E))-1,Hu.push(yu(u,E,_))-1)}}for(i=0;i<g;++i)(o=Iu[i])&&(o.halfedges.length||delete Iu[i])}(u,a,l,c)}this.edges=Hu,this.cells=Iu,Fu=ju=Hu=Iu=null}Qu.prototype={constructor:Qu,polygons:function(){var e=this.edges;return this.cells.map((function(t){var n=t.halfedges.map((function(n){return ku(t,e[n])}));return n.data=t.site.data,n}))},triangles:function(){var e=[],t=this.edges;return this.cells.forEach((function(n,r){if(o=(i=n.halfedges).length)for(var i,o,u,a,l,c,s=n.site,f=-1,d=t[i[o-1]],p=d.left===s?d.right:d.left;++f<o;)u=p,p=(d=t[i[f]]).left===s?d.right:d.left,u&&p&&r<u.index&&r<p.index&&(l=u,c=p,((a=s)[0]-c[0])*(l[1]-a[1])-(a[0]-l[0])*(c[1]-a[1])<0)&&e.push([s.data,u.data,p.data])})),e},links:function(){return this.edges.filter((function(e){return e.right})).map((function(e){return{source:e.left.data,target:e.right.data}}))},find:function(e,t,n){for(var r,i,o=this,u=o._found||0,a=o.cells.length;!(i=o.cells[u]);)if(++u>=a)return null;var l=e-i.site[0],c=t-i.site[1],s=l*l+c*c;do{i=o.cells[r=u],u=null,i.halfedges.forEach((function(n){var r=o.edges[n],a=r.left;if(a!==i.site&&a||(a=r.right)){var l=e-a[0],c=t-a[1],f=l*l+c*c;f<s&&(s=f,u=a.index)}}))}while(null!==u);return o._found=r,null==n||s<=n*n?i.site:null}};Math.SQRT2;function qu(e,t,n){this.k=e,this.x=t,this.y=n}qu.prototype={constructor:qu,scale:function(e){return 1===e?this:new qu(this.k*e,this.x,this.y)},translate:function(e,t){return 0===e&0===t?this:new qu(this.k,this.x+this.k*e,this.y+this.k*t)},apply:function(e){return[e[0]*this.k+this.x,e[1]*this.k+this.y]},applyX:function(e){return e*this.k+this.x},applyY:function(e){return e*this.k+this.y},invert:function(e){return[(e[0]-this.x)/this.k,(e[1]-this.y)/this.k]},invertX:function(e){return(e-this.x)/this.k},invertY:function(e){return(e-this.y)/this.k},rescaleX:function(e){return e.copy().domain(e.range().map(this.invertX,this).map(e.invert,e))},rescaleY:function(e){return e.copy().domain(e.range().map(this.invertY,this).map(e.invert,e))},toString:function(){return"translate("+this.x+","+this.y+") scale("+this.k+")"}};new qu(1,0,0);qu.prototype},,,,,,function(e,t,n){"use strict";!function e(){if("undefined"!==typeof __REACT_DEVTOOLS_GLOBAL_HOOK__&&"function"===typeof __REACT_DEVTOOLS_GLOBAL_HOOK__.checkDCE){0;try{__REACT_DEVTOOLS_GLOBAL_HOOK__.checkDCE(e)}catch(t){console.error(t)}}}(),e.exports=n(30)},function(e,t,n){"use strict";function r(e,t){if(!(e instanceof t))throw new TypeError("Cannot call a class as a function")}n.d(t,"a",(function(){return r}))},function(e,t,n){"use strict";function r(e,t){for(var n=0;n<t.length;n++){var r=t[n];r.enumerable=r.enumerable||!1,r.configurable=!0,"value"in r&&(r.writable=!0),Object.defineProperty(e,r.key,r)}}function i(e,t,n){return t&&r(e.prototype,t),n&&r(e,n),e}n.d(t,"a",(function(){return i}))},function(e,t,n){"use strict";function r(e){if(void 0===e)throw new ReferenceError("this hasn't been initialised - super() hasn't been called");return e}n.d(t,"a",(function(){return r}))},function(e,t,n){"use strict";function r(e){return(r=Object.setPrototypeOf?Object.getPrototypeOf:function(e){return e.__proto__||Object.getPrototypeOf(e)})(e)}function i(){if("undefined"===typeof Reflect||!Reflect.construct)return!1;if(Reflect.construct.sham)return!1;if("function"===typeof Proxy)return!0;try{return Date.prototype.toString.call(Reflect.construct(Date,[],(function(){}))),!0}catch(e){return!1}}function o(e){return(o="function"===typeof Symbol&&"symbol"===typeof Symbol.iterator?function(e){return typeof e}:function(e){return e&&"function"===typeof Symbol&&e.constructor===Symbol&&e!==Symbol.prototype?"symbol":typeof e})(e)}n.d(t,"a",(function(){return l}));var u=n(11);function a(e,t){return!t||"object"!==o(t)&&"function"!==typeof t?Object(u.a)(e):t}function l(e){return function(){var t,n=r(e);if(i()){var o=r(this).constructor;t=Reflect.construct(n,arguments,o)}else t=n.apply(this,arguments);return a(this,t)}}},function(e,t,n){"use strict";function r(e,t){return(r=Object.setPrototypeOf||function(e,t){return e.__proto__=t,e})(e,t)}function i(e,t){if("function"!==typeof t&&null!==t)throw new TypeError("Super expression must either be null or a function");e.prototype=Object.create(t&&t.prototype,{constructor:{value:e,writable:!0,configurable:!0}}),t&&r(e,t)}n.d(t,"a",(function(){return i}))},,,,,,,,,function(e,t,n){"use strict";var r=Object.getOwnPropertySymbols,i=Object.prototype.hasOwnProperty,o=Object.prototype.propertyIsEnumerable;function u(e){if(null===e||void 0===e)throw new TypeError("Object.assign cannot be called with null or undefined");return Object(e)}e.exports=function(){try{if(!Object.assign)return!1;var e=new String("abc");if(e[5]="de","5"===Object.getOwnPropertyNames(e)[0])return!1;for(var t={},n=0;n<10;n++)t["_"+String.fromCharCode(n)]=n;if("0123456789"!==Object.getOwnPropertyNames(t).map((function(e){return t[e]})).join(""))return!1;var r={};return"abcdefghijklmnopqrst".split("").forEach((function(e){r[e]=e})),"abcdefghijklmnopqrst"===Object.keys(Object.assign({},r)).join("")}catch(i){return!1}}()?Object.assign:function(e,t){for(var n,a,l=u(e),c=1;c<arguments.length;c++){for(var s in n=Object(arguments[c]))i.call(n,s)&&(l[s]=n[s]);if(r){a=r(n);for(var f=0;f<a.length;f++)o.call(n,a[f])&&(l[a[f]]=n[a[f]])}}return l}},,,,,,,function(e,t,n){"use strict";var r=n(22),i="function"===typeof Symbol&&Symbol.for,o=i?Symbol.for("react.element"):60103,u=i?Symbol.for("react.portal"):60106,a=i?Symbol.for("react.fragment"):60107,l=i?Symbol.for("react.strict_mode"):60108,c=i?Symbol.for("react.profiler"):60114,s=i?Symbol.for("react.provider"):60109,f=i?Symbol.for("react.context"):60110,d=i?Symbol.for("react.forward_ref"):60112,p=i?Symbol.for("react.suspense"):60113,h=i?Symbol.for("react.memo"):60115,m=i?Symbol.for("react.lazy"):60116,g="function"===typeof Symbol&&Symbol.iterator;function v(e){for(var t="https://reactjs.org/docs/error-decoder.html?invariant="+e,n=1;n<arguments.length;n++)t+="&args[]="+encodeURIComponent(arguments[n]);return"Minified React error #"+e+"; visit "+t+" for the full message or use the non-minified dev environment for full errors and additional helpful warnings."}var y={isMounted:function(){return!1},enqueueForceUpdate:function(){},enqueueReplaceState:function(){},enqueueSetState:function(){}},b={};function w(e,t,n){this.props=e,this.context=t,this.refs=b,this.updater=n||y}function x(){}function _(e,t,n){this.props=e,this.context=t,this.refs=b,this.updater=n||y}w.prototype.isReactComponent={},w.prototype.setState=function(e,t){if("object"!==typeof e&&"function"!==typeof e&&null!=e)throw Error(v(85));this.updater.enqueueSetState(this,e,t,"setState")},w.prototype.forceUpdate=function(e){this.updater.enqueueForceUpdate(this,e,"forceUpdate")},x.prototype=w.prototype;var k=_.prototype=new x;k.constructor=_,r(k,w.prototype),k.isPureReactComponent=!0;var T={current:null},E=Object.prototype.hasOwnProperty,C={key:!0,ref:!0,__self:!0,__source:!0};function S(e,t,n){var r,i={},u=null,a=null;if(null!=t)for(r in void 0!==t.ref&&(a=t.ref),void 0!==t.key&&(u=""+t.key),t)E.call(t,r)&&!C.hasOwnProperty(r)&&(i[r]=t[r]);var l=arguments.length-2;if(1===l)i.children=n;else if(1<l){for(var c=Array(l),s=0;s<l;s++)c[s]=arguments[s+2];i.children=c}if(e&&e.defaultProps)for(r in l=e.defaultProps)void 0===i[r]&&(i[r]=l[r]);return{$$typeof:o,type:e,key:u,ref:a,props:i,_owner:T.current}}function M(e){return"object"===typeof e&&null!==e&&e.$$typeof===o}var N=/\/+/g,P=[];function A(e,t,n,r){if(P.length){var i=P.pop();return i.result=e,i.keyPrefix=t,i.func=n,i.context=r,i.count=0,i}return{result:e,keyPrefix:t,func:n,context:r,count:0}}function R(e){e.result=null,e.keyPrefix=null,e.func=null,e.context=null,e.count=0,10>P.length&&P.push(e)}function D(e,t,n){return null==e?0:function e(t,n,r,i){var a=typeof t;"undefined"!==a&&"boolean"!==a||(t=null);var l=!1;if(null===t)l=!0;else switch(a){case"string":case"number":l=!0;break;case"object":switch(t.$$typeof){case o:case u:l=!0}}if(l)return r(i,t,""===n?"."+O(t,0):n),1;if(l=0,n=""===n?".":n+":",Array.isArray(t))for(var c=0;c<t.length;c++){var s=n+O(a=t[c],c);l+=e(a,s,r,i)}else if(null===t||"object"!==typeof t?s=null:s="function"===typeof(s=g&&t[g]||t["@@iterator"])?s:null,"function"===typeof s)for(t=s.call(t),c=0;!(a=t.next()).done;)l+=e(a=a.value,s=n+O(a,c++),r,i);else if("object"===a)throw r=""+t,Error(v(31,"[object Object]"===r?"object with keys {"+Object.keys(t).join(", ")+"}":r,""));return l}(e,"",t,n)}function O(e,t){return"object"===typeof e&&null!==e&&null!=e.key?function(e){var t={"=":"=0",":":"=2"};return"$"+(""+e).replace(/[=:]/g,(function(e){return t[e]}))}(e.key):t.toString(36)}function z(e,t){e.func.call(e.context,t,e.count++)}function L(e,t,n){var r=e.result,i=e.keyPrefix;e=e.func.call(e.context,t,e.count++),Array.isArray(e)?U(e,r,n,(function(e){return e})):null!=e&&(M(e)&&(e=function(e,t){return{$$typeof:o,type:e.type,key:t,ref:e.ref,props:e.props,_owner:e._owner}}(e,i+(!e.key||t&&t.key===e.key?"":(""+e.key).replace(N,"$&/")+"/")+n)),r.push(e))}function U(e,t,n,r,i){var o="";null!=n&&(o=(""+n).replace(N,"$&/")+"/"),D(e,L,t=A(t,o,r,i)),R(t)}var F={current:null};function I(){var e=F.current;if(null===e)throw Error(v(321));return e}var j={ReactCurrentDispatcher:F,ReactCurrentBatchConfig:{suspense:null},ReactCurrentOwner:T,IsSomeRendererActing:{current:!1},assign:r};t.Children={map:function(e,t,n){if(null==e)return e;var r=[];return U(e,r,null,t,n),r},forEach:function(e,t,n){if(null==e)return e;D(e,z,t=A(null,null,t,n)),R(t)},count:function(e){return D(e,(function(){return null}),null)},toArray:function(e){var t=[];return U(e,t,null,(function(e){return e})),t},only:function(e){if(!M(e))throw Error(v(143));return e}},t.Component=w,t.Fragment=a,t.Profiler=c,t.PureComponent=_,t.StrictMode=l,t.Suspense=p,t.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED=j,t.cloneElement=function(e,t,n){if(null===e||void 0===e)throw Error(v(267,e));var i=r({},e.props),u=e.key,a=e.ref,l=e._owner;if(null!=t){if(void 0!==t.ref&&(a=t.ref,l=T.current),void 0!==t.key&&(u=""+t.key),e.type&&e.type.defaultProps)var c=e.type.defaultProps;for(s in t)E.call(t,s)&&!C.hasOwnProperty(s)&&(i[s]=void 0===t[s]&&void 0!==c?c[s]:t[s])}var s=arguments.length-2;if(1===s)i.children=n;else if(1<s){c=Array(s);for(var f=0;f<s;f++)c[f]=arguments[f+2];i.children=c}return{$$typeof:o,type:e.type,key:u,ref:a,props:i,_owner:l}},t.createContext=function(e,t){return void 0===t&&(t=null),(e={$$typeof:f,_calculateChangedBits:t,_currentValue:e,_currentValue2:e,_threadCount:0,Provider:null,Consumer:null}).Provider={$$typeof:s,_context:e},e.Consumer=e},t.createElement=S,t.createFactory=function(e){var t=S.bind(null,e);return t.type=e,t},t.createRef=function(){return{current:null}},t.forwardRef=function(e){return{$$typeof:d,render:e}},t.isValidElement=M,t.lazy=function(e){return{$$typeof:m,_ctor:e,_status:-1,_result:null}},t.memo=function(e,t){return{$$typeof:h,type:e,compare:void 0===t?null:t}},t.useCallback=function(e,t){return I().useCallback(e,t)},t.useContext=function(e,t){return I().useContext(e,t)},t.useDebugValue=function(){},t.useEffect=function(e,t){return I().useEffect(e,t)},t.useImperativeHandle=function(e,t,n){return I().useImperativeHandle(e,t,n)},t.useLayoutEffect=function(e,t){return I().useLayoutEffect(e,t)},t.useMemo=function(e,t){return I().useMemo(e,t)},t.useReducer=function(e,t,n){return I().useReducer(e,t,n)},t.useRef=function(e){return I().useRef(e)},t.useState=function(e){return I().useState(e)},t.version="16.13.1"},function(e,t,n){"use strict";var r=n(0),i=n(22),o=n(31);function u(e){for(var t="https://reactjs.org/docs/error-decoder.html?invariant="+e,n=1;n<arguments.length;n++)t+="&args[]="+encodeURIComponent(arguments[n]);return"Minified React error #"+e+"; visit "+t+" for the full message or use the non-minified dev environment for full errors and additional helpful warnings."}if(!r)throw Error(u(227));function a(e,t,n,r,i,o,u,a,l){var c=Array.prototype.slice.call(arguments,3);try{t.apply(n,c)}catch(s){this.onError(s)}}var l=!1,c=null,s=!1,f=null,d={onError:function(e){l=!0,c=e}};function p(e,t,n,r,i,o,u,s,f){l=!1,c=null,a.apply(d,arguments)}var h=null,m=null,g=null;function v(e,t,n){var r=e.type||"unknown-event";e.currentTarget=g(n),function(e,t,n,r,i,o,a,d,h){if(p.apply(this,arguments),l){if(!l)throw Error(u(198));var m=c;l=!1,c=null,s||(s=!0,f=m)}}(r,t,void 0,e),e.currentTarget=null}var y=null,b={};function w(){if(y)for(var e in b){var t=b[e],n=y.indexOf(e);if(!(-1<n))throw Error(u(96,e));if(!_[n]){if(!t.extractEvents)throw Error(u(97,e));for(var r in _[n]=t,n=t.eventTypes){var i=void 0,o=n[r],a=t,l=r;if(k.hasOwnProperty(l))throw Error(u(99,l));k[l]=o;var c=o.phasedRegistrationNames;if(c){for(i in c)c.hasOwnProperty(i)&&x(c[i],a,l);i=!0}else o.registrationName?(x(o.registrationName,a,l),i=!0):i=!1;if(!i)throw Error(u(98,r,e))}}}}function x(e,t,n){if(T[e])throw Error(u(100,e));T[e]=t,E[e]=t.eventTypes[n].dependencies}var _=[],k={},T={},E={};function C(e){var t,n=!1;for(t in e)if(e.hasOwnProperty(t)){var r=e[t];if(!b.hasOwnProperty(t)||b[t]!==r){if(b[t])throw Error(u(102,t));b[t]=r,n=!0}}n&&w()}var S=!("undefined"===typeof window||"undefined"===typeof window.document||"undefined"===typeof window.document.createElement),M=null,N=null,P=null;function A(e){if(e=m(e)){if("function"!==typeof M)throw Error(u(280));var t=e.stateNode;t&&(t=h(t),M(e.stateNode,e.type,t))}}function R(e){N?P?P.push(e):P=[e]:N=e}function D(){if(N){var e=N,t=P;if(P=N=null,A(e),t)for(e=0;e<t.length;e++)A(t[e])}}function O(e,t){return e(t)}function z(e,t,n,r,i){return e(t,n,r,i)}function L(){}var U=O,F=!1,I=!1;function j(){null===N&&null===P||(L(),D())}function H(e,t,n){if(I)return e(t,n);I=!0;try{return U(e,t,n)}finally{I=!1,j()}}var V=/^[:A-Z_a-z\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u02FF\u0370-\u037D\u037F-\u1FFF\u200C-\u200D\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD][:A-Z_a-z\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u02FF\u0370-\u037D\u037F-\u1FFF\u200C-\u200D\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD\-.0-9\u00B7\u0300-\u036F\u203F-\u2040]*$/,$=Object.prototype.hasOwnProperty,W={},Q={};function q(e,t,n,r,i,o){this.acceptsBooleans=2===t||3===t||4===t,this.attributeName=r,this.attributeNamespace=i,this.mustUseProperty=n,this.propertyName=e,this.type=t,this.sanitizeURL=o}var B={};"children dangerouslySetInnerHTML defaultValue defaultChecked innerHTML suppressContentEditableWarning suppressHydrationWarning style".split(" ").forEach((function(e){B[e]=new q(e,0,!1,e,null,!1)})),[["acceptCharset","accept-charset"],["className","class"],["htmlFor","for"],["httpEquiv","http-equiv"]].forEach((function(e){var t=e[0];B[t]=new q(t,1,!1,e[1],null,!1)})),["contentEditable","draggable","spellCheck","value"].forEach((function(e){B[e]=new q(e,2,!1,e.toLowerCase(),null,!1)})),["autoReverse","externalResourcesRequired","focusable","preserveAlpha"].forEach((function(e){B[e]=new q(e,2,!1,e,null,!1)})),"allowFullScreen async autoFocus autoPlay controls default defer disabled disablePictureInPicture formNoValidate hidden loop noModule noValidate open playsInline readOnly required reversed scoped seamless itemScope".split(" ").forEach((function(e){B[e]=new q(e,3,!1,e.toLowerCase(),null,!1)})),["checked","multiple","muted","selected"].forEach((function(e){B[e]=new q(e,3,!0,e,null,!1)})),["capture","download"].forEach((function(e){B[e]=new q(e,4,!1,e,null,!1)})),["cols","rows","size","span"].forEach((function(e){B[e]=new q(e,6,!1,e,null,!1)})),["rowSpan","start"].forEach((function(e){B[e]=new q(e,5,!1,e.toLowerCase(),null,!1)}));var Y=/[\-:]([a-z])/g;function X(e){return e[1].toUpperCase()}"accent-height alignment-baseline arabic-form baseline-shift cap-height clip-path clip-rule color-interpolation color-interpolation-filters color-profile color-rendering dominant-baseline enable-background fill-opacity fill-rule flood-color flood-opacity font-family font-size font-size-adjust font-stretch font-style font-variant font-weight glyph-name glyph-orientation-horizontal glyph-orientation-vertical horiz-adv-x horiz-origin-x image-rendering letter-spacing lighting-color marker-end marker-mid marker-start overline-position overline-thickness paint-order panose-1 pointer-events rendering-intent shape-rendering stop-color stop-opacity strikethrough-position strikethrough-thickness stroke-dasharray stroke-dashoffset stroke-linecap stroke-linejoin stroke-miterlimit stroke-opacity stroke-width text-anchor text-decoration text-rendering underline-position underline-thickness unicode-bidi unicode-range units-per-em v-alphabetic v-hanging v-ideographic v-mathematical vector-effect vert-adv-y vert-origin-x vert-origin-y word-spacing writing-mode xmlns:xlink x-height".split(" ").forEach((function(e){var t=e.replace(Y,X);B[t]=new q(t,1,!1,e,null,!1)})),"xlink:actuate xlink:arcrole xlink:role xlink:show xlink:title xlink:type".split(" ").forEach((function(e){var t=e.replace(Y,X);B[t]=new q(t,1,!1,e,"http://www.w3.org/1999/xlink",!1)})),["xml:base","xml:lang","xml:space"].forEach((function(e){var t=e.replace(Y,X);B[t]=new q(t,1,!1,e,"http://www.w3.org/XML/1998/namespace",!1)})),["tabIndex","crossOrigin"].forEach((function(e){B[e]=new q(e,1,!1,e.toLowerCase(),null,!1)})),B.xlinkHref=new q("xlinkHref",1,!1,"xlink:href","http://www.w3.org/1999/xlink",!0),["src","href","action","formAction"].forEach((function(e){B[e]=new q(e,1,!1,e.toLowerCase(),null,!0)}));var K=r.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED;function Z(e,t,n,r){var i=B.hasOwnProperty(t)?B[t]:null;(null!==i?0===i.type:!r&&(2<t.length&&("o"===t[0]||"O"===t[0])&&("n"===t[1]||"N"===t[1])))||(function(e,t,n,r){if(null===t||"undefined"===typeof t||function(e,t,n,r){if(null!==n&&0===n.type)return!1;switch(typeof t){case"function":case"symbol":return!0;case"boolean":return!r&&(null!==n?!n.acceptsBooleans:"data-"!==(e=e.toLowerCase().slice(0,5))&&"aria-"!==e);default:return!1}}(e,t,n,r))return!0;if(r)return!1;if(null!==n)switch(n.type){case 3:return!t;case 4:return!1===t;case 5:return isNaN(t);case 6:return isNaN(t)||1>t}return!1}(t,n,i,r)&&(n=null),r||null===i?function(e){return!!$.call(Q,e)||!$.call(W,e)&&(V.test(e)?Q[e]=!0:(W[e]=!0,!1))}(t)&&(null===n?e.removeAttribute(t):e.setAttribute(t,""+n)):i.mustUseProperty?e[i.propertyName]=null===n?3!==i.type&&"":n:(t=i.attributeName,r=i.attributeNamespace,null===n?e.removeAttribute(t):(n=3===(i=i.type)||4===i&&!0===n?"":""+n,r?e.setAttributeNS(r,t,n):e.setAttribute(t,n))))}K.hasOwnProperty("ReactCurrentDispatcher")||(K.ReactCurrentDispatcher={current:null}),K.hasOwnProperty("ReactCurrentBatchConfig")||(K.ReactCurrentBatchConfig={suspense:null});var J=/^(.*)[\\\/]/,G="function"===typeof Symbol&&Symbol.for,ee=G?Symbol.for("react.element"):60103,te=G?Symbol.for("react.portal"):60106,ne=G?Symbol.for("react.fragment"):60107,re=G?Symbol.for("react.strict_mode"):60108,ie=G?Symbol.for("react.profiler"):60114,oe=G?Symbol.for("react.provider"):60109,ue=G?Symbol.for("react.context"):60110,ae=G?Symbol.for("react.concurrent_mode"):60111,le=G?Symbol.for("react.forward_ref"):60112,ce=G?Symbol.for("react.suspense"):60113,se=G?Symbol.for("react.suspense_list"):60120,fe=G?Symbol.for("react.memo"):60115,de=G?Symbol.for("react.lazy"):60116,pe=G?Symbol.for("react.block"):60121,he="function"===typeof Symbol&&Symbol.iterator;function me(e){return null===e||"object"!==typeof e?null:"function"===typeof(e=he&&e[he]||e["@@iterator"])?e:null}function ge(e){if(null==e)return null;if("function"===typeof e)return e.displayName||e.name||null;if("string"===typeof e)return e;switch(e){case ne:return"Fragment";case te:return"Portal";case ie:return"Profiler";case re:return"StrictMode";case ce:return"Suspense";case se:return"SuspenseList"}if("object"===typeof e)switch(e.$$typeof){case ue:return"Context.Consumer";case oe:return"Context.Provider";case le:var t=e.render;return t=t.displayName||t.name||"",e.displayName||(""!==t?"ForwardRef("+t+")":"ForwardRef");case fe:return ge(e.type);case pe:return ge(e.render);case de:if(e=1===e._status?e._result:null)return ge(e)}return null}function ve(e){var t="";do{e:switch(e.tag){case 3:case 4:case 6:case 7:case 10:case 9:var n="";break e;default:var r=e._debugOwner,i=e._debugSource,o=ge(e.type);n=null,r&&(n=ge(r.type)),r=o,o="",i?o=" (at "+i.fileName.replace(J,"")+":"+i.lineNumber+")":n&&(o=" (created by "+n+")"),n="\n    in "+(r||"Unknown")+o}t+=n,e=e.return}while(e);return t}function ye(e){switch(typeof e){case"boolean":case"number":case"object":case"string":case"undefined":return e;default:return""}}function be(e){var t=e.type;return(e=e.nodeName)&&"input"===e.toLowerCase()&&("checkbox"===t||"radio"===t)}function we(e){e._valueTracker||(e._valueTracker=function(e){var t=be(e)?"checked":"value",n=Object.getOwnPropertyDescriptor(e.constructor.prototype,t),r=""+e[t];if(!e.hasOwnProperty(t)&&"undefined"!==typeof n&&"function"===typeof n.get&&"function"===typeof n.set){var i=n.get,o=n.set;return Object.defineProperty(e,t,{configurable:!0,get:function(){return i.call(this)},set:function(e){r=""+e,o.call(this,e)}}),Object.defineProperty(e,t,{enumerable:n.enumerable}),{getValue:function(){return r},setValue:function(e){r=""+e},stopTracking:function(){e._valueTracker=null,delete e[t]}}}}(e))}function xe(e){if(!e)return!1;var t=e._valueTracker;if(!t)return!0;var n=t.getValue(),r="";return e&&(r=be(e)?e.checked?"true":"false":e.value),(e=r)!==n&&(t.setValue(e),!0)}function _e(e,t){var n=t.checked;return i({},t,{defaultChecked:void 0,defaultValue:void 0,value:void 0,checked:null!=n?n:e._wrapperState.initialChecked})}function ke(e,t){var n=null==t.defaultValue?"":t.defaultValue,r=null!=t.checked?t.checked:t.defaultChecked;n=ye(null!=t.value?t.value:n),e._wrapperState={initialChecked:r,initialValue:n,controlled:"checkbox"===t.type||"radio"===t.type?null!=t.checked:null!=t.value}}function Te(e,t){null!=(t=t.checked)&&Z(e,"checked",t,!1)}function Ee(e,t){Te(e,t);var n=ye(t.value),r=t.type;if(null!=n)"number"===r?(0===n&&""===e.value||e.value!=n)&&(e.value=""+n):e.value!==""+n&&(e.value=""+n);else if("submit"===r||"reset"===r)return void e.removeAttribute("value");t.hasOwnProperty("value")?Se(e,t.type,n):t.hasOwnProperty("defaultValue")&&Se(e,t.type,ye(t.defaultValue)),null==t.checked&&null!=t.defaultChecked&&(e.defaultChecked=!!t.defaultChecked)}function Ce(e,t,n){if(t.hasOwnProperty("value")||t.hasOwnProperty("defaultValue")){var r=t.type;if(!("submit"!==r&&"reset"!==r||void 0!==t.value&&null!==t.value))return;t=""+e._wrapperState.initialValue,n||t===e.value||(e.value=t),e.defaultValue=t}""!==(n=e.name)&&(e.name=""),e.defaultChecked=!!e._wrapperState.initialChecked,""!==n&&(e.name=n)}function Se(e,t,n){"number"===t&&e.ownerDocument.activeElement===e||(null==n?e.defaultValue=""+e._wrapperState.initialValue:e.defaultValue!==""+n&&(e.defaultValue=""+n))}function Me(e,t){return e=i({children:void 0},t),(t=function(e){var t="";return r.Children.forEach(e,(function(e){null!=e&&(t+=e)})),t}(t.children))&&(e.children=t),e}function Ne(e,t,n,r){if(e=e.options,t){t={};for(var i=0;i<n.length;i++)t["$"+n[i]]=!0;for(n=0;n<e.length;n++)i=t.hasOwnProperty("$"+e[n].value),e[n].selected!==i&&(e[n].selected=i),i&&r&&(e[n].defaultSelected=!0)}else{for(n=""+ye(n),t=null,i=0;i<e.length;i++){if(e[i].value===n)return e[i].selected=!0,void(r&&(e[i].defaultSelected=!0));null!==t||e[i].disabled||(t=e[i])}null!==t&&(t.selected=!0)}}function Pe(e,t){if(null!=t.dangerouslySetInnerHTML)throw Error(u(91));return i({},t,{value:void 0,defaultValue:void 0,children:""+e._wrapperState.initialValue})}function Ae(e,t){var n=t.value;if(null==n){if(n=t.children,t=t.defaultValue,null!=n){if(null!=t)throw Error(u(92));if(Array.isArray(n)){if(!(1>=n.length))throw Error(u(93));n=n[0]}t=n}null==t&&(t=""),n=t}e._wrapperState={initialValue:ye(n)}}function Re(e,t){var n=ye(t.value),r=ye(t.defaultValue);null!=n&&((n=""+n)!==e.value&&(e.value=n),null==t.defaultValue&&e.defaultValue!==n&&(e.defaultValue=n)),null!=r&&(e.defaultValue=""+r)}function De(e){var t=e.textContent;t===e._wrapperState.initialValue&&""!==t&&null!==t&&(e.value=t)}var Oe="http://www.w3.org/1999/xhtml",ze="http://www.w3.org/2000/svg";function Le(e){switch(e){case"svg":return"http://www.w3.org/2000/svg";case"math":return"http://www.w3.org/1998/Math/MathML";default:return"http://www.w3.org/1999/xhtml"}}function Ue(e,t){return null==e||"http://www.w3.org/1999/xhtml"===e?Le(t):"http://www.w3.org/2000/svg"===e&&"foreignObject"===t?"http://www.w3.org/1999/xhtml":e}var Fe,Ie=function(e){return"undefined"!==typeof MSApp&&MSApp.execUnsafeLocalFunction?function(t,n,r,i){MSApp.execUnsafeLocalFunction((function(){return e(t,n)}))}:e}((function(e,t){if(e.namespaceURI!==ze||"innerHTML"in e)e.innerHTML=t;else{for((Fe=Fe||document.createElement("div")).innerHTML="<svg>"+t.valueOf().toString()+"</svg>",t=Fe.firstChild;e.firstChild;)e.removeChild(e.firstChild);for(;t.firstChild;)e.appendChild(t.firstChild)}}));function je(e,t){if(t){var n=e.firstChild;if(n&&n===e.lastChild&&3===n.nodeType)return void(n.nodeValue=t)}e.textContent=t}function He(e,t){var n={};return n[e.toLowerCase()]=t.toLowerCase(),n["Webkit"+e]="webkit"+t,n["Moz"+e]="moz"+t,n}var Ve={animationend:He("Animation","AnimationEnd"),animationiteration:He("Animation","AnimationIteration"),animationstart:He("Animation","AnimationStart"),transitionend:He("Transition","TransitionEnd")},$e={},We={};function Qe(e){if($e[e])return $e[e];if(!Ve[e])return e;var t,n=Ve[e];for(t in n)if(n.hasOwnProperty(t)&&t in We)return $e[e]=n[t];return e}S&&(We=document.createElement("div").style,"AnimationEvent"in window||(delete Ve.animationend.animation,delete Ve.animationiteration.animation,delete Ve.animationstart.animation),"TransitionEvent"in window||delete Ve.transitionend.transition);var qe=Qe("animationend"),Be=Qe("animationiteration"),Ye=Qe("animationstart"),Xe=Qe("transitionend"),Ke="abort canplay canplaythrough durationchange emptied encrypted ended error loadeddata loadedmetadata loadstart pause play playing progress ratechange seeked seeking stalled suspend timeupdate volumechange waiting".split(" "),Ze=new("function"===typeof WeakMap?WeakMap:Map);function Je(e){var t=Ze.get(e);return void 0===t&&(t=new Map,Ze.set(e,t)),t}function Ge(e){var t=e,n=e;if(e.alternate)for(;t.return;)t=t.return;else{e=t;do{0!==(1026&(t=e).effectTag)&&(n=t.return),e=t.return}while(e)}return 3===t.tag?n:null}function et(e){if(13===e.tag){var t=e.memoizedState;if(null===t&&(null!==(e=e.alternate)&&(t=e.memoizedState)),null!==t)return t.dehydrated}return null}function tt(e){if(Ge(e)!==e)throw Error(u(188))}function nt(e){if(!(e=function(e){var t=e.alternate;if(!t){if(null===(t=Ge(e)))throw Error(u(188));return t!==e?null:e}for(var n=e,r=t;;){var i=n.return;if(null===i)break;var o=i.alternate;if(null===o){if(null!==(r=i.return)){n=r;continue}break}if(i.child===o.child){for(o=i.child;o;){if(o===n)return tt(i),e;if(o===r)return tt(i),t;o=o.sibling}throw Error(u(188))}if(n.return!==r.return)n=i,r=o;else{for(var a=!1,l=i.child;l;){if(l===n){a=!0,n=i,r=o;break}if(l===r){a=!0,r=i,n=o;break}l=l.sibling}if(!a){for(l=o.child;l;){if(l===n){a=!0,n=o,r=i;break}if(l===r){a=!0,r=o,n=i;break}l=l.sibling}if(!a)throw Error(u(189))}}if(n.alternate!==r)throw Error(u(190))}if(3!==n.tag)throw Error(u(188));return n.stateNode.current===n?e:t}(e)))return null;for(var t=e;;){if(5===t.tag||6===t.tag)return t;if(t.child)t.child.return=t,t=t.child;else{if(t===e)break;for(;!t.sibling;){if(!t.return||t.return===e)return null;t=t.return}t.sibling.return=t.return,t=t.sibling}}return null}function rt(e,t){if(null==t)throw Error(u(30));return null==e?t:Array.isArray(e)?Array.isArray(t)?(e.push.apply(e,t),e):(e.push(t),e):Array.isArray(t)?[e].concat(t):[e,t]}function it(e,t,n){Array.isArray(e)?e.forEach(t,n):e&&t.call(n,e)}var ot=null;function ut(e){if(e){var t=e._dispatchListeners,n=e._dispatchInstances;if(Array.isArray(t))for(var r=0;r<t.length&&!e.isPropagationStopped();r++)v(e,t[r],n[r]);else t&&v(e,t,n);e._dispatchListeners=null,e._dispatchInstances=null,e.isPersistent()||e.constructor.release(e)}}function at(e){if(null!==e&&(ot=rt(ot,e)),e=ot,ot=null,e){if(it(e,ut),ot)throw Error(u(95));if(s)throw e=f,s=!1,f=null,e}}function lt(e){return(e=e.target||e.srcElement||window).correspondingUseElement&&(e=e.correspondingUseElement),3===e.nodeType?e.parentNode:e}function ct(e){if(!S)return!1;var t=(e="on"+e)in document;return t||((t=document.createElement("div")).setAttribute(e,"return;"),t="function"===typeof t[e]),t}var st=[];function ft(e){e.topLevelType=null,e.nativeEvent=null,e.targetInst=null,e.ancestors.length=0,10>st.length&&st.push(e)}function dt(e,t,n,r){if(st.length){var i=st.pop();return i.topLevelType=e,i.eventSystemFlags=r,i.nativeEvent=t,i.targetInst=n,i}return{topLevelType:e,eventSystemFlags:r,nativeEvent:t,targetInst:n,ancestors:[]}}function pt(e){var t=e.targetInst,n=t;do{if(!n){e.ancestors.push(n);break}var r=n;if(3===r.tag)r=r.stateNode.containerInfo;else{for(;r.return;)r=r.return;r=3!==r.tag?null:r.stateNode.containerInfo}if(!r)break;5!==(t=n.tag)&&6!==t||e.ancestors.push(n),n=Sn(r)}while(n);for(n=0;n<e.ancestors.length;n++){t=e.ancestors[n];var i=lt(e.nativeEvent);r=e.topLevelType;var o=e.nativeEvent,u=e.eventSystemFlags;0===n&&(u|=64);for(var a=null,l=0;l<_.length;l++){var c=_[l];c&&(c=c.extractEvents(r,t,o,i,u))&&(a=rt(a,c))}at(a)}}function ht(e,t,n){if(!n.has(e)){switch(e){case"scroll":Yt(t,"scroll",!0);break;case"focus":case"blur":Yt(t,"focus",!0),Yt(t,"blur",!0),n.set("blur",null),n.set("focus",null);break;case"cancel":case"close":ct(e)&&Yt(t,e,!0);break;case"invalid":case"submit":case"reset":break;default:-1===Ke.indexOf(e)&&Bt(e,t)}n.set(e,null)}}var mt,gt,vt,yt=!1,bt=[],wt=null,xt=null,_t=null,kt=new Map,Tt=new Map,Et=[],Ct="mousedown mouseup touchcancel touchend touchstart auxclick dblclick pointercancel pointerdown pointerup dragend dragstart drop compositionend compositionstart keydown keypress keyup input textInput close cancel copy cut paste click change contextmenu reset submit".split(" "),St="focus blur dragenter dragleave mouseover mouseout pointerover pointerout gotpointercapture lostpointercapture".split(" ");function Mt(e,t,n,r,i){return{blockedOn:e,topLevelType:t,eventSystemFlags:32|n,nativeEvent:i,container:r}}function Nt(e,t){switch(e){case"focus":case"blur":wt=null;break;case"dragenter":case"dragleave":xt=null;break;case"mouseover":case"mouseout":_t=null;break;case"pointerover":case"pointerout":kt.delete(t.pointerId);break;case"gotpointercapture":case"lostpointercapture":Tt.delete(t.pointerId)}}function Pt(e,t,n,r,i,o){return null===e||e.nativeEvent!==o?(e=Mt(t,n,r,i,o),null!==t&&(null!==(t=Mn(t))&&gt(t)),e):(e.eventSystemFlags|=r,e)}function At(e){var t=Sn(e.target);if(null!==t){var n=Ge(t);if(null!==n)if(13===(t=n.tag)){if(null!==(t=et(n)))return e.blockedOn=t,void o.unstable_runWithPriority(e.priority,(function(){vt(n)}))}else if(3===t&&n.stateNode.hydrate)return void(e.blockedOn=3===n.tag?n.stateNode.containerInfo:null)}e.blockedOn=null}function Rt(e){if(null!==e.blockedOn)return!1;var t=Jt(e.topLevelType,e.eventSystemFlags,e.container,e.nativeEvent);if(null!==t){var n=Mn(t);return null!==n&&gt(n),e.blockedOn=t,!1}return!0}function Dt(e,t,n){Rt(e)&&n.delete(t)}function Ot(){for(yt=!1;0<bt.length;){var e=bt[0];if(null!==e.blockedOn){null!==(e=Mn(e.blockedOn))&&mt(e);break}var t=Jt(e.topLevelType,e.eventSystemFlags,e.container,e.nativeEvent);null!==t?e.blockedOn=t:bt.shift()}null!==wt&&Rt(wt)&&(wt=null),null!==xt&&Rt(xt)&&(xt=null),null!==_t&&Rt(_t)&&(_t=null),kt.forEach(Dt),Tt.forEach(Dt)}function zt(e,t){e.blockedOn===t&&(e.blockedOn=null,yt||(yt=!0,o.unstable_scheduleCallback(o.unstable_NormalPriority,Ot)))}function Lt(e){function t(t){return zt(t,e)}if(0<bt.length){zt(bt[0],e);for(var n=1;n<bt.length;n++){var r=bt[n];r.blockedOn===e&&(r.blockedOn=null)}}for(null!==wt&&zt(wt,e),null!==xt&&zt(xt,e),null!==_t&&zt(_t,e),kt.forEach(t),Tt.forEach(t),n=0;n<Et.length;n++)(r=Et[n]).blockedOn===e&&(r.blockedOn=null);for(;0<Et.length&&null===(n=Et[0]).blockedOn;)At(n),null===n.blockedOn&&Et.shift()}var Ut={},Ft=new Map,It=new Map,jt=["abort","abort",qe,"animationEnd",Be,"animationIteration",Ye,"animationStart","canplay","canPlay","canplaythrough","canPlayThrough","durationchange","durationChange","emptied","emptied","encrypted","encrypted","ended","ended","error","error","gotpointercapture","gotPointerCapture","load","load","loadeddata","loadedData","loadedmetadata","loadedMetadata","loadstart","loadStart","lostpointercapture","lostPointerCapture","playing","playing","progress","progress","seeking","seeking","stalled","stalled","suspend","suspend","timeupdate","timeUpdate",Xe,"transitionEnd","waiting","waiting"];function Ht(e,t){for(var n=0;n<e.length;n+=2){var r=e[n],i=e[n+1],o="on"+(i[0].toUpperCase()+i.slice(1));o={phasedRegistrationNames:{bubbled:o,captured:o+"Capture"},dependencies:[r],eventPriority:t},It.set(r,t),Ft.set(r,o),Ut[i]=o}}Ht("blur blur cancel cancel click click close close contextmenu contextMenu copy copy cut cut auxclick auxClick dblclick doubleClick dragend dragEnd dragstart dragStart drop drop focus focus input input invalid invalid keydown keyDown keypress keyPress keyup keyUp mousedown mouseDown mouseup mouseUp paste paste pause pause play play pointercancel pointerCancel pointerdown pointerDown pointerup pointerUp ratechange rateChange reset reset seeked seeked submit submit touchcancel touchCancel touchend touchEnd touchstart touchStart volumechange volumeChange".split(" "),0),Ht("drag drag dragenter dragEnter dragexit dragExit dragleave dragLeave dragover dragOver mousemove mouseMove mouseout mouseOut mouseover mouseOver pointermove pointerMove pointerout pointerOut pointerover pointerOver scroll scroll toggle toggle touchmove touchMove wheel wheel".split(" "),1),Ht(jt,2);for(var Vt="change selectionchange textInput compositionstart compositionend compositionupdate".split(" "),$t=0;$t<Vt.length;$t++)It.set(Vt[$t],0);var Wt=o.unstable_UserBlockingPriority,Qt=o.unstable_runWithPriority,qt=!0;function Bt(e,t){Yt(t,e,!1)}function Yt(e,t,n){var r=It.get(t);switch(void 0===r?2:r){case 0:r=Xt.bind(null,t,1,e);break;case 1:r=Kt.bind(null,t,1,e);break;default:r=Zt.bind(null,t,1,e)}n?e.addEventListener(t,r,!0):e.addEventListener(t,r,!1)}function Xt(e,t,n,r){F||L();var i=Zt,o=F;F=!0;try{z(i,e,t,n,r)}finally{(F=o)||j()}}function Kt(e,t,n,r){Qt(Wt,Zt.bind(null,e,t,n,r))}function Zt(e,t,n,r){if(qt)if(0<bt.length&&-1<Ct.indexOf(e))e=Mt(null,e,t,n,r),bt.push(e);else{var i=Jt(e,t,n,r);if(null===i)Nt(e,r);else if(-1<Ct.indexOf(e))e=Mt(i,e,t,n,r),bt.push(e);else if(!function(e,t,n,r,i){switch(t){case"focus":return wt=Pt(wt,e,t,n,r,i),!0;case"dragenter":return xt=Pt(xt,e,t,n,r,i),!0;case"mouseover":return _t=Pt(_t,e,t,n,r,i),!0;case"pointerover":var o=i.pointerId;return kt.set(o,Pt(kt.get(o)||null,e,t,n,r,i)),!0;case"gotpointercapture":return o=i.pointerId,Tt.set(o,Pt(Tt.get(o)||null,e,t,n,r,i)),!0}return!1}(i,e,t,n,r)){Nt(e,r),e=dt(e,r,null,t);try{H(pt,e)}finally{ft(e)}}}}function Jt(e,t,n,r){if(null!==(n=Sn(n=lt(r)))){var i=Ge(n);if(null===i)n=null;else{var o=i.tag;if(13===o){if(null!==(n=et(i)))return n;n=null}else if(3===o){if(i.stateNode.hydrate)return 3===i.tag?i.stateNode.containerInfo:null;n=null}else i!==n&&(n=null)}}e=dt(e,r,n,t);try{H(pt,e)}finally{ft(e)}return null}var Gt={animationIterationCount:!0,borderImageOutset:!0,borderImageSlice:!0,borderImageWidth:!0,boxFlex:!0,boxFlexGroup:!0,boxOrdinalGroup:!0,columnCount:!0,columns:!0,flex:!0,flexGrow:!0,flexPositive:!0,flexShrink:!0,flexNegative:!0,flexOrder:!0,gridArea:!0,gridRow:!0,gridRowEnd:!0,gridRowSpan:!0,gridRowStart:!0,gridColumn:!0,gridColumnEnd:!0,gridColumnSpan:!0,gridColumnStart:!0,fontWeight:!0,lineClamp:!0,lineHeight:!0,opacity:!0,order:!0,orphans:!0,tabSize:!0,widows:!0,zIndex:!0,zoom:!0,fillOpacity:!0,floodOpacity:!0,stopOpacity:!0,strokeDasharray:!0,strokeDashoffset:!0,strokeMiterlimit:!0,strokeOpacity:!0,strokeWidth:!0},en=["Webkit","ms","Moz","O"];function tn(e,t,n){return null==t||"boolean"===typeof t||""===t?"":n||"number"!==typeof t||0===t||Gt.hasOwnProperty(e)&&Gt[e]?(""+t).trim():t+"px"}function nn(e,t){for(var n in e=e.style,t)if(t.hasOwnProperty(n)){var r=0===n.indexOf("--"),i=tn(n,t[n],r);"float"===n&&(n="cssFloat"),r?e.setProperty(n,i):e[n]=i}}Object.keys(Gt).forEach((function(e){en.forEach((function(t){t=t+e.charAt(0).toUpperCase()+e.substring(1),Gt[t]=Gt[e]}))}));var rn=i({menuitem:!0},{area:!0,base:!0,br:!0,col:!0,embed:!0,hr:!0,img:!0,input:!0,keygen:!0,link:!0,meta:!0,param:!0,source:!0,track:!0,wbr:!0});function on(e,t){if(t){if(rn[e]&&(null!=t.children||null!=t.dangerouslySetInnerHTML))throw Error(u(137,e,""));if(null!=t.dangerouslySetInnerHTML){if(null!=t.children)throw Error(u(60));if("object"!==typeof t.dangerouslySetInnerHTML||!("__html"in t.dangerouslySetInnerHTML))throw Error(u(61))}if(null!=t.style&&"object"!==typeof t.style)throw Error(u(62,""))}}function un(e,t){if(-1===e.indexOf("-"))return"string"===typeof t.is;switch(e){case"annotation-xml":case"color-profile":case"font-face":case"font-face-src":case"font-face-uri":case"font-face-format":case"font-face-name":case"missing-glyph":return!1;default:return!0}}var an=Oe;function ln(e,t){var n=Je(e=9===e.nodeType||11===e.nodeType?e:e.ownerDocument);t=E[t];for(var r=0;r<t.length;r++)ht(t[r],e,n)}function cn(){}function sn(e){if("undefined"===typeof(e=e||("undefined"!==typeof document?document:void 0)))return null;try{return e.activeElement||e.body}catch(t){return e.body}}function fn(e){for(;e&&e.firstChild;)e=e.firstChild;return e}function dn(e,t){var n,r=fn(e);for(e=0;r;){if(3===r.nodeType){if(n=e+r.textContent.length,e<=t&&n>=t)return{node:r,offset:t-e};e=n}e:{for(;r;){if(r.nextSibling){r=r.nextSibling;break e}r=r.parentNode}r=void 0}r=fn(r)}}function pn(){for(var e=window,t=sn();t instanceof e.HTMLIFrameElement;){try{var n="string"===typeof t.contentWindow.location.href}catch(r){n=!1}if(!n)break;t=sn((e=t.contentWindow).document)}return t}function hn(e){var t=e&&e.nodeName&&e.nodeName.toLowerCase();return t&&("input"===t&&("text"===e.type||"search"===e.type||"tel"===e.type||"url"===e.type||"password"===e.type)||"textarea"===t||"true"===e.contentEditable)}var mn=null,gn=null;function vn(e,t){switch(e){case"button":case"input":case"select":case"textarea":return!!t.autoFocus}return!1}function yn(e,t){return"textarea"===e||"option"===e||"noscript"===e||"string"===typeof t.children||"number"===typeof t.children||"object"===typeof t.dangerouslySetInnerHTML&&null!==t.dangerouslySetInnerHTML&&null!=t.dangerouslySetInnerHTML.__html}var bn="function"===typeof setTimeout?setTimeout:void 0,wn="function"===typeof clearTimeout?clearTimeout:void 0;function xn(e){for(;null!=e;e=e.nextSibling){var t=e.nodeType;if(1===t||3===t)break}return e}function _n(e){e=e.previousSibling;for(var t=0;e;){if(8===e.nodeType){var n=e.data;if("$"===n||"$!"===n||"$?"===n){if(0===t)return e;t--}else"/$"===n&&t++}e=e.previousSibling}return null}var kn=Math.random().toString(36).slice(2),Tn="__reactInternalInstance$"+kn,En="__reactEventHandlers$"+kn,Cn="__reactContainere$"+kn;function Sn(e){var t=e[Tn];if(t)return t;for(var n=e.parentNode;n;){if(t=n[Cn]||n[Tn]){if(n=t.alternate,null!==t.child||null!==n&&null!==n.child)for(e=_n(e);null!==e;){if(n=e[Tn])return n;e=_n(e)}return t}n=(e=n).parentNode}return null}function Mn(e){return!(e=e[Tn]||e[Cn])||5!==e.tag&&6!==e.tag&&13!==e.tag&&3!==e.tag?null:e}function Nn(e){if(5===e.tag||6===e.tag)return e.stateNode;throw Error(u(33))}function Pn(e){return e[En]||null}function An(e){do{e=e.return}while(e&&5!==e.tag);return e||null}function Rn(e,t){var n=e.stateNode;if(!n)return null;var r=h(n);if(!r)return null;n=r[t];e:switch(t){case"onClick":case"onClickCapture":case"onDoubleClick":case"onDoubleClickCapture":case"onMouseDown":case"onMouseDownCapture":case"onMouseMove":case"onMouseMoveCapture":case"onMouseUp":case"onMouseUpCapture":case"onMouseEnter":(r=!r.disabled)||(r=!("button"===(e=e.type)||"input"===e||"select"===e||"textarea"===e)),e=!r;break e;default:e=!1}if(e)return null;if(n&&"function"!==typeof n)throw Error(u(231,t,typeof n));return n}function Dn(e,t,n){(t=Rn(e,n.dispatchConfig.phasedRegistrationNames[t]))&&(n._dispatchListeners=rt(n._dispatchListeners,t),n._dispatchInstances=rt(n._dispatchInstances,e))}function On(e){if(e&&e.dispatchConfig.phasedRegistrationNames){for(var t=e._targetInst,n=[];t;)n.push(t),t=An(t);for(t=n.length;0<t--;)Dn(n[t],"captured",e);for(t=0;t<n.length;t++)Dn(n[t],"bubbled",e)}}function zn(e,t,n){e&&n&&n.dispatchConfig.registrationName&&(t=Rn(e,n.dispatchConfig.registrationName))&&(n._dispatchListeners=rt(n._dispatchListeners,t),n._dispatchInstances=rt(n._dispatchInstances,e))}function Ln(e){e&&e.dispatchConfig.registrationName&&zn(e._targetInst,null,e)}function Un(e){it(e,On)}var Fn=null,In=null,jn=null;function Hn(){if(jn)return jn;var e,t,n=In,r=n.length,i="value"in Fn?Fn.value:Fn.textContent,o=i.length;for(e=0;e<r&&n[e]===i[e];e++);var u=r-e;for(t=1;t<=u&&n[r-t]===i[o-t];t++);return jn=i.slice(e,1<t?1-t:void 0)}function Vn(){return!0}function $n(){return!1}function Wn(e,t,n,r){for(var i in this.dispatchConfig=e,this._targetInst=t,this.nativeEvent=n,e=this.constructor.Interface)e.hasOwnProperty(i)&&((t=e[i])?this[i]=t(n):"target"===i?this.target=r:this[i]=n[i]);return this.isDefaultPrevented=(null!=n.defaultPrevented?n.defaultPrevented:!1===n.returnValue)?Vn:$n,this.isPropagationStopped=$n,this}function Qn(e,t,n,r){if(this.eventPool.length){var i=this.eventPool.pop();return this.call(i,e,t,n,r),i}return new this(e,t,n,r)}function qn(e){if(!(e instanceof this))throw Error(u(279));e.destructor(),10>this.eventPool.length&&this.eventPool.push(e)}function Bn(e){e.eventPool=[],e.getPooled=Qn,e.release=qn}i(Wn.prototype,{preventDefault:function(){this.defaultPrevented=!0;var e=this.nativeEvent;e&&(e.preventDefault?e.preventDefault():"unknown"!==typeof e.returnValue&&(e.returnValue=!1),this.isDefaultPrevented=Vn)},stopPropagation:function(){var e=this.nativeEvent;e&&(e.stopPropagation?e.stopPropagation():"unknown"!==typeof e.cancelBubble&&(e.cancelBubble=!0),this.isPropagationStopped=Vn)},persist:function(){this.isPersistent=Vn},isPersistent:$n,destructor:function(){var e,t=this.constructor.Interface;for(e in t)this[e]=null;this.nativeEvent=this._targetInst=this.dispatchConfig=null,this.isPropagationStopped=this.isDefaultPrevented=$n,this._dispatchInstances=this._dispatchListeners=null}}),Wn.Interface={type:null,target:null,currentTarget:function(){return null},eventPhase:null,bubbles:null,cancelable:null,timeStamp:function(e){return e.timeStamp||Date.now()},defaultPrevented:null,isTrusted:null},Wn.extend=function(e){function t(){}function n(){return r.apply(this,arguments)}var r=this;t.prototype=r.prototype;var o=new t;return i(o,n.prototype),n.prototype=o,n.prototype.constructor=n,n.Interface=i({},r.Interface,e),n.extend=r.extend,Bn(n),n},Bn(Wn);var Yn=Wn.extend({data:null}),Xn=Wn.extend({data:null}),Kn=[9,13,27,32],Zn=S&&"CompositionEvent"in window,Jn=null;S&&"documentMode"in document&&(Jn=document.documentMode);var Gn=S&&"TextEvent"in window&&!Jn,er=S&&(!Zn||Jn&&8<Jn&&11>=Jn),tr=String.fromCharCode(32),nr={beforeInput:{phasedRegistrationNames:{bubbled:"onBeforeInput",captured:"onBeforeInputCapture"},dependencies:["compositionend","keypress","textInput","paste"]},compositionEnd:{phasedRegistrationNames:{bubbled:"onCompositionEnd",captured:"onCompositionEndCapture"},dependencies:"blur compositionend keydown keypress keyup mousedown".split(" ")},compositionStart:{phasedRegistrationNames:{bubbled:"onCompositionStart",captured:"onCompositionStartCapture"},dependencies:"blur compositionstart keydown keypress keyup mousedown".split(" ")},compositionUpdate:{phasedRegistrationNames:{bubbled:"onCompositionUpdate",captured:"onCompositionUpdateCapture"},dependencies:"blur compositionupdate keydown keypress keyup mousedown".split(" ")}},rr=!1;function ir(e,t){switch(e){case"keyup":return-1!==Kn.indexOf(t.keyCode);case"keydown":return 229!==t.keyCode;case"keypress":case"mousedown":case"blur":return!0;default:return!1}}function or(e){return"object"===typeof(e=e.detail)&&"data"in e?e.data:null}var ur=!1;var ar={eventTypes:nr,extractEvents:function(e,t,n,r){var i;if(Zn)e:{switch(e){case"compositionstart":var o=nr.compositionStart;break e;case"compositionend":o=nr.compositionEnd;break e;case"compositionupdate":o=nr.compositionUpdate;break e}o=void 0}else ur?ir(e,n)&&(o=nr.compositionEnd):"keydown"===e&&229===n.keyCode&&(o=nr.compositionStart);return o?(er&&"ko"!==n.locale&&(ur||o!==nr.compositionStart?o===nr.compositionEnd&&ur&&(i=Hn()):(In="value"in(Fn=r)?Fn.value:Fn.textContent,ur=!0)),o=Yn.getPooled(o,t,n,r),i?o.data=i:null!==(i=or(n))&&(o.data=i),Un(o),i=o):i=null,(e=Gn?function(e,t){switch(e){case"compositionend":return or(t);case"keypress":return 32!==t.which?null:(rr=!0,tr);case"textInput":return(e=t.data)===tr&&rr?null:e;default:return null}}(e,n):function(e,t){if(ur)return"compositionend"===e||!Zn&&ir(e,t)?(e=Hn(),jn=In=Fn=null,ur=!1,e):null;switch(e){case"paste":return null;case"keypress":if(!(t.ctrlKey||t.altKey||t.metaKey)||t.ctrlKey&&t.altKey){if(t.char&&1<t.char.length)return t.char;if(t.which)return String.fromCharCode(t.which)}return null;case"compositionend":return er&&"ko"!==t.locale?null:t.data;default:return null}}(e,n))?((t=Xn.getPooled(nr.beforeInput,t,n,r)).data=e,Un(t)):t=null,null===i?t:null===t?i:[i,t]}},lr={color:!0,date:!0,datetime:!0,"datetime-local":!0,email:!0,month:!0,number:!0,password:!0,range:!0,search:!0,tel:!0,text:!0,time:!0,url:!0,week:!0};function cr(e){var t=e&&e.nodeName&&e.nodeName.toLowerCase();return"input"===t?!!lr[e.type]:"textarea"===t}var sr={change:{phasedRegistrationNames:{bubbled:"onChange",captured:"onChangeCapture"},dependencies:"blur change click focus input keydown keyup selectionchange".split(" ")}};function fr(e,t,n){return(e=Wn.getPooled(sr.change,e,t,n)).type="change",R(n),Un(e),e}var dr=null,pr=null;function hr(e){at(e)}function mr(e){if(xe(Nn(e)))return e}function gr(e,t){if("change"===e)return t}var vr=!1;function yr(){dr&&(dr.detachEvent("onpropertychange",br),pr=dr=null)}function br(e){if("value"===e.propertyName&&mr(pr))if(e=fr(pr,e,lt(e)),F)at(e);else{F=!0;try{O(hr,e)}finally{F=!1,j()}}}function wr(e,t,n){"focus"===e?(yr(),pr=n,(dr=t).attachEvent("onpropertychange",br)):"blur"===e&&yr()}function xr(e){if("selectionchange"===e||"keyup"===e||"keydown"===e)return mr(pr)}function _r(e,t){if("click"===e)return mr(t)}function kr(e,t){if("input"===e||"change"===e)return mr(t)}S&&(vr=ct("input")&&(!document.documentMode||9<document.documentMode));var Tr={eventTypes:sr,_isInputEventSupported:vr,extractEvents:function(e,t,n,r){var i=t?Nn(t):window,o=i.nodeName&&i.nodeName.toLowerCase();if("select"===o||"input"===o&&"file"===i.type)var u=gr;else if(cr(i))if(vr)u=kr;else{u=xr;var a=wr}else(o=i.nodeName)&&"input"===o.toLowerCase()&&("checkbox"===i.type||"radio"===i.type)&&(u=_r);if(u&&(u=u(e,t)))return fr(u,n,r);a&&a(e,i,t),"blur"===e&&(e=i._wrapperState)&&e.controlled&&"number"===i.type&&Se(i,"number",i.value)}},Er=Wn.extend({view:null,detail:null}),Cr={Alt:"altKey",Control:"ctrlKey",Meta:"metaKey",Shift:"shiftKey"};function Sr(e){var t=this.nativeEvent;return t.getModifierState?t.getModifierState(e):!!(e=Cr[e])&&!!t[e]}function Mr(){return Sr}var Nr=0,Pr=0,Ar=!1,Rr=!1,Dr=Er.extend({screenX:null,screenY:null,clientX:null,clientY:null,pageX:null,pageY:null,ctrlKey:null,shiftKey:null,altKey:null,metaKey:null,getModifierState:Mr,button:null,buttons:null,relatedTarget:function(e){return e.relatedTarget||(e.fromElement===e.srcElement?e.toElement:e.fromElement)},movementX:function(e){if("movementX"in e)return e.movementX;var t=Nr;return Nr=e.screenX,Ar?"mousemove"===e.type?e.screenX-t:0:(Ar=!0,0)},movementY:function(e){if("movementY"in e)return e.movementY;var t=Pr;return Pr=e.screenY,Rr?"mousemove"===e.type?e.screenY-t:0:(Rr=!0,0)}}),Or=Dr.extend({pointerId:null,width:null,height:null,pressure:null,tangentialPressure:null,tiltX:null,tiltY:null,twist:null,pointerType:null,isPrimary:null}),zr={mouseEnter:{registrationName:"onMouseEnter",dependencies:["mouseout","mouseover"]},mouseLeave:{registrationName:"onMouseLeave",dependencies:["mouseout","mouseover"]},pointerEnter:{registrationName:"onPointerEnter",dependencies:["pointerout","pointerover"]},pointerLeave:{registrationName:"onPointerLeave",dependencies:["pointerout","pointerover"]}},Lr={eventTypes:zr,extractEvents:function(e,t,n,r,i){var o="mouseover"===e||"pointerover"===e,u="mouseout"===e||"pointerout"===e;if(o&&0===(32&i)&&(n.relatedTarget||n.fromElement)||!u&&!o)return null;(o=r.window===r?r:(o=r.ownerDocument)?o.defaultView||o.parentWindow:window,u)?(u=t,null!==(t=(t=n.relatedTarget||n.toElement)?Sn(t):null)&&(t!==Ge(t)||5!==t.tag&&6!==t.tag)&&(t=null)):u=null;if(u===t)return null;if("mouseout"===e||"mouseover"===e)var a=Dr,l=zr.mouseLeave,c=zr.mouseEnter,s="mouse";else"pointerout"!==e&&"pointerover"!==e||(a=Or,l=zr.pointerLeave,c=zr.pointerEnter,s="pointer");if(e=null==u?o:Nn(u),o=null==t?o:Nn(t),(l=a.getPooled(l,u,n,r)).type=s+"leave",l.target=e,l.relatedTarget=o,(n=a.getPooled(c,t,n,r)).type=s+"enter",n.target=o,n.relatedTarget=e,s=t,(r=u)&&s)e:{for(c=s,u=0,e=a=r;e;e=An(e))u++;for(e=0,t=c;t;t=An(t))e++;for(;0<u-e;)a=An(a),u--;for(;0<e-u;)c=An(c),e--;for(;u--;){if(a===c||a===c.alternate)break e;a=An(a),c=An(c)}a=null}else a=null;for(c=a,a=[];r&&r!==c&&(null===(u=r.alternate)||u!==c);)a.push(r),r=An(r);for(r=[];s&&s!==c&&(null===(u=s.alternate)||u!==c);)r.push(s),s=An(s);for(s=0;s<a.length;s++)zn(a[s],"bubbled",l);for(s=r.length;0<s--;)zn(r[s],"captured",n);return 0===(64&i)?[l]:[l,n]}};var Ur="function"===typeof Object.is?Object.is:function(e,t){return e===t&&(0!==e||1/e===1/t)||e!==e&&t!==t},Fr=Object.prototype.hasOwnProperty;function Ir(e,t){if(Ur(e,t))return!0;if("object"!==typeof e||null===e||"object"!==typeof t||null===t)return!1;var n=Object.keys(e),r=Object.keys(t);if(n.length!==r.length)return!1;for(r=0;r<n.length;r++)if(!Fr.call(t,n[r])||!Ur(e[n[r]],t[n[r]]))return!1;return!0}var jr=S&&"documentMode"in document&&11>=document.documentMode,Hr={select:{phasedRegistrationNames:{bubbled:"onSelect",captured:"onSelectCapture"},dependencies:"blur contextmenu dragend focus keydown keyup mousedown mouseup selectionchange".split(" ")}},Vr=null,$r=null,Wr=null,Qr=!1;function qr(e,t){var n=t.window===t?t.document:9===t.nodeType?t:t.ownerDocument;return Qr||null==Vr||Vr!==sn(n)?null:("selectionStart"in(n=Vr)&&hn(n)?n={start:n.selectionStart,end:n.selectionEnd}:n={anchorNode:(n=(n.ownerDocument&&n.ownerDocument.defaultView||window).getSelection()).anchorNode,anchorOffset:n.anchorOffset,focusNode:n.focusNode,focusOffset:n.focusOffset},Wr&&Ir(Wr,n)?null:(Wr=n,(e=Wn.getPooled(Hr.select,$r,e,t)).type="select",e.target=Vr,Un(e),e))}var Br={eventTypes:Hr,extractEvents:function(e,t,n,r,i,o){if(!(o=!(i=o||(r.window===r?r.document:9===r.nodeType?r:r.ownerDocument)))){e:{i=Je(i),o=E.onSelect;for(var u=0;u<o.length;u++)if(!i.has(o[u])){i=!1;break e}i=!0}o=!i}if(o)return null;switch(i=t?Nn(t):window,e){case"focus":(cr(i)||"true"===i.contentEditable)&&(Vr=i,$r=t,Wr=null);break;case"blur":Wr=$r=Vr=null;break;case"mousedown":Qr=!0;break;case"contextmenu":case"mouseup":case"dragend":return Qr=!1,qr(n,r);case"selectionchange":if(jr)break;case"keydown":case"keyup":return qr(n,r)}return null}},Yr=Wn.extend({animationName:null,elapsedTime:null,pseudoElement:null}),Xr=Wn.extend({clipboardData:function(e){return"clipboardData"in e?e.clipboardData:window.clipboardData}}),Kr=Er.extend({relatedTarget:null});function Zr(e){var t=e.keyCode;return"charCode"in e?0===(e=e.charCode)&&13===t&&(e=13):e=t,10===e&&(e=13),32<=e||13===e?e:0}var Jr={Esc:"Escape",Spacebar:" ",Left:"ArrowLeft",Up:"ArrowUp",Right:"ArrowRight",Down:"ArrowDown",Del:"Delete",Win:"OS",Menu:"ContextMenu",Apps:"ContextMenu",Scroll:"ScrollLock",MozPrintableKey:"Unidentified"},Gr={8:"Backspace",9:"Tab",12:"Clear",13:"Enter",16:"Shift",17:"Control",18:"Alt",19:"Pause",20:"CapsLock",27:"Escape",32:" ",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"ArrowLeft",38:"ArrowUp",39:"ArrowRight",40:"ArrowDown",45:"Insert",46:"Delete",112:"F1",113:"F2",114:"F3",115:"F4",116:"F5",117:"F6",118:"F7",119:"F8",120:"F9",121:"F10",122:"F11",123:"F12",144:"NumLock",145:"ScrollLock",224:"Meta"},ei=Er.extend({key:function(e){if(e.key){var t=Jr[e.key]||e.key;if("Unidentified"!==t)return t}return"keypress"===e.type?13===(e=Zr(e))?"Enter":String.fromCharCode(e):"keydown"===e.type||"keyup"===e.type?Gr[e.keyCode]||"Unidentified":""},location:null,ctrlKey:null,shiftKey:null,altKey:null,metaKey:null,repeat:null,locale:null,getModifierState:Mr,charCode:function(e){return"keypress"===e.type?Zr(e):0},keyCode:function(e){return"keydown"===e.type||"keyup"===e.type?e.keyCode:0},which:function(e){return"keypress"===e.type?Zr(e):"keydown"===e.type||"keyup"===e.type?e.keyCode:0}}),ti=Dr.extend({dataTransfer:null}),ni=Er.extend({touches:null,targetTouches:null,changedTouches:null,altKey:null,metaKey:null,ctrlKey:null,shiftKey:null,getModifierState:Mr}),ri=Wn.extend({propertyName:null,elapsedTime:null,pseudoElement:null}),ii=Dr.extend({deltaX:function(e){return"deltaX"in e?e.deltaX:"wheelDeltaX"in e?-e.wheelDeltaX:0},deltaY:function(e){return"deltaY"in e?e.deltaY:"wheelDeltaY"in e?-e.wheelDeltaY:"wheelDelta"in e?-e.wheelDelta:0},deltaZ:null,deltaMode:null}),oi={eventTypes:Ut,extractEvents:function(e,t,n,r){var i=Ft.get(e);if(!i)return null;switch(e){case"keypress":if(0===Zr(n))return null;case"keydown":case"keyup":e=ei;break;case"blur":case"focus":e=Kr;break;case"click":if(2===n.button)return null;case"auxclick":case"dblclick":case"mousedown":case"mousemove":case"mouseup":case"mouseout":case"mouseover":case"contextmenu":e=Dr;break;case"drag":case"dragend":case"dragenter":case"dragexit":case"dragleave":case"dragover":case"dragstart":case"drop":e=ti;break;case"touchcancel":case"touchend":case"touchmove":case"touchstart":e=ni;break;case qe:case Be:case Ye:e=Yr;break;case Xe:e=ri;break;case"scroll":e=Er;break;case"wheel":e=ii;break;case"copy":case"cut":case"paste":e=Xr;break;case"gotpointercapture":case"lostpointercapture":case"pointercancel":case"pointerdown":case"pointermove":case"pointerout":case"pointerover":case"pointerup":e=Or;break;default:e=Wn}return Un(t=e.getPooled(i,t,n,r)),t}};if(y)throw Error(u(101));y=Array.prototype.slice.call("ResponderEventPlugin SimpleEventPlugin EnterLeaveEventPlugin ChangeEventPlugin SelectEventPlugin BeforeInputEventPlugin".split(" ")),w(),h=Pn,m=Mn,g=Nn,C({SimpleEventPlugin:oi,EnterLeaveEventPlugin:Lr,ChangeEventPlugin:Tr,SelectEventPlugin:Br,BeforeInputEventPlugin:ar});var ui=[],ai=-1;function li(e){0>ai||(e.current=ui[ai],ui[ai]=null,ai--)}function ci(e,t){ai++,ui[ai]=e.current,e.current=t}var si={},fi={current:si},di={current:!1},pi=si;function hi(e,t){var n=e.type.contextTypes;if(!n)return si;var r=e.stateNode;if(r&&r.__reactInternalMemoizedUnmaskedChildContext===t)return r.__reactInternalMemoizedMaskedChildContext;var i,o={};for(i in n)o[i]=t[i];return r&&((e=e.stateNode).__reactInternalMemoizedUnmaskedChildContext=t,e.__reactInternalMemoizedMaskedChildContext=o),o}function mi(e){return null!==(e=e.childContextTypes)&&void 0!==e}function gi(){li(di),li(fi)}function vi(e,t,n){if(fi.current!==si)throw Error(u(168));ci(fi,t),ci(di,n)}function yi(e,t,n){var r=e.stateNode;if(e=t.childContextTypes,"function"!==typeof r.getChildContext)return n;for(var o in r=r.getChildContext())if(!(o in e))throw Error(u(108,ge(t)||"Unknown",o));return i({},n,{},r)}function bi(e){return e=(e=e.stateNode)&&e.__reactInternalMemoizedMergedChildContext||si,pi=fi.current,ci(fi,e),ci(di,di.current),!0}function wi(e,t,n){var r=e.stateNode;if(!r)throw Error(u(169));n?(e=yi(e,t,pi),r.__reactInternalMemoizedMergedChildContext=e,li(di),li(fi),ci(fi,e)):li(di),ci(di,n)}var xi=o.unstable_runWithPriority,_i=o.unstable_scheduleCallback,ki=o.unstable_cancelCallback,Ti=o.unstable_requestPaint,Ei=o.unstable_now,Ci=o.unstable_getCurrentPriorityLevel,Si=o.unstable_ImmediatePriority,Mi=o.unstable_UserBlockingPriority,Ni=o.unstable_NormalPriority,Pi=o.unstable_LowPriority,Ai=o.unstable_IdlePriority,Ri={},Di=o.unstable_shouldYield,Oi=void 0!==Ti?Ti:function(){},zi=null,Li=null,Ui=!1,Fi=Ei(),Ii=1e4>Fi?Ei:function(){return Ei()-Fi};function ji(){switch(Ci()){case Si:return 99;case Mi:return 98;case Ni:return 97;case Pi:return 96;case Ai:return 95;default:throw Error(u(332))}}function Hi(e){switch(e){case 99:return Si;case 98:return Mi;case 97:return Ni;case 96:return Pi;case 95:return Ai;default:throw Error(u(332))}}function Vi(e,t){return e=Hi(e),xi(e,t)}function $i(e,t,n){return e=Hi(e),_i(e,t,n)}function Wi(e){return null===zi?(zi=[e],Li=_i(Si,qi)):zi.push(e),Ri}function Qi(){if(null!==Li){var e=Li;Li=null,ki(e)}qi()}function qi(){if(!Ui&&null!==zi){Ui=!0;var e=0;try{var t=zi;Vi(99,(function(){for(;e<t.length;e++){var n=t[e];do{n=n(!0)}while(null!==n)}})),zi=null}catch(n){throw null!==zi&&(zi=zi.slice(e+1)),_i(Si,Qi),n}finally{Ui=!1}}}function Bi(e,t,n){return 1073741821-(1+((1073741821-e+t/10)/(n/=10)|0))*n}function Yi(e,t){if(e&&e.defaultProps)for(var n in t=i({},t),e=e.defaultProps)void 0===t[n]&&(t[n]=e[n]);return t}var Xi={current:null},Ki=null,Zi=null,Ji=null;function Gi(){Ji=Zi=Ki=null}function eo(e){var t=Xi.current;li(Xi),e.type._context._currentValue=t}function to(e,t){for(;null!==e;){var n=e.alternate;if(e.childExpirationTime<t)e.childExpirationTime=t,null!==n&&n.childExpirationTime<t&&(n.childExpirationTime=t);else{if(!(null!==n&&n.childExpirationTime<t))break;n.childExpirationTime=t}e=e.return}}function no(e,t){Ki=e,Ji=Zi=null,null!==(e=e.dependencies)&&null!==e.firstContext&&(e.expirationTime>=t&&(Pu=!0),e.firstContext=null)}function ro(e,t){if(Ji!==e&&!1!==t&&0!==t)if("number"===typeof t&&1073741823!==t||(Ji=e,t=1073741823),t={context:e,observedBits:t,next:null},null===Zi){if(null===Ki)throw Error(u(308));Zi=t,Ki.dependencies={expirationTime:0,firstContext:t,responders:null}}else Zi=Zi.next=t;return e._currentValue}var io=!1;function oo(e){e.updateQueue={baseState:e.memoizedState,baseQueue:null,shared:{pending:null},effects:null}}function uo(e,t){e=e.updateQueue,t.updateQueue===e&&(t.updateQueue={baseState:e.baseState,baseQueue:e.baseQueue,shared:e.shared,effects:e.effects})}function ao(e,t){return(e={expirationTime:e,suspenseConfig:t,tag:0,payload:null,callback:null,next:null}).next=e}function lo(e,t){if(null!==(e=e.updateQueue)){var n=(e=e.shared).pending;null===n?t.next=t:(t.next=n.next,n.next=t),e.pending=t}}function co(e,t){var n=e.alternate;null!==n&&uo(n,e),null===(n=(e=e.updateQueue).baseQueue)?(e.baseQueue=t.next=t,t.next=t):(t.next=n.next,n.next=t)}function so(e,t,n,r){var o=e.updateQueue;io=!1;var u=o.baseQueue,a=o.shared.pending;if(null!==a){if(null!==u){var l=u.next;u.next=a.next,a.next=l}u=a,o.shared.pending=null,null!==(l=e.alternate)&&(null!==(l=l.updateQueue)&&(l.baseQueue=a))}if(null!==u){l=u.next;var c=o.baseState,s=0,f=null,d=null,p=null;if(null!==l)for(var h=l;;){if((a=h.expirationTime)<r){var m={expirationTime:h.expirationTime,suspenseConfig:h.suspenseConfig,tag:h.tag,payload:h.payload,callback:h.callback,next:null};null===p?(d=p=m,f=c):p=p.next=m,a>s&&(s=a)}else{null!==p&&(p=p.next={expirationTime:1073741823,suspenseConfig:h.suspenseConfig,tag:h.tag,payload:h.payload,callback:h.callback,next:null}),ol(a,h.suspenseConfig);e:{var g=e,v=h;switch(a=t,m=n,v.tag){case 1:if("function"===typeof(g=v.payload)){c=g.call(m,c,a);break e}c=g;break e;case 3:g.effectTag=-4097&g.effectTag|64;case 0:if(null===(a="function"===typeof(g=v.payload)?g.call(m,c,a):g)||void 0===a)break e;c=i({},c,a);break e;case 2:io=!0}}null!==h.callback&&(e.effectTag|=32,null===(a=o.effects)?o.effects=[h]:a.push(h))}if(null===(h=h.next)||h===l){if(null===(a=o.shared.pending))break;h=u.next=a.next,a.next=l,o.baseQueue=u=a,o.shared.pending=null}}null===p?f=c:p.next=d,o.baseState=f,o.baseQueue=p,ul(s),e.expirationTime=s,e.memoizedState=c}}function fo(e,t,n){if(e=t.effects,t.effects=null,null!==e)for(t=0;t<e.length;t++){var r=e[t],i=r.callback;if(null!==i){if(r.callback=null,r=i,i=n,"function"!==typeof r)throw Error(u(191,r));r.call(i)}}}var po=K.ReactCurrentBatchConfig,ho=(new r.Component).refs;function mo(e,t,n,r){n=null===(n=n(r,t=e.memoizedState))||void 0===n?t:i({},t,n),e.memoizedState=n,0===e.expirationTime&&(e.updateQueue.baseState=n)}var go={isMounted:function(e){return!!(e=e._reactInternalFiber)&&Ge(e)===e},enqueueSetState:function(e,t,n){e=e._reactInternalFiber;var r=qa(),i=po.suspense;(i=ao(r=Ba(r,e,i),i)).payload=t,void 0!==n&&null!==n&&(i.callback=n),lo(e,i),Ya(e,r)},enqueueReplaceState:function(e,t,n){e=e._reactInternalFiber;var r=qa(),i=po.suspense;(i=ao(r=Ba(r,e,i),i)).tag=1,i.payload=t,void 0!==n&&null!==n&&(i.callback=n),lo(e,i),Ya(e,r)},enqueueForceUpdate:function(e,t){e=e._reactInternalFiber;var n=qa(),r=po.suspense;(r=ao(n=Ba(n,e,r),r)).tag=2,void 0!==t&&null!==t&&(r.callback=t),lo(e,r),Ya(e,n)}};function vo(e,t,n,r,i,o,u){return"function"===typeof(e=e.stateNode).shouldComponentUpdate?e.shouldComponentUpdate(r,o,u):!t.prototype||!t.prototype.isPureReactComponent||(!Ir(n,r)||!Ir(i,o))}function yo(e,t,n){var r=!1,i=si,o=t.contextType;return"object"===typeof o&&null!==o?o=ro(o):(i=mi(t)?pi:fi.current,o=(r=null!==(r=t.contextTypes)&&void 0!==r)?hi(e,i):si),t=new t(n,o),e.memoizedState=null!==t.state&&void 0!==t.state?t.state:null,t.updater=go,e.stateNode=t,t._reactInternalFiber=e,r&&((e=e.stateNode).__reactInternalMemoizedUnmaskedChildContext=i,e.__reactInternalMemoizedMaskedChildContext=o),t}function bo(e,t,n,r){e=t.state,"function"===typeof t.componentWillReceiveProps&&t.componentWillReceiveProps(n,r),"function"===typeof t.UNSAFE_componentWillReceiveProps&&t.UNSAFE_componentWillReceiveProps(n,r),t.state!==e&&go.enqueueReplaceState(t,t.state,null)}function wo(e,t,n,r){var i=e.stateNode;i.props=n,i.state=e.memoizedState,i.refs=ho,oo(e);var o=t.contextType;"object"===typeof o&&null!==o?i.context=ro(o):(o=mi(t)?pi:fi.current,i.context=hi(e,o)),so(e,n,i,r),i.state=e.memoizedState,"function"===typeof(o=t.getDerivedStateFromProps)&&(mo(e,t,o,n),i.state=e.memoizedState),"function"===typeof t.getDerivedStateFromProps||"function"===typeof i.getSnapshotBeforeUpdate||"function"!==typeof i.UNSAFE_componentWillMount&&"function"!==typeof i.componentWillMount||(t=i.state,"function"===typeof i.componentWillMount&&i.componentWillMount(),"function"===typeof i.UNSAFE_componentWillMount&&i.UNSAFE_componentWillMount(),t!==i.state&&go.enqueueReplaceState(i,i.state,null),so(e,n,i,r),i.state=e.memoizedState),"function"===typeof i.componentDidMount&&(e.effectTag|=4)}var xo=Array.isArray;function _o(e,t,n){if(null!==(e=n.ref)&&"function"!==typeof e&&"object"!==typeof e){if(n._owner){if(n=n._owner){if(1!==n.tag)throw Error(u(309));var r=n.stateNode}if(!r)throw Error(u(147,e));var i=""+e;return null!==t&&null!==t.ref&&"function"===typeof t.ref&&t.ref._stringRef===i?t.ref:((t=function(e){var t=r.refs;t===ho&&(t=r.refs={}),null===e?delete t[i]:t[i]=e})._stringRef=i,t)}if("string"!==typeof e)throw Error(u(284));if(!n._owner)throw Error(u(290,e))}return e}function ko(e,t){if("textarea"!==e.type)throw Error(u(31,"[object Object]"===Object.prototype.toString.call(t)?"object with keys {"+Object.keys(t).join(", ")+"}":t,""))}function To(e){function t(t,n){if(e){var r=t.lastEffect;null!==r?(r.nextEffect=n,t.lastEffect=n):t.firstEffect=t.lastEffect=n,n.nextEffect=null,n.effectTag=8}}function n(n,r){if(!e)return null;for(;null!==r;)t(n,r),r=r.sibling;return null}function r(e,t){for(e=new Map;null!==t;)null!==t.key?e.set(t.key,t):e.set(t.index,t),t=t.sibling;return e}function i(e,t){return(e=Cl(e,t)).index=0,e.sibling=null,e}function o(t,n,r){return t.index=r,e?null!==(r=t.alternate)?(r=r.index)<n?(t.effectTag=2,n):r:(t.effectTag=2,n):n}function a(t){return e&&null===t.alternate&&(t.effectTag=2),t}function l(e,t,n,r){return null===t||6!==t.tag?((t=Nl(n,e.mode,r)).return=e,t):((t=i(t,n)).return=e,t)}function c(e,t,n,r){return null!==t&&t.elementType===n.type?((r=i(t,n.props)).ref=_o(e,t,n),r.return=e,r):((r=Sl(n.type,n.key,n.props,null,e.mode,r)).ref=_o(e,t,n),r.return=e,r)}function s(e,t,n,r){return null===t||4!==t.tag||t.stateNode.containerInfo!==n.containerInfo||t.stateNode.implementation!==n.implementation?((t=Pl(n,e.mode,r)).return=e,t):((t=i(t,n.children||[])).return=e,t)}function f(e,t,n,r,o){return null===t||7!==t.tag?((t=Ml(n,e.mode,r,o)).return=e,t):((t=i(t,n)).return=e,t)}function d(e,t,n){if("string"===typeof t||"number"===typeof t)return(t=Nl(""+t,e.mode,n)).return=e,t;if("object"===typeof t&&null!==t){switch(t.$$typeof){case ee:return(n=Sl(t.type,t.key,t.props,null,e.mode,n)).ref=_o(e,null,t),n.return=e,n;case te:return(t=Pl(t,e.mode,n)).return=e,t}if(xo(t)||me(t))return(t=Ml(t,e.mode,n,null)).return=e,t;ko(e,t)}return null}function p(e,t,n,r){var i=null!==t?t.key:null;if("string"===typeof n||"number"===typeof n)return null!==i?null:l(e,t,""+n,r);if("object"===typeof n&&null!==n){switch(n.$$typeof){case ee:return n.key===i?n.type===ne?f(e,t,n.props.children,r,i):c(e,t,n,r):null;case te:return n.key===i?s(e,t,n,r):null}if(xo(n)||me(n))return null!==i?null:f(e,t,n,r,null);ko(e,n)}return null}function h(e,t,n,r,i){if("string"===typeof r||"number"===typeof r)return l(t,e=e.get(n)||null,""+r,i);if("object"===typeof r&&null!==r){switch(r.$$typeof){case ee:return e=e.get(null===r.key?n:r.key)||null,r.type===ne?f(t,e,r.props.children,i,r.key):c(t,e,r,i);case te:return s(t,e=e.get(null===r.key?n:r.key)||null,r,i)}if(xo(r)||me(r))return f(t,e=e.get(n)||null,r,i,null);ko(t,r)}return null}function m(i,u,a,l){for(var c=null,s=null,f=u,m=u=0,g=null;null!==f&&m<a.length;m++){f.index>m?(g=f,f=null):g=f.sibling;var v=p(i,f,a[m],l);if(null===v){null===f&&(f=g);break}e&&f&&null===v.alternate&&t(i,f),u=o(v,u,m),null===s?c=v:s.sibling=v,s=v,f=g}if(m===a.length)return n(i,f),c;if(null===f){for(;m<a.length;m++)null!==(f=d(i,a[m],l))&&(u=o(f,u,m),null===s?c=f:s.sibling=f,s=f);return c}for(f=r(i,f);m<a.length;m++)null!==(g=h(f,i,m,a[m],l))&&(e&&null!==g.alternate&&f.delete(null===g.key?m:g.key),u=o(g,u,m),null===s?c=g:s.sibling=g,s=g);return e&&f.forEach((function(e){return t(i,e)})),c}function g(i,a,l,c){var s=me(l);if("function"!==typeof s)throw Error(u(150));if(null==(l=s.call(l)))throw Error(u(151));for(var f=s=null,m=a,g=a=0,v=null,y=l.next();null!==m&&!y.done;g++,y=l.next()){m.index>g?(v=m,m=null):v=m.sibling;var b=p(i,m,y.value,c);if(null===b){null===m&&(m=v);break}e&&m&&null===b.alternate&&t(i,m),a=o(b,a,g),null===f?s=b:f.sibling=b,f=b,m=v}if(y.done)return n(i,m),s;if(null===m){for(;!y.done;g++,y=l.next())null!==(y=d(i,y.value,c))&&(a=o(y,a,g),null===f?s=y:f.sibling=y,f=y);return s}for(m=r(i,m);!y.done;g++,y=l.next())null!==(y=h(m,i,g,y.value,c))&&(e&&null!==y.alternate&&m.delete(null===y.key?g:y.key),a=o(y,a,g),null===f?s=y:f.sibling=y,f=y);return e&&m.forEach((function(e){return t(i,e)})),s}return function(e,r,o,l){var c="object"===typeof o&&null!==o&&o.type===ne&&null===o.key;c&&(o=o.props.children);var s="object"===typeof o&&null!==o;if(s)switch(o.$$typeof){case ee:e:{for(s=o.key,c=r;null!==c;){if(c.key===s){switch(c.tag){case 7:if(o.type===ne){n(e,c.sibling),(r=i(c,o.props.children)).return=e,e=r;break e}break;default:if(c.elementType===o.type){n(e,c.sibling),(r=i(c,o.props)).ref=_o(e,c,o),r.return=e,e=r;break e}}n(e,c);break}t(e,c),c=c.sibling}o.type===ne?((r=Ml(o.props.children,e.mode,l,o.key)).return=e,e=r):((l=Sl(o.type,o.key,o.props,null,e.mode,l)).ref=_o(e,r,o),l.return=e,e=l)}return a(e);case te:e:{for(c=o.key;null!==r;){if(r.key===c){if(4===r.tag&&r.stateNode.containerInfo===o.containerInfo&&r.stateNode.implementation===o.implementation){n(e,r.sibling),(r=i(r,o.children||[])).return=e,e=r;break e}n(e,r);break}t(e,r),r=r.sibling}(r=Pl(o,e.mode,l)).return=e,e=r}return a(e)}if("string"===typeof o||"number"===typeof o)return o=""+o,null!==r&&6===r.tag?(n(e,r.sibling),(r=i(r,o)).return=e,e=r):(n(e,r),(r=Nl(o,e.mode,l)).return=e,e=r),a(e);if(xo(o))return m(e,r,o,l);if(me(o))return g(e,r,o,l);if(s&&ko(e,o),"undefined"===typeof o&&!c)switch(e.tag){case 1:case 0:throw e=e.type,Error(u(152,e.displayName||e.name||"Component"))}return n(e,r)}}var Eo=To(!0),Co=To(!1),So={},Mo={current:So},No={current:So},Po={current:So};function Ao(e){if(e===So)throw Error(u(174));return e}function Ro(e,t){switch(ci(Po,t),ci(No,e),ci(Mo,So),e=t.nodeType){case 9:case 11:t=(t=t.documentElement)?t.namespaceURI:Ue(null,"");break;default:t=Ue(t=(e=8===e?t.parentNode:t).namespaceURI||null,e=e.tagName)}li(Mo),ci(Mo,t)}function Do(){li(Mo),li(No),li(Po)}function Oo(e){Ao(Po.current);var t=Ao(Mo.current),n=Ue(t,e.type);t!==n&&(ci(No,e),ci(Mo,n))}function zo(e){No.current===e&&(li(Mo),li(No))}var Lo={current:0};function Uo(e){for(var t=e;null!==t;){if(13===t.tag){var n=t.memoizedState;if(null!==n&&(null===(n=n.dehydrated)||"$?"===n.data||"$!"===n.data))return t}else if(19===t.tag&&void 0!==t.memoizedProps.revealOrder){if(0!==(64&t.effectTag))return t}else if(null!==t.child){t.child.return=t,t=t.child;continue}if(t===e)break;for(;null===t.sibling;){if(null===t.return||t.return===e)return null;t=t.return}t.sibling.return=t.return,t=t.sibling}return null}function Fo(e,t){return{responder:e,props:t}}var Io=K.ReactCurrentDispatcher,jo=K.ReactCurrentBatchConfig,Ho=0,Vo=null,$o=null,Wo=null,Qo=!1;function qo(){throw Error(u(321))}function Bo(e,t){if(null===t)return!1;for(var n=0;n<t.length&&n<e.length;n++)if(!Ur(e[n],t[n]))return!1;return!0}function Yo(e,t,n,r,i,o){if(Ho=o,Vo=t,t.memoizedState=null,t.updateQueue=null,t.expirationTime=0,Io.current=null===e||null===e.memoizedState?vu:yu,e=n(r,i),t.expirationTime===Ho){o=0;do{if(t.expirationTime=0,!(25>o))throw Error(u(301));o+=1,Wo=$o=null,t.updateQueue=null,Io.current=bu,e=n(r,i)}while(t.expirationTime===Ho)}if(Io.current=gu,t=null!==$o&&null!==$o.next,Ho=0,Wo=$o=Vo=null,Qo=!1,t)throw Error(u(300));return e}function Xo(){var e={memoizedState:null,baseState:null,baseQueue:null,queue:null,next:null};return null===Wo?Vo.memoizedState=Wo=e:Wo=Wo.next=e,Wo}function Ko(){if(null===$o){var e=Vo.alternate;e=null!==e?e.memoizedState:null}else e=$o.next;var t=null===Wo?Vo.memoizedState:Wo.next;if(null!==t)Wo=t,$o=e;else{if(null===e)throw Error(u(310));e={memoizedState:($o=e).memoizedState,baseState:$o.baseState,baseQueue:$o.baseQueue,queue:$o.queue,next:null},null===Wo?Vo.memoizedState=Wo=e:Wo=Wo.next=e}return Wo}function Zo(e,t){return"function"===typeof t?t(e):t}function Jo(e){var t=Ko(),n=t.queue;if(null===n)throw Error(u(311));n.lastRenderedReducer=e;var r=$o,i=r.baseQueue,o=n.pending;if(null!==o){if(null!==i){var a=i.next;i.next=o.next,o.next=a}r.baseQueue=i=o,n.pending=null}if(null!==i){i=i.next,r=r.baseState;var l=a=o=null,c=i;do{var s=c.expirationTime;if(s<Ho){var f={expirationTime:c.expirationTime,suspenseConfig:c.suspenseConfig,action:c.action,eagerReducer:c.eagerReducer,eagerState:c.eagerState,next:null};null===l?(a=l=f,o=r):l=l.next=f,s>Vo.expirationTime&&(Vo.expirationTime=s,ul(s))}else null!==l&&(l=l.next={expirationTime:1073741823,suspenseConfig:c.suspenseConfig,action:c.action,eagerReducer:c.eagerReducer,eagerState:c.eagerState,next:null}),ol(s,c.suspenseConfig),r=c.eagerReducer===e?c.eagerState:e(r,c.action);c=c.next}while(null!==c&&c!==i);null===l?o=r:l.next=a,Ur(r,t.memoizedState)||(Pu=!0),t.memoizedState=r,t.baseState=o,t.baseQueue=l,n.lastRenderedState=r}return[t.memoizedState,n.dispatch]}function Go(e){var t=Ko(),n=t.queue;if(null===n)throw Error(u(311));n.lastRenderedReducer=e;var r=n.dispatch,i=n.pending,o=t.memoizedState;if(null!==i){n.pending=null;var a=i=i.next;do{o=e(o,a.action),a=a.next}while(a!==i);Ur(o,t.memoizedState)||(Pu=!0),t.memoizedState=o,null===t.baseQueue&&(t.baseState=o),n.lastRenderedState=o}return[o,r]}function eu(e){var t=Xo();return"function"===typeof e&&(e=e()),t.memoizedState=t.baseState=e,e=(e=t.queue={pending:null,dispatch:null,lastRenderedReducer:Zo,lastRenderedState:e}).dispatch=mu.bind(null,Vo,e),[t.memoizedState,e]}function tu(e,t,n,r){return e={tag:e,create:t,destroy:n,deps:r,next:null},null===(t=Vo.updateQueue)?(t={lastEffect:null},Vo.updateQueue=t,t.lastEffect=e.next=e):null===(n=t.lastEffect)?t.lastEffect=e.next=e:(r=n.next,n.next=e,e.next=r,t.lastEffect=e),e}function nu(){return Ko().memoizedState}function ru(e,t,n,r){var i=Xo();Vo.effectTag|=e,i.memoizedState=tu(1|t,n,void 0,void 0===r?null:r)}function iu(e,t,n,r){var i=Ko();r=void 0===r?null:r;var o=void 0;if(null!==$o){var u=$o.memoizedState;if(o=u.destroy,null!==r&&Bo(r,u.deps))return void tu(t,n,o,r)}Vo.effectTag|=e,i.memoizedState=tu(1|t,n,o,r)}function ou(e,t){return ru(516,4,e,t)}function uu(e,t){return iu(516,4,e,t)}function au(e,t){return iu(4,2,e,t)}function lu(e,t){return"function"===typeof t?(e=e(),t(e),function(){t(null)}):null!==t&&void 0!==t?(e=e(),t.current=e,function(){t.current=null}):void 0}function cu(e,t,n){return n=null!==n&&void 0!==n?n.concat([e]):null,iu(4,2,lu.bind(null,t,e),n)}function su(){}function fu(e,t){return Xo().memoizedState=[e,void 0===t?null:t],e}function du(e,t){var n=Ko();t=void 0===t?null:t;var r=n.memoizedState;return null!==r&&null!==t&&Bo(t,r[1])?r[0]:(n.memoizedState=[e,t],e)}function pu(e,t){var n=Ko();t=void 0===t?null:t;var r=n.memoizedState;return null!==r&&null!==t&&Bo(t,r[1])?r[0]:(e=e(),n.memoizedState=[e,t],e)}function hu(e,t,n){var r=ji();Vi(98>r?98:r,(function(){e(!0)})),Vi(97<r?97:r,(function(){var r=jo.suspense;jo.suspense=void 0===t?null:t;try{e(!1),n()}finally{jo.suspense=r}}))}function mu(e,t,n){var r=qa(),i=po.suspense;i={expirationTime:r=Ba(r,e,i),suspenseConfig:i,action:n,eagerReducer:null,eagerState:null,next:null};var o=t.pending;if(null===o?i.next=i:(i.next=o.next,o.next=i),t.pending=i,o=e.alternate,e===Vo||null!==o&&o===Vo)Qo=!0,i.expirationTime=Ho,Vo.expirationTime=Ho;else{if(0===e.expirationTime&&(null===o||0===o.expirationTime)&&null!==(o=t.lastRenderedReducer))try{var u=t.lastRenderedState,a=o(u,n);if(i.eagerReducer=o,i.eagerState=a,Ur(a,u))return}catch(l){}Ya(e,r)}}var gu={readContext:ro,useCallback:qo,useContext:qo,useEffect:qo,useImperativeHandle:qo,useLayoutEffect:qo,useMemo:qo,useReducer:qo,useRef:qo,useState:qo,useDebugValue:qo,useResponder:qo,useDeferredValue:qo,useTransition:qo},vu={readContext:ro,useCallback:fu,useContext:ro,useEffect:ou,useImperativeHandle:function(e,t,n){return n=null!==n&&void 0!==n?n.concat([e]):null,ru(4,2,lu.bind(null,t,e),n)},useLayoutEffect:function(e,t){return ru(4,2,e,t)},useMemo:function(e,t){var n=Xo();return t=void 0===t?null:t,e=e(),n.memoizedState=[e,t],e},useReducer:function(e,t,n){var r=Xo();return t=void 0!==n?n(t):t,r.memoizedState=r.baseState=t,e=(e=r.queue={pending:null,dispatch:null,lastRenderedReducer:e,lastRenderedState:t}).dispatch=mu.bind(null,Vo,e),[r.memoizedState,e]},useRef:function(e){return e={current:e},Xo().memoizedState=e},useState:eu,useDebugValue:su,useResponder:Fo,useDeferredValue:function(e,t){var n=eu(e),r=n[0],i=n[1];return ou((function(){var n=jo.suspense;jo.suspense=void 0===t?null:t;try{i(e)}finally{jo.suspense=n}}),[e,t]),r},useTransition:function(e){var t=eu(!1),n=t[0];return t=t[1],[fu(hu.bind(null,t,e),[t,e]),n]}},yu={readContext:ro,useCallback:du,useContext:ro,useEffect:uu,useImperativeHandle:cu,useLayoutEffect:au,useMemo:pu,useReducer:Jo,useRef:nu,useState:function(){return Jo(Zo)},useDebugValue:su,useResponder:Fo,useDeferredValue:function(e,t){var n=Jo(Zo),r=n[0],i=n[1];return uu((function(){var n=jo.suspense;jo.suspense=void 0===t?null:t;try{i(e)}finally{jo.suspense=n}}),[e,t]),r},useTransition:function(e){var t=Jo(Zo),n=t[0];return t=t[1],[du(hu.bind(null,t,e),[t,e]),n]}},bu={readContext:ro,useCallback:du,useContext:ro,useEffect:uu,useImperativeHandle:cu,useLayoutEffect:au,useMemo:pu,useReducer:Go,useRef:nu,useState:function(){return Go(Zo)},useDebugValue:su,useResponder:Fo,useDeferredValue:function(e,t){var n=Go(Zo),r=n[0],i=n[1];return uu((function(){var n=jo.suspense;jo.suspense=void 0===t?null:t;try{i(e)}finally{jo.suspense=n}}),[e,t]),r},useTransition:function(e){var t=Go(Zo),n=t[0];return t=t[1],[du(hu.bind(null,t,e),[t,e]),n]}},wu=null,xu=null,_u=!1;function ku(e,t){var n=Tl(5,null,null,0);n.elementType="DELETED",n.type="DELETED",n.stateNode=t,n.return=e,n.effectTag=8,null!==e.lastEffect?(e.lastEffect.nextEffect=n,e.lastEffect=n):e.firstEffect=e.lastEffect=n}function Tu(e,t){switch(e.tag){case 5:var n=e.type;return null!==(t=1!==t.nodeType||n.toLowerCase()!==t.nodeName.toLowerCase()?null:t)&&(e.stateNode=t,!0);case 6:return null!==(t=""===e.pendingProps||3!==t.nodeType?null:t)&&(e.stateNode=t,!0);case 13:default:return!1}}function Eu(e){if(_u){var t=xu;if(t){var n=t;if(!Tu(e,t)){if(!(t=xn(n.nextSibling))||!Tu(e,t))return e.effectTag=-1025&e.effectTag|2,_u=!1,void(wu=e);ku(wu,n)}wu=e,xu=xn(t.firstChild)}else e.effectTag=-1025&e.effectTag|2,_u=!1,wu=e}}function Cu(e){for(e=e.return;null!==e&&5!==e.tag&&3!==e.tag&&13!==e.tag;)e=e.return;wu=e}function Su(e){if(e!==wu)return!1;if(!_u)return Cu(e),_u=!0,!1;var t=e.type;if(5!==e.tag||"head"!==t&&"body"!==t&&!yn(t,e.memoizedProps))for(t=xu;t;)ku(e,t),t=xn(t.nextSibling);if(Cu(e),13===e.tag){if(!(e=null!==(e=e.memoizedState)?e.dehydrated:null))throw Error(u(317));e:{for(e=e.nextSibling,t=0;e;){if(8===e.nodeType){var n=e.data;if("/$"===n){if(0===t){xu=xn(e.nextSibling);break e}t--}else"$"!==n&&"$!"!==n&&"$?"!==n||t++}e=e.nextSibling}xu=null}}else xu=wu?xn(e.stateNode.nextSibling):null;return!0}function Mu(){xu=wu=null,_u=!1}var Nu=K.ReactCurrentOwner,Pu=!1;function Au(e,t,n,r){t.child=null===e?Co(t,null,n,r):Eo(t,e.child,n,r)}function Ru(e,t,n,r,i){n=n.render;var o=t.ref;return no(t,i),r=Yo(e,t,n,r,o,i),null===e||Pu?(t.effectTag|=1,Au(e,t,r,i),t.child):(t.updateQueue=e.updateQueue,t.effectTag&=-517,e.expirationTime<=i&&(e.expirationTime=0),Yu(e,t,i))}function Du(e,t,n,r,i,o){if(null===e){var u=n.type;return"function"!==typeof u||El(u)||void 0!==u.defaultProps||null!==n.compare||void 0!==n.defaultProps?((e=Sl(n.type,null,r,null,t.mode,o)).ref=t.ref,e.return=t,t.child=e):(t.tag=15,t.type=u,Ou(e,t,u,r,i,o))}return u=e.child,i<o&&(i=u.memoizedProps,(n=null!==(n=n.compare)?n:Ir)(i,r)&&e.ref===t.ref)?Yu(e,t,o):(t.effectTag|=1,(e=Cl(u,r)).ref=t.ref,e.return=t,t.child=e)}function Ou(e,t,n,r,i,o){return null!==e&&Ir(e.memoizedProps,r)&&e.ref===t.ref&&(Pu=!1,i<o)?(t.expirationTime=e.expirationTime,Yu(e,t,o)):Lu(e,t,n,r,o)}function zu(e,t){var n=t.ref;(null===e&&null!==n||null!==e&&e.ref!==n)&&(t.effectTag|=128)}function Lu(e,t,n,r,i){var o=mi(n)?pi:fi.current;return o=hi(t,o),no(t,i),n=Yo(e,t,n,r,o,i),null===e||Pu?(t.effectTag|=1,Au(e,t,n,i),t.child):(t.updateQueue=e.updateQueue,t.effectTag&=-517,e.expirationTime<=i&&(e.expirationTime=0),Yu(e,t,i))}function Uu(e,t,n,r,i){if(mi(n)){var o=!0;bi(t)}else o=!1;if(no(t,i),null===t.stateNode)null!==e&&(e.alternate=null,t.alternate=null,t.effectTag|=2),yo(t,n,r),wo(t,n,r,i),r=!0;else if(null===e){var u=t.stateNode,a=t.memoizedProps;u.props=a;var l=u.context,c=n.contextType;"object"===typeof c&&null!==c?c=ro(c):c=hi(t,c=mi(n)?pi:fi.current);var s=n.getDerivedStateFromProps,f="function"===typeof s||"function"===typeof u.getSnapshotBeforeUpdate;f||"function"!==typeof u.UNSAFE_componentWillReceiveProps&&"function"!==typeof u.componentWillReceiveProps||(a!==r||l!==c)&&bo(t,u,r,c),io=!1;var d=t.memoizedState;u.state=d,so(t,r,u,i),l=t.memoizedState,a!==r||d!==l||di.current||io?("function"===typeof s&&(mo(t,n,s,r),l=t.memoizedState),(a=io||vo(t,n,a,r,d,l,c))?(f||"function"!==typeof u.UNSAFE_componentWillMount&&"function"!==typeof u.componentWillMount||("function"===typeof u.componentWillMount&&u.componentWillMount(),"function"===typeof u.UNSAFE_componentWillMount&&u.UNSAFE_componentWillMount()),"function"===typeof u.componentDidMount&&(t.effectTag|=4)):("function"===typeof u.componentDidMount&&(t.effectTag|=4),t.memoizedProps=r,t.memoizedState=l),u.props=r,u.state=l,u.context=c,r=a):("function"===typeof u.componentDidMount&&(t.effectTag|=4),r=!1)}else u=t.stateNode,uo(e,t),a=t.memoizedProps,u.props=t.type===t.elementType?a:Yi(t.type,a),l=u.context,"object"===typeof(c=n.contextType)&&null!==c?c=ro(c):c=hi(t,c=mi(n)?pi:fi.current),(f="function"===typeof(s=n.getDerivedStateFromProps)||"function"===typeof u.getSnapshotBeforeUpdate)||"function"!==typeof u.UNSAFE_componentWillReceiveProps&&"function"!==typeof u.componentWillReceiveProps||(a!==r||l!==c)&&bo(t,u,r,c),io=!1,l=t.memoizedState,u.state=l,so(t,r,u,i),d=t.memoizedState,a!==r||l!==d||di.current||io?("function"===typeof s&&(mo(t,n,s,r),d=t.memoizedState),(s=io||vo(t,n,a,r,l,d,c))?(f||"function"!==typeof u.UNSAFE_componentWillUpdate&&"function"!==typeof u.componentWillUpdate||("function"===typeof u.componentWillUpdate&&u.componentWillUpdate(r,d,c),"function"===typeof u.UNSAFE_componentWillUpdate&&u.UNSAFE_componentWillUpdate(r,d,c)),"function"===typeof u.componentDidUpdate&&(t.effectTag|=4),"function"===typeof u.getSnapshotBeforeUpdate&&(t.effectTag|=256)):("function"!==typeof u.componentDidUpdate||a===e.memoizedProps&&l===e.memoizedState||(t.effectTag|=4),"function"!==typeof u.getSnapshotBeforeUpdate||a===e.memoizedProps&&l===e.memoizedState||(t.effectTag|=256),t.memoizedProps=r,t.memoizedState=d),u.props=r,u.state=d,u.context=c,r=s):("function"!==typeof u.componentDidUpdate||a===e.memoizedProps&&l===e.memoizedState||(t.effectTag|=4),"function"!==typeof u.getSnapshotBeforeUpdate||a===e.memoizedProps&&l===e.memoizedState||(t.effectTag|=256),r=!1);return Fu(e,t,n,r,o,i)}function Fu(e,t,n,r,i,o){zu(e,t);var u=0!==(64&t.effectTag);if(!r&&!u)return i&&wi(t,n,!1),Yu(e,t,o);r=t.stateNode,Nu.current=t;var a=u&&"function"!==typeof n.getDerivedStateFromError?null:r.render();return t.effectTag|=1,null!==e&&u?(t.child=Eo(t,e.child,null,o),t.child=Eo(t,null,a,o)):Au(e,t,a,o),t.memoizedState=r.state,i&&wi(t,n,!0),t.child}function Iu(e){var t=e.stateNode;t.pendingContext?vi(0,t.pendingContext,t.pendingContext!==t.context):t.context&&vi(0,t.context,!1),Ro(e,t.containerInfo)}var ju,Hu,Vu,$u={dehydrated:null,retryTime:0};function Wu(e,t,n){var r,i=t.mode,o=t.pendingProps,u=Lo.current,a=!1;if((r=0!==(64&t.effectTag))||(r=0!==(2&u)&&(null===e||null!==e.memoizedState)),r?(a=!0,t.effectTag&=-65):null!==e&&null===e.memoizedState||void 0===o.fallback||!0===o.unstable_avoidThisFallback||(u|=1),ci(Lo,1&u),null===e){if(void 0!==o.fallback&&Eu(t),a){if(a=o.fallback,(o=Ml(null,i,0,null)).return=t,0===(2&t.mode))for(e=null!==t.memoizedState?t.child.child:t.child,o.child=e;null!==e;)e.return=o,e=e.sibling;return(n=Ml(a,i,n,null)).return=t,o.sibling=n,t.memoizedState=$u,t.child=o,n}return i=o.children,t.memoizedState=null,t.child=Co(t,null,i,n)}if(null!==e.memoizedState){if(i=(e=e.child).sibling,a){if(o=o.fallback,(n=Cl(e,e.pendingProps)).return=t,0===(2&t.mode)&&(a=null!==t.memoizedState?t.child.child:t.child)!==e.child)for(n.child=a;null!==a;)a.return=n,a=a.sibling;return(i=Cl(i,o)).return=t,n.sibling=i,n.childExpirationTime=0,t.memoizedState=$u,t.child=n,i}return n=Eo(t,e.child,o.children,n),t.memoizedState=null,t.child=n}if(e=e.child,a){if(a=o.fallback,(o=Ml(null,i,0,null)).return=t,o.child=e,null!==e&&(e.return=o),0===(2&t.mode))for(e=null!==t.memoizedState?t.child.child:t.child,o.child=e;null!==e;)e.return=o,e=e.sibling;return(n=Ml(a,i,n,null)).return=t,o.sibling=n,n.effectTag|=2,o.childExpirationTime=0,t.memoizedState=$u,t.child=o,n}return t.memoizedState=null,t.child=Eo(t,e,o.children,n)}function Qu(e,t){e.expirationTime<t&&(e.expirationTime=t);var n=e.alternate;null!==n&&n.expirationTime<t&&(n.expirationTime=t),to(e.return,t)}function qu(e,t,n,r,i,o){var u=e.memoizedState;null===u?e.memoizedState={isBackwards:t,rendering:null,renderingStartTime:0,last:r,tail:n,tailExpiration:0,tailMode:i,lastEffect:o}:(u.isBackwards=t,u.rendering=null,u.renderingStartTime=0,u.last=r,u.tail=n,u.tailExpiration=0,u.tailMode=i,u.lastEffect=o)}function Bu(e,t,n){var r=t.pendingProps,i=r.revealOrder,o=r.tail;if(Au(e,t,r.children,n),0!==(2&(r=Lo.current)))r=1&r|2,t.effectTag|=64;else{if(null!==e&&0!==(64&e.effectTag))e:for(e=t.child;null!==e;){if(13===e.tag)null!==e.memoizedState&&Qu(e,n);else if(19===e.tag)Qu(e,n);else if(null!==e.child){e.child.return=e,e=e.child;continue}if(e===t)break e;for(;null===e.sibling;){if(null===e.return||e.return===t)break e;e=e.return}e.sibling.return=e.return,e=e.sibling}r&=1}if(ci(Lo,r),0===(2&t.mode))t.memoizedState=null;else switch(i){case"forwards":for(n=t.child,i=null;null!==n;)null!==(e=n.alternate)&&null===Uo(e)&&(i=n),n=n.sibling;null===(n=i)?(i=t.child,t.child=null):(i=n.sibling,n.sibling=null),qu(t,!1,i,n,o,t.lastEffect);break;case"backwards":for(n=null,i=t.child,t.child=null;null!==i;){if(null!==(e=i.alternate)&&null===Uo(e)){t.child=i;break}e=i.sibling,i.sibling=n,n=i,i=e}qu(t,!0,n,null,o,t.lastEffect);break;case"together":qu(t,!1,null,null,void 0,t.lastEffect);break;default:t.memoizedState=null}return t.child}function Yu(e,t,n){null!==e&&(t.dependencies=e.dependencies);var r=t.expirationTime;if(0!==r&&ul(r),t.childExpirationTime<n)return null;if(null!==e&&t.child!==e.child)throw Error(u(153));if(null!==t.child){for(n=Cl(e=t.child,e.pendingProps),t.child=n,n.return=t;null!==e.sibling;)e=e.sibling,(n=n.sibling=Cl(e,e.pendingProps)).return=t;n.sibling=null}return t.child}function Xu(e,t){switch(e.tailMode){case"hidden":t=e.tail;for(var n=null;null!==t;)null!==t.alternate&&(n=t),t=t.sibling;null===n?e.tail=null:n.sibling=null;break;case"collapsed":n=e.tail;for(var r=null;null!==n;)null!==n.alternate&&(r=n),n=n.sibling;null===r?t||null===e.tail?e.tail=null:e.tail.sibling=null:r.sibling=null}}function Ku(e,t,n){var r=t.pendingProps;switch(t.tag){case 2:case 16:case 15:case 0:case 11:case 7:case 8:case 12:case 9:case 14:return null;case 1:return mi(t.type)&&gi(),null;case 3:return Do(),li(di),li(fi),(n=t.stateNode).pendingContext&&(n.context=n.pendingContext,n.pendingContext=null),null!==e&&null!==e.child||!Su(t)||(t.effectTag|=4),null;case 5:zo(t),n=Ao(Po.current);var o=t.type;if(null!==e&&null!=t.stateNode)Hu(e,t,o,r,n),e.ref!==t.ref&&(t.effectTag|=128);else{if(!r){if(null===t.stateNode)throw Error(u(166));return null}if(e=Ao(Mo.current),Su(t)){r=t.stateNode,o=t.type;var a=t.memoizedProps;switch(r[Tn]=t,r[En]=a,o){case"iframe":case"object":case"embed":Bt("load",r);break;case"video":case"audio":for(e=0;e<Ke.length;e++)Bt(Ke[e],r);break;case"source":Bt("error",r);break;case"img":case"image":case"link":Bt("error",r),Bt("load",r);break;case"form":Bt("reset",r),Bt("submit",r);break;case"details":Bt("toggle",r);break;case"input":ke(r,a),Bt("invalid",r),ln(n,"onChange");break;case"select":r._wrapperState={wasMultiple:!!a.multiple},Bt("invalid",r),ln(n,"onChange");break;case"textarea":Ae(r,a),Bt("invalid",r),ln(n,"onChange")}for(var l in on(o,a),e=null,a)if(a.hasOwnProperty(l)){var c=a[l];"children"===l?"string"===typeof c?r.textContent!==c&&(e=["children",c]):"number"===typeof c&&r.textContent!==""+c&&(e=["children",""+c]):T.hasOwnProperty(l)&&null!=c&&ln(n,l)}switch(o){case"input":we(r),Ce(r,a,!0);break;case"textarea":we(r),De(r);break;case"select":case"option":break;default:"function"===typeof a.onClick&&(r.onclick=cn)}n=e,t.updateQueue=n,null!==n&&(t.effectTag|=4)}else{switch(l=9===n.nodeType?n:n.ownerDocument,e===an&&(e=Le(o)),e===an?"script"===o?((e=l.createElement("div")).innerHTML="<script><\/script>",e=e.removeChild(e.firstChild)):"string"===typeof r.is?e=l.createElement(o,{is:r.is}):(e=l.createElement(o),"select"===o&&(l=e,r.multiple?l.multiple=!0:r.size&&(l.size=r.size))):e=l.createElementNS(e,o),e[Tn]=t,e[En]=r,ju(e,t),t.stateNode=e,l=un(o,r),o){case"iframe":case"object":case"embed":Bt("load",e),c=r;break;case"video":case"audio":for(c=0;c<Ke.length;c++)Bt(Ke[c],e);c=r;break;case"source":Bt("error",e),c=r;break;case"img":case"image":case"link":Bt("error",e),Bt("load",e),c=r;break;case"form":Bt("reset",e),Bt("submit",e),c=r;break;case"details":Bt("toggle",e),c=r;break;case"input":ke(e,r),c=_e(e,r),Bt("invalid",e),ln(n,"onChange");break;case"option":c=Me(e,r);break;case"select":e._wrapperState={wasMultiple:!!r.multiple},c=i({},r,{value:void 0}),Bt("invalid",e),ln(n,"onChange");break;case"textarea":Ae(e,r),c=Pe(e,r),Bt("invalid",e),ln(n,"onChange");break;default:c=r}on(o,c);var s=c;for(a in s)if(s.hasOwnProperty(a)){var f=s[a];"style"===a?nn(e,f):"dangerouslySetInnerHTML"===a?null!=(f=f?f.__html:void 0)&&Ie(e,f):"children"===a?"string"===typeof f?("textarea"!==o||""!==f)&&je(e,f):"number"===typeof f&&je(e,""+f):"suppressContentEditableWarning"!==a&&"suppressHydrationWarning"!==a&&"autoFocus"!==a&&(T.hasOwnProperty(a)?null!=f&&ln(n,a):null!=f&&Z(e,a,f,l))}switch(o){case"input":we(e),Ce(e,r,!1);break;case"textarea":we(e),De(e);break;case"option":null!=r.value&&e.setAttribute("value",""+ye(r.value));break;case"select":e.multiple=!!r.multiple,null!=(n=r.value)?Ne(e,!!r.multiple,n,!1):null!=r.defaultValue&&Ne(e,!!r.multiple,r.defaultValue,!0);break;default:"function"===typeof c.onClick&&(e.onclick=cn)}vn(o,r)&&(t.effectTag|=4)}null!==t.ref&&(t.effectTag|=128)}return null;case 6:if(e&&null!=t.stateNode)Vu(0,t,e.memoizedProps,r);else{if("string"!==typeof r&&null===t.stateNode)throw Error(u(166));n=Ao(Po.current),Ao(Mo.current),Su(t)?(n=t.stateNode,r=t.memoizedProps,n[Tn]=t,n.nodeValue!==r&&(t.effectTag|=4)):((n=(9===n.nodeType?n:n.ownerDocument).createTextNode(r))[Tn]=t,t.stateNode=n)}return null;case 13:return li(Lo),r=t.memoizedState,0!==(64&t.effectTag)?(t.expirationTime=n,t):(n=null!==r,r=!1,null===e?void 0!==t.memoizedProps.fallback&&Su(t):(r=null!==(o=e.memoizedState),n||null===o||null!==(o=e.child.sibling)&&(null!==(a=t.firstEffect)?(t.firstEffect=o,o.nextEffect=a):(t.firstEffect=t.lastEffect=o,o.nextEffect=null),o.effectTag=8)),n&&!r&&0!==(2&t.mode)&&(null===e&&!0!==t.memoizedProps.unstable_avoidThisFallback||0!==(1&Lo.current)?Sa===wa&&(Sa=xa):(Sa!==wa&&Sa!==xa||(Sa=_a),0!==Ra&&null!==Ta&&(Dl(Ta,Ca),Ol(Ta,Ra)))),(n||r)&&(t.effectTag|=4),null);case 4:return Do(),null;case 10:return eo(t),null;case 17:return mi(t.type)&&gi(),null;case 19:if(li(Lo),null===(r=t.memoizedState))return null;if(o=0!==(64&t.effectTag),null===(a=r.rendering)){if(o)Xu(r,!1);else if(Sa!==wa||null!==e&&0!==(64&e.effectTag))for(a=t.child;null!==a;){if(null!==(e=Uo(a))){for(t.effectTag|=64,Xu(r,!1),null!==(o=e.updateQueue)&&(t.updateQueue=o,t.effectTag|=4),null===r.lastEffect&&(t.firstEffect=null),t.lastEffect=r.lastEffect,r=t.child;null!==r;)a=n,(o=r).effectTag&=2,o.nextEffect=null,o.firstEffect=null,o.lastEffect=null,null===(e=o.alternate)?(o.childExpirationTime=0,o.expirationTime=a,o.child=null,o.memoizedProps=null,o.memoizedState=null,o.updateQueue=null,o.dependencies=null):(o.childExpirationTime=e.childExpirationTime,o.expirationTime=e.expirationTime,o.child=e.child,o.memoizedProps=e.memoizedProps,o.memoizedState=e.memoizedState,o.updateQueue=e.updateQueue,a=e.dependencies,o.dependencies=null===a?null:{expirationTime:a.expirationTime,firstContext:a.firstContext,responders:a.responders}),r=r.sibling;return ci(Lo,1&Lo.current|2),t.child}a=a.sibling}}else{if(!o)if(null!==(e=Uo(a))){if(t.effectTag|=64,o=!0,null!==(n=e.updateQueue)&&(t.updateQueue=n,t.effectTag|=4),Xu(r,!0),null===r.tail&&"hidden"===r.tailMode&&!a.alternate)return null!==(t=t.lastEffect=r.lastEffect)&&(t.nextEffect=null),null}else 2*Ii()-r.renderingStartTime>r.tailExpiration&&1<n&&(t.effectTag|=64,o=!0,Xu(r,!1),t.expirationTime=t.childExpirationTime=n-1);r.isBackwards?(a.sibling=t.child,t.child=a):(null!==(n=r.last)?n.sibling=a:t.child=a,r.last=a)}return null!==r.tail?(0===r.tailExpiration&&(r.tailExpiration=Ii()+500),n=r.tail,r.rendering=n,r.tail=n.sibling,r.lastEffect=t.lastEffect,r.renderingStartTime=Ii(),n.sibling=null,t=Lo.current,ci(Lo,o?1&t|2:1&t),n):null}throw Error(u(156,t.tag))}function Zu(e){switch(e.tag){case 1:mi(e.type)&&gi();var t=e.effectTag;return 4096&t?(e.effectTag=-4097&t|64,e):null;case 3:if(Do(),li(di),li(fi),0!==(64&(t=e.effectTag)))throw Error(u(285));return e.effectTag=-4097&t|64,e;case 5:return zo(e),null;case 13:return li(Lo),4096&(t=e.effectTag)?(e.effectTag=-4097&t|64,e):null;case 19:return li(Lo),null;case 4:return Do(),null;case 10:return eo(e),null;default:return null}}function Ju(e,t){return{value:e,source:t,stack:ve(t)}}ju=function(e,t){for(var n=t.child;null!==n;){if(5===n.tag||6===n.tag)e.appendChild(n.stateNode);else if(4!==n.tag&&null!==n.child){n.child.return=n,n=n.child;continue}if(n===t)break;for(;null===n.sibling;){if(null===n.return||n.return===t)return;n=n.return}n.sibling.return=n.return,n=n.sibling}},Hu=function(e,t,n,r,o){var u=e.memoizedProps;if(u!==r){var a,l,c=t.stateNode;switch(Ao(Mo.current),e=null,n){case"input":u=_e(c,u),r=_e(c,r),e=[];break;case"option":u=Me(c,u),r=Me(c,r),e=[];break;case"select":u=i({},u,{value:void 0}),r=i({},r,{value:void 0}),e=[];break;case"textarea":u=Pe(c,u),r=Pe(c,r),e=[];break;default:"function"!==typeof u.onClick&&"function"===typeof r.onClick&&(c.onclick=cn)}for(a in on(n,r),n=null,u)if(!r.hasOwnProperty(a)&&u.hasOwnProperty(a)&&null!=u[a])if("style"===a)for(l in c=u[a])c.hasOwnProperty(l)&&(n||(n={}),n[l]="");else"dangerouslySetInnerHTML"!==a&&"children"!==a&&"suppressContentEditableWarning"!==a&&"suppressHydrationWarning"!==a&&"autoFocus"!==a&&(T.hasOwnProperty(a)?e||(e=[]):(e=e||[]).push(a,null));for(a in r){var s=r[a];if(c=null!=u?u[a]:void 0,r.hasOwnProperty(a)&&s!==c&&(null!=s||null!=c))if("style"===a)if(c){for(l in c)!c.hasOwnProperty(l)||s&&s.hasOwnProperty(l)||(n||(n={}),n[l]="");for(l in s)s.hasOwnProperty(l)&&c[l]!==s[l]&&(n||(n={}),n[l]=s[l])}else n||(e||(e=[]),e.push(a,n)),n=s;else"dangerouslySetInnerHTML"===a?(s=s?s.__html:void 0,c=c?c.__html:void 0,null!=s&&c!==s&&(e=e||[]).push(a,s)):"children"===a?c===s||"string"!==typeof s&&"number"!==typeof s||(e=e||[]).push(a,""+s):"suppressContentEditableWarning"!==a&&"suppressHydrationWarning"!==a&&(T.hasOwnProperty(a)?(null!=s&&ln(o,a),e||c===s||(e=[])):(e=e||[]).push(a,s))}n&&(e=e||[]).push("style",n),o=e,(t.updateQueue=o)&&(t.effectTag|=4)}},Vu=function(e,t,n,r){n!==r&&(t.effectTag|=4)};var Gu="function"===typeof WeakSet?WeakSet:Set;function ea(e,t){var n=t.source,r=t.stack;null===r&&null!==n&&(r=ve(n)),null!==n&&ge(n.type),t=t.value,null!==e&&1===e.tag&&ge(e.type);try{console.error(t)}catch(i){setTimeout((function(){throw i}))}}function ta(e){var t=e.ref;if(null!==t)if("function"===typeof t)try{t(null)}catch(n){yl(e,n)}else t.current=null}function na(e,t){switch(t.tag){case 0:case 11:case 15:case 22:return;case 1:if(256&t.effectTag&&null!==e){var n=e.memoizedProps,r=e.memoizedState;t=(e=t.stateNode).getSnapshotBeforeUpdate(t.elementType===t.type?n:Yi(t.type,n),r),e.__reactInternalSnapshotBeforeUpdate=t}return;case 3:case 5:case 6:case 4:case 17:return}throw Error(u(163))}function ra(e,t){if(null!==(t=null!==(t=t.updateQueue)?t.lastEffect:null)){var n=t=t.next;do{if((n.tag&e)===e){var r=n.destroy;n.destroy=void 0,void 0!==r&&r()}n=n.next}while(n!==t)}}function ia(e,t){if(null!==(t=null!==(t=t.updateQueue)?t.lastEffect:null)){var n=t=t.next;do{if((n.tag&e)===e){var r=n.create;n.destroy=r()}n=n.next}while(n!==t)}}function oa(e,t,n){switch(n.tag){case 0:case 11:case 15:case 22:return void ia(3,n);case 1:if(e=n.stateNode,4&n.effectTag)if(null===t)e.componentDidMount();else{var r=n.elementType===n.type?t.memoizedProps:Yi(n.type,t.memoizedProps);e.componentDidUpdate(r,t.memoizedState,e.__reactInternalSnapshotBeforeUpdate)}return void(null!==(t=n.updateQueue)&&fo(n,t,e));case 3:if(null!==(t=n.updateQueue)){if(e=null,null!==n.child)switch(n.child.tag){case 5:e=n.child.stateNode;break;case 1:e=n.child.stateNode}fo(n,t,e)}return;case 5:return e=n.stateNode,void(null===t&&4&n.effectTag&&vn(n.type,n.memoizedProps)&&e.focus());case 6:case 4:case 12:return;case 13:return void(null===n.memoizedState&&(n=n.alternate,null!==n&&(n=n.memoizedState,null!==n&&(n=n.dehydrated,null!==n&&Lt(n)))));case 19:case 17:case 20:case 21:return}throw Error(u(163))}function ua(e,t,n){switch("function"===typeof _l&&_l(t),t.tag){case 0:case 11:case 14:case 15:case 22:if(null!==(e=t.updateQueue)&&null!==(e=e.lastEffect)){var r=e.next;Vi(97<n?97:n,(function(){var e=r;do{var n=e.destroy;if(void 0!==n){var i=t;try{n()}catch(o){yl(i,o)}}e=e.next}while(e!==r)}))}break;case 1:ta(t),"function"===typeof(n=t.stateNode).componentWillUnmount&&function(e,t){try{t.props=e.memoizedProps,t.state=e.memoizedState,t.componentWillUnmount()}catch(n){yl(e,n)}}(t,n);break;case 5:ta(t);break;case 4:sa(e,t,n)}}function aa(e){var t=e.alternate;e.return=null,e.child=null,e.memoizedState=null,e.updateQueue=null,e.dependencies=null,e.alternate=null,e.firstEffect=null,e.lastEffect=null,e.pendingProps=null,e.memoizedProps=null,e.stateNode=null,null!==t&&aa(t)}function la(e){return 5===e.tag||3===e.tag||4===e.tag}function ca(e){e:{for(var t=e.return;null!==t;){if(la(t)){var n=t;break e}t=t.return}throw Error(u(160))}switch(t=n.stateNode,n.tag){case 5:var r=!1;break;case 3:case 4:t=t.containerInfo,r=!0;break;default:throw Error(u(161))}16&n.effectTag&&(je(t,""),n.effectTag&=-17);e:t:for(n=e;;){for(;null===n.sibling;){if(null===n.return||la(n.return)){n=null;break e}n=n.return}for(n.sibling.return=n.return,n=n.sibling;5!==n.tag&&6!==n.tag&&18!==n.tag;){if(2&n.effectTag)continue t;if(null===n.child||4===n.tag)continue t;n.child.return=n,n=n.child}if(!(2&n.effectTag)){n=n.stateNode;break e}}r?function e(t,n,r){var i=t.tag,o=5===i||6===i;if(o)t=o?t.stateNode:t.stateNode.instance,n?8===r.nodeType?r.parentNode.insertBefore(t,n):r.insertBefore(t,n):(8===r.nodeType?(n=r.parentNode).insertBefore(t,r):(n=r).appendChild(t),null!==(r=r._reactRootContainer)&&void 0!==r||null!==n.onclick||(n.onclick=cn));else if(4!==i&&null!==(t=t.child))for(e(t,n,r),t=t.sibling;null!==t;)e(t,n,r),t=t.sibling}(e,n,t):function e(t,n,r){var i=t.tag,o=5===i||6===i;if(o)t=o?t.stateNode:t.stateNode.instance,n?r.insertBefore(t,n):r.appendChild(t);else if(4!==i&&null!==(t=t.child))for(e(t,n,r),t=t.sibling;null!==t;)e(t,n,r),t=t.sibling}(e,n,t)}function sa(e,t,n){for(var r,i,o=t,a=!1;;){if(!a){a=o.return;e:for(;;){if(null===a)throw Error(u(160));switch(r=a.stateNode,a.tag){case 5:i=!1;break e;case 3:case 4:r=r.containerInfo,i=!0;break e}a=a.return}a=!0}if(5===o.tag||6===o.tag){e:for(var l=e,c=o,s=n,f=c;;)if(ua(l,f,s),null!==f.child&&4!==f.tag)f.child.return=f,f=f.child;else{if(f===c)break e;for(;null===f.sibling;){if(null===f.return||f.return===c)break e;f=f.return}f.sibling.return=f.return,f=f.sibling}i?(l=r,c=o.stateNode,8===l.nodeType?l.parentNode.removeChild(c):l.removeChild(c)):r.removeChild(o.stateNode)}else if(4===o.tag){if(null!==o.child){r=o.stateNode.containerInfo,i=!0,o.child.return=o,o=o.child;continue}}else if(ua(e,o,n),null!==o.child){o.child.return=o,o=o.child;continue}if(o===t)break;for(;null===o.sibling;){if(null===o.return||o.return===t)return;4===(o=o.return).tag&&(a=!1)}o.sibling.return=o.return,o=o.sibling}}function fa(e,t){switch(t.tag){case 0:case 11:case 14:case 15:case 22:return void ra(3,t);case 1:return;case 5:var n=t.stateNode;if(null!=n){var r=t.memoizedProps,i=null!==e?e.memoizedProps:r;e=t.type;var o=t.updateQueue;if(t.updateQueue=null,null!==o){for(n[En]=r,"input"===e&&"radio"===r.type&&null!=r.name&&Te(n,r),un(e,i),t=un(e,r),i=0;i<o.length;i+=2){var a=o[i],l=o[i+1];"style"===a?nn(n,l):"dangerouslySetInnerHTML"===a?Ie(n,l):"children"===a?je(n,l):Z(n,a,l,t)}switch(e){case"input":Ee(n,r);break;case"textarea":Re(n,r);break;case"select":t=n._wrapperState.wasMultiple,n._wrapperState.wasMultiple=!!r.multiple,null!=(e=r.value)?Ne(n,!!r.multiple,e,!1):t!==!!r.multiple&&(null!=r.defaultValue?Ne(n,!!r.multiple,r.defaultValue,!0):Ne(n,!!r.multiple,r.multiple?[]:"",!1))}}}return;case 6:if(null===t.stateNode)throw Error(u(162));return void(t.stateNode.nodeValue=t.memoizedProps);case 3:return void((t=t.stateNode).hydrate&&(t.hydrate=!1,Lt(t.containerInfo)));case 12:return;case 13:if(n=t,null===t.memoizedState?r=!1:(r=!0,n=t.child,Oa=Ii()),null!==n)e:for(e=n;;){if(5===e.tag)o=e.stateNode,r?"function"===typeof(o=o.style).setProperty?o.setProperty("display","none","important"):o.display="none":(o=e.stateNode,i=void 0!==(i=e.memoizedProps.style)&&null!==i&&i.hasOwnProperty("display")?i.display:null,o.style.display=tn("display",i));else if(6===e.tag)e.stateNode.nodeValue=r?"":e.memoizedProps;else{if(13===e.tag&&null!==e.memoizedState&&null===e.memoizedState.dehydrated){(o=e.child.sibling).return=e,e=o;continue}if(null!==e.child){e.child.return=e,e=e.child;continue}}if(e===n)break;for(;null===e.sibling;){if(null===e.return||e.return===n)break e;e=e.return}e.sibling.return=e.return,e=e.sibling}return void da(t);case 19:return void da(t);case 17:return}throw Error(u(163))}function da(e){var t=e.updateQueue;if(null!==t){e.updateQueue=null;var n=e.stateNode;null===n&&(n=e.stateNode=new Gu),t.forEach((function(t){var r=wl.bind(null,e,t);n.has(t)||(n.add(t),t.then(r,r))}))}}var pa="function"===typeof WeakMap?WeakMap:Map;function ha(e,t,n){(n=ao(n,null)).tag=3,n.payload={element:null};var r=t.value;return n.callback=function(){La||(La=!0,Ua=r),ea(e,t)},n}function ma(e,t,n){(n=ao(n,null)).tag=3;var r=e.type.getDerivedStateFromError;if("function"===typeof r){var i=t.value;n.payload=function(){return ea(e,t),r(i)}}var o=e.stateNode;return null!==o&&"function"===typeof o.componentDidCatch&&(n.callback=function(){"function"!==typeof r&&(null===Fa?Fa=new Set([this]):Fa.add(this),ea(e,t));var n=t.stack;this.componentDidCatch(t.value,{componentStack:null!==n?n:""})}),n}var ga,va=Math.ceil,ya=K.ReactCurrentDispatcher,ba=K.ReactCurrentOwner,wa=0,xa=3,_a=4,ka=0,Ta=null,Ea=null,Ca=0,Sa=wa,Ma=null,Na=1073741823,Pa=1073741823,Aa=null,Ra=0,Da=!1,Oa=0,za=null,La=!1,Ua=null,Fa=null,Ia=!1,ja=null,Ha=90,Va=null,$a=0,Wa=null,Qa=0;function qa(){return 0!==(48&ka)?1073741821-(Ii()/10|0):0!==Qa?Qa:Qa=1073741821-(Ii()/10|0)}function Ba(e,t,n){if(0===(2&(t=t.mode)))return 1073741823;var r=ji();if(0===(4&t))return 99===r?1073741823:1073741822;if(0!==(16&ka))return Ca;if(null!==n)e=Bi(e,0|n.timeoutMs||5e3,250);else switch(r){case 99:e=1073741823;break;case 98:e=Bi(e,150,100);break;case 97:case 96:e=Bi(e,5e3,250);break;case 95:e=2;break;default:throw Error(u(326))}return null!==Ta&&e===Ca&&--e,e}function Ya(e,t){if(50<$a)throw $a=0,Wa=null,Error(u(185));if(null!==(e=Xa(e,t))){var n=ji();1073741823===t?0!==(8&ka)&&0===(48&ka)?Ga(e):(Za(e),0===ka&&Qi()):Za(e),0===(4&ka)||98!==n&&99!==n||(null===Va?Va=new Map([[e,t]]):(void 0===(n=Va.get(e))||n>t)&&Va.set(e,t))}}function Xa(e,t){e.expirationTime<t&&(e.expirationTime=t);var n=e.alternate;null!==n&&n.expirationTime<t&&(n.expirationTime=t);var r=e.return,i=null;if(null===r&&3===e.tag)i=e.stateNode;else for(;null!==r;){if(n=r.alternate,r.childExpirationTime<t&&(r.childExpirationTime=t),null!==n&&n.childExpirationTime<t&&(n.childExpirationTime=t),null===r.return&&3===r.tag){i=r.stateNode;break}r=r.return}return null!==i&&(Ta===i&&(ul(t),Sa===_a&&Dl(i,Ca)),Ol(i,t)),i}function Ka(e){var t=e.lastExpiredTime;if(0!==t)return t;if(!Rl(e,t=e.firstPendingTime))return t;var n=e.lastPingedTime;return 2>=(e=n>(e=e.nextKnownPendingLevel)?n:e)&&t!==e?0:e}function Za(e){if(0!==e.lastExpiredTime)e.callbackExpirationTime=1073741823,e.callbackPriority=99,e.callbackNode=Wi(Ga.bind(null,e));else{var t=Ka(e),n=e.callbackNode;if(0===t)null!==n&&(e.callbackNode=null,e.callbackExpirationTime=0,e.callbackPriority=90);else{var r=qa();if(1073741823===t?r=99:1===t||2===t?r=95:r=0>=(r=10*(1073741821-t)-10*(1073741821-r))?99:250>=r?98:5250>=r?97:95,null!==n){var i=e.callbackPriority;if(e.callbackExpirationTime===t&&i>=r)return;n!==Ri&&ki(n)}e.callbackExpirationTime=t,e.callbackPriority=r,t=1073741823===t?Wi(Ga.bind(null,e)):$i(r,Ja.bind(null,e),{timeout:10*(1073741821-t)-Ii()}),e.callbackNode=t}}}function Ja(e,t){if(Qa=0,t)return zl(e,t=qa()),Za(e),null;var n=Ka(e);if(0!==n){if(t=e.callbackNode,0!==(48&ka))throw Error(u(327));if(ml(),e===Ta&&n===Ca||nl(e,n),null!==Ea){var r=ka;ka|=16;for(var i=il();;)try{ll();break}catch(l){rl(e,l)}if(Gi(),ka=r,ya.current=i,1===Sa)throw t=Ma,nl(e,n),Dl(e,n),Za(e),t;if(null===Ea)switch(i=e.finishedWork=e.current.alternate,e.finishedExpirationTime=n,r=Sa,Ta=null,r){case wa:case 1:throw Error(u(345));case 2:zl(e,2<n?2:n);break;case xa:if(Dl(e,n),n===(r=e.lastSuspendedTime)&&(e.nextKnownPendingLevel=fl(i)),1073741823===Na&&10<(i=Oa+500-Ii())){if(Da){var o=e.lastPingedTime;if(0===o||o>=n){e.lastPingedTime=n,nl(e,n);break}}if(0!==(o=Ka(e))&&o!==n)break;if(0!==r&&r!==n){e.lastPingedTime=r;break}e.timeoutHandle=bn(dl.bind(null,e),i);break}dl(e);break;case _a:if(Dl(e,n),n===(r=e.lastSuspendedTime)&&(e.nextKnownPendingLevel=fl(i)),Da&&(0===(i=e.lastPingedTime)||i>=n)){e.lastPingedTime=n,nl(e,n);break}if(0!==(i=Ka(e))&&i!==n)break;if(0!==r&&r!==n){e.lastPingedTime=r;break}if(1073741823!==Pa?r=10*(1073741821-Pa)-Ii():1073741823===Na?r=0:(r=10*(1073741821-Na)-5e3,0>(r=(i=Ii())-r)&&(r=0),(n=10*(1073741821-n)-i)<(r=(120>r?120:480>r?480:1080>r?1080:1920>r?1920:3e3>r?3e3:4320>r?4320:1960*va(r/1960))-r)&&(r=n)),10<r){e.timeoutHandle=bn(dl.bind(null,e),r);break}dl(e);break;case 5:if(1073741823!==Na&&null!==Aa){o=Na;var a=Aa;if(0>=(r=0|a.busyMinDurationMs)?r=0:(i=0|a.busyDelayMs,r=(o=Ii()-(10*(1073741821-o)-(0|a.timeoutMs||5e3)))<=i?0:i+r-o),10<r){Dl(e,n),e.timeoutHandle=bn(dl.bind(null,e),r);break}}dl(e);break;default:throw Error(u(329))}if(Za(e),e.callbackNode===t)return Ja.bind(null,e)}}return null}function Ga(e){var t=e.lastExpiredTime;if(t=0!==t?t:1073741823,0!==(48&ka))throw Error(u(327));if(ml(),e===Ta&&t===Ca||nl(e,t),null!==Ea){var n=ka;ka|=16;for(var r=il();;)try{al();break}catch(i){rl(e,i)}if(Gi(),ka=n,ya.current=r,1===Sa)throw n=Ma,nl(e,t),Dl(e,t),Za(e),n;if(null!==Ea)throw Error(u(261));e.finishedWork=e.current.alternate,e.finishedExpirationTime=t,Ta=null,dl(e),Za(e)}return null}function el(e,t){var n=ka;ka|=1;try{return e(t)}finally{0===(ka=n)&&Qi()}}function tl(e,t){var n=ka;ka&=-2,ka|=8;try{return e(t)}finally{0===(ka=n)&&Qi()}}function nl(e,t){e.finishedWork=null,e.finishedExpirationTime=0;var n=e.timeoutHandle;if(-1!==n&&(e.timeoutHandle=-1,wn(n)),null!==Ea)for(n=Ea.return;null!==n;){var r=n;switch(r.tag){case 1:null!==(r=r.type.childContextTypes)&&void 0!==r&&gi();break;case 3:Do(),li(di),li(fi);break;case 5:zo(r);break;case 4:Do();break;case 13:case 19:li(Lo);break;case 10:eo(r)}n=n.return}Ta=e,Ea=Cl(e.current,null),Ca=t,Sa=wa,Ma=null,Pa=Na=1073741823,Aa=null,Ra=0,Da=!1}function rl(e,t){for(;;){try{if(Gi(),Io.current=gu,Qo)for(var n=Vo.memoizedState;null!==n;){var r=n.queue;null!==r&&(r.pending=null),n=n.next}if(Ho=0,Wo=$o=Vo=null,Qo=!1,null===Ea||null===Ea.return)return Sa=1,Ma=t,Ea=null;e:{var i=e,o=Ea.return,u=Ea,a=t;if(t=Ca,u.effectTag|=2048,u.firstEffect=u.lastEffect=null,null!==a&&"object"===typeof a&&"function"===typeof a.then){var l=a;if(0===(2&u.mode)){var c=u.alternate;c?(u.updateQueue=c.updateQueue,u.memoizedState=c.memoizedState,u.expirationTime=c.expirationTime):(u.updateQueue=null,u.memoizedState=null)}var s=0!==(1&Lo.current),f=o;do{var d;if(d=13===f.tag){var p=f.memoizedState;if(null!==p)d=null!==p.dehydrated;else{var h=f.memoizedProps;d=void 0!==h.fallback&&(!0!==h.unstable_avoidThisFallback||!s)}}if(d){var m=f.updateQueue;if(null===m){var g=new Set;g.add(l),f.updateQueue=g}else m.add(l);if(0===(2&f.mode)){if(f.effectTag|=64,u.effectTag&=-2981,1===u.tag)if(null===u.alternate)u.tag=17;else{var v=ao(1073741823,null);v.tag=2,lo(u,v)}u.expirationTime=1073741823;break e}a=void 0,u=t;var y=i.pingCache;if(null===y?(y=i.pingCache=new pa,a=new Set,y.set(l,a)):void 0===(a=y.get(l))&&(a=new Set,y.set(l,a)),!a.has(u)){a.add(u);var b=bl.bind(null,i,l,u);l.then(b,b)}f.effectTag|=4096,f.expirationTime=t;break e}f=f.return}while(null!==f);a=Error((ge(u.type)||"A React component")+" suspended while rendering, but no fallback UI was specified.\n\nAdd a <Suspense fallback=...> component higher in the tree to provide a loading indicator or placeholder to display."+ve(u))}5!==Sa&&(Sa=2),a=Ju(a,u),f=o;do{switch(f.tag){case 3:l=a,f.effectTag|=4096,f.expirationTime=t,co(f,ha(f,l,t));break e;case 1:l=a;var w=f.type,x=f.stateNode;if(0===(64&f.effectTag)&&("function"===typeof w.getDerivedStateFromError||null!==x&&"function"===typeof x.componentDidCatch&&(null===Fa||!Fa.has(x)))){f.effectTag|=4096,f.expirationTime=t,co(f,ma(f,l,t));break e}}f=f.return}while(null!==f)}Ea=sl(Ea)}catch(_){t=_;continue}break}}function il(){var e=ya.current;return ya.current=gu,null===e?gu:e}function ol(e,t){e<Na&&2<e&&(Na=e),null!==t&&e<Pa&&2<e&&(Pa=e,Aa=t)}function ul(e){e>Ra&&(Ra=e)}function al(){for(;null!==Ea;)Ea=cl(Ea)}function ll(){for(;null!==Ea&&!Di();)Ea=cl(Ea)}function cl(e){var t=ga(e.alternate,e,Ca);return e.memoizedProps=e.pendingProps,null===t&&(t=sl(e)),ba.current=null,t}function sl(e){Ea=e;do{var t=Ea.alternate;if(e=Ea.return,0===(2048&Ea.effectTag)){if(t=Ku(t,Ea,Ca),1===Ca||1!==Ea.childExpirationTime){for(var n=0,r=Ea.child;null!==r;){var i=r.expirationTime,o=r.childExpirationTime;i>n&&(n=i),o>n&&(n=o),r=r.sibling}Ea.childExpirationTime=n}if(null!==t)return t;null!==e&&0===(2048&e.effectTag)&&(null===e.firstEffect&&(e.firstEffect=Ea.firstEffect),null!==Ea.lastEffect&&(null!==e.lastEffect&&(e.lastEffect.nextEffect=Ea.firstEffect),e.lastEffect=Ea.lastEffect),1<Ea.effectTag&&(null!==e.lastEffect?e.lastEffect.nextEffect=Ea:e.firstEffect=Ea,e.lastEffect=Ea))}else{if(null!==(t=Zu(Ea)))return t.effectTag&=2047,t;null!==e&&(e.firstEffect=e.lastEffect=null,e.effectTag|=2048)}if(null!==(t=Ea.sibling))return t;Ea=e}while(null!==Ea);return Sa===wa&&(Sa=5),null}function fl(e){var t=e.expirationTime;return t>(e=e.childExpirationTime)?t:e}function dl(e){var t=ji();return Vi(99,pl.bind(null,e,t)),null}function pl(e,t){do{ml()}while(null!==ja);if(0!==(48&ka))throw Error(u(327));var n=e.finishedWork,r=e.finishedExpirationTime;if(null===n)return null;if(e.finishedWork=null,e.finishedExpirationTime=0,n===e.current)throw Error(u(177));e.callbackNode=null,e.callbackExpirationTime=0,e.callbackPriority=90,e.nextKnownPendingLevel=0;var i=fl(n);if(e.firstPendingTime=i,r<=e.lastSuspendedTime?e.firstSuspendedTime=e.lastSuspendedTime=e.nextKnownPendingLevel=0:r<=e.firstSuspendedTime&&(e.firstSuspendedTime=r-1),r<=e.lastPingedTime&&(e.lastPingedTime=0),r<=e.lastExpiredTime&&(e.lastExpiredTime=0),e===Ta&&(Ea=Ta=null,Ca=0),1<n.effectTag?null!==n.lastEffect?(n.lastEffect.nextEffect=n,i=n.firstEffect):i=n:i=n.firstEffect,null!==i){var o=ka;ka|=32,ba.current=null,mn=qt;var a=pn();if(hn(a)){if("selectionStart"in a)var l={start:a.selectionStart,end:a.selectionEnd};else e:{var c=(l=(l=a.ownerDocument)&&l.defaultView||window).getSelection&&l.getSelection();if(c&&0!==c.rangeCount){l=c.anchorNode;var s=c.anchorOffset,f=c.focusNode;c=c.focusOffset;try{l.nodeType,f.nodeType}catch(C){l=null;break e}var d=0,p=-1,h=-1,m=0,g=0,v=a,y=null;t:for(;;){for(var b;v!==l||0!==s&&3!==v.nodeType||(p=d+s),v!==f||0!==c&&3!==v.nodeType||(h=d+c),3===v.nodeType&&(d+=v.nodeValue.length),null!==(b=v.firstChild);)y=v,v=b;for(;;){if(v===a)break t;if(y===l&&++m===s&&(p=d),y===f&&++g===c&&(h=d),null!==(b=v.nextSibling))break;y=(v=y).parentNode}v=b}l=-1===p||-1===h?null:{start:p,end:h}}else l=null}l=l||{start:0,end:0}}else l=null;gn={activeElementDetached:null,focusedElem:a,selectionRange:l},qt=!1,za=i;do{try{hl()}catch(C){if(null===za)throw Error(u(330));yl(za,C),za=za.nextEffect}}while(null!==za);za=i;do{try{for(a=e,l=t;null!==za;){var w=za.effectTag;if(16&w&&je(za.stateNode,""),128&w){var x=za.alternate;if(null!==x){var _=x.ref;null!==_&&("function"===typeof _?_(null):_.current=null)}}switch(1038&w){case 2:ca(za),za.effectTag&=-3;break;case 6:ca(za),za.effectTag&=-3,fa(za.alternate,za);break;case 1024:za.effectTag&=-1025;break;case 1028:za.effectTag&=-1025,fa(za.alternate,za);break;case 4:fa(za.alternate,za);break;case 8:sa(a,s=za,l),aa(s)}za=za.nextEffect}}catch(C){if(null===za)throw Error(u(330));yl(za,C),za=za.nextEffect}}while(null!==za);if(_=gn,x=pn(),w=_.focusedElem,l=_.selectionRange,x!==w&&w&&w.ownerDocument&&function e(t,n){return!(!t||!n)&&(t===n||(!t||3!==t.nodeType)&&(n&&3===n.nodeType?e(t,n.parentNode):"contains"in t?t.contains(n):!!t.compareDocumentPosition&&!!(16&t.compareDocumentPosition(n))))}(w.ownerDocument.documentElement,w)){null!==l&&hn(w)&&(x=l.start,void 0===(_=l.end)&&(_=x),"selectionStart"in w?(w.selectionStart=x,w.selectionEnd=Math.min(_,w.value.length)):(_=(x=w.ownerDocument||document)&&x.defaultView||window).getSelection&&(_=_.getSelection(),s=w.textContent.length,a=Math.min(l.start,s),l=void 0===l.end?a:Math.min(l.end,s),!_.extend&&a>l&&(s=l,l=a,a=s),s=dn(w,a),f=dn(w,l),s&&f&&(1!==_.rangeCount||_.anchorNode!==s.node||_.anchorOffset!==s.offset||_.focusNode!==f.node||_.focusOffset!==f.offset)&&((x=x.createRange()).setStart(s.node,s.offset),_.removeAllRanges(),a>l?(_.addRange(x),_.extend(f.node,f.offset)):(x.setEnd(f.node,f.offset),_.addRange(x))))),x=[];for(_=w;_=_.parentNode;)1===_.nodeType&&x.push({element:_,left:_.scrollLeft,top:_.scrollTop});for("function"===typeof w.focus&&w.focus(),w=0;w<x.length;w++)(_=x[w]).element.scrollLeft=_.left,_.element.scrollTop=_.top}qt=!!mn,gn=mn=null,e.current=n,za=i;do{try{for(w=e;null!==za;){var k=za.effectTag;if(36&k&&oa(w,za.alternate,za),128&k){x=void 0;var T=za.ref;if(null!==T){var E=za.stateNode;switch(za.tag){case 5:x=E;break;default:x=E}"function"===typeof T?T(x):T.current=x}}za=za.nextEffect}}catch(C){if(null===za)throw Error(u(330));yl(za,C),za=za.nextEffect}}while(null!==za);za=null,Oi(),ka=o}else e.current=n;if(Ia)Ia=!1,ja=e,Ha=t;else for(za=i;null!==za;)t=za.nextEffect,za.nextEffect=null,za=t;if(0===(t=e.firstPendingTime)&&(Fa=null),1073741823===t?e===Wa?$a++:($a=0,Wa=e):$a=0,"function"===typeof xl&&xl(n.stateNode,r),Za(e),La)throw La=!1,e=Ua,Ua=null,e;return 0!==(8&ka)||Qi(),null}function hl(){for(;null!==za;){var e=za.effectTag;0!==(256&e)&&na(za.alternate,za),0===(512&e)||Ia||(Ia=!0,$i(97,(function(){return ml(),null}))),za=za.nextEffect}}function ml(){if(90!==Ha){var e=97<Ha?97:Ha;return Ha=90,Vi(e,gl)}}function gl(){if(null===ja)return!1;var e=ja;if(ja=null,0!==(48&ka))throw Error(u(331));var t=ka;for(ka|=32,e=e.current.firstEffect;null!==e;){try{var n=e;if(0!==(512&n.effectTag))switch(n.tag){case 0:case 11:case 15:case 22:ra(5,n),ia(5,n)}}catch(r){if(null===e)throw Error(u(330));yl(e,r)}n=e.nextEffect,e.nextEffect=null,e=n}return ka=t,Qi(),!0}function vl(e,t,n){lo(e,t=ha(e,t=Ju(n,t),1073741823)),null!==(e=Xa(e,1073741823))&&Za(e)}function yl(e,t){if(3===e.tag)vl(e,e,t);else for(var n=e.return;null!==n;){if(3===n.tag){vl(n,e,t);break}if(1===n.tag){var r=n.stateNode;if("function"===typeof n.type.getDerivedStateFromError||"function"===typeof r.componentDidCatch&&(null===Fa||!Fa.has(r))){lo(n,e=ma(n,e=Ju(t,e),1073741823)),null!==(n=Xa(n,1073741823))&&Za(n);break}}n=n.return}}function bl(e,t,n){var r=e.pingCache;null!==r&&r.delete(t),Ta===e&&Ca===n?Sa===_a||Sa===xa&&1073741823===Na&&Ii()-Oa<500?nl(e,Ca):Da=!0:Rl(e,n)&&(0!==(t=e.lastPingedTime)&&t<n||(e.lastPingedTime=n,Za(e)))}function wl(e,t){var n=e.stateNode;null!==n&&n.delete(t),0===(t=0)&&(t=Ba(t=qa(),e,null)),null!==(e=Xa(e,t))&&Za(e)}ga=function(e,t,n){var r=t.expirationTime;if(null!==e){var i=t.pendingProps;if(e.memoizedProps!==i||di.current)Pu=!0;else{if(r<n){switch(Pu=!1,t.tag){case 3:Iu(t),Mu();break;case 5:if(Oo(t),4&t.mode&&1!==n&&i.hidden)return t.expirationTime=t.childExpirationTime=1,null;break;case 1:mi(t.type)&&bi(t);break;case 4:Ro(t,t.stateNode.containerInfo);break;case 10:r=t.memoizedProps.value,i=t.type._context,ci(Xi,i._currentValue),i._currentValue=r;break;case 13:if(null!==t.memoizedState)return 0!==(r=t.child.childExpirationTime)&&r>=n?Wu(e,t,n):(ci(Lo,1&Lo.current),null!==(t=Yu(e,t,n))?t.sibling:null);ci(Lo,1&Lo.current);break;case 19:if(r=t.childExpirationTime>=n,0!==(64&e.effectTag)){if(r)return Bu(e,t,n);t.effectTag|=64}if(null!==(i=t.memoizedState)&&(i.rendering=null,i.tail=null),ci(Lo,Lo.current),!r)return null}return Yu(e,t,n)}Pu=!1}}else Pu=!1;switch(t.expirationTime=0,t.tag){case 2:if(r=t.type,null!==e&&(e.alternate=null,t.alternate=null,t.effectTag|=2),e=t.pendingProps,i=hi(t,fi.current),no(t,n),i=Yo(null,t,r,e,i,n),t.effectTag|=1,"object"===typeof i&&null!==i&&"function"===typeof i.render&&void 0===i.$$typeof){if(t.tag=1,t.memoizedState=null,t.updateQueue=null,mi(r)){var o=!0;bi(t)}else o=!1;t.memoizedState=null!==i.state&&void 0!==i.state?i.state:null,oo(t);var a=r.getDerivedStateFromProps;"function"===typeof a&&mo(t,r,a,e),i.updater=go,t.stateNode=i,i._reactInternalFiber=t,wo(t,r,e,n),t=Fu(null,t,r,!0,o,n)}else t.tag=0,Au(null,t,i,n),t=t.child;return t;case 16:e:{if(i=t.elementType,null!==e&&(e.alternate=null,t.alternate=null,t.effectTag|=2),e=t.pendingProps,function(e){if(-1===e._status){e._status=0;var t=e._ctor;t=t(),e._result=t,t.then((function(t){0===e._status&&(t=t.default,e._status=1,e._result=t)}),(function(t){0===e._status&&(e._status=2,e._result=t)}))}}(i),1!==i._status)throw i._result;switch(i=i._result,t.type=i,o=t.tag=function(e){if("function"===typeof e)return El(e)?1:0;if(void 0!==e&&null!==e){if((e=e.$$typeof)===le)return 11;if(e===fe)return 14}return 2}(i),e=Yi(i,e),o){case 0:t=Lu(null,t,i,e,n);break e;case 1:t=Uu(null,t,i,e,n);break e;case 11:t=Ru(null,t,i,e,n);break e;case 14:t=Du(null,t,i,Yi(i.type,e),r,n);break e}throw Error(u(306,i,""))}return t;case 0:return r=t.type,i=t.pendingProps,Lu(e,t,r,i=t.elementType===r?i:Yi(r,i),n);case 1:return r=t.type,i=t.pendingProps,Uu(e,t,r,i=t.elementType===r?i:Yi(r,i),n);case 3:if(Iu(t),r=t.updateQueue,null===e||null===r)throw Error(u(282));if(r=t.pendingProps,i=null!==(i=t.memoizedState)?i.element:null,uo(e,t),so(t,r,null,n),(r=t.memoizedState.element)===i)Mu(),t=Yu(e,t,n);else{if((i=t.stateNode.hydrate)&&(xu=xn(t.stateNode.containerInfo.firstChild),wu=t,i=_u=!0),i)for(n=Co(t,null,r,n),t.child=n;n;)n.effectTag=-3&n.effectTag|1024,n=n.sibling;else Au(e,t,r,n),Mu();t=t.child}return t;case 5:return Oo(t),null===e&&Eu(t),r=t.type,i=t.pendingProps,o=null!==e?e.memoizedProps:null,a=i.children,yn(r,i)?a=null:null!==o&&yn(r,o)&&(t.effectTag|=16),zu(e,t),4&t.mode&&1!==n&&i.hidden?(t.expirationTime=t.childExpirationTime=1,t=null):(Au(e,t,a,n),t=t.child),t;case 6:return null===e&&Eu(t),null;case 13:return Wu(e,t,n);case 4:return Ro(t,t.stateNode.containerInfo),r=t.pendingProps,null===e?t.child=Eo(t,null,r,n):Au(e,t,r,n),t.child;case 11:return r=t.type,i=t.pendingProps,Ru(e,t,r,i=t.elementType===r?i:Yi(r,i),n);case 7:return Au(e,t,t.pendingProps,n),t.child;case 8:case 12:return Au(e,t,t.pendingProps.children,n),t.child;case 10:e:{r=t.type._context,i=t.pendingProps,a=t.memoizedProps,o=i.value;var l=t.type._context;if(ci(Xi,l._currentValue),l._currentValue=o,null!==a)if(l=a.value,0===(o=Ur(l,o)?0:0|("function"===typeof r._calculateChangedBits?r._calculateChangedBits(l,o):1073741823))){if(a.children===i.children&&!di.current){t=Yu(e,t,n);break e}}else for(null!==(l=t.child)&&(l.return=t);null!==l;){var c=l.dependencies;if(null!==c){a=l.child;for(var s=c.firstContext;null!==s;){if(s.context===r&&0!==(s.observedBits&o)){1===l.tag&&((s=ao(n,null)).tag=2,lo(l,s)),l.expirationTime<n&&(l.expirationTime=n),null!==(s=l.alternate)&&s.expirationTime<n&&(s.expirationTime=n),to(l.return,n),c.expirationTime<n&&(c.expirationTime=n);break}s=s.next}}else a=10===l.tag&&l.type===t.type?null:l.child;if(null!==a)a.return=l;else for(a=l;null!==a;){if(a===t){a=null;break}if(null!==(l=a.sibling)){l.return=a.return,a=l;break}a=a.return}l=a}Au(e,t,i.children,n),t=t.child}return t;case 9:return i=t.type,r=(o=t.pendingProps).children,no(t,n),r=r(i=ro(i,o.unstable_observedBits)),t.effectTag|=1,Au(e,t,r,n),t.child;case 14:return o=Yi(i=t.type,t.pendingProps),Du(e,t,i,o=Yi(i.type,o),r,n);case 15:return Ou(e,t,t.type,t.pendingProps,r,n);case 17:return r=t.type,i=t.pendingProps,i=t.elementType===r?i:Yi(r,i),null!==e&&(e.alternate=null,t.alternate=null,t.effectTag|=2),t.tag=1,mi(r)?(e=!0,bi(t)):e=!1,no(t,n),yo(t,r,i),wo(t,r,i,n),Fu(null,t,r,!0,e,n);case 19:return Bu(e,t,n)}throw Error(u(156,t.tag))};var xl=null,_l=null;function kl(e,t,n,r){this.tag=e,this.key=n,this.sibling=this.child=this.return=this.stateNode=this.type=this.elementType=null,this.index=0,this.ref=null,this.pendingProps=t,this.dependencies=this.memoizedState=this.updateQueue=this.memoizedProps=null,this.mode=r,this.effectTag=0,this.lastEffect=this.firstEffect=this.nextEffect=null,this.childExpirationTime=this.expirationTime=0,this.alternate=null}function Tl(e,t,n,r){return new kl(e,t,n,r)}function El(e){return!(!(e=e.prototype)||!e.isReactComponent)}function Cl(e,t){var n=e.alternate;return null===n?((n=Tl(e.tag,t,e.key,e.mode)).elementType=e.elementType,n.type=e.type,n.stateNode=e.stateNode,n.alternate=e,e.alternate=n):(n.pendingProps=t,n.effectTag=0,n.nextEffect=null,n.firstEffect=null,n.lastEffect=null),n.childExpirationTime=e.childExpirationTime,n.expirationTime=e.expirationTime,n.child=e.child,n.memoizedProps=e.memoizedProps,n.memoizedState=e.memoizedState,n.updateQueue=e.updateQueue,t=e.dependencies,n.dependencies=null===t?null:{expirationTime:t.expirationTime,firstContext:t.firstContext,responders:t.responders},n.sibling=e.sibling,n.index=e.index,n.ref=e.ref,n}function Sl(e,t,n,r,i,o){var a=2;if(r=e,"function"===typeof e)El(e)&&(a=1);else if("string"===typeof e)a=5;else e:switch(e){case ne:return Ml(n.children,i,o,t);case ae:a=8,i|=7;break;case re:a=8,i|=1;break;case ie:return(e=Tl(12,n,t,8|i)).elementType=ie,e.type=ie,e.expirationTime=o,e;case ce:return(e=Tl(13,n,t,i)).type=ce,e.elementType=ce,e.expirationTime=o,e;case se:return(e=Tl(19,n,t,i)).elementType=se,e.expirationTime=o,e;default:if("object"===typeof e&&null!==e)switch(e.$$typeof){case oe:a=10;break e;case ue:a=9;break e;case le:a=11;break e;case fe:a=14;break e;case de:a=16,r=null;break e;case pe:a=22;break e}throw Error(u(130,null==e?e:typeof e,""))}return(t=Tl(a,n,t,i)).elementType=e,t.type=r,t.expirationTime=o,t}function Ml(e,t,n,r){return(e=Tl(7,e,r,t)).expirationTime=n,e}function Nl(e,t,n){return(e=Tl(6,e,null,t)).expirationTime=n,e}function Pl(e,t,n){return(t=Tl(4,null!==e.children?e.children:[],e.key,t)).expirationTime=n,t.stateNode={containerInfo:e.containerInfo,pendingChildren:null,implementation:e.implementation},t}function Al(e,t,n){this.tag=t,this.current=null,this.containerInfo=e,this.pingCache=this.pendingChildren=null,this.finishedExpirationTime=0,this.finishedWork=null,this.timeoutHandle=-1,this.pendingContext=this.context=null,this.hydrate=n,this.callbackNode=null,this.callbackPriority=90,this.lastExpiredTime=this.lastPingedTime=this.nextKnownPendingLevel=this.lastSuspendedTime=this.firstSuspendedTime=this.firstPendingTime=0}function Rl(e,t){var n=e.firstSuspendedTime;return e=e.lastSuspendedTime,0!==n&&n>=t&&e<=t}function Dl(e,t){var n=e.firstSuspendedTime,r=e.lastSuspendedTime;n<t&&(e.firstSuspendedTime=t),(r>t||0===n)&&(e.lastSuspendedTime=t),t<=e.lastPingedTime&&(e.lastPingedTime=0),t<=e.lastExpiredTime&&(e.lastExpiredTime=0)}function Ol(e,t){t>e.firstPendingTime&&(e.firstPendingTime=t);var n=e.firstSuspendedTime;0!==n&&(t>=n?e.firstSuspendedTime=e.lastSuspendedTime=e.nextKnownPendingLevel=0:t>=e.lastSuspendedTime&&(e.lastSuspendedTime=t+1),t>e.nextKnownPendingLevel&&(e.nextKnownPendingLevel=t))}function zl(e,t){var n=e.lastExpiredTime;(0===n||n>t)&&(e.lastExpiredTime=t)}function Ll(e,t,n,r){var i=t.current,o=qa(),a=po.suspense;o=Ba(o,i,a);e:if(n){t:{if(Ge(n=n._reactInternalFiber)!==n||1!==n.tag)throw Error(u(170));var l=n;do{switch(l.tag){case 3:l=l.stateNode.context;break t;case 1:if(mi(l.type)){l=l.stateNode.__reactInternalMemoizedMergedChildContext;break t}}l=l.return}while(null!==l);throw Error(u(171))}if(1===n.tag){var c=n.type;if(mi(c)){n=yi(n,c,l);break e}}n=l}else n=si;return null===t.context?t.context=n:t.pendingContext=n,(t=ao(o,a)).payload={element:e},null!==(r=void 0===r?null:r)&&(t.callback=r),lo(i,t),Ya(i,o),o}function Ul(e){if(!(e=e.current).child)return null;switch(e.child.tag){case 5:default:return e.child.stateNode}}function Fl(e,t){null!==(e=e.memoizedState)&&null!==e.dehydrated&&e.retryTime<t&&(e.retryTime=t)}function Il(e,t){Fl(e,t),(e=e.alternate)&&Fl(e,t)}function jl(e,t,n){var r=new Al(e,t,n=null!=n&&!0===n.hydrate),i=Tl(3,null,null,2===t?7:1===t?3:0);r.current=i,i.stateNode=r,oo(i),e[Cn]=r.current,n&&0!==t&&function(e,t){var n=Je(t);Ct.forEach((function(e){ht(e,t,n)})),St.forEach((function(e){ht(e,t,n)}))}(0,9===e.nodeType?e:e.ownerDocument),this._internalRoot=r}function Hl(e){return!(!e||1!==e.nodeType&&9!==e.nodeType&&11!==e.nodeType&&(8!==e.nodeType||" react-mount-point-unstable "!==e.nodeValue))}function Vl(e,t,n,r,i){var o=n._reactRootContainer;if(o){var u=o._internalRoot;if("function"===typeof i){var a=i;i=function(){var e=Ul(u);a.call(e)}}Ll(t,u,e,i)}else{if(o=n._reactRootContainer=function(e,t){if(t||(t=!(!(t=e?9===e.nodeType?e.documentElement:e.firstChild:null)||1!==t.nodeType||!t.hasAttribute("data-reactroot"))),!t)for(var n;n=e.lastChild;)e.removeChild(n);return new jl(e,0,t?{hydrate:!0}:void 0)}(n,r),u=o._internalRoot,"function"===typeof i){var l=i;i=function(){var e=Ul(u);l.call(e)}}tl((function(){Ll(t,u,e,i)}))}return Ul(u)}function $l(e,t,n){var r=3<arguments.length&&void 0!==arguments[3]?arguments[3]:null;return{$$typeof:te,key:null==r?null:""+r,children:e,containerInfo:t,implementation:n}}function Wl(e,t){var n=2<arguments.length&&void 0!==arguments[2]?arguments[2]:null;if(!Hl(t))throw Error(u(200));return $l(e,t,null,n)}jl.prototype.render=function(e){Ll(e,this._internalRoot,null,null)},jl.prototype.unmount=function(){var e=this._internalRoot,t=e.containerInfo;Ll(null,e,null,(function(){t[Cn]=null}))},mt=function(e){if(13===e.tag){var t=Bi(qa(),150,100);Ya(e,t),Il(e,t)}},gt=function(e){13===e.tag&&(Ya(e,3),Il(e,3))},vt=function(e){if(13===e.tag){var t=qa();Ya(e,t=Ba(t,e,null)),Il(e,t)}},M=function(e,t,n){switch(t){case"input":if(Ee(e,n),t=n.name,"radio"===n.type&&null!=t){for(n=e;n.parentNode;)n=n.parentNode;for(n=n.querySelectorAll("input[name="+JSON.stringify(""+t)+'][type="radio"]'),t=0;t<n.length;t++){var r=n[t];if(r!==e&&r.form===e.form){var i=Pn(r);if(!i)throw Error(u(90));xe(r),Ee(r,i)}}}break;case"textarea":Re(e,n);break;case"select":null!=(t=n.value)&&Ne(e,!!n.multiple,t,!1)}},O=el,z=function(e,t,n,r,i){var o=ka;ka|=4;try{return Vi(98,e.bind(null,t,n,r,i))}finally{0===(ka=o)&&Qi()}},L=function(){0===(49&ka)&&(function(){if(null!==Va){var e=Va;Va=null,e.forEach((function(e,t){zl(t,e),Za(t)})),Qi()}}(),ml())},U=function(e,t){var n=ka;ka|=2;try{return e(t)}finally{0===(ka=n)&&Qi()}};var Ql={Events:[Mn,Nn,Pn,C,k,Un,function(e){it(e,Ln)},R,D,Zt,at,ml,{current:!1}]};!function(e){var t=e.findFiberByHostInstance;(function(e){if("undefined"===typeof __REACT_DEVTOOLS_GLOBAL_HOOK__)return!1;var t=__REACT_DEVTOOLS_GLOBAL_HOOK__;if(t.isDisabled||!t.supportsFiber)return!0;try{var n=t.inject(e);xl=function(e){try{t.onCommitFiberRoot(n,e,void 0,64===(64&e.current.effectTag))}catch(r){}},_l=function(e){try{t.onCommitFiberUnmount(n,e)}catch(r){}}}catch(r){}})(i({},e,{overrideHookState:null,overrideProps:null,setSuspenseHandler:null,scheduleUpdate:null,currentDispatcherRef:K.ReactCurrentDispatcher,findHostInstanceByFiber:function(e){return null===(e=nt(e))?null:e.stateNode},findFiberByHostInstance:function(e){return t?t(e):null},findHostInstancesForRefresh:null,scheduleRefresh:null,scheduleRoot:null,setRefreshHandler:null,getCurrentFiber:null}))}({findFiberByHostInstance:Sn,bundleType:0,version:"16.13.1",rendererPackageName:"react-dom"}),t.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED=Ql,t.createPortal=Wl,t.findDOMNode=function(e){if(null==e)return null;if(1===e.nodeType)return e;var t=e._reactInternalFiber;if(void 0===t){if("function"===typeof e.render)throw Error(u(188));throw Error(u(268,Object.keys(e)))}return e=null===(e=nt(t))?null:e.stateNode},t.flushSync=function(e,t){if(0!==(48&ka))throw Error(u(187));var n=ka;ka|=1;try{return Vi(99,e.bind(null,t))}finally{ka=n,Qi()}},t.hydrate=function(e,t,n){if(!Hl(t))throw Error(u(200));return Vl(null,e,t,!0,n)},t.render=function(e,t,n){if(!Hl(t))throw Error(u(200));return Vl(null,e,t,!1,n)},t.unmountComponentAtNode=function(e){if(!Hl(e))throw Error(u(40));return!!e._reactRootContainer&&(tl((function(){Vl(null,null,e,!1,(function(){e._reactRootContainer=null,e[Cn]=null}))})),!0)},t.unstable_batchedUpdates=el,t.unstable_createPortal=function(e,t){return Wl(e,t,2<arguments.length&&void 0!==arguments[2]?arguments[2]:null)},t.unstable_renderSubtreeIntoContainer=function(e,t,n,r){if(!Hl(n))throw Error(u(200));if(null==e||void 0===e._reactInternalFiber)throw Error(u(38));return Vl(e,t,n,!1,r)},t.version="16.13.1"},function(e,t,n){"use strict";e.exports=n(32)},function(e,t,n){"use strict";var r,i,o,u,a;if("undefined"===typeof window||"function"!==typeof MessageChannel){var l=null,c=null,s=function e(){if(null!==l)try{var n=t.unstable_now();l(!0,n),l=null}catch(r){throw setTimeout(e,0),r}},f=Date.now();t.unstable_now=function(){return Date.now()-f},r=function(e){null!==l?setTimeout(r,0,e):(l=e,setTimeout(s,0))},i=function(e,t){c=setTimeout(e,t)},o=function(){clearTimeout(c)},u=function(){return!1},a=t.unstable_forceFrameRate=function(){}}else{var d=window.performance,p=window.Date,h=window.setTimeout,m=window.clearTimeout;if("undefined"!==typeof console){var g=window.cancelAnimationFrame;"function"!==typeof window.requestAnimationFrame&&console.error("This browser doesn't support requestAnimationFrame. Make sure that you load a polyfill in older browsers. https://fb.me/react-polyfills"),"function"!==typeof g&&console.error("This browser doesn't support cancelAnimationFrame. Make sure that you load a polyfill in older browsers. https://fb.me/react-polyfills")}if("object"===typeof d&&"function"===typeof d.now)t.unstable_now=function(){return d.now()};else{var v=p.now();t.unstable_now=function(){return p.now()-v}}var y=!1,b=null,w=-1,x=5,_=0;u=function(){return t.unstable_now()>=_},a=function(){},t.unstable_forceFrameRate=function(e){0>e||125<e?console.error("forceFrameRate takes a positive int between 0 and 125, forcing framerates higher than 125 fps is not unsupported"):x=0<e?Math.floor(1e3/e):5};var k=new MessageChannel,T=k.port2;k.port1.onmessage=function(){if(null!==b){var e=t.unstable_now();_=e+x;try{b(!0,e)?T.postMessage(null):(y=!1,b=null)}catch(n){throw T.postMessage(null),n}}else y=!1},r=function(e){b=e,y||(y=!0,T.postMessage(null))},i=function(e,n){w=h((function(){e(t.unstable_now())}),n)},o=function(){m(w),w=-1}}function E(e,t){var n=e.length;e.push(t);e:for(;;){var r=n-1>>>1,i=e[r];if(!(void 0!==i&&0<M(i,t)))break e;e[r]=t,e[n]=i,n=r}}function C(e){return void 0===(e=e[0])?null:e}function S(e){var t=e[0];if(void 0!==t){var n=e.pop();if(n!==t){e[0]=n;e:for(var r=0,i=e.length;r<i;){var o=2*(r+1)-1,u=e[o],a=o+1,l=e[a];if(void 0!==u&&0>M(u,n))void 0!==l&&0>M(l,u)?(e[r]=l,e[a]=n,r=a):(e[r]=u,e[o]=n,r=o);else{if(!(void 0!==l&&0>M(l,n)))break e;e[r]=l,e[a]=n,r=a}}}return t}return null}function M(e,t){var n=e.sortIndex-t.sortIndex;return 0!==n?n:e.id-t.id}var N=[],P=[],A=1,R=null,D=3,O=!1,z=!1,L=!1;function U(e){for(var t=C(P);null!==t;){if(null===t.callback)S(P);else{if(!(t.startTime<=e))break;S(P),t.sortIndex=t.expirationTime,E(N,t)}t=C(P)}}function F(e){if(L=!1,U(e),!z)if(null!==C(N))z=!0,r(I);else{var t=C(P);null!==t&&i(F,t.startTime-e)}}function I(e,n){z=!1,L&&(L=!1,o()),O=!0;var r=D;try{for(U(n),R=C(N);null!==R&&(!(R.expirationTime>n)||e&&!u());){var a=R.callback;if(null!==a){R.callback=null,D=R.priorityLevel;var l=a(R.expirationTime<=n);n=t.unstable_now(),"function"===typeof l?R.callback=l:R===C(N)&&S(N),U(n)}else S(N);R=C(N)}if(null!==R)var c=!0;else{var s=C(P);null!==s&&i(F,s.startTime-n),c=!1}return c}finally{R=null,D=r,O=!1}}function j(e){switch(e){case 1:return-1;case 2:return 250;case 5:return 1073741823;case 4:return 1e4;default:return 5e3}}var H=a;t.unstable_IdlePriority=5,t.unstable_ImmediatePriority=1,t.unstable_LowPriority=4,t.unstable_NormalPriority=3,t.unstable_Profiling=null,t.unstable_UserBlockingPriority=2,t.unstable_cancelCallback=function(e){e.callback=null},t.unstable_continueExecution=function(){z||O||(z=!0,r(I))},t.unstable_getCurrentPriorityLevel=function(){return D},t.unstable_getFirstCallbackNode=function(){return C(N)},t.unstable_next=function(e){switch(D){case 1:case 2:case 3:var t=3;break;default:t=D}var n=D;D=t;try{return e()}finally{D=n}},t.unstable_pauseExecution=function(){},t.unstable_requestPaint=H,t.unstable_runWithPriority=function(e,t){switch(e){case 1:case 2:case 3:case 4:case 5:break;default:e=3}var n=D;D=e;try{return t()}finally{D=n}},t.unstable_scheduleCallback=function(e,n,u){var a=t.unstable_now();if("object"===typeof u&&null!==u){var l=u.delay;l="number"===typeof l&&0<l?a+l:a,u="number"===typeof u.timeout?u.timeout:j(e)}else u=j(e),l=a;return e={id:A++,callback:n,priorityLevel:e,startTime:l,expirationTime:u=l+u,sortIndex:-1},l>a?(e.sortIndex=l,E(P,e),null===C(N)&&e===C(P)&&(L?o():L=!0,i(F,l-a))):(e.sortIndex=u,E(N,e),z||O||(z=!0,r(I))),e},t.unstable_shouldYield=function(){var e=t.unstable_now();U(e);var n=C(N);return n!==R&&null!==R&&null!==n&&null!==n.callback&&n.startTime<=e&&n.expirationTime<R.expirationTime||u()},t.unstable_wrapCallback=function(e){var t=D;return function(){var n=D;D=t;try{return e.apply(this,arguments)}finally{D=n}}}}]]);

(this["webpackJsonpvisualisation-d3"]=this["webpackJsonpvisualisation-d3"]||[]).push([[0],{18:function(e,t,a){},28:function(e,t,a){e.exports=a(33)},33:function(e,t,a){"use strict";a.r(t);var r=a(0),n=a.n(r),i=a(8),c=a(2),l=(a(18),a(9)),o=a(10),s=a(11),y=a(13),m=a(12),p=function(e){Object(y.a)(a,e);var t=Object(m.a)(a);function a(){return Object(l.a)(this,a),t.apply(this,arguments)}return Object(o.a)(a,[{key:"render",value:function(){var e="0 0 ".concat(500," ").concat(900),t={paddingBottom:"180%"},a={stroke:"rgba(70, 130, 180,.3)",strokeWidth:1},r={stroke:"rgba(70, 130, 180,.7)",strokeWidth:1},i={stroke:"rgba(70, 130, 180,.7)",strokeWidth:2},l=this.props.data_terms,o=(this.props.itemsDisplay,this.props.data_weight),s=this.props.cat,y=l.length,m=75+27*y,p=230/c.b(l,(function(e){return+Math.round(e.Total)})),u=c.e().domain([c.c(o,(function(e){return+e.proportion})),c.b(o,(function(e){return+e.proportion}))]).range([5,17]);return n.a.createElement(n.a.Fragment,null,n.a.createElement("div",{className:"svg-container",style:t},n.a.createElement("svg",{preserveAspectRatio:"xMidYMidmeet",viewBox:e,className:"svg-content-responsive"},n.a.createElement("line",{x1:5,x2:495,y1:5,y2:5,style:i}),n.a.createElement("text",{className:"svg_h1",x:12,y:50},"Topic weight"),n.a.createElement("text",{className:"svg_h1",x:110,y:50},"Default"==s?"Term frequency - All Topics":"Term frequency - "+s," "),n.a.createElement("line",{x1:5,x2:495,y1:55,y2:55,style:r}),n.a.createElement("line",{x1:5,x2:495,y1:m,y2:m,style:i}),n.a.createElement("rect",{x:345,y:24,width:"12",height:"10",fill:"rgba(243, 157, 65,1)"}),n.a.createElement("text",{x:365,y:34,className:"svg_text_2"},"Frequency within topic"),n.a.createElement("rect",{x:345,y:40,width:"12",height:"10",fill:"rgba(70, 130, 180,.7)"}),n.a.createElement("text",{x:365,y:50,className:"svg_text_2"},"Overall term frequency"),n.a.createElement("line",{x1:230,x2:230,y1:55,y2:m,style:a}),n.a.createElement("line",{x1:5,x2:5,y1:5,y2:m,style:i}),n.a.createElement("line",{x1:100,x2:100,y1:5,y2:m,style:i}),n.a.createElement("line",{x1:495,x2:495,y1:5,y2:m,style:i}),o.map((function(e,t){return n.a.createElement(n.a.Fragment,{key:"frag_"+t},n.a.createElement("line",{key:t+"_1_",x1:5,x2:100,y1:135+80*t,y2:135+80*t,style:r}),n.a.createElement("text",{key:t+"2_",x:50,y:87+80*t,className:"svg_h2"},"Topic "+(+e.topic_id+1)),n.a.createElement("circle",{key:t+"_c",cx:25,cy:95+80*t,r:u(e.proportion),fill:"Topic"+(+e.topic_id+1)==s?"rgba(243, 157, 65,.8)":"rgba(70, 130, 180,.8)"}),n.a.createElement("text",{key:t+"_prc",x:55,y:102+80*t,className:"svg_h1"},Math.round(100*e.proportion,2)+"%"))})),l.map((function(e,t){return n.a.createElement(n.a.Fragment,{key:"frag_2_"+t},n.a.createElement("text",{key:t+"_term_",x:110,y:90+27*t,className:"svg_text"},e.Term),n.a.createElement("line",{key:t+"_l",x1:105,x2:495,y1:91+27*t,y2:91+27*t,style:a}),n.a.createElement("rect",{key:t+"_r1",x:230,y:"Default"==e.Category?80+27*t:82+27*t,width:+e.Total*p,height:"Default"==e.Category?"12":"8",fill:"rgba(70, 130, 180,.8)"}),n.a.createElement("text",{key:t+"_tot_",x:232+ +e.Total*p,y:89+27*t,className:"svg_text_3"},"Default"==e.Category?Number(e.Total).toFixed(0):Number(e.Total).toFixed(1)),n.a.createElement("rect",{key:t+"_r2",x:230,y:72+27*t,width:+e.Freq*p,height:"8",fill:"Default"==e.Category?"rgba(70, 130, 180,0)":"rgba(243, 157, 65,.8)"}),n.a.createElement("text",{key:t+"_tot2_",x:232+ +e.Freq*p,y:79+27*t,className:"svg_text_3",fill:"Default"==e.Category?"rgba(70, 130, 180,0)":"rgb(0, 0, 0)"},Number(e.Freq).toFixed(1)))})))))}}]),a}(n.a.Component),u=function(e){Object(y.a)(a,e);var t=Object(m.a)(a);function a(e){var r;return Object(l.a)(this,a),(r=t.call(this,e)).state={Category:"Default",Color:"rgba(70, 130, 180,.8)"},r.handleChange=r.handleChange.bind(Object(s.a)(r)),r}return Object(o.a)(a,[{key:"handleChange",value:function(e){this.setState({Category:e.target.value})}},{key:"render",value:function(){var e=this.props.topic_info,t=this.props.topic_proportion,a=c.d().key((function(e){return e.Category})).entries(e),r=this.state.Category,i=e.filter((function(e){return e.Category===r})).slice(0,30),l=t;return n.a.createElement(n.a.Fragment,null,n.a.createElement("div",{className:"select_box"},n.a.createElement("select",{value:this.state.Category,onChange:this.handleChange},a.map((function(e){return n.a.createElement("option",{key:e.key},e.key)})))),n.a.createElement("div",{className:"main_container_svg"},n.a.createElement(p,{data_terms:i,data_weight:l,itemsDisplay:30,cat:this.state.Category})))}}]),a}(n.a.Component);Promise.all([c.a("/nbextensions/topic_info.csv"),c.a("/nbextensions/topic_proportion.csv")]).then((function(e){var t=e[0],a=e[1],r=document.getElementById("vis_tm_1");r.hasChildNodes()?Object(i.hydrate)(n.a.createElement(u,{topic_info:t,topic_proportion:a}),r):Object(i.render)(n.a.createElement(u,{topic_info:t,topic_proportion:a}),r)})).catch((function(e){}))}},[[28,1,2]]]);

# kiara\TopicModelling-\vis-files\tm_1\jscode-test.js
!function(e){function t(t){for(var n,i,l=t[0],a=t[1],f=t[2],c=0,s=[];c<l.length;c++)i=l[c],Object.prototype.hasOwnProperty.call(o,i)&&o[i]&&s.push(o[i][0]),o[i]=0;for(n in a)Object.prototype.hasOwnProperty.call(a,n)&&(e[n]=a[n]);for(p&&p(t);s.length;)s.shift()();return u.push.apply(u,f||[]),r()}function r(){for(var e,t=0;t<u.length;t++){for(var r=u[t],n=!0,l=1;l<r.length;l++){var a=r[l];0!==o[a]&&(n=!1)}n&&(u.splice(t--,1),e=i(i.s=r[0]))}return e}var n={},o={1:0},u=[];function i(t){if(n[t])return n[t].exports;var r=n[t]={i:t,l:!1,exports:{}};return e[t].call(r.exports,r,r.exports,i),r.l=!0,r.exports}i.m=e,i.c=n,i.d=function(e,t,r){i.o(e,t)||Object.defineProperty(e,t,{enumerable:!0,get:r})},i.r=function(e){"undefined"!=typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(e,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(e,"__esModule",{value:!0})},i.t=function(e,t){if(1&t&&(e=i(e)),8&t)return e;if(4&t&&"object"==typeof e&&e&&e.__esModule)return e;var r=Object.create(null);if(i.r(r),Object.defineProperty(r,"default",{enumerable:!0,value:e}),2&t&&"string"!=typeof e)for(var n in e)i.d(r,n,function(t){return e[t]}.bind(null,n));return r},i.n=function(e){var t=e&&e.__esModule?function(){return e.default}:function(){return e};return i.d(t,"a",t),t},i.o=function(e,t){return Object.prototype.hasOwnProperty.call(e,t)},i.p="/";var l=this["webpackJsonpvisualisation-d3"]=this["webpackJsonpvisualisation-d3"]||[],a=l.push.bind(l);l.push=t,l=l.slice();for(var f=0;f<l.length;f++)t(l[f]);var p=a;r()}([]);

(this["webpackJsonpvisualisation-d3"]=this["webpackJsonpvisualisation-d3"]||[]).push([[2],[function(e,t,n){"use strict";e.exports=n(29)},,function(e,t,n){"use strict";n.d(t,"b",(function(){return m})),n.d(t,"c",(function(){return g})),n.d(t,"d",(function(){return Zn})),n.d(t,"a",(function(){return br})),n.d(t,"e",(function(){return pi}));var r=function(e,t){return e<t?-1:e>t?1:e>=t?0:NaN},i=function(e){var t;return 1===e.length&&(t=e,e=function(e,n){return r(t(e),n)}),{left:function(t,n,r,i){for(null==r&&(r=0),null==i&&(i=t.length);r<i;){var o=r+i>>>1;e(t[o],n)<0?r=o+1:i=o}return r},right:function(t,n,r,i){for(null==r&&(r=0),null==i&&(i=t.length);r<i;){var o=r+i>>>1;e(t[o],n)>0?i=o:r=o+1}return r}}};var o=i(r),u=o.right,a=(o.left,u);var l=Array.prototype,c=(l.slice,l.map,Math.sqrt(50)),s=Math.sqrt(10),f=Math.sqrt(2),d=function(e,t,n){var r,i,o,u,a=-1;if(n=+n,(e=+e)===(t=+t)&&n>0)return[e];if((r=t<e)&&(i=e,e=t,t=i),0===(u=p(e,t,n))||!isFinite(u))return[];if(u>0)for(e=Math.ceil(e/u),t=Math.floor(t/u),o=new Array(i=Math.ceil(t-e+1));++a<i;)o[a]=(e+a)*u;else for(e=Math.floor(e*u),t=Math.ceil(t*u),o=new Array(i=Math.ceil(e-t+1));++a<i;)o[a]=(e-a)/u;return r&&o.reverse(),o};function p(e,t,n){var r=(t-e)/Math.max(0,n),i=Math.floor(Math.log(r)/Math.LN10),o=r/Math.pow(10,i);return i>=0?(o>=c?10:o>=s?5:o>=f?2:1)*Math.pow(10,i):-Math.pow(10,-i)/(o>=c?10:o>=s?5:o>=f?2:1)}function h(e,t,n){var r=Math.abs(t-e)/Math.max(0,n),i=Math.pow(10,Math.floor(Math.log(r)/Math.LN10)),o=r/i;return o>=c?i*=10:o>=s?i*=5:o>=f&&(i*=2),t<e?-i:i}var m=function(e,t){var n,r,i=e.length,o=-1;if(null==t){for(;++o<i;)if(null!=(n=e[o])&&n>=n)for(r=n;++o<i;)null!=(n=e[o])&&n>r&&(r=n)}else for(;++o<i;)if(null!=(n=t(e[o],o,e))&&n>=n)for(r=n;++o<i;)null!=(n=t(e[o],o,e))&&n>r&&(r=n);return r},g=function(e,t){var n,r,i=e.length,o=-1;if(null==t){for(;++o<i;)if(null!=(n=e[o])&&n>=n)for(r=n;++o<i;)null!=(n=e[o])&&r>n&&(r=n)}else for(;++o<i;)if(null!=(n=t(e[o],o,e))&&n>=n)for(r=n;++o<i;)null!=(n=t(e[o],o,e))&&r>n&&(r=n);return r};Array.prototype.slice;var v={value:function(){}};function y(){for(var e,t=0,n=arguments.length,r={};t<n;++t){if(!(e=arguments[t]+"")||e in r||/[\s.]/.test(e))throw new Error("illegal type: "+e);r[e]=[]}return new b(r)}function b(e){this._=e}function w(e,t){return e.trim().split(/^|\s+/).map((function(e){var n="",r=e.indexOf(".");if(r>=0&&(n=e.slice(r+1),e=e.slice(0,r)),e&&!t.hasOwnProperty(e))throw new Error("unknown type: "+e);return{type:e,name:n}}))}function x(e,t){for(var n,r=0,i=e.length;r<i;++r)if((n=e[r]).name===t)return n.value}function _(e,t,n){for(var r=0,i=e.length;r<i;++r)if(e[r].name===t){e[r]=v,e=e.slice(0,r).concat(e.slice(r+1));break}return null!=n&&e.push({name:t,value:n}),e}b.prototype=y.prototype={constructor:b,on:function(e,t){var n,r=this._,i=w(e+"",r),o=-1,u=i.length;if(!(arguments.length<2)){if(null!=t&&"function"!==typeof t)throw new Error("invalid callback: "+t);for(;++o<u;)if(n=(e=i[o]).type)r[n]=_(r[n],e.name,t);else if(null==t)for(n in r)r[n]=_(r[n],e.name,null);return this}for(;++o<u;)if((n=(e=i[o]).type)&&(n=x(r[n],e.name)))return n},copy:function(){var e={},t=this._;for(var n in t)e[n]=t[n].slice();return new b(e)},call:function(e,t){if((n=arguments.length-2)>0)for(var n,r,i=new Array(n),o=0;o<n;++o)i[o]=arguments[o+2];if(!this._.hasOwnProperty(e))throw new Error("unknown type: "+e);for(o=0,n=(r=this._[e]).length;o<n;++o)r[o].value.apply(t,i)},apply:function(e,t,n){if(!this._.hasOwnProperty(e))throw new Error("unknown type: "+e);for(var r=this._[e],i=0,o=r.length;i<o;++i)r[i].value.apply(t,n)}};var k=y;function T(){}var E=function(e){return null==e?T:function(){return this.querySelector(e)}};function C(){return[]}var S=function(e){return null==e?C:function(){return this.querySelectorAll(e)}},M=function(e){return function(){return this.matches(e)}},N=function(e){return new Array(e.length)};function P(e,t){this.ownerDocument=e.ownerDocument,this.namespaceURI=e.namespaceURI,this._next=null,this._parent=e,this.__data__=t}P.prototype={constructor:P,appendChild:function(e){return this._parent.insertBefore(e,this._next)},insertBefore:function(e,t){return this._parent.insertBefore(e,t)},querySelector:function(e){return this._parent.querySelector(e)},querySelectorAll:function(e){return this._parent.querySelectorAll(e)}};function A(e,t,n,r,i,o){for(var u,a=0,l=t.length,c=o.length;a<c;++a)(u=t[a])?(u.__data__=o[a],r[a]=u):n[a]=new P(e,o[a]);for(;a<l;++a)(u=t[a])&&(i[a]=u)}function R(e,t,n,r,i,o,u){var a,l,c,s={},f=t.length,d=o.length,p=new Array(f);for(a=0;a<f;++a)(l=t[a])&&(p[a]=c="$"+u.call(l,l.__data__,a,t),c in s?i[a]=l:s[c]=l);for(a=0;a<d;++a)(l=s[c="$"+u.call(e,o[a],a,o)])?(r[a]=l,l.__data__=o[a],s[c]=null):n[a]=new P(e,o[a]);for(a=0;a<f;++a)(l=t[a])&&s[p[a]]===l&&(i[a]=l)}function D(e,t){return e<t?-1:e>t?1:e>=t?0:NaN}var O="http://www.w3.org/1999/xhtml",z={svg:"http://www.w3.org/2000/svg",xhtml:O,xlink:"http://www.w3.org/1999/xlink",xml:"http://www.w3.org/XML/1998/namespace",xmlns:"http://www.w3.org/2000/xmlns/"},L=function(e){var t=e+="",n=t.indexOf(":");return n>=0&&"xmlns"!==(t=e.slice(0,n))&&(e=e.slice(n+1)),z.hasOwnProperty(t)?{space:z[t],local:e}:e};function U(e){return function(){this.removeAttribute(e)}}function F(e){return function(){this.removeAttributeNS(e.space,e.local)}}function I(e,t){return function(){this.setAttribute(e,t)}}function j(e,t){return function(){this.setAttributeNS(e.space,e.local,t)}}function H(e,t){return function(){var n=t.apply(this,arguments);null==n?this.removeAttribute(e):this.setAttribute(e,n)}}function V(e,t){return function(){var n=t.apply(this,arguments);null==n?this.removeAttributeNS(e.space,e.local):this.setAttributeNS(e.space,e.local,n)}}var $=function(e){return e.ownerDocument&&e.ownerDocument.defaultView||e.document&&e||e.defaultView};function W(e){return function(){this.style.removeProperty(e)}}function Q(e,t,n){return function(){this.style.setProperty(e,t,n)}}function q(e,t,n){return function(){var r=t.apply(this,arguments);null==r?this.style.removeProperty(e):this.style.setProperty(e,r,n)}}function B(e,t){return e.style.getPropertyValue(t)||$(e).getComputedStyle(e,null).getPropertyValue(t)}function Y(e){return function(){delete this[e]}}function X(e,t){return function(){this[e]=t}}function K(e,t){return function(){var n=t.apply(this,arguments);null==n?delete this[e]:this[e]=n}}function Z(e){return e.trim().split(/^|\s+/)}function J(e){return e.classList||new G(e)}function G(e){this._node=e,this._names=Z(e.getAttribute("class")||"")}function ee(e,t){for(var n=J(e),r=-1,i=t.length;++r<i;)n.add(t[r])}function te(e,t){for(var n=J(e),r=-1,i=t.length;++r<i;)n.remove(t[r])}function ne(e){return function(){ee(this,e)}}function re(e){return function(){te(this,e)}}function ie(e,t){return function(){(t.apply(this,arguments)?ee:te)(this,e)}}G.prototype={add:function(e){this._names.indexOf(e)<0&&(this._names.push(e),this._node.setAttribute("class",this._names.join(" ")))},remove:function(e){var t=this._names.indexOf(e);t>=0&&(this._names.splice(t,1),this._node.setAttribute("class",this._names.join(" ")))},contains:function(e){return this._names.indexOf(e)>=0}};function oe(){this.textContent=""}function ue(e){return function(){this.textContent=e}}function ae(e){return function(){var t=e.apply(this,arguments);this.textContent=null==t?"":t}}function le(){this.innerHTML=""}function ce(e){return function(){this.innerHTML=e}}function se(e){return function(){var t=e.apply(this,arguments);this.innerHTML=null==t?"":t}}function fe(){this.nextSibling&&this.parentNode.appendChild(this)}function de(){this.previousSibling&&this.parentNode.insertBefore(this,this.parentNode.firstChild)}function pe(e){return function(){var t=this.ownerDocument,n=this.namespaceURI;return n===O&&t.documentElement.namespaceURI===O?t.createElement(e):t.createElementNS(n,e)}}function he(e){return function(){return this.ownerDocument.createElementNS(e.space,e.local)}}var me=function(e){var t=L(e);return(t.local?he:pe)(t)};function ge(){return null}function ve(){var e=this.parentNode;e&&e.removeChild(this)}function ye(){var e=this.cloneNode(!1),t=this.parentNode;return t?t.insertBefore(e,this.nextSibling):e}function be(){var e=this.cloneNode(!0),t=this.parentNode;return t?t.insertBefore(e,this.nextSibling):e}var we={},xe=null;"undefined"!==typeof document&&("onmouseenter"in document.documentElement||(we={mouseenter:"mouseover",mouseleave:"mouseout"}));function _e(e,t,n){return e=ke(e,t,n),function(t){var n=t.relatedTarget;n&&(n===this||8&n.compareDocumentPosition(this))||e.call(this,t)}}function ke(e,t,n){return function(r){var i=xe;xe=r;try{e.call(this,this.__data__,t,n)}finally{xe=i}}}function Te(e){return e.trim().split(/^|\s+/).map((function(e){var t="",n=e.indexOf(".");return n>=0&&(t=e.slice(n+1),e=e.slice(0,n)),{type:e,name:t}}))}function Ee(e){return function(){var t=this.__on;if(t){for(var n,r=0,i=-1,o=t.length;r<o;++r)n=t[r],e.type&&n.type!==e.type||n.name!==e.name?t[++i]=n:this.removeEventListener(n.type,n.listener,n.capture);++i?t.length=i:delete this.__on}}}function Ce(e,t,n){var r=we.hasOwnProperty(e.type)?_e:ke;return function(i,o,u){var a,l=this.__on,c=r(t,o,u);if(l)for(var s=0,f=l.length;s<f;++s)if((a=l[s]).type===e.type&&a.name===e.name)return this.removeEventListener(a.type,a.listener,a.capture),this.addEventListener(a.type,a.listener=c,a.capture=n),void(a.value=t);this.addEventListener(e.type,c,n),a={type:e.type,name:e.name,value:t,listener:c,capture:n},l?l.push(a):this.__on=[a]}}function Se(e,t,n){var r=$(e),i=r.CustomEvent;"function"===typeof i?i=new i(t,n):(i=r.document.createEvent("Event"),n?(i.initEvent(t,n.bubbles,n.cancelable),i.detail=n.detail):i.initEvent(t,!1,!1)),e.dispatchEvent(i)}function Me(e,t){return function(){return Se(this,e,t)}}function Ne(e,t){return function(){return Se(this,e,t.apply(this,arguments))}}var Pe=[null];function Ae(e,t){this._groups=e,this._parents=t}function Re(){return new Ae([[document.documentElement]],Pe)}Ae.prototype=Re.prototype={constructor:Ae,select:function(e){"function"!==typeof e&&(e=E(e));for(var t=this._groups,n=t.length,r=new Array(n),i=0;i<n;++i)for(var o,u,a=t[i],l=a.length,c=r[i]=new Array(l),s=0;s<l;++s)(o=a[s])&&(u=e.call(o,o.__data__,s,a))&&("__data__"in o&&(u.__data__=o.__data__),c[s]=u);return new Ae(r,this._parents)},selectAll:function(e){"function"!==typeof e&&(e=S(e));for(var t=this._groups,n=t.length,r=[],i=[],o=0;o<n;++o)for(var u,a=t[o],l=a.length,c=0;c<l;++c)(u=a[c])&&(r.push(e.call(u,u.__data__,c,a)),i.push(u));return new Ae(r,i)},filter:function(e){"function"!==typeof e&&(e=M(e));for(var t=this._groups,n=t.length,r=new Array(n),i=0;i<n;++i)for(var o,u=t[i],a=u.length,l=r[i]=[],c=0;c<a;++c)(o=u[c])&&e.call(o,o.__data__,c,u)&&l.push(o);return new Ae(r,this._parents)},data:function(e,t){if(!e)return h=new Array(this.size()),s=-1,this.each((function(e){h[++s]=e})),h;var n,r=t?R:A,i=this._parents,o=this._groups;"function"!==typeof e&&(n=e,e=function(){return n});for(var u=o.length,a=new Array(u),l=new Array(u),c=new Array(u),s=0;s<u;++s){var f=i[s],d=o[s],p=d.length,h=e.call(f,f&&f.__data__,s,i),m=h.length,g=l[s]=new Array(m),v=a[s]=new Array(m);r(f,d,g,v,c[s]=new Array(p),h,t);for(var y,b,w=0,x=0;w<m;++w)if(y=g[w]){for(w>=x&&(x=w+1);!(b=v[x])&&++x<m;);y._next=b||null}}return(a=new Ae(a,i))._enter=l,a._exit=c,a},enter:function(){return new Ae(this._enter||this._groups.map(N),this._parents)},exit:function(){return new Ae(this._exit||this._groups.map(N),this._parents)},join:function(e,t,n){var r=this.enter(),i=this,o=this.exit();return r="function"===typeof e?e(r):r.append(e+""),null!=t&&(i=t(i)),null==n?o.remove():n(o),r&&i?r.merge(i).order():i},merge:function(e){for(var t=this._groups,n=e._groups,r=t.length,i=n.length,o=Math.min(r,i),u=new Array(r),a=0;a<o;++a)for(var l,c=t[a],s=n[a],f=c.length,d=u[a]=new Array(f),p=0;p<f;++p)(l=c[p]||s[p])&&(d[p]=l);for(;a<r;++a)u[a]=t[a];return new Ae(u,this._parents)},order:function(){for(var e=this._groups,t=-1,n=e.length;++t<n;)for(var r,i=e[t],o=i.length-1,u=i[o];--o>=0;)(r=i[o])&&(u&&4^r.compareDocumentPosition(u)&&u.parentNode.insertBefore(r,u),u=r);return this},sort:function(e){function t(t,n){return t&&n?e(t.__data__,n.__data__):!t-!n}e||(e=D);for(var n=this._groups,r=n.length,i=new Array(r),o=0;o<r;++o){for(var u,a=n[o],l=a.length,c=i[o]=new Array(l),s=0;s<l;++s)(u=a[s])&&(c[s]=u);c.sort(t)}return new Ae(i,this._parents).order()},call:function(){var e=arguments[0];return arguments[0]=this,e.apply(null,arguments),this},nodes:function(){var e=new Array(this.size()),t=-1;return this.each((function(){e[++t]=this})),e},node:function(){for(var e=this._groups,t=0,n=e.length;t<n;++t)for(var r=e[t],i=0,o=r.length;i<o;++i){var u=r[i];if(u)return u}return null},size:function(){var e=0;return this.each((function(){++e})),e},empty:function(){return!this.node()},each:function(e){for(var t=this._groups,n=0,r=t.length;n<r;++n)for(var i,o=t[n],u=0,a=o.length;u<a;++u)(i=o[u])&&e.call(i,i.__data__,u,o);return this},attr:function(e,t){var n=L(e);if(arguments.length<2){var r=this.node();return n.local?r.getAttributeNS(n.space,n.local):r.getAttribute(n)}return this.each((null==t?n.local?F:U:"function"===typeof t?n.local?V:H:n.local?j:I)(n,t))},style:function(e,t,n){return arguments.length>1?this.each((null==t?W:"function"===typeof t?q:Q)(e,t,null==n?"":n)):B(this.node(),e)},property:function(e,t){return arguments.length>1?this.each((null==t?Y:"function"===typeof t?K:X)(e,t)):this.node()[e]},classed:function(e,t){var n=Z(e+"");if(arguments.length<2){for(var r=J(this.node()),i=-1,o=n.length;++i<o;)if(!r.contains(n[i]))return!1;return!0}return this.each(("function"===typeof t?ie:t?ne:re)(n,t))},text:function(e){return arguments.length?this.each(null==e?oe:("function"===typeof e?ae:ue)(e)):this.node().textContent},html:function(e){return arguments.length?this.each(null==e?le:("function"===typeof e?se:ce)(e)):this.node().innerHTML},raise:function(){return this.each(fe)},lower:function(){return this.each(de)},append:function(e){var t="function"===typeof e?e:me(e);return this.select((function(){return this.appendChild(t.apply(this,arguments))}))},insert:function(e,t){var n="function"===typeof e?e:me(e),r=null==t?ge:"function"===typeof t?t:E(t);return this.select((function(){return this.insertBefore(n.apply(this,arguments),r.apply(this,arguments)||null)}))},remove:function(){return this.each(ve)},clone:function(e){return this.select(e?be:ye)},datum:function(e){return arguments.length?this.property("__data__",e):this.node().__data__},on:function(e,t,n){var r,i,o=Te(e+""),u=o.length;if(!(arguments.length<2)){for(a=t?Ce:Ee,null==n&&(n=!1),r=0;r<u;++r)this.each(a(o[r],t,n));return this}var a=this.node().__on;if(a)for(var l,c=0,s=a.length;c<s;++c)for(r=0,l=a[c];r<u;++r)if((i=o[r]).type===l.type&&i.name===l.name)return l.value},dispatch:function(e,t){return this.each(("function"===typeof t?Ne:Me)(e,t))}};var De=Re;var Oe=function(e,t,n){e.prototype=t.prototype=n,n.constructor=e};function ze(e,t){var n=Object.create(e.prototype);for(var r in t)n[r]=t[r];return n}function Le(){}var Ue="\\s*([+-]?\\d+)\\s*",Fe="\\s*([+-]?\\d*\\.?\\d+(?:[eE][+-]?\\d+)?)\\s*",Ie="\\s*([+-]?\\d*\\.?\\d+(?:[eE][+-]?\\d+)?)%\\s*",je=/^#([0-9a-f]{3,8})$/,He=new RegExp("^rgb\\("+[Ue,Ue,Ue]+"\\)$"),Ve=new RegExp("^rgb\\("+[Ie,Ie,Ie]+"\\)$"),$e=new RegExp("^rgba\\("+[Ue,Ue,Ue,Fe]+"\\)$"),We=new RegExp("^rgba\\("+[Ie,Ie,Ie,Fe]+"\\)$"),Qe=new RegExp("^hsl\\("+[Fe,Ie,Ie]+"\\)$"),qe=new RegExp("^hsla\\("+[Fe,Ie,Ie,Fe]+"\\)$"),Be={aliceblue:15792383,antiquewhite:16444375,aqua:65535,aquamarine:8388564,azure:15794175,beige:16119260,bisque:16770244,black:0,blanchedalmond:16772045,blue:255,blueviolet:9055202,brown:10824234,burlywood:14596231,cadetblue:6266528,chartreuse:8388352,chocolate:13789470,coral:16744272,cornflowerblue:6591981,cornsilk:16775388,crimson:14423100,cyan:65535,darkblue:139,darkcyan:35723,darkgoldenrod:12092939,darkgray:11119017,darkgreen:25600,darkgrey:11119017,darkkhaki:12433259,darkmagenta:9109643,darkolivegreen:5597999,darkorange:16747520,darkorchid:10040012,darkred:9109504,darksalmon:15308410,darkseagreen:9419919,darkslateblue:4734347,darkslategray:3100495,darkslategrey:3100495,darkturquoise:52945,darkviolet:9699539,deeppink:16716947,deepskyblue:49151,dimgray:6908265,dimgrey:6908265,dodgerblue:2003199,firebrick:11674146,floralwhite:16775920,forestgreen:2263842,fuchsia:16711935,gainsboro:14474460,ghostwhite:16316671,gold:16766720,goldenrod:14329120,gray:8421504,green:32768,greenyellow:11403055,grey:8421504,honeydew:15794160,hotpink:16738740,indianred:13458524,indigo:4915330,ivory:16777200,khaki:15787660,lavender:15132410,lavenderblush:16773365,lawngreen:8190976,lemonchiffon:16775885,lightblue:11393254,lightcoral:15761536,lightcyan:14745599,lightgoldenrodyellow:16448210,lightgray:13882323,lightgreen:9498256,lightgrey:13882323,lightpink:16758465,lightsalmon:16752762,lightseagreen:2142890,lightskyblue:8900346,lightslategray:7833753,lightslategrey:7833753,lightsteelblue:11584734,lightyellow:16777184,lime:65280,limegreen:3329330,linen:16445670,magenta:16711935,maroon:8388608,mediumaquamarine:6737322,mediumblue:205,mediumorchid:12211667,mediumpurple:9662683,mediumseagreen:3978097,mediumslateblue:8087790,mediumspringgreen:64154,mediumturquoise:4772300,mediumvioletred:13047173,midnightblue:1644912,mintcream:16121850,mistyrose:16770273,moccasin:16770229,navajowhite:16768685,navy:128,oldlace:16643558,olive:8421376,olivedrab:7048739,orange:16753920,orangered:16729344,orchid:14315734,palegoldenrod:15657130,palegreen:10025880,paleturquoise:11529966,palevioletred:14381203,papayawhip:16773077,peachpuff:16767673,peru:13468991,pink:16761035,plum:14524637,powderblue:11591910,purple:8388736,rebeccapurple:6697881,red:16711680,rosybrown:12357519,royalblue:4286945,saddlebrown:9127187,salmon:16416882,sandybrown:16032864,seagreen:3050327,seashell:16774638,sienna:10506797,silver:12632256,skyblue:8900331,slateblue:6970061,slategray:7372944,slategrey:7372944,snow:16775930,springgreen:65407,steelblue:4620980,tan:13808780,teal:32896,thistle:14204888,tomato:16737095,turquoise:4251856,violet:15631086,wheat:16113331,white:16777215,whitesmoke:16119285,yellow:16776960,yellowgreen:10145074};function Ye(){return this.rgb().formatHex()}function Xe(){return this.rgb().formatRgb()}function Ke(e){var t,n;return e=(e+"").trim().toLowerCase(),(t=je.exec(e))?(n=t[1].length,t=parseInt(t[1],16),6===n?Ze(t):3===n?new tt(t>>8&15|t>>4&240,t>>4&15|240&t,(15&t)<<4|15&t,1):8===n?Je(t>>24&255,t>>16&255,t>>8&255,(255&t)/255):4===n?Je(t>>12&15|t>>8&240,t>>8&15|t>>4&240,t>>4&15|240&t,((15&t)<<4|15&t)/255):null):(t=He.exec(e))?new tt(t[1],t[2],t[3],1):(t=Ve.exec(e))?new tt(255*t[1]/100,255*t[2]/100,255*t[3]/100,1):(t=$e.exec(e))?Je(t[1],t[2],t[3],t[4]):(t=We.exec(e))?Je(255*t[1]/100,255*t[2]/100,255*t[3]/100,t[4]):(t=Qe.exec(e))?ot(t[1],t[2]/100,t[3]/100,1):(t=qe.exec(e))?ot(t[1],t[2]/100,t[3]/100,t[4]):Be.hasOwnProperty(e)?Ze(Be[e]):"transparent"===e?new tt(NaN,NaN,NaN,0):null}function Ze(e){return new tt(e>>16&255,e>>8&255,255&e,1)}function Je(e,t,n,r){return r<=0&&(e=t=n=NaN),new tt(e,t,n,r)}function Ge(e){return e instanceof Le||(e=Ke(e)),e?new tt((e=e.rgb()).r,e.g,e.b,e.opacity):new tt}function et(e,t,n,r){return 1===arguments.length?Ge(e):new tt(e,t,n,null==r?1:r)}function tt(e,t,n,r){this.r=+e,this.g=+t,this.b=+n,this.opacity=+r}function nt(){return"#"+it(this.r)+it(this.g)+it(this.b)}function rt(){var e=this.opacity;return(1===(e=isNaN(e)?1:Math.max(0,Math.min(1,e)))?"rgb(":"rgba(")+Math.max(0,Math.min(255,Math.round(this.r)||0))+", "+Math.max(0,Math.min(255,Math.round(this.g)||0))+", "+Math.max(0,Math.min(255,Math.round(this.b)||0))+(1===e?")":", "+e+")")}function it(e){return((e=Math.max(0,Math.min(255,Math.round(e)||0)))<16?"0":"")+e.toString(16)}function ot(e,t,n,r){return r<=0?e=t=n=NaN:n<=0||n>=1?e=t=NaN:t<=0&&(e=NaN),new at(e,t,n,r)}function ut(e){if(e instanceof at)return new at(e.h,e.s,e.l,e.opacity);if(e instanceof Le||(e=Ke(e)),!e)return new at;if(e instanceof at)return e;var t=(e=e.rgb()).r/255,n=e.g/255,r=e.b/255,i=Math.min(t,n,r),o=Math.max(t,n,r),u=NaN,a=o-i,l=(o+i)/2;return a?(u=t===o?(n-r)/a+6*(n<r):n===o?(r-t)/a+2:(t-n)/a+4,a/=l<.5?o+i:2-o-i,u*=60):a=l>0&&l<1?0:u,new at(u,a,l,e.opacity)}function at(e,t,n,r){this.h=+e,this.s=+t,this.l=+n,this.opacity=+r}function lt(e,t,n){return 255*(e<60?t+(n-t)*e/60:e<180?n:e<240?t+(n-t)*(240-e)/60:t)}function ct(e,t,n,r,i){var o=e*e,u=o*e;return((1-3*e+3*o-u)*t+(4-6*o+3*u)*n+(1+3*e+3*o-3*u)*r+u*i)/6}Oe(Le,Ke,{copy:function(e){return Object.assign(new this.constructor,this,e)},displayable:function(){return this.rgb().displayable()},hex:Ye,formatHex:Ye,formatHsl:function(){return ut(this).formatHsl()},formatRgb:Xe,toString:Xe}),Oe(tt,et,ze(Le,{brighter:function(e){return e=null==e?1/.7:Math.pow(1/.7,e),new tt(this.r*e,this.g*e,this.b*e,this.opacity)},darker:function(e){return e=null==e?.7:Math.pow(.7,e),new tt(this.r*e,this.g*e,this.b*e,this.opacity)},rgb:function(){return this},displayable:function(){return-.5<=this.r&&this.r<255.5&&-.5<=this.g&&this.g<255.5&&-.5<=this.b&&this.b<255.5&&0<=this.opacity&&this.opacity<=1},hex:nt,formatHex:nt,formatRgb:rt,toString:rt})),Oe(at,(function(e,t,n,r){return 1===arguments.length?ut(e):new at(e,t,n,null==r?1:r)}),ze(Le,{brighter:function(e){return e=null==e?1/.7:Math.pow(1/.7,e),new at(this.h,this.s,this.l*e,this.opacity)},darker:function(e){return e=null==e?.7:Math.pow(.7,e),new at(this.h,this.s,this.l*e,this.opacity)},rgb:function(){var e=this.h%360+360*(this.h<0),t=isNaN(e)||isNaN(this.s)?0:this.s,n=this.l,r=n+(n<.5?n:1-n)*t,i=2*n-r;return new tt(lt(e>=240?e-240:e+120,i,r),lt(e,i,r),lt(e<120?e+240:e-120,i,r),this.opacity)},displayable:function(){return(0<=this.s&&this.s<=1||isNaN(this.s))&&0<=this.l&&this.l<=1&&0<=this.opacity&&this.opacity<=1},formatHsl:function(){var e=this.opacity;return(1===(e=isNaN(e)?1:Math.max(0,Math.min(1,e)))?"hsl(":"hsla(")+(this.h||0)+", "+100*(this.s||0)+"%, "+100*(this.l||0)+"%"+(1===e?")":", "+e+")")}}));var st=function(e){return function(){return e}};function ft(e,t){return function(n){return e+n*t}}function dt(e){return 1===(e=+e)?pt:function(t,n){return n-t?function(e,t,n){return e=Math.pow(e,n),t=Math.pow(t,n)-e,n=1/n,function(r){return Math.pow(e+r*t,n)}}(t,n,e):st(isNaN(t)?n:t)}}function pt(e,t){var n=t-e;return n?ft(e,n):st(isNaN(e)?t:e)}var ht=function e(t){var n=dt(t);function r(e,t){var r=n((e=et(e)).r,(t=et(t)).r),i=n(e.g,t.g),o=n(e.b,t.b),u=pt(e.opacity,t.opacity);return function(t){return e.r=r(t),e.g=i(t),e.b=o(t),e.opacity=u(t),e+""}}return r.gamma=e,r}(1);function mt(e){return function(t){var n,r,i=t.length,o=new Array(i),u=new Array(i),a=new Array(i);for(n=0;n<i;++n)r=et(t[n]),o[n]=r.r||0,u[n]=r.g||0,a[n]=r.b||0;return o=e(o),u=e(u),a=e(a),r.opacity=1,function(e){return r.r=o(e),r.g=u(e),r.b=a(e),r+""}}}mt((function(e){var t=e.length-1;return function(n){var r=n<=0?n=0:n>=1?(n=1,t-1):Math.floor(n*t),i=e[r],o=e[r+1],u=r>0?e[r-1]:2*i-o,a=r<t-1?e[r+2]:2*o-i;return ct((n-r/t)*t,u,i,o,a)}})),mt((function(e){var t=e.length;return function(n){var r=Math.floor(((n%=1)<0?++n:n)*t),i=e[(r+t-1)%t],o=e[r%t],u=e[(r+1)%t],a=e[(r+2)%t];return ct((n-r/t)*t,i,o,u,a)}}));var gt=function(e,t){t||(t=[]);var n,r=e?Math.min(t.length,e.length):0,i=t.slice();return function(o){for(n=0;n<r;++n)i[n]=e[n]*(1-o)+t[n]*o;return i}};function vt(e){return ArrayBuffer.isView(e)&&!(e instanceof DataView)}function yt(e,t){var n,r=t?t.length:0,i=e?Math.min(r,e.length):0,o=new Array(i),u=new Array(r);for(n=0;n<i;++n)o[n]=St(e[n],t[n]);for(;n<r;++n)u[n]=t[n];return function(e){for(n=0;n<i;++n)u[n]=o[n](e);return u}}var bt=function(e,t){var n=new Date;return e=+e,t=+t,function(r){return n.setTime(e*(1-r)+t*r),n}},wt=function(e,t){return e=+e,t=+t,function(n){return e*(1-n)+t*n}},xt=function(e,t){var n,r={},i={};for(n in null!==e&&"object"===typeof e||(e={}),null!==t&&"object"===typeof t||(t={}),t)n in e?r[n]=St(e[n],t[n]):i[n]=t[n];return function(e){for(n in r)i[n]=r[n](e);return i}},_t=/[-+]?(?:\d+\.?\d*|\.?\d+)(?:[eE][-+]?\d+)?/g,kt=new RegExp(_t.source,"g");var Tt,Et,Ct=function(e,t){var n,r,i,o=_t.lastIndex=kt.lastIndex=0,u=-1,a=[],l=[];for(e+="",t+="";(n=_t.exec(e))&&(r=kt.exec(t));)(i=r.index)>o&&(i=t.slice(o,i),a[u]?a[u]+=i:a[++u]=i),(n=n[0])===(r=r[0])?a[u]?a[u]+=r:a[++u]=r:(a[++u]=null,l.push({i:u,x:wt(n,r)})),o=kt.lastIndex;return o<t.length&&(i=t.slice(o),a[u]?a[u]+=i:a[++u]=i),a.length<2?l[0]?function(e){return function(t){return e(t)+""}}(l[0].x):function(e){return function(){return e}}(t):(t=l.length,function(e){for(var n,r=0;r<t;++r)a[(n=l[r]).i]=n.x(e);return a.join("")})},St=function(e,t){var n,r=typeof t;return null==t||"boolean"===r?st(t):("number"===r?wt:"string"===r?(n=Ke(t))?(t=n,ht):Ct:t instanceof Ke?ht:t instanceof Date?bt:vt(t)?gt:Array.isArray(t)?yt:"function"!==typeof t.valueOf&&"function"!==typeof t.toString||isNaN(t)?xt:wt)(e,t)},Mt=0,Nt=0,Pt=0,At=0,Rt=0,Dt=0,Ot="object"===typeof performance&&performance.now?performance:Date,zt="object"===typeof window&&window.requestAnimationFrame?window.requestAnimationFrame.bind(window):function(e){setTimeout(e,17)};function Lt(){return Rt||(zt(Ut),Rt=Ot.now()+Dt)}function Ut(){Rt=0}function Ft(){this._call=this._time=this._next=null}function It(e,t,n){var r=new Ft;return r.restart(e,t,n),r}function jt(){Rt=(At=Ot.now())+Dt,Mt=Nt=0;try{!function(){Lt(),++Mt;for(var e,t=Tt;t;)(e=Rt-t._time)>=0&&t._call.call(null,e),t=t._next;--Mt}()}finally{Mt=0,function(){var e,t,n=Tt,r=1/0;for(;n;)n._call?(r>n._time&&(r=n._time),e=n,n=n._next):(t=n._next,n._next=null,n=e?e._next=t:Tt=t);Et=e,Vt(r)}(),Rt=0}}function Ht(){var e=Ot.now(),t=e-At;t>1e3&&(Dt-=t,At=e)}function Vt(e){Mt||(Nt&&(Nt=clearTimeout(Nt)),e-Rt>24?(e<1/0&&(Nt=setTimeout(jt,e-Ot.now()-Dt)),Pt&&(Pt=clearInterval(Pt))):(Pt||(At=Ot.now(),Pt=setInterval(Ht,1e3)),Mt=1,zt(jt)))}Ft.prototype=It.prototype={constructor:Ft,restart:function(e,t,n){if("function"!==typeof e)throw new TypeError("callback is not a function");n=(null==n?Lt():+n)+(null==t?0:+t),this._next||Et===this||(Et?Et._next=this:Tt=this,Et=this),this._call=e,this._time=n,Vt()},stop:function(){this._call&&(this._call=null,this._time=1/0,Vt())}};var $t=function(e,t,n){var r=new Ft;return t=null==t?0:+t,r.restart((function(n){r.stop(),e(n+t)}),t,n),r},Wt=k("start","end","cancel","interrupt"),Qt=[],qt=function(e,t,n,r,i,o){var u=e.__transition;if(u){if(n in u)return}else e.__transition={};!function(e,t,n){var r,i=e.__transition;function o(l){var c,s,f,d;if(1!==n.state)return a();for(c in i)if((d=i[c]).name===n.name){if(3===d.state)return $t(o);4===d.state?(d.state=6,d.timer.stop(),d.on.call("interrupt",e,e.__data__,d.index,d.group),delete i[c]):+c<t&&(d.state=6,d.timer.stop(),d.on.call("cancel",e,e.__data__,d.index,d.group),delete i[c])}if($t((function(){3===n.state&&(n.state=4,n.timer.restart(u,n.delay,n.time),u(l))})),n.state=2,n.on.call("start",e,e.__data__,n.index,n.group),2===n.state){for(n.state=3,r=new Array(f=n.tween.length),c=0,s=-1;c<f;++c)(d=n.tween[c].value.call(e,e.__data__,n.index,n.group))&&(r[++s]=d);r.length=s+1}}function u(t){for(var i=t<n.duration?n.ease.call(null,t/n.duration):(n.timer.restart(a),n.state=5,1),o=-1,u=r.length;++o<u;)r[o].call(e,i);5===n.state&&(n.on.call("end",e,e.__data__,n.index,n.group),a())}function a(){for(var r in n.state=6,n.timer.stop(),delete i[t],i)return;delete e.__transition}i[t]=n,n.timer=It((function(e){n.state=1,n.timer.restart(o,n.delay,n.time),n.delay<=e&&o(e-n.delay)}),0,n.time)}(e,n,{name:t,index:r,group:i,on:Wt,tween:Qt,time:o.time,delay:o.delay,duration:o.duration,ease:o.ease,timer:null,state:0})};function Bt(e,t){var n=Xt(e,t);if(n.state>0)throw new Error("too late; already scheduled");return n}function Yt(e,t){var n=Xt(e,t);if(n.state>3)throw new Error("too late; already running");return n}function Xt(e,t){var n=e.__transition;if(!n||!(n=n[t]))throw new Error("transition not found");return n}var Kt,Zt,Jt,Gt,en=function(e,t){var n,r,i,o=e.__transition,u=!0;if(o){for(i in t=null==t?null:t+"",o)(n=o[i]).name===t?(r=n.state>2&&n.state<5,n.state=6,n.timer.stop(),n.on.call(r?"interrupt":"cancel",e,e.__data__,n.index,n.group),delete o[i]):u=!1;u&&delete e.__transition}},tn=180/Math.PI,nn={translateX:0,translateY:0,rotate:0,skewX:0,scaleX:1,scaleY:1},rn=function(e,t,n,r,i,o){var u,a,l;return(u=Math.sqrt(e*e+t*t))&&(e/=u,t/=u),(l=e*n+t*r)&&(n-=e*l,r-=t*l),(a=Math.sqrt(n*n+r*r))&&(n/=a,r/=a,l/=a),e*r<t*n&&(e=-e,t=-t,l=-l,u=-u),{translateX:i,translateY:o,rotate:Math.atan2(t,e)*tn,skewX:Math.atan(l)*tn,scaleX:u,scaleY:a}};function on(e,t,n,r){function i(e){return e.length?e.pop()+" ":""}return function(o,u){var a=[],l=[];return o=e(o),u=e(u),function(e,r,i,o,u,a){if(e!==i||r!==o){var l=u.push("translate(",null,t,null,n);a.push({i:l-4,x:wt(e,i)},{i:l-2,x:wt(r,o)})}else(i||o)&&u.push("translate("+i+t+o+n)}(o.translateX,o.translateY,u.translateX,u.translateY,a,l),function(e,t,n,o){e!==t?(e-t>180?t+=360:t-e>180&&(e+=360),o.push({i:n.push(i(n)+"rotate(",null,r)-2,x:wt(e,t)})):t&&n.push(i(n)+"rotate("+t+r)}(o.rotate,u.rotate,a,l),function(e,t,n,o){e!==t?o.push({i:n.push(i(n)+"skewX(",null,r)-2,x:wt(e,t)}):t&&n.push(i(n)+"skewX("+t+r)}(o.skewX,u.skewX,a,l),function(e,t,n,r,o,u){if(e!==n||t!==r){var a=o.push(i(o)+"scale(",null,",",null,")");u.push({i:a-4,x:wt(e,n)},{i:a-2,x:wt(t,r)})}else 1===n&&1===r||o.push(i(o)+"scale("+n+","+r+")")}(o.scaleX,o.scaleY,u.scaleX,u.scaleY,a,l),o=u=null,function(e){for(var t,n=-1,r=l.length;++n<r;)a[(t=l[n]).i]=t.x(e);return a.join("")}}}var un=on((function(e){return"none"===e?nn:(Kt||(Kt=document.createElement("DIV"),Zt=document.documentElement,Jt=document.defaultView),Kt.style.transform=e,e=Jt.getComputedStyle(Zt.appendChild(Kt),null).getPropertyValue("transform"),Zt.removeChild(Kt),e=e.slice(7,-1).split(","),rn(+e[0],+e[1],+e[2],+e[3],+e[4],+e[5]))}),"px, ","px)","deg)"),an=on((function(e){return null==e?nn:(Gt||(Gt=document.createElementNS("http://www.w3.org/2000/svg","g")),Gt.setAttribute("transform",e),(e=Gt.transform.baseVal.consolidate())?(e=e.matrix,rn(e.a,e.b,e.c,e.d,e.e,e.f)):nn)}),", ",")",")");function ln(e,t){var n,r;return function(){var i=Yt(this,e),o=i.tween;if(o!==n)for(var u=0,a=(r=n=o).length;u<a;++u)if(r[u].name===t){(r=r.slice()).splice(u,1);break}i.tween=r}}function cn(e,t,n){var r,i;if("function"!==typeof n)throw new Error;return function(){var o=Yt(this,e),u=o.tween;if(u!==r){i=(r=u).slice();for(var a={name:t,value:n},l=0,c=i.length;l<c;++l)if(i[l].name===t){i[l]=a;break}l===c&&i.push(a)}o.tween=i}}function sn(e,t,n){var r=e._id;return e.each((function(){var e=Yt(this,r);(e.value||(e.value={}))[t]=n.apply(this,arguments)})),function(e){return Xt(e,r).value[t]}}var fn=function(e,t){var n;return("number"===typeof t?wt:t instanceof Ke?ht:(n=Ke(t))?(t=n,ht):Ct)(e,t)};function dn(e){return function(){this.removeAttribute(e)}}function pn(e){return function(){this.removeAttributeNS(e.space,e.local)}}function hn(e,t,n){var r,i,o=n+"";return function(){var u=this.getAttribute(e);return u===o?null:u===r?i:i=t(r=u,n)}}function mn(e,t,n){var r,i,o=n+"";return function(){var u=this.getAttributeNS(e.space,e.local);return u===o?null:u===r?i:i=t(r=u,n)}}function gn(e,t,n){var r,i,o;return function(){var u,a,l=n(this);if(null!=l)return(u=this.getAttribute(e))===(a=l+"")?null:u===r&&a===i?o:(i=a,o=t(r=u,l));this.removeAttribute(e)}}function vn(e,t,n){var r,i,o;return function(){var u,a,l=n(this);if(null!=l)return(u=this.getAttributeNS(e.space,e.local))===(a=l+"")?null:u===r&&a===i?o:(i=a,o=t(r=u,l));this.removeAttributeNS(e.space,e.local)}}function yn(e,t){return function(n){this.setAttribute(e,t.call(this,n))}}function bn(e,t){return function(n){this.setAttributeNS(e.space,e.local,t.call(this,n))}}function wn(e,t){var n,r;function i(){var i=t.apply(this,arguments);return i!==r&&(n=(r=i)&&bn(e,i)),n}return i._value=t,i}function xn(e,t){var n,r;function i(){var i=t.apply(this,arguments);return i!==r&&(n=(r=i)&&yn(e,i)),n}return i._value=t,i}function _n(e,t){return function(){Bt(this,e).delay=+t.apply(this,arguments)}}function kn(e,t){return t=+t,function(){Bt(this,e).delay=t}}function Tn(e,t){return function(){Yt(this,e).duration=+t.apply(this,arguments)}}function En(e,t){return t=+t,function(){Yt(this,e).duration=t}}function Cn(e,t){if("function"!==typeof t)throw new Error;return function(){Yt(this,e).ease=t}}function Sn(e,t,n){var r,i,o=function(e){return(e+"").trim().split(/^|\s+/).every((function(e){var t=e.indexOf(".");return t>=0&&(e=e.slice(0,t)),!e||"start"===e}))}(t)?Bt:Yt;return function(){var u=o(this,e),a=u.on;a!==r&&(i=(r=a).copy()).on(t,n),u.on=i}}var Mn=De.prototype.constructor;function Nn(e){return function(){this.style.removeProperty(e)}}function Pn(e,t,n){return function(r){this.style.setProperty(e,t.call(this,r),n)}}function An(e,t,n){var r,i;function o(){var o=t.apply(this,arguments);return o!==i&&(r=(i=o)&&Pn(e,o,n)),r}return o._value=t,o}function Rn(e){return function(t){this.textContent=e.call(this,t)}}function Dn(e){var t,n;function r(){var r=e.apply(this,arguments);return r!==n&&(t=(n=r)&&Rn(r)),t}return r._value=e,r}var On=0;function zn(e,t,n,r){this._groups=e,this._parents=t,this._name=n,this._id=r}function Ln(){return++On}var Un=De.prototype;zn.prototype=function(e){return De().transition(e)}.prototype={constructor:zn,select:function(e){var t=this._name,n=this._id;"function"!==typeof e&&(e=E(e));for(var r=this._groups,i=r.length,o=new Array(i),u=0;u<i;++u)for(var a,l,c=r[u],s=c.length,f=o[u]=new Array(s),d=0;d<s;++d)(a=c[d])&&(l=e.call(a,a.__data__,d,c))&&("__data__"in a&&(l.__data__=a.__data__),f[d]=l,qt(f[d],t,n,d,f,Xt(a,n)));return new zn(o,this._parents,t,n)},selectAll:function(e){var t=this._name,n=this._id;"function"!==typeof e&&(e=S(e));for(var r=this._groups,i=r.length,o=[],u=[],a=0;a<i;++a)for(var l,c=r[a],s=c.length,f=0;f<s;++f)if(l=c[f]){for(var d,p=e.call(l,l.__data__,f,c),h=Xt(l,n),m=0,g=p.length;m<g;++m)(d=p[m])&&qt(d,t,n,m,p,h);o.push(p),u.push(l)}return new zn(o,u,t,n)},filter:function(e){"function"!==typeof e&&(e=M(e));for(var t=this._groups,n=t.length,r=new Array(n),i=0;i<n;++i)for(var o,u=t[i],a=u.length,l=r[i]=[],c=0;c<a;++c)(o=u[c])&&e.call(o,o.__data__,c,u)&&l.push(o);return new zn(r,this._parents,this._name,this._id)},merge:function(e){if(e._id!==this._id)throw new Error;for(var t=this._groups,n=e._groups,r=t.length,i=n.length,o=Math.min(r,i),u=new Array(r),a=0;a<o;++a)for(var l,c=t[a],s=n[a],f=c.length,d=u[a]=new Array(f),p=0;p<f;++p)(l=c[p]||s[p])&&(d[p]=l);for(;a<r;++a)u[a]=t[a];return new zn(u,this._parents,this._name,this._id)},selection:function(){return new Mn(this._groups,this._parents)},transition:function(){for(var e=this._name,t=this._id,n=Ln(),r=this._groups,i=r.length,o=0;o<i;++o)for(var u,a=r[o],l=a.length,c=0;c<l;++c)if(u=a[c]){var s=Xt(u,t);qt(u,e,n,c,a,{time:s.time+s.delay+s.duration,delay:0,duration:s.duration,ease:s.ease})}return new zn(r,this._parents,e,n)},call:Un.call,nodes:Un.nodes,node:Un.node,size:Un.size,empty:Un.empty,each:Un.each,on:function(e,t){var n=this._id;return arguments.length<2?Xt(this.node(),n).on.on(e):this.each(Sn(n,e,t))},attr:function(e,t){var n=L(e),r="transform"===n?an:fn;return this.attrTween(e,"function"===typeof t?(n.local?vn:gn)(n,r,sn(this,"attr."+e,t)):null==t?(n.local?pn:dn)(n):(n.local?mn:hn)(n,r,t))},attrTween:function(e,t){var n="attr."+e;if(arguments.length<2)return(n=this.tween(n))&&n._value;if(null==t)return this.tween(n,null);if("function"!==typeof t)throw new Error;var r=L(e);return this.tween(n,(r.local?wn:xn)(r,t))},style:function(e,t,n){var r="transform"===(e+="")?un:fn;return null==t?this.styleTween(e,function(e,t){var n,r,i;return function(){var o=B(this,e),u=(this.style.removeProperty(e),B(this,e));return o===u?null:o===n&&u===r?i:i=t(n=o,r=u)}}(e,r)).on("end.style."+e,Nn(e)):"function"===typeof t?this.styleTween(e,function(e,t,n){var r,i,o;return function(){var u=B(this,e),a=n(this),l=a+"";return null==a&&(this.style.removeProperty(e),l=a=B(this,e)),u===l?null:u===r&&l===i?o:(i=l,o=t(r=u,a))}}(e,r,sn(this,"style."+e,t))).each(function(e,t){var n,r,i,o,u="style."+t,a="end."+u;return function(){var l=Yt(this,e),c=l.on,s=null==l.value[u]?o||(o=Nn(t)):void 0;c===n&&i===s||(r=(n=c).copy()).on(a,i=s),l.on=r}}(this._id,e)):this.styleTween(e,function(e,t,n){var r,i,o=n+"";return function(){var u=B(this,e);return u===o?null:u===r?i:i=t(r=u,n)}}(e,r,t),n).on("end.style."+e,null)},styleTween:function(e,t,n){var r="style."+(e+="");if(arguments.length<2)return(r=this.tween(r))&&r._value;if(null==t)return this.tween(r,null);if("function"!==typeof t)throw new Error;return this.tween(r,An(e,t,null==n?"":n))},text:function(e){return this.tween("text","function"===typeof e?function(e){return function(){var t=e(this);this.textContent=null==t?"":t}}(sn(this,"text",e)):function(e){return function(){this.textContent=e}}(null==e?"":e+""))},textTween:function(e){var t="text";if(arguments.length<1)return(t=this.tween(t))&&t._value;if(null==e)return this.tween(t,null);if("function"!==typeof e)throw new Error;return this.tween(t,Dn(e))},remove:function(){return this.on("end.remove",(e=this._id,function(){var t=this.parentNode;for(var n in this.__transition)if(+n!==e)return;t&&t.removeChild(this)}));var e},tween:function(e,t){var n=this._id;if(e+="",arguments.length<2){for(var r,i=Xt(this.node(),n).tween,o=0,u=i.length;o<u;++o)if((r=i[o]).name===e)return r.value;return null}return this.each((null==t?ln:cn)(n,e,t))},delay:function(e){var t=this._id;return arguments.length?this.each(("function"===typeof e?_n:kn)(t,e)):Xt(this.node(),t).delay},duration:function(e){var t=this._id;return arguments.length?this.each(("function"===typeof e?Tn:En)(t,e)):Xt(this.node(),t).duration},ease:function(e){var t=this._id;return arguments.length?this.each(Cn(t,e)):Xt(this.node(),t).ease},end:function(){var e,t,n=this,r=n._id,i=n.size();return new Promise((function(o,u){var a={value:u},l={value:function(){0===--i&&o()}};n.each((function(){var n=Yt(this,r),i=n.on;i!==e&&((t=(e=i).copy())._.cancel.push(a),t._.interrupt.push(a),t._.end.push(l)),n.on=t}))}))}};var Fn={time:null,delay:0,duration:250,ease:function(e){return((e*=2)<=1?e*e*e:(e-=2)*e*e+2)/2}};function In(e,t){for(var n;!(n=e.__transition)||!(n=n[t]);)if(!(e=e.parentNode))return Fn.time=Lt(),Fn;return n}De.prototype.interrupt=function(e){return this.each((function(){en(this,e)}))},De.prototype.transition=function(e){var t,n;e instanceof zn?(t=e._id,e=e._name):(t=Ln(),(n=Fn).time=Lt(),e=null==e?null:e+"");for(var r=this._groups,i=r.length,o=0;o<i;++o)for(var u,a=r[o],l=a.length,c=0;c<l;++c)(u=a[c])&&qt(u,e,t,c,a,n||In(u,t));return new zn(r,this._parents,e,t)};function jn(e){return[+e[0],+e[1]]}function Hn(e){return[jn(e[0]),jn(e[1])]}["w","e"].map(Vn),["n","s"].map(Vn),["n","w","e","s","nw","ne","sw","se"].map(Vn);function Vn(e){return{type:e}}Math.cos,Math.sin,Math.PI,Math.max;Array.prototype.slice;var $n=Math.PI,Wn=2*$n,Qn=Wn-1e-6;function qn(){this._x0=this._y0=this._x1=this._y1=null,this._=""}function Bn(){return new qn}qn.prototype=Bn.prototype={constructor:qn,moveTo:function(e,t){this._+="M"+(this._x0=this._x1=+e)+","+(this._y0=this._y1=+t)},closePath:function(){null!==this._x1&&(this._x1=this._x0,this._y1=this._y0,this._+="Z")},lineTo:function(e,t){this._+="L"+(this._x1=+e)+","+(this._y1=+t)},quadraticCurveTo:function(e,t,n,r){this._+="Q"+ +e+","+ +t+","+(this._x1=+n)+","+(this._y1=+r)},bezierCurveTo:function(e,t,n,r,i,o){this._+="C"+ +e+","+ +t+","+ +n+","+ +r+","+(this._x1=+i)+","+(this._y1=+o)},arcTo:function(e,t,n,r,i){e=+e,t=+t,n=+n,r=+r,i=+i;var o=this._x1,u=this._y1,a=n-e,l=r-t,c=o-e,s=u-t,f=c*c+s*s;if(i<0)throw new Error("negative radius: "+i);if(null===this._x1)this._+="M"+(this._x1=e)+","+(this._y1=t);else if(f>1e-6)if(Math.abs(s*a-l*c)>1e-6&&i){var d=n-o,p=r-u,h=a*a+l*l,m=d*d+p*p,g=Math.sqrt(h),v=Math.sqrt(f),y=i*Math.tan(($n-Math.acos((h+f-m)/(2*g*v)))/2),b=y/v,w=y/g;Math.abs(b-1)>1e-6&&(this._+="L"+(e+b*c)+","+(t+b*s)),this._+="A"+i+","+i+",0,0,"+ +(s*d>c*p)+","+(this._x1=e+w*a)+","+(this._y1=t+w*l)}else this._+="L"+(this._x1=e)+","+(this._y1=t);else;},arc:function(e,t,n,r,i,o){e=+e,t=+t,o=!!o;var u=(n=+n)*Math.cos(r),a=n*Math.sin(r),l=e+u,c=t+a,s=1^o,f=o?r-i:i-r;if(n<0)throw new Error("negative radius: "+n);null===this._x1?this._+="M"+l+","+c:(Math.abs(this._x1-l)>1e-6||Math.abs(this._y1-c)>1e-6)&&(this._+="L"+l+","+c),n&&(f<0&&(f=f%Wn+Wn),f>Qn?this._+="A"+n+","+n+",0,1,"+s+","+(e-u)+","+(t-a)+"A"+n+","+n+",0,1,"+s+","+(this._x1=l)+","+(this._y1=c):f>1e-6&&(this._+="A"+n+","+n+",0,"+ +(f>=$n)+","+s+","+(this._x1=e+n*Math.cos(i))+","+(this._y1=t+n*Math.sin(i))))},rect:function(e,t,n,r){this._+="M"+(this._x0=this._x1=+e)+","+(this._y0=this._y1=+t)+"h"+ +n+"v"+ +r+"h"+-n+"Z"},toString:function(){return this._}};function Yn(){}function Xn(e,t){var n=new Yn;if(e instanceof Yn)e.each((function(e,t){n.set(t,e)}));else if(Array.isArray(e)){var r,i=-1,o=e.length;if(null==t)for(;++i<o;)n.set(i,e[i]);else for(;++i<o;)n.set(t(r=e[i],i,e),r)}else if(e)for(var u in e)n.set(u,e[u]);return n}Yn.prototype=Xn.prototype={constructor:Yn,has:function(e){return"$"+e in this},get:function(e){return this["$"+e]},set:function(e,t){return this["$"+e]=t,this},remove:function(e){var t="$"+e;return t in this&&delete this[t]},clear:function(){for(var e in this)"$"===e[0]&&delete this[e]},keys:function(){var e=[];for(var t in this)"$"===t[0]&&e.push(t.slice(1));return e},values:function(){var e=[];for(var t in this)"$"===t[0]&&e.push(this[t]);return e},entries:function(){var e=[];for(var t in this)"$"===t[0]&&e.push({key:t.slice(1),value:this[t]});return e},size:function(){var e=0;for(var t in this)"$"===t[0]&&++e;return e},empty:function(){for(var e in this)if("$"===e[0])return!1;return!0},each:function(e){for(var t in this)"$"===t[0]&&e(this[t],t.slice(1),this)}};var Kn=Xn,Zn=function(){var e,t,n,r=[],i=[];function o(n,i,u,a){if(i>=r.length)return null!=e&&n.sort(e),null!=t?t(n):n;for(var l,c,s,f=-1,d=n.length,p=r[i++],h=Kn(),m=u();++f<d;)(s=h.get(l=p(c=n[f])+""))?s.push(c):h.set(l,[c]);return h.each((function(e,t){a(m,t,o(e,i,u,a))})),m}return n={object:function(e){return o(e,0,Jn,Gn)},map:function(e){return o(e,0,er,tr)},entries:function(e){return function e(n,o){if(++o>r.length)return n;var u,a=i[o-1];return null!=t&&o>=r.length?u=n.entries():(u=[],n.each((function(t,n){u.push({key:n,values:e(t,o)})}))),null!=a?u.sort((function(e,t){return a(e.key,t.key)})):u}(o(e,0,er,tr),0)},key:function(e){return r.push(e),n},sortKeys:function(e){return i[r.length-1]=e,n},sortValues:function(t){return e=t,n},rollup:function(e){return t=e,n}}};function Jn(){return{}}function Gn(e,t,n){e[t]=n}function er(){return Kn()}function tr(e,t,n){e.set(t,n)}function nr(){}var rr=Kn.prototype;function ir(e,t){var n=new nr;if(e instanceof nr)e.each((function(e){n.add(e)}));else if(e){var r=-1,i=e.length;if(null==t)for(;++r<i;)n.add(e[r]);else for(;++r<i;)n.add(t(e[r],r,e))}return n}nr.prototype=ir.prototype={constructor:nr,has:rr.has,add:function(e){return this["$"+(e+="")]=e,this},remove:rr.remove,clear:rr.clear,values:rr.keys,size:rr.size,empty:rr.empty,each:rr.each};Array.prototype.slice;var or={},ur={};function ar(e){return new Function("d","return {"+e.map((function(e,t){return JSON.stringify(e)+": d["+t+'] || ""'})).join(",")+"}")}function lr(e){var t=Object.create(null),n=[];return e.forEach((function(e){for(var r in e)r in t||n.push(t[r]=r)})),n}function cr(e,t){var n=e+"",r=n.length;return r<t?new Array(t-r+1).join(0)+n:n}function sr(e){var t,n=e.getUTCHours(),r=e.getUTCMinutes(),i=e.getUTCSeconds(),o=e.getUTCMilliseconds();return isNaN(e)?"Invalid Date":((t=e.getUTCFullYear())<0?"-"+cr(-t,6):t>9999?"+"+cr(t,6):cr(t,4))+"-"+cr(e.getUTCMonth()+1,2)+"-"+cr(e.getUTCDate(),2)+(o?"T"+cr(n,2)+":"+cr(r,2)+":"+cr(i,2)+"."+cr(o,3)+"Z":i?"T"+cr(n,2)+":"+cr(r,2)+":"+cr(i,2)+"Z":r||n?"T"+cr(n,2)+":"+cr(r,2)+"Z":"")}var fr=function(e){var t=new RegExp('["'+e+"\n\r]"),n=e.charCodeAt(0);function r(e,t){var r,i=[],o=e.length,u=0,a=0,l=o<=0,c=!1;function s(){if(l)return ur;if(c)return c=!1,or;var t,r,i=u;if(34===e.charCodeAt(i)){for(;u++<o&&34!==e.charCodeAt(u)||34===e.charCodeAt(++u););return(t=u)>=o?l=!0:10===(r=e.charCodeAt(u++))?c=!0:13===r&&(c=!0,10===e.charCodeAt(u)&&++u),e.slice(i+1,t-1).replace(/""/g,'"')}for(;u<o;){if(10===(r=e.charCodeAt(t=u++)))c=!0;else if(13===r)c=!0,10===e.charCodeAt(u)&&++u;else if(r!==n)continue;return e.slice(i,t)}return l=!0,e.slice(i,o)}for(10===e.charCodeAt(o-1)&&--o,13===e.charCodeAt(o-1)&&--o;(r=s())!==ur;){for(var f=[];r!==or&&r!==ur;)f.push(r),r=s();t&&null==(f=t(f,a++))||i.push(f)}return i}function i(t,n){return t.map((function(t){return n.map((function(e){return u(t[e])})).join(e)}))}function o(t){return t.map(u).join(e)}function u(e){return null==e?"":e instanceof Date?sr(e):t.test(e+="")?'"'+e.replace(/"/g,'""')+'"':e}return{parse:function(e,t){var n,i,o=r(e,(function(e,r){if(n)return n(e,r-1);i=e,n=t?function(e,t){var n=ar(e);return function(r,i){return t(n(r),i,e)}}(e,t):ar(e)}));return o.columns=i||[],o},parseRows:r,format:function(t,n){return null==n&&(n=lr(t)),[n.map(u).join(e)].concat(i(t,n)).join("\n")},formatBody:function(e,t){return null==t&&(t=lr(e)),i(e,t).join("\n")},formatRows:function(e){return e.map(o).join("\n")},formatRow:o,formatValue:u}},dr=fr(","),pr=dr.parse,hr=(dr.parseRows,dr.format,dr.formatBody,dr.formatRows,dr.formatRow,dr.formatValue,fr("\t")),mr=hr.parse;hr.parseRows,hr.format,hr.formatBody,hr.formatRows,hr.formatRow,hr.formatValue;function gr(e){if(!e.ok)throw new Error(e.status+" "+e.statusText);return e.text()}var vr=function(e,t){return fetch(e,t).then(gr)};function yr(e){return function(t,n,r){return 2===arguments.length&&"function"===typeof n&&(r=n,n=void 0),vr(t,n).then((function(t){return e(t,r)}))}}var br=yr(pr);yr(mr);function wr(e){return function(t,n){return vr(t,n).then((function(t){return(new DOMParser).parseFromString(t,e)}))}}wr("application/xml"),wr("text/html"),wr("image/svg+xml");function xr(e,t,n,r){if(isNaN(t)||isNaN(n))return e;var i,o,u,a,l,c,s,f,d,p=e._root,h={data:r},m=e._x0,g=e._y0,v=e._x1,y=e._y1;if(!p)return e._root=h,e;for(;p.length;)if((c=t>=(o=(m+v)/2))?m=o:v=o,(s=n>=(u=(g+y)/2))?g=u:y=u,i=p,!(p=p[f=s<<1|c]))return i[f]=h,e;if(a=+e._x.call(null,p.data),l=+e._y.call(null,p.data),t===a&&n===l)return h.next=p,i?i[f]=h:e._root=h,e;do{i=i?i[f]=new Array(4):e._root=new Array(4),(c=t>=(o=(m+v)/2))?m=o:v=o,(s=n>=(u=(g+y)/2))?g=u:y=u}while((f=s<<1|c)===(d=(l>=u)<<1|a>=o));return i[d]=p,i[f]=h,e}var _r=function(e,t,n,r,i){this.node=e,this.x0=t,this.y0=n,this.x1=r,this.y1=i};function kr(e){return e[0]}function Tr(e){return e[1]}function Er(e,t,n){var r=new Cr(null==t?kr:t,null==n?Tr:n,NaN,NaN,NaN,NaN);return null==e?r:r.addAll(e)}function Cr(e,t,n,r,i,o){this._x=e,this._y=t,this._x0=n,this._y0=r,this._x1=i,this._y1=o,this._root=void 0}function Sr(e){for(var t={data:e.data},n=t;e=e.next;)n=n.next={data:e.data};return t}var Mr=Er.prototype=Cr.prototype;Mr.copy=function(){var e,t,n=new Cr(this._x,this._y,this._x0,this._y0,this._x1,this._y1),r=this._root;if(!r)return n;if(!r.length)return n._root=Sr(r),n;for(e=[{source:r,target:n._root=new Array(4)}];r=e.pop();)for(var i=0;i<4;++i)(t=r.source[i])&&(t.length?e.push({source:t,target:r.target[i]=new Array(4)}):r.target[i]=Sr(t));return n},Mr.add=function(e){var t=+this._x.call(null,e),n=+this._y.call(null,e);return xr(this.cover(t,n),t,n,e)},Mr.addAll=function(e){var t,n,r,i,o=e.length,u=new Array(o),a=new Array(o),l=1/0,c=1/0,s=-1/0,f=-1/0;for(n=0;n<o;++n)isNaN(r=+this._x.call(null,t=e[n]))||isNaN(i=+this._y.call(null,t))||(u[n]=r,a[n]=i,r<l&&(l=r),r>s&&(s=r),i<c&&(c=i),i>f&&(f=i));if(l>s||c>f)return this;for(this.cover(l,c).cover(s,f),n=0;n<o;++n)xr(this,u[n],a[n],e[n]);return this},Mr.cover=function(e,t){if(isNaN(e=+e)||isNaN(t=+t))return this;var n=this._x0,r=this._y0,i=this._x1,o=this._y1;if(isNaN(n))i=(n=Math.floor(e))+1,o=(r=Math.floor(t))+1;else{for(var u,a,l=i-n,c=this._root;n>e||e>=i||r>t||t>=o;)switch(a=(t<r)<<1|e<n,(u=new Array(4))[a]=c,c=u,l*=2,a){case 0:i=n+l,o=r+l;break;case 1:n=i-l,o=r+l;break;case 2:i=n+l,r=o-l;break;case 3:n=i-l,r=o-l}this._root&&this._root.length&&(this._root=c)}return this._x0=n,this._y0=r,this._x1=i,this._y1=o,this},Mr.data=function(){var e=[];return this.visit((function(t){if(!t.length)do{e.push(t.data)}while(t=t.next)})),e},Mr.extent=function(e){return arguments.length?this.cover(+e[0][0],+e[0][1]).cover(+e[1][0],+e[1][1]):isNaN(this._x0)?void 0:[[this._x0,this._y0],[this._x1,this._y1]]},Mr.find=function(e,t,n){var r,i,o,u,a,l,c,s=this._x0,f=this._y0,d=this._x1,p=this._y1,h=[],m=this._root;for(m&&h.push(new _r(m,s,f,d,p)),null==n?n=1/0:(s=e-n,f=t-n,d=e+n,p=t+n,n*=n);l=h.pop();)if(!(!(m=l.node)||(i=l.x0)>d||(o=l.y0)>p||(u=l.x1)<s||(a=l.y1)<f))if(m.length){var g=(i+u)/2,v=(o+a)/2;h.push(new _r(m[3],g,v,u,a),new _r(m[2],i,v,g,a),new _r(m[1],g,o,u,v),new _r(m[0],i,o,g,v)),(c=(t>=v)<<1|e>=g)&&(l=h[h.length-1],h[h.length-1]=h[h.length-1-c],h[h.length-1-c]=l)}else{var y=e-+this._x.call(null,m.data),b=t-+this._y.call(null,m.data),w=y*y+b*b;if(w<n){var x=Math.sqrt(n=w);s=e-x,f=t-x,d=e+x,p=t+x,r=m.data}}return r},Mr.remove=function(e){if(isNaN(o=+this._x.call(null,e))||isNaN(u=+this._y.call(null,e)))return this;var t,n,r,i,o,u,a,l,c,s,f,d,p=this._root,h=this._x0,m=this._y0,g=this._x1,v=this._y1;if(!p)return this;if(p.length)for(;;){if((c=o>=(a=(h+g)/2))?h=a:g=a,(s=u>=(l=(m+v)/2))?m=l:v=l,t=p,!(p=p[f=s<<1|c]))return this;if(!p.length)break;(t[f+1&3]||t[f+2&3]||t[f+3&3])&&(n=t,d=f)}for(;p.data!==e;)if(r=p,!(p=p.next))return this;return(i=p.next)&&delete p.next,r?(i?r.next=i:delete r.next,this):t?(i?t[f]=i:delete t[f],(p=t[0]||t[1]||t[2]||t[3])&&p===(t[3]||t[2]||t[1]||t[0])&&!p.length&&(n?n[d]=p:this._root=p),this):(this._root=i,this)},Mr.removeAll=function(e){for(var t=0,n=e.length;t<n;++t)this.remove(e[t]);return this},Mr.root=function(){return this._root},Mr.size=function(){var e=0;return this.visit((function(t){if(!t.length)do{++e}while(t=t.next)})),e},Mr.visit=function(e){var t,n,r,i,o,u,a=[],l=this._root;for(l&&a.push(new _r(l,this._x0,this._y0,this._x1,this._y1));t=a.pop();)if(!e(l=t.node,r=t.x0,i=t.y0,o=t.x1,u=t.y1)&&l.length){var c=(r+o)/2,s=(i+u)/2;(n=l[3])&&a.push(new _r(n,c,s,o,u)),(n=l[2])&&a.push(new _r(n,r,s,c,u)),(n=l[1])&&a.push(new _r(n,c,i,o,s)),(n=l[0])&&a.push(new _r(n,r,i,c,s))}return this},Mr.visitAfter=function(e){var t,n=[],r=[];for(this._root&&n.push(new _r(this._root,this._x0,this._y0,this._x1,this._y1));t=n.pop();){var i=t.node;if(i.length){var o,u=t.x0,a=t.y0,l=t.x1,c=t.y1,s=(u+l)/2,f=(a+c)/2;(o=i[0])&&n.push(new _r(o,u,a,s,f)),(o=i[1])&&n.push(new _r(o,s,a,l,f)),(o=i[2])&&n.push(new _r(o,u,f,s,c)),(o=i[3])&&n.push(new _r(o,s,f,l,c))}r.push(t)}for(;t=r.pop();)e(t.node,t.x0,t.y0,t.x1,t.y1);return this},Mr.x=function(e){return arguments.length?(this._x=e,this):this._x},Mr.y=function(e){return arguments.length?(this._y=e,this):this._y};Math.PI,Math.sqrt(5);var Nr=function(){return Math.random()},Pr=(function e(t){function n(e,n){return e=null==e?0:+e,n=null==n?1:+n,1===arguments.length?(n=e,e=0):n-=e,function(){return t()*n+e}}return n.source=e,n}(Nr),function e(t){function n(e,n){var r,i;return e=null==e?0:+e,n=null==n?1:+n,function(){var o;if(null!=r)o=r,r=null;else do{r=2*t()-1,o=2*t()-1,i=r*r+o*o}while(!i||i>1);return e+n*o*Math.sqrt(-2*Math.log(i)/i)}}return n.source=e,n}(Nr)),Ar=(function e(t){function n(){var e=Pr.source(t).apply(this,arguments);return function(){return Math.exp(e())}}return n.source=e,n}(Nr),function e(t){function n(e){return function(){for(var n=0,r=0;r<e;++r)n+=t();return n}}return n.source=e,n}(Nr));(function e(t){function n(e){var n=Ar.source(t)(e);return function(){return n()/e}}return n.source=e,n})(Nr),function e(t){function n(e){return function(){return-Math.log(1-t())/e}}return n.source=e,n}(Nr);function Rr(e,t){switch(arguments.length){case 0:break;case 1:this.range(e);break;default:this.range(t).domain(e)}return this}var Dr=Array.prototype,Or=Dr.map,zr=Dr.slice;var Lr=function(e,t){return e=+e,t=+t,function(n){return Math.round(e*(1-n)+t*n)}},Ur=function(e){return+e},Fr=[0,1];function Ir(e){return e}function jr(e,t){return(t-=e=+e)?function(n){return(n-e)/t}:(n=isNaN(t)?NaN:.5,function(){return n});var n}function Hr(e){var t,n=e[0],r=e[e.length-1];return n>r&&(t=n,n=r,r=t),function(e){return Math.max(n,Math.min(r,e))}}function Vr(e,t,n){var r=e[0],i=e[1],o=t[0],u=t[1];return i<r?(r=jr(i,r),o=n(u,o)):(r=jr(r,i),o=n(o,u)),function(e){return o(r(e))}}function $r(e,t,n){var r=Math.min(e.length,t.length)-1,i=new Array(r),o=new Array(r),u=-1;for(e[r]<e[0]&&(e=e.slice().reverse(),t=t.slice().reverse());++u<r;)i[u]=jr(e[u],e[u+1]),o[u]=n(t[u],t[u+1]);return function(t){var n=a(e,t,1,r)-1;return o[n](i[n](t))}}function Wr(e,t){return t.domain(e.domain()).range(e.range()).interpolate(e.interpolate()).clamp(e.clamp()).unknown(e.unknown())}function Qr(){var e,t,n,r,i,o,u=Fr,a=Fr,l=St,c=Ir;function s(){return r=Math.min(u.length,a.length)>2?$r:Vr,i=o=null,f}function f(t){return isNaN(t=+t)?n:(i||(i=r(u.map(e),a,l)))(e(c(t)))}return f.invert=function(n){return c(t((o||(o=r(a,u.map(e),wt)))(n)))},f.domain=function(e){return arguments.length?(u=Or.call(e,Ur),c===Ir||(c=Hr(u)),s()):u.slice()},f.range=function(e){return arguments.length?(a=zr.call(e),s()):a.slice()},f.rangeRound=function(e){return a=zr.call(e),l=Lr,s()},f.clamp=function(e){return arguments.length?(c=e?Hr(u):Ir,f):c!==Ir},f.interpolate=function(e){return arguments.length?(l=e,s()):l},f.unknown=function(e){return arguments.length?(n=e,f):n},function(n,r){return e=n,t=r,s()}}var qr=/^(?:(.)?([<>=^]))?([+\-( ])?([$#])?(0)?(\d+)?(,)?(\.\d+)?(~)?([a-z%])?$/i;function Br(e){if(!(t=qr.exec(e)))throw new Error("invalid format: "+e);var t;return new Yr({fill:t[1],align:t[2],sign:t[3],symbol:t[4],zero:t[5],width:t[6],comma:t[7],precision:t[8]&&t[8].slice(1),trim:t[9],type:t[10]})}function Yr(e){this.fill=void 0===e.fill?" ":e.fill+"",this.align=void 0===e.align?">":e.align+"",this.sign=void 0===e.sign?"-":e.sign+"",this.symbol=void 0===e.symbol?"":e.symbol+"",this.zero=!!e.zero,this.width=void 0===e.width?void 0:+e.width,this.comma=!!e.comma,this.precision=void 0===e.precision?void 0:+e.precision,this.trim=!!e.trim,this.type=void 0===e.type?"":e.type+""}Br.prototype=Yr.prototype,Yr.prototype.toString=function(){return this.fill+this.align+this.sign+this.symbol+(this.zero?"0":"")+(void 0===this.width?"":Math.max(1,0|this.width))+(this.comma?",":"")+(void 0===this.precision?"":"."+Math.max(0,0|this.precision))+(this.trim?"~":"")+this.type};var Xr,Kr,Zr,Jr,Gr=function(e,t){if((n=(e=t?e.toExponential(t-1):e.toExponential()).indexOf("e"))<0)return null;var n,r=e.slice(0,n);return[r.length>1?r[0]+r.slice(2):r,+e.slice(n+1)]},ei=function(e){return(e=Gr(Math.abs(e)))?e[1]:NaN},ti=function(e,t){var n=Gr(e,t);if(!n)return e+"";var r=n[0],i=n[1];return i<0?"0."+new Array(-i).join("0")+r:r.length>i+1?r.slice(0,i+1)+"."+r.slice(i+1):r+new Array(i-r.length+2).join("0")},ni={"%":function(e,t){return(100*e).toFixed(t)},b:function(e){return Math.round(e).toString(2)},c:function(e){return e+""},d:function(e){return Math.round(e).toString(10)},e:function(e,t){return e.toExponential(t)},f:function(e,t){return e.toFixed(t)},g:function(e,t){return e.toPrecision(t)},o:function(e){return Math.round(e).toString(8)},p:function(e,t){return ti(100*e,t)},r:ti,s:function(e,t){var n=Gr(e,t);if(!n)return e+"";var r=n[0],i=n[1],o=i-(Xr=3*Math.max(-8,Math.min(8,Math.floor(i/3))))+1,u=r.length;return o===u?r:o>u?r+new Array(o-u+1).join("0"):o>0?r.slice(0,o)+"."+r.slice(o):"0."+new Array(1-o).join("0")+Gr(e,Math.max(0,t+o-1))[0]},X:function(e){return Math.round(e).toString(16).toUpperCase()},x:function(e){return Math.round(e).toString(16)}},ri=function(e){return e},ii=Array.prototype.map,oi=["y","z","a","f","p","n","\xb5","m","","k","M","G","T","P","E","Z","Y"];Kr=function(e){var t,n,r=void 0===e.grouping||void 0===e.thousands?ri:(t=ii.call(e.grouping,Number),n=e.thousands+"",function(e,r){for(var i=e.length,o=[],u=0,a=t[0],l=0;i>0&&a>0&&(l+a+1>r&&(a=Math.max(1,r-l)),o.push(e.substring(i-=a,i+a)),!((l+=a+1)>r));)a=t[u=(u+1)%t.length];return o.reverse().join(n)}),i=void 0===e.currency?"":e.currency[0]+"",o=void 0===e.currency?"":e.currency[1]+"",u=void 0===e.decimal?".":e.decimal+"",a=void 0===e.numerals?ri:function(e){return function(t){return t.replace(/[0-9]/g,(function(t){return e[+t]}))}}(ii.call(e.numerals,String)),l=void 0===e.percent?"%":e.percent+"",c=void 0===e.minus?"-":e.minus+"",s=void 0===e.nan?"NaN":e.nan+"";function f(e){var t=(e=Br(e)).fill,n=e.align,f=e.sign,d=e.symbol,p=e.zero,h=e.width,m=e.comma,g=e.precision,v=e.trim,y=e.type;"n"===y?(m=!0,y="g"):ni[y]||(void 0===g&&(g=12),v=!0,y="g"),(p||"0"===t&&"="===n)&&(p=!0,t="0",n="=");var b="$"===d?i:"#"===d&&/[boxX]/.test(y)?"0"+y.toLowerCase():"",w="$"===d?o:/[%p]/.test(y)?l:"",x=ni[y],_=/[defgprs%]/.test(y);function k(e){var i,o,l,d=b,k=w;if("c"===y)k=x(e)+k,e="";else{var T=(e=+e)<0||1/e<0;if(e=isNaN(e)?s:x(Math.abs(e),g),v&&(e=function(e){e:for(var t,n=e.length,r=1,i=-1;r<n;++r)switch(e[r]){case".":i=t=r;break;case"0":0===i&&(i=r),t=r;break;default:if(!+e[r])break e;i>0&&(i=0)}return i>0?e.slice(0,i)+e.slice(t+1):e}(e)),T&&0===+e&&"+"!==f&&(T=!1),d=(T?"("===f?f:c:"-"===f||"("===f?"":f)+d,k=("s"===y?oi[8+Xr/3]:"")+k+(T&&"("===f?")":""),_)for(i=-1,o=e.length;++i<o;)if(48>(l=e.charCodeAt(i))||l>57){k=(46===l?u+e.slice(i+1):e.slice(i))+k,e=e.slice(0,i);break}}m&&!p&&(e=r(e,1/0));var E=d.length+e.length+k.length,C=E<h?new Array(h-E+1).join(t):"";switch(m&&p&&(e=r(C+e,C.length?h-k.length:1/0),C=""),n){case"<":e=d+e+k+C;break;case"=":e=d+C+e+k;break;case"^":e=C.slice(0,E=C.length>>1)+d+e+k+C.slice(E);break;default:e=C+d+e+k}return a(e)}return g=void 0===g?6:/[gprs]/.test(y)?Math.max(1,Math.min(21,g)):Math.max(0,Math.min(20,g)),k.toString=function(){return e+""},k}return{format:f,formatPrefix:function(e,t){var n=f(((e=Br(e)).type="f",e)),r=3*Math.max(-8,Math.min(8,Math.floor(ei(t)/3))),i=Math.pow(10,-r),o=oi[8+r/3];return function(e){return n(i*e)+o}}}}({decimal:".",thousands:",",grouping:[3],currency:["$",""],minus:"-"}),Zr=Kr.format,Jr=Kr.formatPrefix;var ui=function(e,t,n,r){var i,o=h(e,t,n);switch((r=Br(null==r?",f":r)).type){case"s":var u=Math.max(Math.abs(e),Math.abs(t));return null!=r.precision||isNaN(i=function(e,t){return Math.max(0,3*Math.max(-8,Math.min(8,Math.floor(ei(t)/3)))-ei(Math.abs(e)))}(o,u))||(r.precision=i),Jr(r,u);case"":case"e":case"g":case"p":case"r":null!=r.precision||isNaN(i=function(e,t){return e=Math.abs(e),t=Math.abs(t)-e,Math.max(0,ei(t)-ei(e))+1}(o,Math.max(Math.abs(e),Math.abs(t))))||(r.precision=i-("e"===r.type));break;case"f":case"%":null!=r.precision||isNaN(i=function(e){return Math.max(0,-ei(Math.abs(e)))}(o))||(r.precision=i-2*("%"===r.type))}return Zr(r)};function ai(e){var t=e.domain;return e.ticks=function(e){var n=t();return d(n[0],n[n.length-1],null==e?10:e)},e.tickFormat=function(e,n){var r=t();return ui(r[0],r[r.length-1],null==e?10:e,n)},e.nice=function(n){null==n&&(n=10);var r,i=t(),o=0,u=i.length-1,a=i[o],l=i[u];return l<a&&(r=a,a=l,l=r,r=o,o=u,u=r),(r=p(a,l,n))>0?r=p(a=Math.floor(a/r)*r,l=Math.ceil(l/r)*r,n):r<0&&(r=p(a=Math.ceil(a*r)/r,l=Math.floor(l*r)/r,n)),r>0?(i[o]=Math.floor(a/r)*r,i[u]=Math.ceil(l/r)*r,t(i)):r<0&&(i[o]=Math.ceil(a*r)/r,i[u]=Math.floor(l*r)/r,t(i)),e},e}function li(e){return function(t){return t<0?-Math.pow(-t,e):Math.pow(t,e)}}function ci(e){return e<0?-Math.sqrt(-e):Math.sqrt(e)}function si(e){return e<0?-e*e:e*e}function fi(e){var t=e(Ir,Ir),n=1;function r(){return 1===n?e(Ir,Ir):.5===n?e(ci,si):e(li(n),li(1/n))}return t.exponent=function(e){return arguments.length?(n=+e,r()):n},ai(t)}function di(){var e=fi(Qr());return e.copy=function(){return Wr(e,di()).exponent(e.exponent())},Rr.apply(e,arguments),e}function pi(){return di.apply(null,arguments).exponent(.5)}var hi=new Date,mi=new Date;function gi(e,t,n,r){function i(t){return e(t=0===arguments.length?new Date:new Date(+t)),t}return i.floor=function(t){return e(t=new Date(+t)),t},i.ceil=function(n){return e(n=new Date(n-1)),t(n,1),e(n),n},i.round=function(e){var t=i(e),n=i.ceil(e);return e-t<n-e?t:n},i.offset=function(e,n){return t(e=new Date(+e),null==n?1:Math.floor(n)),e},i.range=function(n,r,o){var u,a=[];if(n=i.ceil(n),o=null==o?1:Math.floor(o),!(n<r)||!(o>0))return a;do{a.push(u=new Date(+n)),t(n,o),e(n)}while(u<n&&n<r);return a},i.filter=function(n){return gi((function(t){if(t>=t)for(;e(t),!n(t);)t.setTime(t-1)}),(function(e,r){if(e>=e)if(r<0)for(;++r<=0;)for(;t(e,-1),!n(e););else for(;--r>=0;)for(;t(e,1),!n(e););}))},n&&(i.count=function(t,r){return hi.setTime(+t),mi.setTime(+r),e(hi),e(mi),Math.floor(n(hi,mi))},i.every=function(e){return e=Math.floor(e),isFinite(e)&&e>0?e>1?i.filter(r?function(t){return r(t)%e===0}:function(t){return i.count(0,t)%e===0}):i:null}),i}var vi=gi((function(e){e.setMonth(0,1),e.setHours(0,0,0,0)}),(function(e,t){e.setFullYear(e.getFullYear()+t)}),(function(e,t){return t.getFullYear()-e.getFullYear()}),(function(e){return e.getFullYear()}));vi.every=function(e){return isFinite(e=Math.floor(e))&&e>0?gi((function(t){t.setFullYear(Math.floor(t.getFullYear()/e)*e),t.setMonth(0,1),t.setHours(0,0,0,0)}),(function(t,n){t.setFullYear(t.getFullYear()+n*e)})):null};var yi=vi,bi=(vi.range,gi((function(e){e.setDate(1),e.setHours(0,0,0,0)}),(function(e,t){e.setMonth(e.getMonth()+t)}),(function(e,t){return t.getMonth()-e.getMonth()+12*(t.getFullYear()-e.getFullYear())}),(function(e){return e.getMonth()})));bi.range;function wi(e){return gi((function(t){t.setDate(t.getDate()-(t.getDay()+7-e)%7),t.setHours(0,0,0,0)}),(function(e,t){e.setDate(e.getDate()+7*t)}),(function(e,t){return(t-e-6e4*(t.getTimezoneOffset()-e.getTimezoneOffset()))/6048e5}))}var xi=wi(0),_i=wi(1),ki=wi(2),Ti=wi(3),Ei=wi(4),Ci=wi(5),Si=wi(6),Mi=(xi.range,_i.range,ki.range,Ti.range,Ei.range,Ci.range,Si.range,gi((function(e){e.setHours(0,0,0,0)}),(function(e,t){e.setDate(e.getDate()+t)}),(function(e,t){return(t-e-6e4*(t.getTimezoneOffset()-e.getTimezoneOffset()))/864e5}),(function(e){return e.getDate()-1}))),Ni=Mi,Pi=(Mi.range,gi((function(e){e.setTime(e-e.getMilliseconds()-1e3*e.getSeconds()-6e4*e.getMinutes())}),(function(e,t){e.setTime(+e+36e5*t)}),(function(e,t){return(t-e)/36e5}),(function(e){return e.getHours()}))),Ai=(Pi.range,gi((function(e){e.setTime(e-e.getMilliseconds()-1e3*e.getSeconds())}),(function(e,t){e.setTime(+e+6e4*t)}),(function(e,t){return(t-e)/6e4}),(function(e){return e.getMinutes()}))),Ri=(Ai.range,gi((function(e){e.setTime(e-e.getMilliseconds())}),(function(e,t){e.setTime(+e+1e3*t)}),(function(e,t){return(t-e)/1e3}),(function(e){return e.getUTCSeconds()}))),Di=(Ri.range,gi((function(){}),(function(e,t){e.setTime(+e+t)}),(function(e,t){return t-e})));Di.every=function(e){return e=Math.floor(e),isFinite(e)&&e>0?e>1?gi((function(t){t.setTime(Math.floor(t/e)*e)}),(function(t,n){t.setTime(+t+n*e)}),(function(t,n){return(n-t)/e})):Di:null};Di.range;function Oi(e){return gi((function(t){t.setUTCDate(t.getUTCDate()-(t.getUTCDay()+7-e)%7),t.setUTCHours(0,0,0,0)}),(function(e,t){e.setUTCDate(e.getUTCDate()+7*t)}),(function(e,t){return(t-e)/6048e5}))}var zi=Oi(0),Li=Oi(1),Ui=Oi(2),Fi=Oi(3),Ii=Oi(4),ji=Oi(5),Hi=Oi(6),Vi=(zi.range,Li.range,Ui.range,Fi.range,Ii.range,ji.range,Hi.range,gi((function(e){e.setUTCHours(0,0,0,0)}),(function(e,t){e.setUTCDate(e.getUTCDate()+t)}),(function(e,t){return(t-e)/864e5}),(function(e){return e.getUTCDate()-1}))),$i=Vi,Wi=(Vi.range,gi((function(e){e.setUTCMonth(0,1),e.setUTCHours(0,0,0,0)}),(function(e,t){e.setUTCFullYear(e.getUTCFullYear()+t)}),(function(e,t){return t.getUTCFullYear()-e.getUTCFullYear()}),(function(e){return e.getUTCFullYear()})));Wi.every=function(e){return isFinite(e=Math.floor(e))&&e>0?gi((function(t){t.setUTCFullYear(Math.floor(t.getUTCFullYear()/e)*e),t.setUTCMonth(0,1),t.setUTCHours(0,0,0,0)}),(function(t,n){t.setUTCFullYear(t.getUTCFullYear()+n*e)})):null};var Qi=Wi;Wi.range;function qi(e){if(0<=e.y&&e.y<100){var t=new Date(-1,e.m,e.d,e.H,e.M,e.S,e.L);return t.setFullYear(e.y),t}return new Date(e.y,e.m,e.d,e.H,e.M,e.S,e.L)}function Bi(e){if(0<=e.y&&e.y<100){var t=new Date(Date.UTC(-1,e.m,e.d,e.H,e.M,e.S,e.L));return t.setUTCFullYear(e.y),t}return new Date(Date.UTC(e.y,e.m,e.d,e.H,e.M,e.S,e.L))}function Yi(e,t,n){return{y:e,m:t,d:n,H:0,M:0,S:0,L:0}}var Xi,Ki={"-":"",_:" ",0:"0"},Zi=/^\s*\d+/,Ji=/^%/,Gi=/[\\^$*+?|[\]().{}]/g;function eo(e,t,n){var r=e<0?"-":"",i=(r?-e:e)+"",o=i.length;return r+(o<n?new Array(n-o+1).join(t)+i:i)}function to(e){return e.replace(Gi,"\\$&")}function no(e){return new RegExp("^(?:"+e.map(to).join("|")+")","i")}function ro(e){for(var t={},n=-1,r=e.length;++n<r;)t[e[n].toLowerCase()]=n;return t}function io(e,t,n){var r=Zi.exec(t.slice(n,n+1));return r?(e.w=+r[0],n+r[0].length):-1}function oo(e,t,n){var r=Zi.exec(t.slice(n,n+1));return r?(e.u=+r[0],n+r[0].length):-1}function uo(e,t,n){var r=Zi.exec(t.slice(n,n+2));return r?(e.U=+r[0],n+r[0].length):-1}function ao(e,t,n){var r=Zi.exec(t.slice(n,n+2));return r?(e.V=+r[0],n+r[0].length):-1}function lo(e,t,n){var r=Zi.exec(t.slice(n,n+2));return r?(e.W=+r[0],n+r[0].length):-1}function co(e,t,n){var r=Zi.exec(t.slice(n,n+4));return r?(e.y=+r[0],n+r[0].length):-1}function so(e,t,n){var r=Zi.exec(t.slice(n,n+2));return r?(e.y=+r[0]+(+r[0]>68?1900:2e3),n+r[0].length):-1}function fo(e,t,n){var r=/^(Z)|([+-]\d\d)(?::?(\d\d))?/.exec(t.slice(n,n+6));return r?(e.Z=r[1]?0:-(r[2]+(r[3]||"00")),n+r[0].length):-1}function po(e,t,n){var r=Zi.exec(t.slice(n,n+1));return r?(e.q=3*r[0]-3,n+r[0].length):-1}function ho(e,t,n){var r=Zi.exec(t.slice(n,n+2));return r?(e.m=r[0]-1,n+r[0].length):-1}function mo(e,t,n){var r=Zi.exec(t.slice(n,n+2));return r?(e.d=+r[0],n+r[0].length):-1}function go(e,t,n){var r=Zi.exec(t.slice(n,n+3));return r?(e.m=0,e.d=+r[0],n+r[0].length):-1}function vo(e,t,n){var r=Zi.exec(t.slice(n,n+2));return r?(e.H=+r[0],n+r[0].length):-1}function yo(e,t,n){var r=Zi.exec(t.slice(n,n+2));return r?(e.M=+r[0],n+r[0].length):-1}function bo(e,t,n){var r=Zi.exec(t.slice(n,n+2));return r?(e.S=+r[0],n+r[0].length):-1}function wo(e,t,n){var r=Zi.exec(t.slice(n,n+3));return r?(e.L=+r[0],n+r[0].length):-1}function xo(e,t,n){var r=Zi.exec(t.slice(n,n+6));return r?(e.L=Math.floor(r[0]/1e3),n+r[0].length):-1}function _o(e,t,n){var r=Ji.exec(t.slice(n,n+1));return r?n+r[0].length:-1}function ko(e,t,n){var r=Zi.exec(t.slice(n));return r?(e.Q=+r[0],n+r[0].length):-1}function To(e,t,n){var r=Zi.exec(t.slice(n));return r?(e.s=+r[0],n+r[0].length):-1}function Eo(e,t){return eo(e.getDate(),t,2)}function Co(e,t){return eo(e.getHours(),t,2)}function So(e,t){return eo(e.getHours()%12||12,t,2)}function Mo(e,t){return eo(1+Ni.count(yi(e),e),t,3)}function No(e,t){return eo(e.getMilliseconds(),t,3)}function Po(e,t){return No(e,t)+"000"}function Ao(e,t){return eo(e.getMonth()+1,t,2)}function Ro(e,t){return eo(e.getMinutes(),t,2)}function Do(e,t){return eo(e.getSeconds(),t,2)}function Oo(e){var t=e.getDay();return 0===t?7:t}function zo(e,t){return eo(xi.count(yi(e)-1,e),t,2)}function Lo(e,t){var n=e.getDay();return e=n>=4||0===n?Ei(e):Ei.ceil(e),eo(Ei.count(yi(e),e)+(4===yi(e).getDay()),t,2)}function Uo(e){return e.getDay()}function Fo(e,t){return eo(_i.count(yi(e)-1,e),t,2)}function Io(e,t){return eo(e.getFullYear()%100,t,2)}function jo(e,t){return eo(e.getFullYear()%1e4,t,4)}function Ho(e){var t=e.getTimezoneOffset();return(t>0?"-":(t*=-1,"+"))+eo(t/60|0,"0",2)+eo(t%60,"0",2)}function Vo(e,t){return eo(e.getUTCDate(),t,2)}function $o(e,t){return eo(e.getUTCHours(),t,2)}function Wo(e,t){return eo(e.getUTCHours()%12||12,t,2)}function Qo(e,t){return eo(1+$i.count(Qi(e),e),t,3)}function qo(e,t){return eo(e.getUTCMilliseconds(),t,3)}function Bo(e,t){return qo(e,t)+"000"}function Yo(e,t){return eo(e.getUTCMonth()+1,t,2)}function Xo(e,t){return eo(e.getUTCMinutes(),t,2)}function Ko(e,t){return eo(e.getUTCSeconds(),t,2)}function Zo(e){var t=e.getUTCDay();return 0===t?7:t}function Jo(e,t){return eo(zi.count(Qi(e)-1,e),t,2)}function Go(e,t){var n=e.getUTCDay();return e=n>=4||0===n?Ii(e):Ii.ceil(e),eo(Ii.count(Qi(e),e)+(4===Qi(e).getUTCDay()),t,2)}function eu(e){return e.getUTCDay()}function tu(e,t){return eo(Li.count(Qi(e)-1,e),t,2)}function nu(e,t){return eo(e.getUTCFullYear()%100,t,2)}function ru(e,t){return eo(e.getUTCFullYear()%1e4,t,4)}function iu(){return"+0000"}function ou(){return"%"}function uu(e){return+e}function au(e){return Math.floor(+e/1e3)}!function(e){Xi=function(e){var t=e.dateTime,n=e.date,r=e.time,i=e.periods,o=e.days,u=e.shortDays,a=e.months,l=e.shortMonths,c=no(i),s=ro(i),f=no(o),d=ro(o),p=no(u),h=ro(u),m=no(a),g=ro(a),v=no(l),y=ro(l),b={a:function(e){return u[e.getDay()]},A:function(e){return o[e.getDay()]},b:function(e){return l[e.getMonth()]},B:function(e){return a[e.getMonth()]},c:null,d:Eo,e:Eo,f:Po,H:Co,I:So,j:Mo,L:No,m:Ao,M:Ro,p:function(e){return i[+(e.getHours()>=12)]},q:function(e){return 1+~~(e.getMonth()/3)},Q:uu,s:au,S:Do,u:Oo,U:zo,V:Lo,w:Uo,W:Fo,x:null,X:null,y:Io,Y:jo,Z:Ho,"%":ou},w={a:function(e){return u[e.getUTCDay()]},A:function(e){return o[e.getUTCDay()]},b:function(e){return l[e.getUTCMonth()]},B:function(e){return a[e.getUTCMonth()]},c:null,d:Vo,e:Vo,f:Bo,H:$o,I:Wo,j:Qo,L:qo,m:Yo,M:Xo,p:function(e){return i[+(e.getUTCHours()>=12)]},q:function(e){return 1+~~(e.getUTCMonth()/3)},Q:uu,s:au,S:Ko,u:Zo,U:Jo,V:Go,w:eu,W:tu,x:null,X:null,y:nu,Y:ru,Z:iu,"%":ou},x={a:function(e,t,n){var r=p.exec(t.slice(n));return r?(e.w=h[r[0].toLowerCase()],n+r[0].length):-1},A:function(e,t,n){var r=f.exec(t.slice(n));return r?(e.w=d[r[0].toLowerCase()],n+r[0].length):-1},b:function(e,t,n){var r=v.exec(t.slice(n));return r?(e.m=y[r[0].toLowerCase()],n+r[0].length):-1},B:function(e,t,n){var r=m.exec(t.slice(n));return r?(e.m=g[r[0].toLowerCase()],n+r[0].length):-1},c:function(e,n,r){return T(e,t,n,r)},d:mo,e:mo,f:xo,H:vo,I:vo,j:go,L:wo,m:ho,M:yo,p:function(e,t,n){var r=c.exec(t.slice(n));return r?(e.p=s[r[0].toLowerCase()],n+r[0].length):-1},q:po,Q:ko,s:To,S:bo,u:oo,U:uo,V:ao,w:io,W:lo,x:function(e,t,r){return T(e,n,t,r)},X:function(e,t,n){return T(e,r,t,n)},y:so,Y:co,Z:fo,"%":_o};function _(e,t){return function(n){var r,i,o,u=[],a=-1,l=0,c=e.length;for(n instanceof Date||(n=new Date(+n));++a<c;)37===e.charCodeAt(a)&&(u.push(e.slice(l,a)),null!=(i=Ki[r=e.charAt(++a)])?r=e.charAt(++a):i="e"===r?" ":"0",(o=t[r])&&(r=o(n,i)),u.push(r),l=a+1);return u.push(e.slice(l,a)),u.join("")}}function k(e,t){return function(n){var r,i,o=Yi(1900,void 0,1);if(T(o,e,n+="",0)!=n.length)return null;if("Q"in o)return new Date(o.Q);if("s"in o)return new Date(1e3*o.s+("L"in o?o.L:0));if(t&&!("Z"in o)&&(o.Z=0),"p"in o&&(o.H=o.H%12+12*o.p),void 0===o.m&&(o.m="q"in o?o.q:0),"V"in o){if(o.V<1||o.V>53)return null;"w"in o||(o.w=1),"Z"in o?(i=(r=Bi(Yi(o.y,0,1))).getUTCDay(),r=i>4||0===i?Li.ceil(r):Li(r),r=$i.offset(r,7*(o.V-1)),o.y=r.getUTCFullYear(),o.m=r.getUTCMonth(),o.d=r.getUTCDate()+(o.w+6)%7):(i=(r=qi(Yi(o.y,0,1))).getDay(),r=i>4||0===i?_i.ceil(r):_i(r),r=Ni.offset(r,7*(o.V-1)),o.y=r.getFullYear(),o.m=r.getMonth(),o.d=r.getDate()+(o.w+6)%7)}else("W"in o||"U"in o)&&("w"in o||(o.w="u"in o?o.u%7:"W"in o?1:0),i="Z"in o?Bi(Yi(o.y,0,1)).getUTCDay():qi(Yi(o.y,0,1)).getDay(),o.m=0,o.d="W"in o?(o.w+6)%7+7*o.W-(i+5)%7:o.w+7*o.U-(i+6)%7);return"Z"in o?(o.H+=o.Z/100|0,o.M+=o.Z%100,Bi(o)):qi(o)}}function T(e,t,n,r){for(var i,o,u=0,a=t.length,l=n.length;u<a;){if(r>=l)return-1;if(37===(i=t.charCodeAt(u++))){if(i=t.charAt(u++),!(o=x[i in Ki?t.charAt(u++):i])||(r=o(e,n,r))<0)return-1}else if(i!=n.charCodeAt(r++))return-1}return r}return b.x=_(n,b),b.X=_(r,b),b.c=_(t,b),w.x=_(n,w),w.X=_(r,w),w.c=_(t,w),{format:function(e){var t=_(e+="",b);return t.toString=function(){return e},t},parse:function(e){var t=k(e+="",!1);return t.toString=function(){return e},t},utcFormat:function(e){var t=_(e+="",w);return t.toString=function(){return e},t},utcParse:function(e){var t=k(e+="",!0);return t.toString=function(){return e},t}}}(e),Xi.format,Xi.parse,Xi.utcFormat,Xi.utcParse}({dateTime:"%x, %X",date:"%-m/%-d/%Y",time:"%-I:%M:%S %p",periods:["AM","PM"],days:["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],shortDays:["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],months:["January","February","March","April","May","June","July","August","September","October","November","December"],shortMonths:["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"]});var lu=gi((function(e){e.setUTCDate(1),e.setUTCHours(0,0,0,0)}),(function(e,t){e.setUTCMonth(e.getUTCMonth()+t)}),(function(e,t){return t.getUTCMonth()-e.getUTCMonth()+12*(t.getUTCFullYear()-e.getUTCFullYear())}),(function(e){return e.getUTCMonth()})),cu=(lu.range,gi((function(e){e.setUTCMinutes(0,0,0)}),(function(e,t){e.setTime(+e+36e5*t)}),(function(e,t){return(t-e)/36e5}),(function(e){return e.getUTCHours()}))),su=(cu.range,gi((function(e){e.setUTCSeconds(0,0)}),(function(e,t){e.setTime(+e+6e4*t)}),(function(e,t){return(t-e)/6e4}),(function(e){return e.getUTCMinutes()})));su.range;function fu(){this._=null}function du(e){e.U=e.C=e.L=e.R=e.P=e.N=null}function pu(e,t){var n=t,r=t.R,i=n.U;i?i.L===n?i.L=r:i.R=r:e._=r,r.U=i,n.U=r,n.R=r.L,n.R&&(n.R.U=n),r.L=n}function hu(e,t){var n=t,r=t.L,i=n.U;i?i.L===n?i.L=r:i.R=r:e._=r,r.U=i,n.U=r,n.L=r.R,n.L&&(n.L.U=n),r.R=n}function mu(e){for(;e.L;)e=e.L;return e}fu.prototype={constructor:fu,insert:function(e,t){var n,r,i;if(e){if(t.P=e,t.N=e.N,e.N&&(e.N.P=t),e.N=t,e.R){for(e=e.R;e.L;)e=e.L;e.L=t}else e.R=t;n=e}else this._?(e=mu(this._),t.P=null,t.N=e,e.P=e.L=t,n=e):(t.P=t.N=null,this._=t,n=null);for(t.L=t.R=null,t.U=n,t.C=!0,e=t;n&&n.C;)n===(r=n.U).L?(i=r.R)&&i.C?(n.C=i.C=!1,r.C=!0,e=r):(e===n.R&&(pu(this,n),n=(e=n).U),n.C=!1,r.C=!0,hu(this,r)):(i=r.L)&&i.C?(n.C=i.C=!1,r.C=!0,e=r):(e===n.L&&(hu(this,n),n=(e=n).U),n.C=!1,r.C=!0,pu(this,r)),n=e.U;this._.C=!1},remove:function(e){e.N&&(e.N.P=e.P),e.P&&(e.P.N=e.N),e.N=e.P=null;var t,n,r,i=e.U,o=e.L,u=e.R;if(n=o?u?mu(u):o:u,i?i.L===e?i.L=n:i.R=n:this._=n,o&&u?(r=n.C,n.C=e.C,n.L=o,o.U=n,n!==u?(i=n.U,n.U=e.U,e=n.R,i.L=e,n.R=u,u.U=n):(n.U=i,i=n,e=n.R)):(r=e.C,e=n),e&&(e.U=i),!r)if(e&&e.C)e.C=!1;else{do{if(e===this._)break;if(e===i.L){if((t=i.R).C&&(t.C=!1,i.C=!0,pu(this,i),t=i.R),t.L&&t.L.C||t.R&&t.R.C){t.R&&t.R.C||(t.L.C=!1,t.C=!0,hu(this,t),t=i.R),t.C=i.C,i.C=t.R.C=!1,pu(this,i),e=this._;break}}else if((t=i.L).C&&(t.C=!1,i.C=!0,hu(this,i),t=i.L),t.L&&t.L.C||t.R&&t.R.C){t.L&&t.L.C||(t.R.C=!1,t.C=!0,pu(this,t),t=i.L),t.C=i.C,i.C=t.L.C=!1,hu(this,i),e=this._;break}t.C=!0,e=i,i=i.U}while(!e.C);e&&(e.C=!1)}}};var gu=fu;function vu(e,t,n,r){var i=[null,null],o=Hu.push(i)-1;return i.left=e,i.right=t,n&&bu(i,e,t,n),r&&bu(i,t,e,r),Iu[e.index].halfedges.push(o),Iu[t.index].halfedges.push(o),i}function yu(e,t,n){var r=[t,n];return r.left=e,r}function bu(e,t,n,r){e[0]||e[1]?e.left===n?e[1]=r:e[0]=r:(e[0]=r,e.left=t,e.right=n)}function wu(e,t,n,r,i){var o,u=e[0],a=e[1],l=u[0],c=u[1],s=0,f=1,d=a[0]-l,p=a[1]-c;if(o=t-l,d||!(o>0)){if(o/=d,d<0){if(o<s)return;o<f&&(f=o)}else if(d>0){if(o>f)return;o>s&&(s=o)}if(o=r-l,d||!(o<0)){if(o/=d,d<0){if(o>f)return;o>s&&(s=o)}else if(d>0){if(o<s)return;o<f&&(f=o)}if(o=n-c,p||!(o>0)){if(o/=p,p<0){if(o<s)return;o<f&&(f=o)}else if(p>0){if(o>f)return;o>s&&(s=o)}if(o=i-c,p||!(o<0)){if(o/=p,p<0){if(o>f)return;o>s&&(s=o)}else if(p>0){if(o<s)return;o<f&&(f=o)}return!(s>0||f<1)||(s>0&&(e[0]=[l+s*d,c+s*p]),f<1&&(e[1]=[l+f*d,c+f*p]),!0)}}}}}function xu(e,t,n,r,i){var o=e[1];if(o)return!0;var u,a,l=e[0],c=e.left,s=e.right,f=c[0],d=c[1],p=s[0],h=s[1],m=(f+p)/2,g=(d+h)/2;if(h===d){if(m<t||m>=r)return;if(f>p){if(l){if(l[1]>=i)return}else l=[m,n];o=[m,i]}else{if(l){if(l[1]<n)return}else l=[m,i];o=[m,n]}}else if(a=g-(u=(f-p)/(h-d))*m,u<-1||u>1)if(f>p){if(l){if(l[1]>=i)return}else l=[(n-a)/u,n];o=[(i-a)/u,i]}else{if(l){if(l[1]<n)return}else l=[(i-a)/u,i];o=[(n-a)/u,n]}else if(d<h){if(l){if(l[0]>=r)return}else l=[t,u*t+a];o=[r,u*r+a]}else{if(l){if(l[0]<t)return}else l=[r,u*r+a];o=[t,u*t+a]}return e[0]=l,e[1]=o,!0}function _u(e,t){var n=e.site,r=t.left,i=t.right;return n===i&&(i=r,r=n),i?Math.atan2(i[1]-r[1],i[0]-r[0]):(n===r?(r=t[1],i=t[0]):(r=t[0],i=t[1]),Math.atan2(r[0]-i[0],i[1]-r[1]))}function ku(e,t){return t[+(t.left!==e.site)]}function Tu(e,t){return t[+(t.left===e.site)]}var Eu,Cu=[];function Su(){du(this),this.x=this.y=this.arc=this.site=this.cy=null}function Mu(e){var t=e.P,n=e.N;if(t&&n){var r=t.site,i=e.site,o=n.site;if(r!==o){var u=i[0],a=i[1],l=r[0]-u,c=r[1]-a,s=o[0]-u,f=o[1]-a,d=2*(l*f-c*s);if(!(d>=-$u)){var p=l*l+c*c,h=s*s+f*f,m=(f*p-c*h)/d,g=(l*h-s*p)/d,v=Cu.pop()||new Su;v.arc=e,v.site=i,v.x=m+u,v.y=(v.cy=g+a)+Math.sqrt(m*m+g*g),e.circle=v;for(var y=null,b=ju._;b;)if(v.y<b.y||v.y===b.y&&v.x<=b.x){if(!b.L){y=b.P;break}b=b.L}else{if(!b.R){y=b;break}b=b.R}ju.insert(y,v),y||(Eu=v)}}}}function Nu(e){var t=e.circle;t&&(t.P||(Eu=t.N),ju.remove(t),Cu.push(t),du(t),e.circle=null)}var Pu=[];function Au(){du(this),this.edge=this.site=this.circle=null}function Ru(e){var t=Pu.pop()||new Au;return t.site=e,t}function Du(e){Nu(e),Fu.remove(e),Pu.push(e),du(e)}function Ou(e){var t=e.circle,n=t.x,r=t.cy,i=[n,r],o=e.P,u=e.N,a=[e];Du(e);for(var l=o;l.circle&&Math.abs(n-l.circle.x)<Vu&&Math.abs(r-l.circle.cy)<Vu;)o=l.P,a.unshift(l),Du(l),l=o;a.unshift(l),Nu(l);for(var c=u;c.circle&&Math.abs(n-c.circle.x)<Vu&&Math.abs(r-c.circle.cy)<Vu;)u=c.N,a.push(c),Du(c),c=u;a.push(c),Nu(c);var s,f=a.length;for(s=1;s<f;++s)c=a[s],l=a[s-1],bu(c.edge,l.site,c.site,i);l=a[0],(c=a[f-1]).edge=vu(l.site,c.site,null,i),Mu(l),Mu(c)}function zu(e){for(var t,n,r,i,o=e[0],u=e[1],a=Fu._;a;)if((r=Lu(a,u)-o)>Vu)a=a.L;else{if(!((i=o-Uu(a,u))>Vu)){r>-Vu?(t=a.P,n=a):i>-Vu?(t=a,n=a.N):t=n=a;break}if(!a.R){t=a;break}a=a.R}!function(e){Iu[e.index]={site:e,halfedges:[]}}(e);var l=Ru(e);if(Fu.insert(t,l),t||n){if(t===n)return Nu(t),n=Ru(t.site),Fu.insert(l,n),l.edge=n.edge=vu(t.site,l.site),Mu(t),void Mu(n);if(n){Nu(t),Nu(n);var c=t.site,s=c[0],f=c[1],d=e[0]-s,p=e[1]-f,h=n.site,m=h[0]-s,g=h[1]-f,v=2*(d*g-p*m),y=d*d+p*p,b=m*m+g*g,w=[(g*y-p*b)/v+s,(d*b-m*y)/v+f];bu(n.edge,c,h,w),l.edge=vu(c,e,null,w),n.edge=vu(e,h,null,w),Mu(t),Mu(n)}else l.edge=vu(t.site,l.site)}}function Lu(e,t){var n=e.site,r=n[0],i=n[1],o=i-t;if(!o)return r;var u=e.P;if(!u)return-1/0;var a=(n=u.site)[0],l=n[1],c=l-t;if(!c)return a;var s=a-r,f=1/o-1/c,d=s/c;return f?(-d+Math.sqrt(d*d-2*f*(s*s/(-2*c)-l+c/2+i-o/2)))/f+r:(r+a)/2}function Uu(e,t){var n=e.N;if(n)return Lu(n,t);var r=e.site;return r[1]===t?r[0]:1/0}var Fu,Iu,ju,Hu,Vu=1e-6,$u=1e-12;function Wu(e,t){return t[1]-e[1]||t[0]-e[0]}function Qu(e,t){var n,r,i,o=e.sort(Wu).pop();for(Hu=[],Iu=new Array(e.length),Fu=new gu,ju=new gu;;)if(i=Eu,o&&(!i||o[1]<i.y||o[1]===i.y&&o[0]<i.x))o[0]===n&&o[1]===r||(zu(o),n=o[0],r=o[1]),o=e.pop();else{if(!i)break;Ou(i.arc)}if(function(){for(var e,t,n,r,i=0,o=Iu.length;i<o;++i)if((e=Iu[i])&&(r=(t=e.halfedges).length)){var u=new Array(r),a=new Array(r);for(n=0;n<r;++n)u[n]=n,a[n]=_u(e,Hu[t[n]]);for(u.sort((function(e,t){return a[t]-a[e]})),n=0;n<r;++n)a[n]=t[u[n]];for(n=0;n<r;++n)t[n]=a[n]}}(),t){var u=+t[0][0],a=+t[0][1],l=+t[1][0],c=+t[1][1];!function(e,t,n,r){for(var i,o=Hu.length;o--;)xu(i=Hu[o],e,t,n,r)&&wu(i,e,t,n,r)&&(Math.abs(i[0][0]-i[1][0])>Vu||Math.abs(i[0][1]-i[1][1])>Vu)||delete Hu[o]}(u,a,l,c),function(e,t,n,r){var i,o,u,a,l,c,s,f,d,p,h,m,g=Iu.length,v=!0;for(i=0;i<g;++i)if(o=Iu[i]){for(u=o.site,a=(l=o.halfedges).length;a--;)Hu[l[a]]||l.splice(a,1);for(a=0,c=l.length;a<c;)h=(p=Tu(o,Hu[l[a]]))[0],m=p[1],f=(s=ku(o,Hu[l[++a%c]]))[0],d=s[1],(Math.abs(h-f)>Vu||Math.abs(m-d)>Vu)&&(l.splice(a,0,Hu.push(yu(u,p,Math.abs(h-e)<Vu&&r-m>Vu?[e,Math.abs(f-e)<Vu?d:r]:Math.abs(m-r)<Vu&&n-h>Vu?[Math.abs(d-r)<Vu?f:n,r]:Math.abs(h-n)<Vu&&m-t>Vu?[n,Math.abs(f-n)<Vu?d:t]:Math.abs(m-t)<Vu&&h-e>Vu?[Math.abs(d-t)<Vu?f:e,t]:null))-1),++c);c&&(v=!1)}if(v){var y,b,w,x=1/0;for(i=0,v=null;i<g;++i)(o=Iu[i])&&(w=(y=(u=o.site)[0]-e)*y+(b=u[1]-t)*b)<x&&(x=w,v=o);if(v){var _=[e,t],k=[e,r],T=[n,r],E=[n,t];v.halfedges.push(Hu.push(yu(u=v.site,_,k))-1,Hu.push(yu(u,k,T))-1,Hu.push(yu(u,T,E))-1,Hu.push(yu(u,E,_))-1)}}for(i=0;i<g;++i)(o=Iu[i])&&(o.halfedges.length||delete Iu[i])}(u,a,l,c)}this.edges=Hu,this.cells=Iu,Fu=ju=Hu=Iu=null}Qu.prototype={constructor:Qu,polygons:function(){var e=this.edges;return this.cells.map((function(t){var n=t.halfedges.map((function(n){return ku(t,e[n])}));return n.data=t.site.data,n}))},triangles:function(){var e=[],t=this.edges;return this.cells.forEach((function(n,r){if(o=(i=n.halfedges).length)for(var i,o,u,a,l,c,s=n.site,f=-1,d=t[i[o-1]],p=d.left===s?d.right:d.left;++f<o;)u=p,p=(d=t[i[f]]).left===s?d.right:d.left,u&&p&&r<u.index&&r<p.index&&(l=u,c=p,((a=s)[0]-c[0])*(l[1]-a[1])-(a[0]-l[0])*(c[1]-a[1])<0)&&e.push([s.data,u.data,p.data])})),e},links:function(){return this.edges.filter((function(e){return e.right})).map((function(e){return{source:e.left.data,target:e.right.data}}))},find:function(e,t,n){for(var r,i,o=this,u=o._found||0,a=o.cells.length;!(i=o.cells[u]);)if(++u>=a)return null;var l=e-i.site[0],c=t-i.site[1],s=l*l+c*c;do{i=o.cells[r=u],u=null,i.halfedges.forEach((function(n){var r=o.edges[n],a=r.left;if(a!==i.site&&a||(a=r.right)){var l=e-a[0],c=t-a[1],f=l*l+c*c;f<s&&(s=f,u=a.index)}}))}while(null!==u);return o._found=r,null==n||s<=n*n?i.site:null}};Math.SQRT2;function qu(e,t,n){this.k=e,this.x=t,this.y=n}qu.prototype={constructor:qu,scale:function(e){return 1===e?this:new qu(this.k*e,this.x,this.y)},translate:function(e,t){return 0===e&0===t?this:new qu(this.k,this.x+this.k*e,this.y+this.k*t)},apply:function(e){return[e[0]*this.k+this.x,e[1]*this.k+this.y]},applyX:function(e){return e*this.k+this.x},applyY:function(e){return e*this.k+this.y},invert:function(e){return[(e[0]-this.x)/this.k,(e[1]-this.y)/this.k]},invertX:function(e){return(e-this.x)/this.k},invertY:function(e){return(e-this.y)/this.k},rescaleX:function(e){return e.copy().domain(e.range().map(this.invertX,this).map(e.invert,e))},rescaleY:function(e){return e.copy().domain(e.range().map(this.invertY,this).map(e.invert,e))},toString:function(){return"translate("+this.x+","+this.y+") scale("+this.k+")"}};new qu(1,0,0);qu.prototype},,,,,,function(e,t,n){"use strict";!function e(){if("undefined"!==typeof __REACT_DEVTOOLS_GLOBAL_HOOK__&&"function"===typeof __REACT_DEVTOOLS_GLOBAL_HOOK__.checkDCE){0;try{__REACT_DEVTOOLS_GLOBAL_HOOK__.checkDCE(e)}catch(t){console.error(t)}}}(),e.exports=n(30)},function(e,t,n){"use strict";function r(e,t){if(!(e instanceof t))throw new TypeError("Cannot call a class as a function")}n.d(t,"a",(function(){return r}))},function(e,t,n){"use strict";function r(e,t){for(var n=0;n<t.length;n++){var r=t[n];r.enumerable=r.enumerable||!1,r.configurable=!0,"value"in r&&(r.writable=!0),Object.defineProperty(e,r.key,r)}}function i(e,t,n){return t&&r(e.prototype,t),n&&r(e,n),e}n.d(t,"a",(function(){return i}))},function(e,t,n){"use strict";function r(e){if(void 0===e)throw new ReferenceError("this hasn't been initialised - super() hasn't been called");return e}n.d(t,"a",(function(){return r}))},function(e,t,n){"use strict";function r(e){return(r=Object.setPrototypeOf?Object.getPrototypeOf:function(e){return e.__proto__||Object.getPrototypeOf(e)})(e)}function i(){if("undefined"===typeof Reflect||!Reflect.construct)return!1;if(Reflect.construct.sham)return!1;if("function"===typeof Proxy)return!0;try{return Date.prototype.toString.call(Reflect.construct(Date,[],(function(){}))),!0}catch(e){return!1}}function o(e){return(o="function"===typeof Symbol&&"symbol"===typeof Symbol.iterator?function(e){return typeof e}:function(e){return e&&"function"===typeof Symbol&&e.constructor===Symbol&&e!==Symbol.prototype?"symbol":typeof e})(e)}n.d(t,"a",(function(){return l}));var u=n(11);function a(e,t){return!t||"object"!==o(t)&&"function"!==typeof t?Object(u.a)(e):t}function l(e){return function(){var t,n=r(e);if(i()){var o=r(this).constructor;t=Reflect.construct(n,arguments,o)}else t=n.apply(this,arguments);return a(this,t)}}},function(e,t,n){"use strict";function r(e,t){return(r=Object.setPrototypeOf||function(e,t){return e.__proto__=t,e})(e,t)}function i(e,t){if("function"!==typeof t&&null!==t)throw new TypeError("Super expression must either be null or a function");e.prototype=Object.create(t&&t.prototype,{constructor:{value:e,writable:!0,configurable:!0}}),t&&r(e,t)}n.d(t,"a",(function(){return i}))},,,,,,,,,function(e,t,n){"use strict";var r=Object.getOwnPropertySymbols,i=Object.prototype.hasOwnProperty,o=Object.prototype.propertyIsEnumerable;function u(e){if(null===e||void 0===e)throw new TypeError("Object.assign cannot be called with null or undefined");return Object(e)}e.exports=function(){try{if(!Object.assign)return!1;var e=new String("abc");if(e[5]="de","5"===Object.getOwnPropertyNames(e)[0])return!1;for(var t={},n=0;n<10;n++)t["_"+String.fromCharCode(n)]=n;if("0123456789"!==Object.getOwnPropertyNames(t).map((function(e){return t[e]})).join(""))return!1;var r={};return"abcdefghijklmnopqrst".split("").forEach((function(e){r[e]=e})),"abcdefghijklmnopqrst"===Object.keys(Object.assign({},r)).join("")}catch(i){return!1}}()?Object.assign:function(e,t){for(var n,a,l=u(e),c=1;c<arguments.length;c++){for(var s in n=Object(arguments[c]))i.call(n,s)&&(l[s]=n[s]);if(r){a=r(n);for(var f=0;f<a.length;f++)o.call(n,a[f])&&(l[a[f]]=n[a[f]])}}return l}},,,,,,,function(e,t,n){"use strict";var r=n(22),i="function"===typeof Symbol&&Symbol.for,o=i?Symbol.for("react.element"):60103,u=i?Symbol.for("react.portal"):60106,a=i?Symbol.for("react.fragment"):60107,l=i?Symbol.for("react.strict_mode"):60108,c=i?Symbol.for("react.profiler"):60114,s=i?Symbol.for("react.provider"):60109,f=i?Symbol.for("react.context"):60110,d=i?Symbol.for("react.forward_ref"):60112,p=i?Symbol.for("react.suspense"):60113,h=i?Symbol.for("react.memo"):60115,m=i?Symbol.for("react.lazy"):60116,g="function"===typeof Symbol&&Symbol.iterator;function v(e){for(var t="https://reactjs.org/docs/error-decoder.html?invariant="+e,n=1;n<arguments.length;n++)t+="&args[]="+encodeURIComponent(arguments[n]);return"Minified React error #"+e+"; visit "+t+" for the full message or use the non-minified dev environment for full errors and additional helpful warnings."}var y={isMounted:function(){return!1},enqueueForceUpdate:function(){},enqueueReplaceState:function(){},enqueueSetState:function(){}},b={};function w(e,t,n){this.props=e,this.context=t,this.refs=b,this.updater=n||y}function x(){}function _(e,t,n){this.props=e,this.context=t,this.refs=b,this.updater=n||y}w.prototype.isReactComponent={},w.prototype.setState=function(e,t){if("object"!==typeof e&&"function"!==typeof e&&null!=e)throw Error(v(85));this.updater.enqueueSetState(this,e,t,"setState")},w.prototype.forceUpdate=function(e){this.updater.enqueueForceUpdate(this,e,"forceUpdate")},x.prototype=w.prototype;var k=_.prototype=new x;k.constructor=_,r(k,w.prototype),k.isPureReactComponent=!0;var T={current:null},E=Object.prototype.hasOwnProperty,C={key:!0,ref:!0,__self:!0,__source:!0};function S(e,t,n){var r,i={},u=null,a=null;if(null!=t)for(r in void 0!==t.ref&&(a=t.ref),void 0!==t.key&&(u=""+t.key),t)E.call(t,r)&&!C.hasOwnProperty(r)&&(i[r]=t[r]);var l=arguments.length-2;if(1===l)i.children=n;else if(1<l){for(var c=Array(l),s=0;s<l;s++)c[s]=arguments[s+2];i.children=c}if(e&&e.defaultProps)for(r in l=e.defaultProps)void 0===i[r]&&(i[r]=l[r]);return{$$typeof:o,type:e,key:u,ref:a,props:i,_owner:T.current}}function M(e){return"object"===typeof e&&null!==e&&e.$$typeof===o}var N=/\/+/g,P=[];function A(e,t,n,r){if(P.length){var i=P.pop();return i.result=e,i.keyPrefix=t,i.func=n,i.context=r,i.count=0,i}return{result:e,keyPrefix:t,func:n,context:r,count:0}}function R(e){e.result=null,e.keyPrefix=null,e.func=null,e.context=null,e.count=0,10>P.length&&P.push(e)}function D(e,t,n){return null==e?0:function e(t,n,r,i){var a=typeof t;"undefined"!==a&&"boolean"!==a||(t=null);var l=!1;if(null===t)l=!0;else switch(a){case"string":case"number":l=!0;break;case"object":switch(t.$$typeof){case o:case u:l=!0}}if(l)return r(i,t,""===n?"."+O(t,0):n),1;if(l=0,n=""===n?".":n+":",Array.isArray(t))for(var c=0;c<t.length;c++){var s=n+O(a=t[c],c);l+=e(a,s,r,i)}else if(null===t||"object"!==typeof t?s=null:s="function"===typeof(s=g&&t[g]||t["@@iterator"])?s:null,"function"===typeof s)for(t=s.call(t),c=0;!(a=t.next()).done;)l+=e(a=a.value,s=n+O(a,c++),r,i);else if("object"===a)throw r=""+t,Error(v(31,"[object Object]"===r?"object with keys {"+Object.keys(t).join(", ")+"}":r,""));return l}(e,"",t,n)}function O(e,t){return"object"===typeof e&&null!==e&&null!=e.key?function(e){var t={"=":"=0",":":"=2"};return"$"+(""+e).replace(/[=:]/g,(function(e){return t[e]}))}(e.key):t.toString(36)}function z(e,t){e.func.call(e.context,t,e.count++)}function L(e,t,n){var r=e.result,i=e.keyPrefix;e=e.func.call(e.context,t,e.count++),Array.isArray(e)?U(e,r,n,(function(e){return e})):null!=e&&(M(e)&&(e=function(e,t){return{$$typeof:o,type:e.type,key:t,ref:e.ref,props:e.props,_owner:e._owner}}(e,i+(!e.key||t&&t.key===e.key?"":(""+e.key).replace(N,"$&/")+"/")+n)),r.push(e))}function U(e,t,n,r,i){var o="";null!=n&&(o=(""+n).replace(N,"$&/")+"/"),D(e,L,t=A(t,o,r,i)),R(t)}var F={current:null};function I(){var e=F.current;if(null===e)throw Error(v(321));return e}var j={ReactCurrentDispatcher:F,ReactCurrentBatchConfig:{suspense:null},ReactCurrentOwner:T,IsSomeRendererActing:{current:!1},assign:r};t.Children={map:function(e,t,n){if(null==e)return e;var r=[];return U(e,r,null,t,n),r},forEach:function(e,t,n){if(null==e)return e;D(e,z,t=A(null,null,t,n)),R(t)},count:function(e){return D(e,(function(){return null}),null)},toArray:function(e){var t=[];return U(e,t,null,(function(e){return e})),t},only:function(e){if(!M(e))throw Error(v(143));return e}},t.Component=w,t.Fragment=a,t.Profiler=c,t.PureComponent=_,t.StrictMode=l,t.Suspense=p,t.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED=j,t.cloneElement=function(e,t,n){if(null===e||void 0===e)throw Error(v(267,e));var i=r({},e.props),u=e.key,a=e.ref,l=e._owner;if(null!=t){if(void 0!==t.ref&&(a=t.ref,l=T.current),void 0!==t.key&&(u=""+t.key),e.type&&e.type.defaultProps)var c=e.type.defaultProps;for(s in t)E.call(t,s)&&!C.hasOwnProperty(s)&&(i[s]=void 0===t[s]&&void 0!==c?c[s]:t[s])}var s=arguments.length-2;if(1===s)i.children=n;else if(1<s){c=Array(s);for(var f=0;f<s;f++)c[f]=arguments[f+2];i.children=c}return{$$typeof:o,type:e.type,key:u,ref:a,props:i,_owner:l}},t.createContext=function(e,t){return void 0===t&&(t=null),(e={$$typeof:f,_calculateChangedBits:t,_currentValue:e,_currentValue2:e,_threadCount:0,Provider:null,Consumer:null}).Provider={$$typeof:s,_context:e},e.Consumer=e},t.createElement=S,t.createFactory=function(e){var t=S.bind(null,e);return t.type=e,t},t.createRef=function(){return{current:null}},t.forwardRef=function(e){return{$$typeof:d,render:e}},t.isValidElement=M,t.lazy=function(e){return{$$typeof:m,_ctor:e,_status:-1,_result:null}},t.memo=function(e,t){return{$$typeof:h,type:e,compare:void 0===t?null:t}},t.useCallback=function(e,t){return I().useCallback(e,t)},t.useContext=function(e,t){return I().useContext(e,t)},t.useDebugValue=function(){},t.useEffect=function(e,t){return I().useEffect(e,t)},t.useImperativeHandle=function(e,t,n){return I().useImperativeHandle(e,t,n)},t.useLayoutEffect=function(e,t){return I().useLayoutEffect(e,t)},t.useMemo=function(e,t){return I().useMemo(e,t)},t.useReducer=function(e,t,n){return I().useReducer(e,t,n)},t.useRef=function(e){return I().useRef(e)},t.useState=function(e){return I().useState(e)},t.version="16.13.1"},function(e,t,n){"use strict";var r=n(0),i=n(22),o=n(31);function u(e){for(var t="https://reactjs.org/docs/error-decoder.html?invariant="+e,n=1;n<arguments.length;n++)t+="&args[]="+encodeURIComponent(arguments[n]);return"Minified React error #"+e+"; visit "+t+" for the full message or use the non-minified dev environment for full errors and additional helpful warnings."}if(!r)throw Error(u(227));function a(e,t,n,r,i,o,u,a,l){var c=Array.prototype.slice.call(arguments,3);try{t.apply(n,c)}catch(s){this.onError(s)}}var l=!1,c=null,s=!1,f=null,d={onError:function(e){l=!0,c=e}};function p(e,t,n,r,i,o,u,s,f){l=!1,c=null,a.apply(d,arguments)}var h=null,m=null,g=null;function v(e,t,n){var r=e.type||"unknown-event";e.currentTarget=g(n),function(e,t,n,r,i,o,a,d,h){if(p.apply(this,arguments),l){if(!l)throw Error(u(198));var m=c;l=!1,c=null,s||(s=!0,f=m)}}(r,t,void 0,e),e.currentTarget=null}var y=null,b={};function w(){if(y)for(var e in b){var t=b[e],n=y.indexOf(e);if(!(-1<n))throw Error(u(96,e));if(!_[n]){if(!t.extractEvents)throw Error(u(97,e));for(var r in _[n]=t,n=t.eventTypes){var i=void 0,o=n[r],a=t,l=r;if(k.hasOwnProperty(l))throw Error(u(99,l));k[l]=o;var c=o.phasedRegistrationNames;if(c){for(i in c)c.hasOwnProperty(i)&&x(c[i],a,l);i=!0}else o.registrationName?(x(o.registrationName,a,l),i=!0):i=!1;if(!i)throw Error(u(98,r,e))}}}}function x(e,t,n){if(T[e])throw Error(u(100,e));T[e]=t,E[e]=t.eventTypes[n].dependencies}var _=[],k={},T={},E={};function C(e){var t,n=!1;for(t in e)if(e.hasOwnProperty(t)){var r=e[t];if(!b.hasOwnProperty(t)||b[t]!==r){if(b[t])throw Error(u(102,t));b[t]=r,n=!0}}n&&w()}var S=!("undefined"===typeof window||"undefined"===typeof window.document||"undefined"===typeof window.document.createElement),M=null,N=null,P=null;function A(e){if(e=m(e)){if("function"!==typeof M)throw Error(u(280));var t=e.stateNode;t&&(t=h(t),M(e.stateNode,e.type,t))}}function R(e){N?P?P.push(e):P=[e]:N=e}function D(){if(N){var e=N,t=P;if(P=N=null,A(e),t)for(e=0;e<t.length;e++)A(t[e])}}function O(e,t){return e(t)}function z(e,t,n,r,i){return e(t,n,r,i)}function L(){}var U=O,F=!1,I=!1;function j(){null===N&&null===P||(L(),D())}function H(e,t,n){if(I)return e(t,n);I=!0;try{return U(e,t,n)}finally{I=!1,j()}}var V=/^[:A-Z_a-z\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u02FF\u0370-\u037D\u037F-\u1FFF\u200C-\u200D\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD][:A-Z_a-z\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u02FF\u0370-\u037D\u037F-\u1FFF\u200C-\u200D\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD\-.0-9\u00B7\u0300-\u036F\u203F-\u2040]*$/,$=Object.prototype.hasOwnProperty,W={},Q={};function q(e,t,n,r,i,o){this.acceptsBooleans=2===t||3===t||4===t,this.attributeName=r,this.attributeNamespace=i,this.mustUseProperty=n,this.propertyName=e,this.type=t,this.sanitizeURL=o}var B={};"children dangerouslySetInnerHTML defaultValue defaultChecked innerHTML suppressContentEditableWarning suppressHydrationWarning style".split(" ").forEach((function(e){B[e]=new q(e,0,!1,e,null,!1)})),[["acceptCharset","accept-charset"],["className","class"],["htmlFor","for"],["httpEquiv","http-equiv"]].forEach((function(e){var t=e[0];B[t]=new q(t,1,!1,e[1],null,!1)})),["contentEditable","draggable","spellCheck","value"].forEach((function(e){B[e]=new q(e,2,!1,e.toLowerCase(),null,!1)})),["autoReverse","externalResourcesRequired","focusable","preserveAlpha"].forEach((function(e){B[e]=new q(e,2,!1,e,null,!1)})),"allowFullScreen async autoFocus autoPlay controls default defer disabled disablePictureInPicture formNoValidate hidden loop noModule noValidate open playsInline readOnly required reversed scoped seamless itemScope".split(" ").forEach((function(e){B[e]=new q(e,3,!1,e.toLowerCase(),null,!1)})),["checked","multiple","muted","selected"].forEach((function(e){B[e]=new q(e,3,!0,e,null,!1)})),["capture","download"].forEach((function(e){B[e]=new q(e,4,!1,e,null,!1)})),["cols","rows","size","span"].forEach((function(e){B[e]=new q(e,6,!1,e,null,!1)})),["rowSpan","start"].forEach((function(e){B[e]=new q(e,5,!1,e.toLowerCase(),null,!1)}));var Y=/[\-:]([a-z])/g;function X(e){return e[1].toUpperCase()}"accent-height alignment-baseline arabic-form baseline-shift cap-height clip-path clip-rule color-interpolation color-interpolation-filters color-profile color-rendering dominant-baseline enable-background fill-opacity fill-rule flood-color flood-opacity font-family font-size font-size-adjust font-stretch font-style font-variant font-weight glyph-name glyph-orientation-horizontal glyph-orientation-vertical horiz-adv-x horiz-origin-x image-rendering letter-spacing lighting-color marker-end marker-mid marker-start overline-position overline-thickness paint-order panose-1 pointer-events rendering-intent shape-rendering stop-color stop-opacity strikethrough-position strikethrough-thickness stroke-dasharray stroke-dashoffset stroke-linecap stroke-linejoin stroke-miterlimit stroke-opacity stroke-width text-anchor text-decoration text-rendering underline-position underline-thickness unicode-bidi unicode-range units-per-em v-alphabetic v-hanging v-ideographic v-mathematical vector-effect vert-adv-y vert-origin-x vert-origin-y word-spacing writing-mode xmlns:xlink x-height".split(" ").forEach((function(e){var t=e.replace(Y,X);B[t]=new q(t,1,!1,e,null,!1)})),"xlink:actuate xlink:arcrole xlink:role xlink:show xlink:title xlink:type".split(" ").forEach((function(e){var t=e.replace(Y,X);B[t]=new q(t,1,!1,e,"http://www.w3.org/1999/xlink",!1)})),["xml:base","xml:lang","xml:space"].forEach((function(e){var t=e.replace(Y,X);B[t]=new q(t,1,!1,e,"http://www.w3.org/XML/1998/namespace",!1)})),["tabIndex","crossOrigin"].forEach((function(e){B[e]=new q(e,1,!1,e.toLowerCase(),null,!1)})),B.xlinkHref=new q("xlinkHref",1,!1,"xlink:href","http://www.w3.org/1999/xlink",!0),["src","href","action","formAction"].forEach((function(e){B[e]=new q(e,1,!1,e.toLowerCase(),null,!0)}));var K=r.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED;function Z(e,t,n,r){var i=B.hasOwnProperty(t)?B[t]:null;(null!==i?0===i.type:!r&&(2<t.length&&("o"===t[0]||"O"===t[0])&&("n"===t[1]||"N"===t[1])))||(function(e,t,n,r){if(null===t||"undefined"===typeof t||function(e,t,n,r){if(null!==n&&0===n.type)return!1;switch(typeof t){case"function":case"symbol":return!0;case"boolean":return!r&&(null!==n?!n.acceptsBooleans:"data-"!==(e=e.toLowerCase().slice(0,5))&&"aria-"!==e);default:return!1}}(e,t,n,r))return!0;if(r)return!1;if(null!==n)switch(n.type){case 3:return!t;case 4:return!1===t;case 5:return isNaN(t);case 6:return isNaN(t)||1>t}return!1}(t,n,i,r)&&(n=null),r||null===i?function(e){return!!$.call(Q,e)||!$.call(W,e)&&(V.test(e)?Q[e]=!0:(W[e]=!0,!1))}(t)&&(null===n?e.removeAttribute(t):e.setAttribute(t,""+n)):i.mustUseProperty?e[i.propertyName]=null===n?3!==i.type&&"":n:(t=i.attributeName,r=i.attributeNamespace,null===n?e.removeAttribute(t):(n=3===(i=i.type)||4===i&&!0===n?"":""+n,r?e.setAttributeNS(r,t,n):e.setAttribute(t,n))))}K.hasOwnProperty("ReactCurrentDispatcher")||(K.ReactCurrentDispatcher={current:null}),K.hasOwnProperty("ReactCurrentBatchConfig")||(K.ReactCurrentBatchConfig={suspense:null});var J=/^(.*)[\\\/]/,G="function"===typeof Symbol&&Symbol.for,ee=G?Symbol.for("react.element"):60103,te=G?Symbol.for("react.portal"):60106,ne=G?Symbol.for("react.fragment"):60107,re=G?Symbol.for("react.strict_mode"):60108,ie=G?Symbol.for("react.profiler"):60114,oe=G?Symbol.for("react.provider"):60109,ue=G?Symbol.for("react.context"):60110,ae=G?Symbol.for("react.concurrent_mode"):60111,le=G?Symbol.for("react.forward_ref"):60112,ce=G?Symbol.for("react.suspense"):60113,se=G?Symbol.for("react.suspense_list"):60120,fe=G?Symbol.for("react.memo"):60115,de=G?Symbol.for("react.lazy"):60116,pe=G?Symbol.for("react.block"):60121,he="function"===typeof Symbol&&Symbol.iterator;function me(e){return null===e||"object"!==typeof e?null:"function"===typeof(e=he&&e[he]||e["@@iterator"])?e:null}function ge(e){if(null==e)return null;if("function"===typeof e)return e.displayName||e.name||null;if("string"===typeof e)return e;switch(e){case ne:return"Fragment";case te:return"Portal";case ie:return"Profiler";case re:return"StrictMode";case ce:return"Suspense";case se:return"SuspenseList"}if("object"===typeof e)switch(e.$$typeof){case ue:return"Context.Consumer";case oe:return"Context.Provider";case le:var t=e.render;return t=t.displayName||t.name||"",e.displayName||(""!==t?"ForwardRef("+t+")":"ForwardRef");case fe:return ge(e.type);case pe:return ge(e.render);case de:if(e=1===e._status?e._result:null)return ge(e)}return null}function ve(e){var t="";do{e:switch(e.tag){case 3:case 4:case 6:case 7:case 10:case 9:var n="";break e;default:var r=e._debugOwner,i=e._debugSource,o=ge(e.type);n=null,r&&(n=ge(r.type)),r=o,o="",i?o=" (at "+i.fileName.replace(J,"")+":"+i.lineNumber+")":n&&(o=" (created by "+n+")"),n="\n    in "+(r||"Unknown")+o}t+=n,e=e.return}while(e);return t}function ye(e){switch(typeof e){case"boolean":case"number":case"object":case"string":case"undefined":return e;default:return""}}function be(e){var t=e.type;return(e=e.nodeName)&&"input"===e.toLowerCase()&&("checkbox"===t||"radio"===t)}function we(e){e._valueTracker||(e._valueTracker=function(e){var t=be(e)?"checked":"value",n=Object.getOwnPropertyDescriptor(e.constructor.prototype,t),r=""+e[t];if(!e.hasOwnProperty(t)&&"undefined"!==typeof n&&"function"===typeof n.get&&"function"===typeof n.set){var i=n.get,o=n.set;return Object.defineProperty(e,t,{configurable:!0,get:function(){return i.call(this)},set:function(e){r=""+e,o.call(this,e)}}),Object.defineProperty(e,t,{enumerable:n.enumerable}),{getValue:function(){return r},setValue:function(e){r=""+e},stopTracking:function(){e._valueTracker=null,delete e[t]}}}}(e))}function xe(e){if(!e)return!1;var t=e._valueTracker;if(!t)return!0;var n=t.getValue(),r="";return e&&(r=be(e)?e.checked?"true":"false":e.value),(e=r)!==n&&(t.setValue(e),!0)}function _e(e,t){var n=t.checked;return i({},t,{defaultChecked:void 0,defaultValue:void 0,value:void 0,checked:null!=n?n:e._wrapperState.initialChecked})}function ke(e,t){var n=null==t.defaultValue?"":t.defaultValue,r=null!=t.checked?t.checked:t.defaultChecked;n=ye(null!=t.value?t.value:n),e._wrapperState={initialChecked:r,initialValue:n,controlled:"checkbox"===t.type||"radio"===t.type?null!=t.checked:null!=t.value}}function Te(e,t){null!=(t=t.checked)&&Z(e,"checked",t,!1)}function Ee(e,t){Te(e,t);var n=ye(t.value),r=t.type;if(null!=n)"number"===r?(0===n&&""===e.value||e.value!=n)&&(e.value=""+n):e.value!==""+n&&(e.value=""+n);else if("submit"===r||"reset"===r)return void e.removeAttribute("value");t.hasOwnProperty("value")?Se(e,t.type,n):t.hasOwnProperty("defaultValue")&&Se(e,t.type,ye(t.defaultValue)),null==t.checked&&null!=t.defaultChecked&&(e.defaultChecked=!!t.defaultChecked)}function Ce(e,t,n){if(t.hasOwnProperty("value")||t.hasOwnProperty("defaultValue")){var r=t.type;if(!("submit"!==r&&"reset"!==r||void 0!==t.value&&null!==t.value))return;t=""+e._wrapperState.initialValue,n||t===e.value||(e.value=t),e.defaultValue=t}""!==(n=e.name)&&(e.name=""),e.defaultChecked=!!e._wrapperState.initialChecked,""!==n&&(e.name=n)}function Se(e,t,n){"number"===t&&e.ownerDocument.activeElement===e||(null==n?e.defaultValue=""+e._wrapperState.initialValue:e.defaultValue!==""+n&&(e.defaultValue=""+n))}function Me(e,t){return e=i({children:void 0},t),(t=function(e){var t="";return r.Children.forEach(e,(function(e){null!=e&&(t+=e)})),t}(t.children))&&(e.children=t),e}function Ne(e,t,n,r){if(e=e.options,t){t={};for(var i=0;i<n.length;i++)t["$"+n[i]]=!0;for(n=0;n<e.length;n++)i=t.hasOwnProperty("$"+e[n].value),e[n].selected!==i&&(e[n].selected=i),i&&r&&(e[n].defaultSelected=!0)}else{for(n=""+ye(n),t=null,i=0;i<e.length;i++){if(e[i].value===n)return e[i].selected=!0,void(r&&(e[i].defaultSelected=!0));null!==t||e[i].disabled||(t=e[i])}null!==t&&(t.selected=!0)}}function Pe(e,t){if(null!=t.dangerouslySetInnerHTML)throw Error(u(91));return i({},t,{value:void 0,defaultValue:void 0,children:""+e._wrapperState.initialValue})}function Ae(e,t){var n=t.value;if(null==n){if(n=t.children,t=t.defaultValue,null!=n){if(null!=t)throw Error(u(92));if(Array.isArray(n)){if(!(1>=n.length))throw Error(u(93));n=n[0]}t=n}null==t&&(t=""),n=t}e._wrapperState={initialValue:ye(n)}}function Re(e,t){var n=ye(t.value),r=ye(t.defaultValue);null!=n&&((n=""+n)!==e.value&&(e.value=n),null==t.defaultValue&&e.defaultValue!==n&&(e.defaultValue=n)),null!=r&&(e.defaultValue=""+r)}function De(e){var t=e.textContent;t===e._wrapperState.initialValue&&""!==t&&null!==t&&(e.value=t)}var Oe="http://www.w3.org/1999/xhtml",ze="http://www.w3.org/2000/svg";function Le(e){switch(e){case"svg":return"http://www.w3.org/2000/svg";case"math":return"http://www.w3.org/1998/Math/MathML";default:return"http://www.w3.org/1999/xhtml"}}function Ue(e,t){return null==e||"http://www.w3.org/1999/xhtml"===e?Le(t):"http://www.w3.org/2000/svg"===e&&"foreignObject"===t?"http://www.w3.org/1999/xhtml":e}var Fe,Ie=function(e){return"undefined"!==typeof MSApp&&MSApp.execUnsafeLocalFunction?function(t,n,r,i){MSApp.execUnsafeLocalFunction((function(){return e(t,n)}))}:e}((function(e,t){if(e.namespaceURI!==ze||"innerHTML"in e)e.innerHTML=t;else{for((Fe=Fe||document.createElement("div")).innerHTML="<svg>"+t.valueOf().toString()+"</svg>",t=Fe.firstChild;e.firstChild;)e.removeChild(e.firstChild);for(;t.firstChild;)e.appendChild(t.firstChild)}}));function je(e,t){if(t){var n=e.firstChild;if(n&&n===e.lastChild&&3===n.nodeType)return void(n.nodeValue=t)}e.textContent=t}function He(e,t){var n={};return n[e.toLowerCase()]=t.toLowerCase(),n["Webkit"+e]="webkit"+t,n["Moz"+e]="moz"+t,n}var Ve={animationend:He("Animation","AnimationEnd"),animationiteration:He("Animation","AnimationIteration"),animationstart:He("Animation","AnimationStart"),transitionend:He("Transition","TransitionEnd")},$e={},We={};function Qe(e){if($e[e])return $e[e];if(!Ve[e])return e;var t,n=Ve[e];for(t in n)if(n.hasOwnProperty(t)&&t in We)return $e[e]=n[t];return e}S&&(We=document.createElement("div").style,"AnimationEvent"in window||(delete Ve.animationend.animation,delete Ve.animationiteration.animation,delete Ve.animationstart.animation),"TransitionEvent"in window||delete Ve.transitionend.transition);var qe=Qe("animationend"),Be=Qe("animationiteration"),Ye=Qe("animationstart"),Xe=Qe("transitionend"),Ke="abort canplay canplaythrough durationchange emptied encrypted ended error loadeddata loadedmetadata loadstart pause play playing progress ratechange seeked seeking stalled suspend timeupdate volumechange waiting".split(" "),Ze=new("function"===typeof WeakMap?WeakMap:Map);function Je(e){var t=Ze.get(e);return void 0===t&&(t=new Map,Ze.set(e,t)),t}function Ge(e){var t=e,n=e;if(e.alternate)for(;t.return;)t=t.return;else{e=t;do{0!==(1026&(t=e).effectTag)&&(n=t.return),e=t.return}while(e)}return 3===t.tag?n:null}function et(e){if(13===e.tag){var t=e.memoizedState;if(null===t&&(null!==(e=e.alternate)&&(t=e.memoizedState)),null!==t)return t.dehydrated}return null}function tt(e){if(Ge(e)!==e)throw Error(u(188))}function nt(e){if(!(e=function(e){var t=e.alternate;if(!t){if(null===(t=Ge(e)))throw Error(u(188));return t!==e?null:e}for(var n=e,r=t;;){var i=n.return;if(null===i)break;var o=i.alternate;if(null===o){if(null!==(r=i.return)){n=r;continue}break}if(i.child===o.child){for(o=i.child;o;){if(o===n)return tt(i),e;if(o===r)return tt(i),t;o=o.sibling}throw Error(u(188))}if(n.return!==r.return)n=i,r=o;else{for(var a=!1,l=i.child;l;){if(l===n){a=!0,n=i,r=o;break}if(l===r){a=!0,r=i,n=o;break}l=l.sibling}if(!a){for(l=o.child;l;){if(l===n){a=!0,n=o,r=i;break}if(l===r){a=!0,r=o,n=i;break}l=l.sibling}if(!a)throw Error(u(189))}}if(n.alternate!==r)throw Error(u(190))}if(3!==n.tag)throw Error(u(188));return n.stateNode.current===n?e:t}(e)))return null;for(var t=e;;){if(5===t.tag||6===t.tag)return t;if(t.child)t.child.return=t,t=t.child;else{if(t===e)break;for(;!t.sibling;){if(!t.return||t.return===e)return null;t=t.return}t.sibling.return=t.return,t=t.sibling}}return null}function rt(e,t){if(null==t)throw Error(u(30));return null==e?t:Array.isArray(e)?Array.isArray(t)?(e.push.apply(e,t),e):(e.push(t),e):Array.isArray(t)?[e].concat(t):[e,t]}function it(e,t,n){Array.isArray(e)?e.forEach(t,n):e&&t.call(n,e)}var ot=null;function ut(e){if(e){var t=e._dispatchListeners,n=e._dispatchInstances;if(Array.isArray(t))for(var r=0;r<t.length&&!e.isPropagationStopped();r++)v(e,t[r],n[r]);else t&&v(e,t,n);e._dispatchListeners=null,e._dispatchInstances=null,e.isPersistent()||e.constructor.release(e)}}function at(e){if(null!==e&&(ot=rt(ot,e)),e=ot,ot=null,e){if(it(e,ut),ot)throw Error(u(95));if(s)throw e=f,s=!1,f=null,e}}function lt(e){return(e=e.target||e.srcElement||window).correspondingUseElement&&(e=e.correspondingUseElement),3===e.nodeType?e.parentNode:e}function ct(e){if(!S)return!1;var t=(e="on"+e)in document;return t||((t=document.createElement("div")).setAttribute(e,"return;"),t="function"===typeof t[e]),t}var st=[];function ft(e){e.topLevelType=null,e.nativeEvent=null,e.targetInst=null,e.ancestors.length=0,10>st.length&&st.push(e)}function dt(e,t,n,r){if(st.length){var i=st.pop();return i.topLevelType=e,i.eventSystemFlags=r,i.nativeEvent=t,i.targetInst=n,i}return{topLevelType:e,eventSystemFlags:r,nativeEvent:t,targetInst:n,ancestors:[]}}function pt(e){var t=e.targetInst,n=t;do{if(!n){e.ancestors.push(n);break}var r=n;if(3===r.tag)r=r.stateNode.containerInfo;else{for(;r.return;)r=r.return;r=3!==r.tag?null:r.stateNode.containerInfo}if(!r)break;5!==(t=n.tag)&&6!==t||e.ancestors.push(n),n=Sn(r)}while(n);for(n=0;n<e.ancestors.length;n++){t=e.ancestors[n];var i=lt(e.nativeEvent);r=e.topLevelType;var o=e.nativeEvent,u=e.eventSystemFlags;0===n&&(u|=64);for(var a=null,l=0;l<_.length;l++){var c=_[l];c&&(c=c.extractEvents(r,t,o,i,u))&&(a=rt(a,c))}at(a)}}function ht(e,t,n){if(!n.has(e)){switch(e){case"scroll":Yt(t,"scroll",!0);break;case"focus":case"blur":Yt(t,"focus",!0),Yt(t,"blur",!0),n.set("blur",null),n.set("focus",null);break;case"cancel":case"close":ct(e)&&Yt(t,e,!0);break;case"invalid":case"submit":case"reset":break;default:-1===Ke.indexOf(e)&&Bt(e,t)}n.set(e,null)}}var mt,gt,vt,yt=!1,bt=[],wt=null,xt=null,_t=null,kt=new Map,Tt=new Map,Et=[],Ct="mousedown mouseup touchcancel touchend touchstart auxclick dblclick pointercancel pointerdown pointerup dragend dragstart drop compositionend compositionstart keydown keypress keyup input textInput close cancel copy cut paste click change contextmenu reset submit".split(" "),St="focus blur dragenter dragleave mouseover mouseout pointerover pointerout gotpointercapture lostpointercapture".split(" ");function Mt(e,t,n,r,i){return{blockedOn:e,topLevelType:t,eventSystemFlags:32|n,nativeEvent:i,container:r}}function Nt(e,t){switch(e){case"focus":case"blur":wt=null;break;case"dragenter":case"dragleave":xt=null;break;case"mouseover":case"mouseout":_t=null;break;case"pointerover":case"pointerout":kt.delete(t.pointerId);break;case"gotpointercapture":case"lostpointercapture":Tt.delete(t.pointerId)}}function Pt(e,t,n,r,i,o){return null===e||e.nativeEvent!==o?(e=Mt(t,n,r,i,o),null!==t&&(null!==(t=Mn(t))&&gt(t)),e):(e.eventSystemFlags|=r,e)}function At(e){var t=Sn(e.target);if(null!==t){var n=Ge(t);if(null!==n)if(13===(t=n.tag)){if(null!==(t=et(n)))return e.blockedOn=t,void o.unstable_runWithPriority(e.priority,(function(){vt(n)}))}else if(3===t&&n.stateNode.hydrate)return void(e.blockedOn=3===n.tag?n.stateNode.containerInfo:null)}e.blockedOn=null}function Rt(e){if(null!==e.blockedOn)return!1;var t=Jt(e.topLevelType,e.eventSystemFlags,e.container,e.nativeEvent);if(null!==t){var n=Mn(t);return null!==n&&gt(n),e.blockedOn=t,!1}return!0}function Dt(e,t,n){Rt(e)&&n.delete(t)}function Ot(){for(yt=!1;0<bt.length;){var e=bt[0];if(null!==e.blockedOn){null!==(e=Mn(e.blockedOn))&&mt(e);break}var t=Jt(e.topLevelType,e.eventSystemFlags,e.container,e.nativeEvent);null!==t?e.blockedOn=t:bt.shift()}null!==wt&&Rt(wt)&&(wt=null),null!==xt&&Rt(xt)&&(xt=null),null!==_t&&Rt(_t)&&(_t=null),kt.forEach(Dt),Tt.forEach(Dt)}function zt(e,t){e.blockedOn===t&&(e.blockedOn=null,yt||(yt=!0,o.unstable_scheduleCallback(o.unstable_NormalPriority,Ot)))}function Lt(e){function t(t){return zt(t,e)}if(0<bt.length){zt(bt[0],e);for(var n=1;n<bt.length;n++){var r=bt[n];r.blockedOn===e&&(r.blockedOn=null)}}for(null!==wt&&zt(wt,e),null!==xt&&zt(xt,e),null!==_t&&zt(_t,e),kt.forEach(t),Tt.forEach(t),n=0;n<Et.length;n++)(r=Et[n]).blockedOn===e&&(r.blockedOn=null);for(;0<Et.length&&null===(n=Et[0]).blockedOn;)At(n),null===n.blockedOn&&Et.shift()}var Ut={},Ft=new Map,It=new Map,jt=["abort","abort",qe,"animationEnd",Be,"animationIteration",Ye,"animationStart","canplay","canPlay","canplaythrough","canPlayThrough","durationchange","durationChange","emptied","emptied","encrypted","encrypted","ended","ended","error","error","gotpointercapture","gotPointerCapture","load","load","loadeddata","loadedData","loadedmetadata","loadedMetadata","loadstart","loadStart","lostpointercapture","lostPointerCapture","playing","playing","progress","progress","seeking","seeking","stalled","stalled","suspend","suspend","timeupdate","timeUpdate",Xe,"transitionEnd","waiting","waiting"];function Ht(e,t){for(var n=0;n<e.length;n+=2){var r=e[n],i=e[n+1],o="on"+(i[0].toUpperCase()+i.slice(1));o={phasedRegistrationNames:{bubbled:o,captured:o+"Capture"},dependencies:[r],eventPriority:t},It.set(r,t),Ft.set(r,o),Ut[i]=o}}Ht("blur blur cancel cancel click click close close contextmenu contextMenu copy copy cut cut auxclick auxClick dblclick doubleClick dragend dragEnd dragstart dragStart drop drop focus focus input input invalid invalid keydown keyDown keypress keyPress keyup keyUp mousedown mouseDown mouseup mouseUp paste paste pause pause play play pointercancel pointerCancel pointerdown pointerDown pointerup pointerUp ratechange rateChange reset reset seeked seeked submit submit touchcancel touchCancel touchend touchEnd touchstart touchStart volumechange volumeChange".split(" "),0),Ht("drag drag dragenter dragEnter dragexit dragExit dragleave dragLeave dragover dragOver mousemove mouseMove mouseout mouseOut mouseover mouseOver pointermove pointerMove pointerout pointerOut pointerover pointerOver scroll scroll toggle toggle touchmove touchMove wheel wheel".split(" "),1),Ht(jt,2);for(var Vt="change selectionchange textInput compositionstart compositionend compositionupdate".split(" "),$t=0;$t<Vt.length;$t++)It.set(Vt[$t],0);var Wt=o.unstable_UserBlockingPriority,Qt=o.unstable_runWithPriority,qt=!0;function Bt(e,t){Yt(t,e,!1)}function Yt(e,t,n){var r=It.get(t);switch(void 0===r?2:r){case 0:r=Xt.bind(null,t,1,e);break;case 1:r=Kt.bind(null,t,1,e);break;default:r=Zt.bind(null,t,1,e)}n?e.addEventListener(t,r,!0):e.addEventListener(t,r,!1)}function Xt(e,t,n,r){F||L();var i=Zt,o=F;F=!0;try{z(i,e,t,n,r)}finally{(F=o)||j()}}function Kt(e,t,n,r){Qt(Wt,Zt.bind(null,e,t,n,r))}function Zt(e,t,n,r){if(qt)if(0<bt.length&&-1<Ct.indexOf(e))e=Mt(null,e,t,n,r),bt.push(e);else{var i=Jt(e,t,n,r);if(null===i)Nt(e,r);else if(-1<Ct.indexOf(e))e=Mt(i,e,t,n,r),bt.push(e);else if(!function(e,t,n,r,i){switch(t){case"focus":return wt=Pt(wt,e,t,n,r,i),!0;case"dragenter":return xt=Pt(xt,e,t,n,r,i),!0;case"mouseover":return _t=Pt(_t,e,t,n,r,i),!0;case"pointerover":var o=i.pointerId;return kt.set(o,Pt(kt.get(o)||null,e,t,n,r,i)),!0;case"gotpointercapture":return o=i.pointerId,Tt.set(o,Pt(Tt.get(o)||null,e,t,n,r,i)),!0}return!1}(i,e,t,n,r)){Nt(e,r),e=dt(e,r,null,t);try{H(pt,e)}finally{ft(e)}}}}function Jt(e,t,n,r){if(null!==(n=Sn(n=lt(r)))){var i=Ge(n);if(null===i)n=null;else{var o=i.tag;if(13===o){if(null!==(n=et(i)))return n;n=null}else if(3===o){if(i.stateNode.hydrate)return 3===i.tag?i.stateNode.containerInfo:null;n=null}else i!==n&&(n=null)}}e=dt(e,r,n,t);try{H(pt,e)}finally{ft(e)}return null}var Gt={animationIterationCount:!0,borderImageOutset:!0,borderImageSlice:!0,borderImageWidth:!0,boxFlex:!0,boxFlexGroup:!0,boxOrdinalGroup:!0,columnCount:!0,columns:!0,flex:!0,flexGrow:!0,flexPositive:!0,flexShrink:!0,flexNegative:!0,flexOrder:!0,gridArea:!0,gridRow:!0,gridRowEnd:!0,gridRowSpan:!0,gridRowStart:!0,gridColumn:!0,gridColumnEnd:!0,gridColumnSpan:!0,gridColumnStart:!0,fontWeight:!0,lineClamp:!0,lineHeight:!0,opacity:!0,order:!0,orphans:!0,tabSize:!0,widows:!0,zIndex:!0,zoom:!0,fillOpacity:!0,floodOpacity:!0,stopOpacity:!0,strokeDasharray:!0,strokeDashoffset:!0,strokeMiterlimit:!0,strokeOpacity:!0,strokeWidth:!0},en=["Webkit","ms","Moz","O"];function tn(e,t,n){return null==t||"boolean"===typeof t||""===t?"":n||"number"!==typeof t||0===t||Gt.hasOwnProperty(e)&&Gt[e]?(""+t).trim():t+"px"}function nn(e,t){for(var n in e=e.style,t)if(t.hasOwnProperty(n)){var r=0===n.indexOf("--"),i=tn(n,t[n],r);"float"===n&&(n="cssFloat"),r?e.setProperty(n,i):e[n]=i}}Object.keys(Gt).forEach((function(e){en.forEach((function(t){t=t+e.charAt(0).toUpperCase()+e.substring(1),Gt[t]=Gt[e]}))}));var rn=i({menuitem:!0},{area:!0,base:!0,br:!0,col:!0,embed:!0,hr:!0,img:!0,input:!0,keygen:!0,link:!0,meta:!0,param:!0,source:!0,track:!0,wbr:!0});function on(e,t){if(t){if(rn[e]&&(null!=t.children||null!=t.dangerouslySetInnerHTML))throw Error(u(137,e,""));if(null!=t.dangerouslySetInnerHTML){if(null!=t.children)throw Error(u(60));if("object"!==typeof t.dangerouslySetInnerHTML||!("__html"in t.dangerouslySetInnerHTML))throw Error(u(61))}if(null!=t.style&&"object"!==typeof t.style)throw Error(u(62,""))}}function un(e,t){if(-1===e.indexOf("-"))return"string"===typeof t.is;switch(e){case"annotation-xml":case"color-profile":case"font-face":case"font-face-src":case"font-face-uri":case"font-face-format":case"font-face-name":case"missing-glyph":return!1;default:return!0}}var an=Oe;function ln(e,t){var n=Je(e=9===e.nodeType||11===e.nodeType?e:e.ownerDocument);t=E[t];for(var r=0;r<t.length;r++)ht(t[r],e,n)}function cn(){}function sn(e){if("undefined"===typeof(e=e||("undefined"!==typeof document?document:void 0)))return null;try{return e.activeElement||e.body}catch(t){return e.body}}function fn(e){for(;e&&e.firstChild;)e=e.firstChild;return e}function dn(e,t){var n,r=fn(e);for(e=0;r;){if(3===r.nodeType){if(n=e+r.textContent.length,e<=t&&n>=t)return{node:r,offset:t-e};e=n}e:{for(;r;){if(r.nextSibling){r=r.nextSibling;break e}r=r.parentNode}r=void 0}r=fn(r)}}function pn(){for(var e=window,t=sn();t instanceof e.HTMLIFrameElement;){try{var n="string"===typeof t.contentWindow.location.href}catch(r){n=!1}if(!n)break;t=sn((e=t.contentWindow).document)}return t}function hn(e){var t=e&&e.nodeName&&e.nodeName.toLowerCase();return t&&("input"===t&&("text"===e.type||"search"===e.type||"tel"===e.type||"url"===e.type||"password"===e.type)||"textarea"===t||"true"===e.contentEditable)}var mn=null,gn=null;function vn(e,t){switch(e){case"button":case"input":case"select":case"textarea":return!!t.autoFocus}return!1}function yn(e,t){return"textarea"===e||"option"===e||"noscript"===e||"string"===typeof t.children||"number"===typeof t.children||"object"===typeof t.dangerouslySetInnerHTML&&null!==t.dangerouslySetInnerHTML&&null!=t.dangerouslySetInnerHTML.__html}var bn="function"===typeof setTimeout?setTimeout:void 0,wn="function"===typeof clearTimeout?clearTimeout:void 0;function xn(e){for(;null!=e;e=e.nextSibling){var t=e.nodeType;if(1===t||3===t)break}return e}function _n(e){e=e.previousSibling;for(var t=0;e;){if(8===e.nodeType){var n=e.data;if("$"===n||"$!"===n||"$?"===n){if(0===t)return e;t--}else"/$"===n&&t++}e=e.previousSibling}return null}var kn=Math.random().toString(36).slice(2),Tn="__reactInternalInstance$"+kn,En="__reactEventHandlers$"+kn,Cn="__reactContainere$"+kn;function Sn(e){var t=e[Tn];if(t)return t;for(var n=e.parentNode;n;){if(t=n[Cn]||n[Tn]){if(n=t.alternate,null!==t.child||null!==n&&null!==n.child)for(e=_n(e);null!==e;){if(n=e[Tn])return n;e=_n(e)}return t}n=(e=n).parentNode}return null}function Mn(e){return!(e=e[Tn]||e[Cn])||5!==e.tag&&6!==e.tag&&13!==e.tag&&3!==e.tag?null:e}function Nn(e){if(5===e.tag||6===e.tag)return e.stateNode;throw Error(u(33))}function Pn(e){return e[En]||null}function An(e){do{e=e.return}while(e&&5!==e.tag);return e||null}function Rn(e,t){var n=e.stateNode;if(!n)return null;var r=h(n);if(!r)return null;n=r[t];e:switch(t){case"onClick":case"onClickCapture":case"onDoubleClick":case"onDoubleClickCapture":case"onMouseDown":case"onMouseDownCapture":case"onMouseMove":case"onMouseMoveCapture":case"onMouseUp":case"onMouseUpCapture":case"onMouseEnter":(r=!r.disabled)||(r=!("button"===(e=e.type)||"input"===e||"select"===e||"textarea"===e)),e=!r;break e;default:e=!1}if(e)return null;if(n&&"function"!==typeof n)throw Error(u(231,t,typeof n));return n}function Dn(e,t,n){(t=Rn(e,n.dispatchConfig.phasedRegistrationNames[t]))&&(n._dispatchListeners=rt(n._dispatchListeners,t),n._dispatchInstances=rt(n._dispatchInstances,e))}function On(e){if(e&&e.dispatchConfig.phasedRegistrationNames){for(var t=e._targetInst,n=[];t;)n.push(t),t=An(t);for(t=n.length;0<t--;)Dn(n[t],"captured",e);for(t=0;t<n.length;t++)Dn(n[t],"bubbled",e)}}function zn(e,t,n){e&&n&&n.dispatchConfig.registrationName&&(t=Rn(e,n.dispatchConfig.registrationName))&&(n._dispatchListeners=rt(n._dispatchListeners,t),n._dispatchInstances=rt(n._dispatchInstances,e))}function Ln(e){e&&e.dispatchConfig.registrationName&&zn(e._targetInst,null,e)}function Un(e){it(e,On)}var Fn=null,In=null,jn=null;function Hn(){if(jn)return jn;var e,t,n=In,r=n.length,i="value"in Fn?Fn.value:Fn.textContent,o=i.length;for(e=0;e<r&&n[e]===i[e];e++);var u=r-e;for(t=1;t<=u&&n[r-t]===i[o-t];t++);return jn=i.slice(e,1<t?1-t:void 0)}function Vn(){return!0}function $n(){return!1}function Wn(e,t,n,r){for(var i in this.dispatchConfig=e,this._targetInst=t,this.nativeEvent=n,e=this.constructor.Interface)e.hasOwnProperty(i)&&((t=e[i])?this[i]=t(n):"target"===i?this.target=r:this[i]=n[i]);return this.isDefaultPrevented=(null!=n.defaultPrevented?n.defaultPrevented:!1===n.returnValue)?Vn:$n,this.isPropagationStopped=$n,this}function Qn(e,t,n,r){if(this.eventPool.length){var i=this.eventPool.pop();return this.call(i,e,t,n,r),i}return new this(e,t,n,r)}function qn(e){if(!(e instanceof this))throw Error(u(279));e.destructor(),10>this.eventPool.length&&this.eventPool.push(e)}function Bn(e){e.eventPool=[],e.getPooled=Qn,e.release=qn}i(Wn.prototype,{preventDefault:function(){this.defaultPrevented=!0;var e=this.nativeEvent;e&&(e.preventDefault?e.preventDefault():"unknown"!==typeof e.returnValue&&(e.returnValue=!1),this.isDefaultPrevented=Vn)},stopPropagation:function(){var e=this.nativeEvent;e&&(e.stopPropagation?e.stopPropagation():"unknown"!==typeof e.cancelBubble&&(e.cancelBubble=!0),this.isPropagationStopped=Vn)},persist:function(){this.isPersistent=Vn},isPersistent:$n,destructor:function(){var e,t=this.constructor.Interface;for(e in t)this[e]=null;this.nativeEvent=this._targetInst=this.dispatchConfig=null,this.isPropagationStopped=this.isDefaultPrevented=$n,this._dispatchInstances=this._dispatchListeners=null}}),Wn.Interface={type:null,target:null,currentTarget:function(){return null},eventPhase:null,bubbles:null,cancelable:null,timeStamp:function(e){return e.timeStamp||Date.now()},defaultPrevented:null,isTrusted:null},Wn.extend=function(e){function t(){}function n(){return r.apply(this,arguments)}var r=this;t.prototype=r.prototype;var o=new t;return i(o,n.prototype),n.prototype=o,n.prototype.constructor=n,n.Interface=i({},r.Interface,e),n.extend=r.extend,Bn(n),n},Bn(Wn);var Yn=Wn.extend({data:null}),Xn=Wn.extend({data:null}),Kn=[9,13,27,32],Zn=S&&"CompositionEvent"in window,Jn=null;S&&"documentMode"in document&&(Jn=document.documentMode);var Gn=S&&"TextEvent"in window&&!Jn,er=S&&(!Zn||Jn&&8<Jn&&11>=Jn),tr=String.fromCharCode(32),nr={beforeInput:{phasedRegistrationNames:{bubbled:"onBeforeInput",captured:"onBeforeInputCapture"},dependencies:["compositionend","keypress","textInput","paste"]},compositionEnd:{phasedRegistrationNames:{bubbled:"onCompositionEnd",captured:"onCompositionEndCapture"},dependencies:"blur compositionend keydown keypress keyup mousedown".split(" ")},compositionStart:{phasedRegistrationNames:{bubbled:"onCompositionStart",captured:"onCompositionStartCapture"},dependencies:"blur compositionstart keydown keypress keyup mousedown".split(" ")},compositionUpdate:{phasedRegistrationNames:{bubbled:"onCompositionUpdate",captured:"onCompositionUpdateCapture"},dependencies:"blur compositionupdate keydown keypress keyup mousedown".split(" ")}},rr=!1;function ir(e,t){switch(e){case"keyup":return-1!==Kn.indexOf(t.keyCode);case"keydown":return 229!==t.keyCode;case"keypress":case"mousedown":case"blur":return!0;default:return!1}}function or(e){return"object"===typeof(e=e.detail)&&"data"in e?e.data:null}var ur=!1;var ar={eventTypes:nr,extractEvents:function(e,t,n,r){var i;if(Zn)e:{switch(e){case"compositionstart":var o=nr.compositionStart;break e;case"compositionend":o=nr.compositionEnd;break e;case"compositionupdate":o=nr.compositionUpdate;break e}o=void 0}else ur?ir(e,n)&&(o=nr.compositionEnd):"keydown"===e&&229===n.keyCode&&(o=nr.compositionStart);return o?(er&&"ko"!==n.locale&&(ur||o!==nr.compositionStart?o===nr.compositionEnd&&ur&&(i=Hn()):(In="value"in(Fn=r)?Fn.value:Fn.textContent,ur=!0)),o=Yn.getPooled(o,t,n,r),i?o.data=i:null!==(i=or(n))&&(o.data=i),Un(o),i=o):i=null,(e=Gn?function(e,t){switch(e){case"compositionend":return or(t);case"keypress":return 32!==t.which?null:(rr=!0,tr);case"textInput":return(e=t.data)===tr&&rr?null:e;default:return null}}(e,n):function(e,t){if(ur)return"compositionend"===e||!Zn&&ir(e,t)?(e=Hn(),jn=In=Fn=null,ur=!1,e):null;switch(e){case"paste":return null;case"keypress":if(!(t.ctrlKey||t.altKey||t.metaKey)||t.ctrlKey&&t.altKey){if(t.char&&1<t.char.length)return t.char;if(t.which)return String.fromCharCode(t.which)}return null;case"compositionend":return er&&"ko"!==t.locale?null:t.data;default:return null}}(e,n))?((t=Xn.getPooled(nr.beforeInput,t,n,r)).data=e,Un(t)):t=null,null===i?t:null===t?i:[i,t]}},lr={color:!0,date:!0,datetime:!0,"datetime-local":!0,email:!0,month:!0,number:!0,password:!0,range:!0,search:!0,tel:!0,text:!0,time:!0,url:!0,week:!0};function cr(e){var t=e&&e.nodeName&&e.nodeName.toLowerCase();return"input"===t?!!lr[e.type]:"textarea"===t}var sr={change:{phasedRegistrationNames:{bubbled:"onChange",captured:"onChangeCapture"},dependencies:"blur change click focus input keydown keyup selectionchange".split(" ")}};function fr(e,t,n){return(e=Wn.getPooled(sr.change,e,t,n)).type="change",R(n),Un(e),e}var dr=null,pr=null;function hr(e){at(e)}function mr(e){if(xe(Nn(e)))return e}function gr(e,t){if("change"===e)return t}var vr=!1;function yr(){dr&&(dr.detachEvent("onpropertychange",br),pr=dr=null)}function br(e){if("value"===e.propertyName&&mr(pr))if(e=fr(pr,e,lt(e)),F)at(e);else{F=!0;try{O(hr,e)}finally{F=!1,j()}}}function wr(e,t,n){"focus"===e?(yr(),pr=n,(dr=t).attachEvent("onpropertychange",br)):"blur"===e&&yr()}function xr(e){if("selectionchange"===e||"keyup"===e||"keydown"===e)return mr(pr)}function _r(e,t){if("click"===e)return mr(t)}function kr(e,t){if("input"===e||"change"===e)return mr(t)}S&&(vr=ct("input")&&(!document.documentMode||9<document.documentMode));var Tr={eventTypes:sr,_isInputEventSupported:vr,extractEvents:function(e,t,n,r){var i=t?Nn(t):window,o=i.nodeName&&i.nodeName.toLowerCase();if("select"===o||"input"===o&&"file"===i.type)var u=gr;else if(cr(i))if(vr)u=kr;else{u=xr;var a=wr}else(o=i.nodeName)&&"input"===o.toLowerCase()&&("checkbox"===i.type||"radio"===i.type)&&(u=_r);if(u&&(u=u(e,t)))return fr(u,n,r);a&&a(e,i,t),"blur"===e&&(e=i._wrapperState)&&e.controlled&&"number"===i.type&&Se(i,"number",i.value)}},Er=Wn.extend({view:null,detail:null}),Cr={Alt:"altKey",Control:"ctrlKey",Meta:"metaKey",Shift:"shiftKey"};function Sr(e){var t=this.nativeEvent;return t.getModifierState?t.getModifierState(e):!!(e=Cr[e])&&!!t[e]}function Mr(){return Sr}var Nr=0,Pr=0,Ar=!1,Rr=!1,Dr=Er.extend({screenX:null,screenY:null,clientX:null,clientY:null,pageX:null,pageY:null,ctrlKey:null,shiftKey:null,altKey:null,metaKey:null,getModifierState:Mr,button:null,buttons:null,relatedTarget:function(e){return e.relatedTarget||(e.fromElement===e.srcElement?e.toElement:e.fromElement)},movementX:function(e){if("movementX"in e)return e.movementX;var t=Nr;return Nr=e.screenX,Ar?"mousemove"===e.type?e.screenX-t:0:(Ar=!0,0)},movementY:function(e){if("movementY"in e)return e.movementY;var t=Pr;return Pr=e.screenY,Rr?"mousemove"===e.type?e.screenY-t:0:(Rr=!0,0)}}),Or=Dr.extend({pointerId:null,width:null,height:null,pressure:null,tangentialPressure:null,tiltX:null,tiltY:null,twist:null,pointerType:null,isPrimary:null}),zr={mouseEnter:{registrationName:"onMouseEnter",dependencies:["mouseout","mouseover"]},mouseLeave:{registrationName:"onMouseLeave",dependencies:["mouseout","mouseover"]},pointerEnter:{registrationName:"onPointerEnter",dependencies:["pointerout","pointerover"]},pointerLeave:{registrationName:"onPointerLeave",dependencies:["pointerout","pointerover"]}},Lr={eventTypes:zr,extractEvents:function(e,t,n,r,i){var o="mouseover"===e||"pointerover"===e,u="mouseout"===e||"pointerout"===e;if(o&&0===(32&i)&&(n.relatedTarget||n.fromElement)||!u&&!o)return null;(o=r.window===r?r:(o=r.ownerDocument)?o.defaultView||o.parentWindow:window,u)?(u=t,null!==(t=(t=n.relatedTarget||n.toElement)?Sn(t):null)&&(t!==Ge(t)||5!==t.tag&&6!==t.tag)&&(t=null)):u=null;if(u===t)return null;if("mouseout"===e||"mouseover"===e)var a=Dr,l=zr.mouseLeave,c=zr.mouseEnter,s="mouse";else"pointerout"!==e&&"pointerover"!==e||(a=Or,l=zr.pointerLeave,c=zr.pointerEnter,s="pointer");if(e=null==u?o:Nn(u),o=null==t?o:Nn(t),(l=a.getPooled(l,u,n,r)).type=s+"leave",l.target=e,l.relatedTarget=o,(n=a.getPooled(c,t,n,r)).type=s+"enter",n.target=o,n.relatedTarget=e,s=t,(r=u)&&s)e:{for(c=s,u=0,e=a=r;e;e=An(e))u++;for(e=0,t=c;t;t=An(t))e++;for(;0<u-e;)a=An(a),u--;for(;0<e-u;)c=An(c),e--;for(;u--;){if(a===c||a===c.alternate)break e;a=An(a),c=An(c)}a=null}else a=null;for(c=a,a=[];r&&r!==c&&(null===(u=r.alternate)||u!==c);)a.push(r),r=An(r);for(r=[];s&&s!==c&&(null===(u=s.alternate)||u!==c);)r.push(s),s=An(s);for(s=0;s<a.length;s++)zn(a[s],"bubbled",l);for(s=r.length;0<s--;)zn(r[s],"captured",n);return 0===(64&i)?[l]:[l,n]}};var Ur="function"===typeof Object.is?Object.is:function(e,t){return e===t&&(0!==e||1/e===1/t)||e!==e&&t!==t},Fr=Object.prototype.hasOwnProperty;function Ir(e,t){if(Ur(e,t))return!0;if("object"!==typeof e||null===e||"object"!==typeof t||null===t)return!1;var n=Object.keys(e),r=Object.keys(t);if(n.length!==r.length)return!1;for(r=0;r<n.length;r++)if(!Fr.call(t,n[r])||!Ur(e[n[r]],t[n[r]]))return!1;return!0}var jr=S&&"documentMode"in document&&11>=document.documentMode,Hr={select:{phasedRegistrationNames:{bubbled:"onSelect",captured:"onSelectCapture"},dependencies:"blur contextmenu dragend focus keydown keyup mousedown mouseup selectionchange".split(" ")}},Vr=null,$r=null,Wr=null,Qr=!1;function qr(e,t){var n=t.window===t?t.document:9===t.nodeType?t:t.ownerDocument;return Qr||null==Vr||Vr!==sn(n)?null:("selectionStart"in(n=Vr)&&hn(n)?n={start:n.selectionStart,end:n.selectionEnd}:n={anchorNode:(n=(n.ownerDocument&&n.ownerDocument.defaultView||window).getSelection()).anchorNode,anchorOffset:n.anchorOffset,focusNode:n.focusNode,focusOffset:n.focusOffset},Wr&&Ir(Wr,n)?null:(Wr=n,(e=Wn.getPooled(Hr.select,$r,e,t)).type="select",e.target=Vr,Un(e),e))}var Br={eventTypes:Hr,extractEvents:function(e,t,n,r,i,o){if(!(o=!(i=o||(r.window===r?r.document:9===r.nodeType?r:r.ownerDocument)))){e:{i=Je(i),o=E.onSelect;for(var u=0;u<o.length;u++)if(!i.has(o[u])){i=!1;break e}i=!0}o=!i}if(o)return null;switch(i=t?Nn(t):window,e){case"focus":(cr(i)||"true"===i.contentEditable)&&(Vr=i,$r=t,Wr=null);break;case"blur":Wr=$r=Vr=null;break;case"mousedown":Qr=!0;break;case"contextmenu":case"mouseup":case"dragend":return Qr=!1,qr(n,r);case"selectionchange":if(jr)break;case"keydown":case"keyup":return qr(n,r)}return null}},Yr=Wn.extend({animationName:null,elapsedTime:null,pseudoElement:null}),Xr=Wn.extend({clipboardData:function(e){return"clipboardData"in e?e.clipboardData:window.clipboardData}}),Kr=Er.extend({relatedTarget:null});function Zr(e){var t=e.keyCode;return"charCode"in e?0===(e=e.charCode)&&13===t&&(e=13):e=t,10===e&&(e=13),32<=e||13===e?e:0}var Jr={Esc:"Escape",Spacebar:" ",Left:"ArrowLeft",Up:"ArrowUp",Right:"ArrowRight",Down:"ArrowDown",Del:"Delete",Win:"OS",Menu:"ContextMenu",Apps:"ContextMenu",Scroll:"ScrollLock",MozPrintableKey:"Unidentified"},Gr={8:"Backspace",9:"Tab",12:"Clear",13:"Enter",16:"Shift",17:"Control",18:"Alt",19:"Pause",20:"CapsLock",27:"Escape",32:" ",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"ArrowLeft",38:"ArrowUp",39:"ArrowRight",40:"ArrowDown",45:"Insert",46:"Delete",112:"F1",113:"F2",114:"F3",115:"F4",116:"F5",117:"F6",118:"F7",119:"F8",120:"F9",121:"F10",122:"F11",123:"F12",144:"NumLock",145:"ScrollLock",224:"Meta"},ei=Er.extend({key:function(e){if(e.key){var t=Jr[e.key]||e.key;if("Unidentified"!==t)return t}return"keypress"===e.type?13===(e=Zr(e))?"Enter":String.fromCharCode(e):"keydown"===e.type||"keyup"===e.type?Gr[e.keyCode]||"Unidentified":""},location:null,ctrlKey:null,shiftKey:null,altKey:null,metaKey:null,repeat:null,locale:null,getModifierState:Mr,charCode:function(e){return"keypress"===e.type?Zr(e):0},keyCode:function(e){return"keydown"===e.type||"keyup"===e.type?e.keyCode:0},which:function(e){return"keypress"===e.type?Zr(e):"keydown"===e.type||"keyup"===e.type?e.keyCode:0}}),ti=Dr.extend({dataTransfer:null}),ni=Er.extend({touches:null,targetTouches:null,changedTouches:null,altKey:null,metaKey:null,ctrlKey:null,shiftKey:null,getModifierState:Mr}),ri=Wn.extend({propertyName:null,elapsedTime:null,pseudoElement:null}),ii=Dr.extend({deltaX:function(e){return"deltaX"in e?e.deltaX:"wheelDeltaX"in e?-e.wheelDeltaX:0},deltaY:function(e){return"deltaY"in e?e.deltaY:"wheelDeltaY"in e?-e.wheelDeltaY:"wheelDelta"in e?-e.wheelDelta:0},deltaZ:null,deltaMode:null}),oi={eventTypes:Ut,extractEvents:function(e,t,n,r){var i=Ft.get(e);if(!i)return null;switch(e){case"keypress":if(0===Zr(n))return null;case"keydown":case"keyup":e=ei;break;case"blur":case"focus":e=Kr;break;case"click":if(2===n.button)return null;case"auxclick":case"dblclick":case"mousedown":case"mousemove":case"mouseup":case"mouseout":case"mouseover":case"contextmenu":e=Dr;break;case"drag":case"dragend":case"dragenter":case"dragexit":case"dragleave":case"dragover":case"dragstart":case"drop":e=ti;break;case"touchcancel":case"touchend":case"touchmove":case"touchstart":e=ni;break;case qe:case Be:case Ye:e=Yr;break;case Xe:e=ri;break;case"scroll":e=Er;break;case"wheel":e=ii;break;case"copy":case"cut":case"paste":e=Xr;break;case"gotpointercapture":case"lostpointercapture":case"pointercancel":case"pointerdown":case"pointermove":case"pointerout":case"pointerover":case"pointerup":e=Or;break;default:e=Wn}return Un(t=e.getPooled(i,t,n,r)),t}};if(y)throw Error(u(101));y=Array.prototype.slice.call("ResponderEventPlugin SimpleEventPlugin EnterLeaveEventPlugin ChangeEventPlugin SelectEventPlugin BeforeInputEventPlugin".split(" ")),w(),h=Pn,m=Mn,g=Nn,C({SimpleEventPlugin:oi,EnterLeaveEventPlugin:Lr,ChangeEventPlugin:Tr,SelectEventPlugin:Br,BeforeInputEventPlugin:ar});var ui=[],ai=-1;function li(e){0>ai||(e.current=ui[ai],ui[ai]=null,ai--)}function ci(e,t){ai++,ui[ai]=e.current,e.current=t}var si={},fi={current:si},di={current:!1},pi=si;function hi(e,t){var n=e.type.contextTypes;if(!n)return si;var r=e.stateNode;if(r&&r.__reactInternalMemoizedUnmaskedChildContext===t)return r.__reactInternalMemoizedMaskedChildContext;var i,o={};for(i in n)o[i]=t[i];return r&&((e=e.stateNode).__reactInternalMemoizedUnmaskedChildContext=t,e.__reactInternalMemoizedMaskedChildContext=o),o}function mi(e){return null!==(e=e.childContextTypes)&&void 0!==e}function gi(){li(di),li(fi)}function vi(e,t,n){if(fi.current!==si)throw Error(u(168));ci(fi,t),ci(di,n)}function yi(e,t,n){var r=e.stateNode;if(e=t.childContextTypes,"function"!==typeof r.getChildContext)return n;for(var o in r=r.getChildContext())if(!(o in e))throw Error(u(108,ge(t)||"Unknown",o));return i({},n,{},r)}function bi(e){return e=(e=e.stateNode)&&e.__reactInternalMemoizedMergedChildContext||si,pi=fi.current,ci(fi,e),ci(di,di.current),!0}function wi(e,t,n){var r=e.stateNode;if(!r)throw Error(u(169));n?(e=yi(e,t,pi),r.__reactInternalMemoizedMergedChildContext=e,li(di),li(fi),ci(fi,e)):li(di),ci(di,n)}var xi=o.unstable_runWithPriority,_i=o.unstable_scheduleCallback,ki=o.unstable_cancelCallback,Ti=o.unstable_requestPaint,Ei=o.unstable_now,Ci=o.unstable_getCurrentPriorityLevel,Si=o.unstable_ImmediatePriority,Mi=o.unstable_UserBlockingPriority,Ni=o.unstable_NormalPriority,Pi=o.unstable_LowPriority,Ai=o.unstable_IdlePriority,Ri={},Di=o.unstable_shouldYield,Oi=void 0!==Ti?Ti:function(){},zi=null,Li=null,Ui=!1,Fi=Ei(),Ii=1e4>Fi?Ei:function(){return Ei()-Fi};function ji(){switch(Ci()){case Si:return 99;case Mi:return 98;case Ni:return 97;case Pi:return 96;case Ai:return 95;default:throw Error(u(332))}}function Hi(e){switch(e){case 99:return Si;case 98:return Mi;case 97:return Ni;case 96:return Pi;case 95:return Ai;default:throw Error(u(332))}}function Vi(e,t){return e=Hi(e),xi(e,t)}function $i(e,t,n){return e=Hi(e),_i(e,t,n)}function Wi(e){return null===zi?(zi=[e],Li=_i(Si,qi)):zi.push(e),Ri}function Qi(){if(null!==Li){var e=Li;Li=null,ki(e)}qi()}function qi(){if(!Ui&&null!==zi){Ui=!0;var e=0;try{var t=zi;Vi(99,(function(){for(;e<t.length;e++){var n=t[e];do{n=n(!0)}while(null!==n)}})),zi=null}catch(n){throw null!==zi&&(zi=zi.slice(e+1)),_i(Si,Qi),n}finally{Ui=!1}}}function Bi(e,t,n){return 1073741821-(1+((1073741821-e+t/10)/(n/=10)|0))*n}function Yi(e,t){if(e&&e.defaultProps)for(var n in t=i({},t),e=e.defaultProps)void 0===t[n]&&(t[n]=e[n]);return t}var Xi={current:null},Ki=null,Zi=null,Ji=null;function Gi(){Ji=Zi=Ki=null}function eo(e){var t=Xi.current;li(Xi),e.type._context._currentValue=t}function to(e,t){for(;null!==e;){var n=e.alternate;if(e.childExpirationTime<t)e.childExpirationTime=t,null!==n&&n.childExpirationTime<t&&(n.childExpirationTime=t);else{if(!(null!==n&&n.childExpirationTime<t))break;n.childExpirationTime=t}e=e.return}}function no(e,t){Ki=e,Ji=Zi=null,null!==(e=e.dependencies)&&null!==e.firstContext&&(e.expirationTime>=t&&(Pu=!0),e.firstContext=null)}function ro(e,t){if(Ji!==e&&!1!==t&&0!==t)if("number"===typeof t&&1073741823!==t||(Ji=e,t=1073741823),t={context:e,observedBits:t,next:null},null===Zi){if(null===Ki)throw Error(u(308));Zi=t,Ki.dependencies={expirationTime:0,firstContext:t,responders:null}}else Zi=Zi.next=t;return e._currentValue}var io=!1;function oo(e){e.updateQueue={baseState:e.memoizedState,baseQueue:null,shared:{pending:null},effects:null}}function uo(e,t){e=e.updateQueue,t.updateQueue===e&&(t.updateQueue={baseState:e.baseState,baseQueue:e.baseQueue,shared:e.shared,effects:e.effects})}function ao(e,t){return(e={expirationTime:e,suspenseConfig:t,tag:0,payload:null,callback:null,next:null}).next=e}function lo(e,t){if(null!==(e=e.updateQueue)){var n=(e=e.shared).pending;null===n?t.next=t:(t.next=n.next,n.next=t),e.pending=t}}function co(e,t){var n=e.alternate;null!==n&&uo(n,e),null===(n=(e=e.updateQueue).baseQueue)?(e.baseQueue=t.next=t,t.next=t):(t.next=n.next,n.next=t)}function so(e,t,n,r){var o=e.updateQueue;io=!1;var u=o.baseQueue,a=o.shared.pending;if(null!==a){if(null!==u){var l=u.next;u.next=a.next,a.next=l}u=a,o.shared.pending=null,null!==(l=e.alternate)&&(null!==(l=l.updateQueue)&&(l.baseQueue=a))}if(null!==u){l=u.next;var c=o.baseState,s=0,f=null,d=null,p=null;if(null!==l)for(var h=l;;){if((a=h.expirationTime)<r){var m={expirationTime:h.expirationTime,suspenseConfig:h.suspenseConfig,tag:h.tag,payload:h.payload,callback:h.callback,next:null};null===p?(d=p=m,f=c):p=p.next=m,a>s&&(s=a)}else{null!==p&&(p=p.next={expirationTime:1073741823,suspenseConfig:h.suspenseConfig,tag:h.tag,payload:h.payload,callback:h.callback,next:null}),ol(a,h.suspenseConfig);e:{var g=e,v=h;switch(a=t,m=n,v.tag){case 1:if("function"===typeof(g=v.payload)){c=g.call(m,c,a);break e}c=g;break e;case 3:g.effectTag=-4097&g.effectTag|64;case 0:if(null===(a="function"===typeof(g=v.payload)?g.call(m,c,a):g)||void 0===a)break e;c=i({},c,a);break e;case 2:io=!0}}null!==h.callback&&(e.effectTag|=32,null===(a=o.effects)?o.effects=[h]:a.push(h))}if(null===(h=h.next)||h===l){if(null===(a=o.shared.pending))break;h=u.next=a.next,a.next=l,o.baseQueue=u=a,o.shared.pending=null}}null===p?f=c:p.next=d,o.baseState=f,o.baseQueue=p,ul(s),e.expirationTime=s,e.memoizedState=c}}function fo(e,t,n){if(e=t.effects,t.effects=null,null!==e)for(t=0;t<e.length;t++){var r=e[t],i=r.callback;if(null!==i){if(r.callback=null,r=i,i=n,"function"!==typeof r)throw Error(u(191,r));r.call(i)}}}var po=K.ReactCurrentBatchConfig,ho=(new r.Component).refs;function mo(e,t,n,r){n=null===(n=n(r,t=e.memoizedState))||void 0===n?t:i({},t,n),e.memoizedState=n,0===e.expirationTime&&(e.updateQueue.baseState=n)}var go={isMounted:function(e){return!!(e=e._reactInternalFiber)&&Ge(e)===e},enqueueSetState:function(e,t,n){e=e._reactInternalFiber;var r=qa(),i=po.suspense;(i=ao(r=Ba(r,e,i),i)).payload=t,void 0!==n&&null!==n&&(i.callback=n),lo(e,i),Ya(e,r)},enqueueReplaceState:function(e,t,n){e=e._reactInternalFiber;var r=qa(),i=po.suspense;(i=ao(r=Ba(r,e,i),i)).tag=1,i.payload=t,void 0!==n&&null!==n&&(i.callback=n),lo(e,i),Ya(e,r)},enqueueForceUpdate:function(e,t){e=e._reactInternalFiber;var n=qa(),r=po.suspense;(r=ao(n=Ba(n,e,r),r)).tag=2,void 0!==t&&null!==t&&(r.callback=t),lo(e,r),Ya(e,n)}};function vo(e,t,n,r,i,o,u){return"function"===typeof(e=e.stateNode).shouldComponentUpdate?e.shouldComponentUpdate(r,o,u):!t.prototype||!t.prototype.isPureReactComponent||(!Ir(n,r)||!Ir(i,o))}function yo(e,t,n){var r=!1,i=si,o=t.contextType;return"object"===typeof o&&null!==o?o=ro(o):(i=mi(t)?pi:fi.current,o=(r=null!==(r=t.contextTypes)&&void 0!==r)?hi(e,i):si),t=new t(n,o),e.memoizedState=null!==t.state&&void 0!==t.state?t.state:null,t.updater=go,e.stateNode=t,t._reactInternalFiber=e,r&&((e=e.stateNode).__reactInternalMemoizedUnmaskedChildContext=i,e.__reactInternalMemoizedMaskedChildContext=o),t}function bo(e,t,n,r){e=t.state,"function"===typeof t.componentWillReceiveProps&&t.componentWillReceiveProps(n,r),"function"===typeof t.UNSAFE_componentWillReceiveProps&&t.UNSAFE_componentWillReceiveProps(n,r),t.state!==e&&go.enqueueReplaceState(t,t.state,null)}function wo(e,t,n,r){var i=e.stateNode;i.props=n,i.state=e.memoizedState,i.refs=ho,oo(e);var o=t.contextType;"object"===typeof o&&null!==o?i.context=ro(o):(o=mi(t)?pi:fi.current,i.context=hi(e,o)),so(e,n,i,r),i.state=e.memoizedState,"function"===typeof(o=t.getDerivedStateFromProps)&&(mo(e,t,o,n),i.state=e.memoizedState),"function"===typeof t.getDerivedStateFromProps||"function"===typeof i.getSnapshotBeforeUpdate||"function"!==typeof i.UNSAFE_componentWillMount&&"function"!==typeof i.componentWillMount||(t=i.state,"function"===typeof i.componentWillMount&&i.componentWillMount(),"function"===typeof i.UNSAFE_componentWillMount&&i.UNSAFE_componentWillMount(),t!==i.state&&go.enqueueReplaceState(i,i.state,null),so(e,n,i,r),i.state=e.memoizedState),"function"===typeof i.componentDidMount&&(e.effectTag|=4)}var xo=Array.isArray;function _o(e,t,n){if(null!==(e=n.ref)&&"function"!==typeof e&&"object"!==typeof e){if(n._owner){if(n=n._owner){if(1!==n.tag)throw Error(u(309));var r=n.stateNode}if(!r)throw Error(u(147,e));var i=""+e;return null!==t&&null!==t.ref&&"function"===typeof t.ref&&t.ref._stringRef===i?t.ref:((t=function(e){var t=r.refs;t===ho&&(t=r.refs={}),null===e?delete t[i]:t[i]=e})._stringRef=i,t)}if("string"!==typeof e)throw Error(u(284));if(!n._owner)throw Error(u(290,e))}return e}function ko(e,t){if("textarea"!==e.type)throw Error(u(31,"[object Object]"===Object.prototype.toString.call(t)?"object with keys {"+Object.keys(t).join(", ")+"}":t,""))}function To(e){function t(t,n){if(e){var r=t.lastEffect;null!==r?(r.nextEffect=n,t.lastEffect=n):t.firstEffect=t.lastEffect=n,n.nextEffect=null,n.effectTag=8}}function n(n,r){if(!e)return null;for(;null!==r;)t(n,r),r=r.sibling;return null}function r(e,t){for(e=new Map;null!==t;)null!==t.key?e.set(t.key,t):e.set(t.index,t),t=t.sibling;return e}function i(e,t){return(e=Cl(e,t)).index=0,e.sibling=null,e}function o(t,n,r){return t.index=r,e?null!==(r=t.alternate)?(r=r.index)<n?(t.effectTag=2,n):r:(t.effectTag=2,n):n}function a(t){return e&&null===t.alternate&&(t.effectTag=2),t}function l(e,t,n,r){return null===t||6!==t.tag?((t=Nl(n,e.mode,r)).return=e,t):((t=i(t,n)).return=e,t)}function c(e,t,n,r){return null!==t&&t.elementType===n.type?((r=i(t,n.props)).ref=_o(e,t,n),r.return=e,r):((r=Sl(n.type,n.key,n.props,null,e.mode,r)).ref=_o(e,t,n),r.return=e,r)}function s(e,t,n,r){return null===t||4!==t.tag||t.stateNode.containerInfo!==n.containerInfo||t.stateNode.implementation!==n.implementation?((t=Pl(n,e.mode,r)).return=e,t):((t=i(t,n.children||[])).return=e,t)}function f(e,t,n,r,o){return null===t||7!==t.tag?((t=Ml(n,e.mode,r,o)).return=e,t):((t=i(t,n)).return=e,t)}function d(e,t,n){if("string"===typeof t||"number"===typeof t)return(t=Nl(""+t,e.mode,n)).return=e,t;if("object"===typeof t&&null!==t){switch(t.$$typeof){case ee:return(n=Sl(t.type,t.key,t.props,null,e.mode,n)).ref=_o(e,null,t),n.return=e,n;case te:return(t=Pl(t,e.mode,n)).return=e,t}if(xo(t)||me(t))return(t=Ml(t,e.mode,n,null)).return=e,t;ko(e,t)}return null}function p(e,t,n,r){var i=null!==t?t.key:null;if("string"===typeof n||"number"===typeof n)return null!==i?null:l(e,t,""+n,r);if("object"===typeof n&&null!==n){switch(n.$$typeof){case ee:return n.key===i?n.type===ne?f(e,t,n.props.children,r,i):c(e,t,n,r):null;case te:return n.key===i?s(e,t,n,r):null}if(xo(n)||me(n))return null!==i?null:f(e,t,n,r,null);ko(e,n)}return null}function h(e,t,n,r,i){if("string"===typeof r||"number"===typeof r)return l(t,e=e.get(n)||null,""+r,i);if("object"===typeof r&&null!==r){switch(r.$$typeof){case ee:return e=e.get(null===r.key?n:r.key)||null,r.type===ne?f(t,e,r.props.children,i,r.key):c(t,e,r,i);case te:return s(t,e=e.get(null===r.key?n:r.key)||null,r,i)}if(xo(r)||me(r))return f(t,e=e.get(n)||null,r,i,null);ko(t,r)}return null}function m(i,u,a,l){for(var c=null,s=null,f=u,m=u=0,g=null;null!==f&&m<a.length;m++){f.index>m?(g=f,f=null):g=f.sibling;var v=p(i,f,a[m],l);if(null===v){null===f&&(f=g);break}e&&f&&null===v.alternate&&t(i,f),u=o(v,u,m),null===s?c=v:s.sibling=v,s=v,f=g}if(m===a.length)return n(i,f),c;if(null===f){for(;m<a.length;m++)null!==(f=d(i,a[m],l))&&(u=o(f,u,m),null===s?c=f:s.sibling=f,s=f);return c}for(f=r(i,f);m<a.length;m++)null!==(g=h(f,i,m,a[m],l))&&(e&&null!==g.alternate&&f.delete(null===g.key?m:g.key),u=o(g,u,m),null===s?c=g:s.sibling=g,s=g);return e&&f.forEach((function(e){return t(i,e)})),c}function g(i,a,l,c){var s=me(l);if("function"!==typeof s)throw Error(u(150));if(null==(l=s.call(l)))throw Error(u(151));for(var f=s=null,m=a,g=a=0,v=null,y=l.next();null!==m&&!y.done;g++,y=l.next()){m.index>g?(v=m,m=null):v=m.sibling;var b=p(i,m,y.value,c);if(null===b){null===m&&(m=v);break}e&&m&&null===b.alternate&&t(i,m),a=o(b,a,g),null===f?s=b:f.sibling=b,f=b,m=v}if(y.done)return n(i,m),s;if(null===m){for(;!y.done;g++,y=l.next())null!==(y=d(i,y.value,c))&&(a=o(y,a,g),null===f?s=y:f.sibling=y,f=y);return s}for(m=r(i,m);!y.done;g++,y=l.next())null!==(y=h(m,i,g,y.value,c))&&(e&&null!==y.alternate&&m.delete(null===y.key?g:y.key),a=o(y,a,g),null===f?s=y:f.sibling=y,f=y);return e&&m.forEach((function(e){return t(i,e)})),s}return function(e,r,o,l){var c="object"===typeof o&&null!==o&&o.type===ne&&null===o.key;c&&(o=o.props.children);var s="object"===typeof o&&null!==o;if(s)switch(o.$$typeof){case ee:e:{for(s=o.key,c=r;null!==c;){if(c.key===s){switch(c.tag){case 7:if(o.type===ne){n(e,c.sibling),(r=i(c,o.props.children)).return=e,e=r;break e}break;default:if(c.elementType===o.type){n(e,c.sibling),(r=i(c,o.props)).ref=_o(e,c,o),r.return=e,e=r;break e}}n(e,c);break}t(e,c),c=c.sibling}o.type===ne?((r=Ml(o.props.children,e.mode,l,o.key)).return=e,e=r):((l=Sl(o.type,o.key,o.props,null,e.mode,l)).ref=_o(e,r,o),l.return=e,e=l)}return a(e);case te:e:{for(c=o.key;null!==r;){if(r.key===c){if(4===r.tag&&r.stateNode.containerInfo===o.containerInfo&&r.stateNode.implementation===o.implementation){n(e,r.sibling),(r=i(r,o.children||[])).return=e,e=r;break e}n(e,r);break}t(e,r),r=r.sibling}(r=Pl(o,e.mode,l)).return=e,e=r}return a(e)}if("string"===typeof o||"number"===typeof o)return o=""+o,null!==r&&6===r.tag?(n(e,r.sibling),(r=i(r,o)).return=e,e=r):(n(e,r),(r=Nl(o,e.mode,l)).return=e,e=r),a(e);if(xo(o))return m(e,r,o,l);if(me(o))return g(e,r,o,l);if(s&&ko(e,o),"undefined"===typeof o&&!c)switch(e.tag){case 1:case 0:throw e=e.type,Error(u(152,e.displayName||e.name||"Component"))}return n(e,r)}}var Eo=To(!0),Co=To(!1),So={},Mo={current:So},No={current:So},Po={current:So};function Ao(e){if(e===So)throw Error(u(174));return e}function Ro(e,t){switch(ci(Po,t),ci(No,e),ci(Mo,So),e=t.nodeType){case 9:case 11:t=(t=t.documentElement)?t.namespaceURI:Ue(null,"");break;default:t=Ue(t=(e=8===e?t.parentNode:t).namespaceURI||null,e=e.tagName)}li(Mo),ci(Mo,t)}function Do(){li(Mo),li(No),li(Po)}function Oo(e){Ao(Po.current);var t=Ao(Mo.current),n=Ue(t,e.type);t!==n&&(ci(No,e),ci(Mo,n))}function zo(e){No.current===e&&(li(Mo),li(No))}var Lo={current:0};function Uo(e){for(var t=e;null!==t;){if(13===t.tag){var n=t.memoizedState;if(null!==n&&(null===(n=n.dehydrated)||"$?"===n.data||"$!"===n.data))return t}else if(19===t.tag&&void 0!==t.memoizedProps.revealOrder){if(0!==(64&t.effectTag))return t}else if(null!==t.child){t.child.return=t,t=t.child;continue}if(t===e)break;for(;null===t.sibling;){if(null===t.return||t.return===e)return null;t=t.return}t.sibling.return=t.return,t=t.sibling}return null}function Fo(e,t){return{responder:e,props:t}}var Io=K.ReactCurrentDispatcher,jo=K.ReactCurrentBatchConfig,Ho=0,Vo=null,$o=null,Wo=null,Qo=!1;function qo(){throw Error(u(321))}function Bo(e,t){if(null===t)return!1;for(var n=0;n<t.length&&n<e.length;n++)if(!Ur(e[n],t[n]))return!1;return!0}function Yo(e,t,n,r,i,o){if(Ho=o,Vo=t,t.memoizedState=null,t.updateQueue=null,t.expirationTime=0,Io.current=null===e||null===e.memoizedState?vu:yu,e=n(r,i),t.expirationTime===Ho){o=0;do{if(t.expirationTime=0,!(25>o))throw Error(u(301));o+=1,Wo=$o=null,t.updateQueue=null,Io.current=bu,e=n(r,i)}while(t.expirationTime===Ho)}if(Io.current=gu,t=null!==$o&&null!==$o.next,Ho=0,Wo=$o=Vo=null,Qo=!1,t)throw Error(u(300));return e}function Xo(){var e={memoizedState:null,baseState:null,baseQueue:null,queue:null,next:null};return null===Wo?Vo.memoizedState=Wo=e:Wo=Wo.next=e,Wo}function Ko(){if(null===$o){var e=Vo.alternate;e=null!==e?e.memoizedState:null}else e=$o.next;var t=null===Wo?Vo.memoizedState:Wo.next;if(null!==t)Wo=t,$o=e;else{if(null===e)throw Error(u(310));e={memoizedState:($o=e).memoizedState,baseState:$o.baseState,baseQueue:$o.baseQueue,queue:$o.queue,next:null},null===Wo?Vo.memoizedState=Wo=e:Wo=Wo.next=e}return Wo}function Zo(e,t){return"function"===typeof t?t(e):t}function Jo(e){var t=Ko(),n=t.queue;if(null===n)throw Error(u(311));n.lastRenderedReducer=e;var r=$o,i=r.baseQueue,o=n.pending;if(null!==o){if(null!==i){var a=i.next;i.next=o.next,o.next=a}r.baseQueue=i=o,n.pending=null}if(null!==i){i=i.next,r=r.baseState;var l=a=o=null,c=i;do{var s=c.expirationTime;if(s<Ho){var f={expirationTime:c.expirationTime,suspenseConfig:c.suspenseConfig,action:c.action,eagerReducer:c.eagerReducer,eagerState:c.eagerState,next:null};null===l?(a=l=f,o=r):l=l.next=f,s>Vo.expirationTime&&(Vo.expirationTime=s,ul(s))}else null!==l&&(l=l.next={expirationTime:1073741823,suspenseConfig:c.suspenseConfig,action:c.action,eagerReducer:c.eagerReducer,eagerState:c.eagerState,next:null}),ol(s,c.suspenseConfig),r=c.eagerReducer===e?c.eagerState:e(r,c.action);c=c.next}while(null!==c&&c!==i);null===l?o=r:l.next=a,Ur(r,t.memoizedState)||(Pu=!0),t.memoizedState=r,t.baseState=o,t.baseQueue=l,n.lastRenderedState=r}return[t.memoizedState,n.dispatch]}function Go(e){var t=Ko(),n=t.queue;if(null===n)throw Error(u(311));n.lastRenderedReducer=e;var r=n.dispatch,i=n.pending,o=t.memoizedState;if(null!==i){n.pending=null;var a=i=i.next;do{o=e(o,a.action),a=a.next}while(a!==i);Ur(o,t.memoizedState)||(Pu=!0),t.memoizedState=o,null===t.baseQueue&&(t.baseState=o),n.lastRenderedState=o}return[o,r]}function eu(e){var t=Xo();return"function"===typeof e&&(e=e()),t.memoizedState=t.baseState=e,e=(e=t.queue={pending:null,dispatch:null,lastRenderedReducer:Zo,lastRenderedState:e}).dispatch=mu.bind(null,Vo,e),[t.memoizedState,e]}function tu(e,t,n,r){return e={tag:e,create:t,destroy:n,deps:r,next:null},null===(t=Vo.updateQueue)?(t={lastEffect:null},Vo.updateQueue=t,t.lastEffect=e.next=e):null===(n=t.lastEffect)?t.lastEffect=e.next=e:(r=n.next,n.next=e,e.next=r,t.lastEffect=e),e}function nu(){return Ko().memoizedState}function ru(e,t,n,r){var i=Xo();Vo.effectTag|=e,i.memoizedState=tu(1|t,n,void 0,void 0===r?null:r)}function iu(e,t,n,r){var i=Ko();r=void 0===r?null:r;var o=void 0;if(null!==$o){var u=$o.memoizedState;if(o=u.destroy,null!==r&&Bo(r,u.deps))return void tu(t,n,o,r)}Vo.effectTag|=e,i.memoizedState=tu(1|t,n,o,r)}function ou(e,t){return ru(516,4,e,t)}function uu(e,t){return iu(516,4,e,t)}function au(e,t){return iu(4,2,e,t)}function lu(e,t){return"function"===typeof t?(e=e(),t(e),function(){t(null)}):null!==t&&void 0!==t?(e=e(),t.current=e,function(){t.current=null}):void 0}function cu(e,t,n){return n=null!==n&&void 0!==n?n.concat([e]):null,iu(4,2,lu.bind(null,t,e),n)}function su(){}function fu(e,t){return Xo().memoizedState=[e,void 0===t?null:t],e}function du(e,t){var n=Ko();t=void 0===t?null:t;var r=n.memoizedState;return null!==r&&null!==t&&Bo(t,r[1])?r[0]:(n.memoizedState=[e,t],e)}function pu(e,t){var n=Ko();t=void 0===t?null:t;var r=n.memoizedState;return null!==r&&null!==t&&Bo(t,r[1])?r[0]:(e=e(),n.memoizedState=[e,t],e)}function hu(e,t,n){var r=ji();Vi(98>r?98:r,(function(){e(!0)})),Vi(97<r?97:r,(function(){var r=jo.suspense;jo.suspense=void 0===t?null:t;try{e(!1),n()}finally{jo.suspense=r}}))}function mu(e,t,n){var r=qa(),i=po.suspense;i={expirationTime:r=Ba(r,e,i),suspenseConfig:i,action:n,eagerReducer:null,eagerState:null,next:null};var o=t.pending;if(null===o?i.next=i:(i.next=o.next,o.next=i),t.pending=i,o=e.alternate,e===Vo||null!==o&&o===Vo)Qo=!0,i.expirationTime=Ho,Vo.expirationTime=Ho;else{if(0===e.expirationTime&&(null===o||0===o.expirationTime)&&null!==(o=t.lastRenderedReducer))try{var u=t.lastRenderedState,a=o(u,n);if(i.eagerReducer=o,i.eagerState=a,Ur(a,u))return}catch(l){}Ya(e,r)}}var gu={readContext:ro,useCallback:qo,useContext:qo,useEffect:qo,useImperativeHandle:qo,useLayoutEffect:qo,useMemo:qo,useReducer:qo,useRef:qo,useState:qo,useDebugValue:qo,useResponder:qo,useDeferredValue:qo,useTransition:qo},vu={readContext:ro,useCallback:fu,useContext:ro,useEffect:ou,useImperativeHandle:function(e,t,n){return n=null!==n&&void 0!==n?n.concat([e]):null,ru(4,2,lu.bind(null,t,e),n)},useLayoutEffect:function(e,t){return ru(4,2,e,t)},useMemo:function(e,t){var n=Xo();return t=void 0===t?null:t,e=e(),n.memoizedState=[e,t],e},useReducer:function(e,t,n){var r=Xo();return t=void 0!==n?n(t):t,r.memoizedState=r.baseState=t,e=(e=r.queue={pending:null,dispatch:null,lastRenderedReducer:e,lastRenderedState:t}).dispatch=mu.bind(null,Vo,e),[r.memoizedState,e]},useRef:function(e){return e={current:e},Xo().memoizedState=e},useState:eu,useDebugValue:su,useResponder:Fo,useDeferredValue:function(e,t){var n=eu(e),r=n[0],i=n[1];return ou((function(){var n=jo.suspense;jo.suspense=void 0===t?null:t;try{i(e)}finally{jo.suspense=n}}),[e,t]),r},useTransition:function(e){var t=eu(!1),n=t[0];return t=t[1],[fu(hu.bind(null,t,e),[t,e]),n]}},yu={readContext:ro,useCallback:du,useContext:ro,useEffect:uu,useImperativeHandle:cu,useLayoutEffect:au,useMemo:pu,useReducer:Jo,useRef:nu,useState:function(){return Jo(Zo)},useDebugValue:su,useResponder:Fo,useDeferredValue:function(e,t){var n=Jo(Zo),r=n[0],i=n[1];return uu((function(){var n=jo.suspense;jo.suspense=void 0===t?null:t;try{i(e)}finally{jo.suspense=n}}),[e,t]),r},useTransition:function(e){var t=Jo(Zo),n=t[0];return t=t[1],[du(hu.bind(null,t,e),[t,e]),n]}},bu={readContext:ro,useCallback:du,useContext:ro,useEffect:uu,useImperativeHandle:cu,useLayoutEffect:au,useMemo:pu,useReducer:Go,useRef:nu,useState:function(){return Go(Zo)},useDebugValue:su,useResponder:Fo,useDeferredValue:function(e,t){var n=Go(Zo),r=n[0],i=n[1];return uu((function(){var n=jo.suspense;jo.suspense=void 0===t?null:t;try{i(e)}finally{jo.suspense=n}}),[e,t]),r},useTransition:function(e){var t=Go(Zo),n=t[0];return t=t[1],[du(hu.bind(null,t,e),[t,e]),n]}},wu=null,xu=null,_u=!1;function ku(e,t){var n=Tl(5,null,null,0);n.elementType="DELETED",n.type="DELETED",n.stateNode=t,n.return=e,n.effectTag=8,null!==e.lastEffect?(e.lastEffect.nextEffect=n,e.lastEffect=n):e.firstEffect=e.lastEffect=n}function Tu(e,t){switch(e.tag){case 5:var n=e.type;return null!==(t=1!==t.nodeType||n.toLowerCase()!==t.nodeName.toLowerCase()?null:t)&&(e.stateNode=t,!0);case 6:return null!==(t=""===e.pendingProps||3!==t.nodeType?null:t)&&(e.stateNode=t,!0);case 13:default:return!1}}function Eu(e){if(_u){var t=xu;if(t){var n=t;if(!Tu(e,t)){if(!(t=xn(n.nextSibling))||!Tu(e,t))return e.effectTag=-1025&e.effectTag|2,_u=!1,void(wu=e);ku(wu,n)}wu=e,xu=xn(t.firstChild)}else e.effectTag=-1025&e.effectTag|2,_u=!1,wu=e}}function Cu(e){for(e=e.return;null!==e&&5!==e.tag&&3!==e.tag&&13!==e.tag;)e=e.return;wu=e}function Su(e){if(e!==wu)return!1;if(!_u)return Cu(e),_u=!0,!1;var t=e.type;if(5!==e.tag||"head"!==t&&"body"!==t&&!yn(t,e.memoizedProps))for(t=xu;t;)ku(e,t),t=xn(t.nextSibling);if(Cu(e),13===e.tag){if(!(e=null!==(e=e.memoizedState)?e.dehydrated:null))throw Error(u(317));e:{for(e=e.nextSibling,t=0;e;){if(8===e.nodeType){var n=e.data;if("/$"===n){if(0===t){xu=xn(e.nextSibling);break e}t--}else"$"!==n&&"$!"!==n&&"$?"!==n||t++}e=e.nextSibling}xu=null}}else xu=wu?xn(e.stateNode.nextSibling):null;return!0}function Mu(){xu=wu=null,_u=!1}var Nu=K.ReactCurrentOwner,Pu=!1;function Au(e,t,n,r){t.child=null===e?Co(t,null,n,r):Eo(t,e.child,n,r)}function Ru(e,t,n,r,i){n=n.render;var o=t.ref;return no(t,i),r=Yo(e,t,n,r,o,i),null===e||Pu?(t.effectTag|=1,Au(e,t,r,i),t.child):(t.updateQueue=e.updateQueue,t.effectTag&=-517,e.expirationTime<=i&&(e.expirationTime=0),Yu(e,t,i))}function Du(e,t,n,r,i,o){if(null===e){var u=n.type;return"function"!==typeof u||El(u)||void 0!==u.defaultProps||null!==n.compare||void 0!==n.defaultProps?((e=Sl(n.type,null,r,null,t.mode,o)).ref=t.ref,e.return=t,t.child=e):(t.tag=15,t.type=u,Ou(e,t,u,r,i,o))}return u=e.child,i<o&&(i=u.memoizedProps,(n=null!==(n=n.compare)?n:Ir)(i,r)&&e.ref===t.ref)?Yu(e,t,o):(t.effectTag|=1,(e=Cl(u,r)).ref=t.ref,e.return=t,t.child=e)}function Ou(e,t,n,r,i,o){return null!==e&&Ir(e.memoizedProps,r)&&e.ref===t.ref&&(Pu=!1,i<o)?(t.expirationTime=e.expirationTime,Yu(e,t,o)):Lu(e,t,n,r,o)}function zu(e,t){var n=t.ref;(null===e&&null!==n||null!==e&&e.ref!==n)&&(t.effectTag|=128)}function Lu(e,t,n,r,i){var o=mi(n)?pi:fi.current;return o=hi(t,o),no(t,i),n=Yo(e,t,n,r,o,i),null===e||Pu?(t.effectTag|=1,Au(e,t,n,i),t.child):(t.updateQueue=e.updateQueue,t.effectTag&=-517,e.expirationTime<=i&&(e.expirationTime=0),Yu(e,t,i))}function Uu(e,t,n,r,i){if(mi(n)){var o=!0;bi(t)}else o=!1;if(no(t,i),null===t.stateNode)null!==e&&(e.alternate=null,t.alternate=null,t.effectTag|=2),yo(t,n,r),wo(t,n,r,i),r=!0;else if(null===e){var u=t.stateNode,a=t.memoizedProps;u.props=a;var l=u.context,c=n.contextType;"object"===typeof c&&null!==c?c=ro(c):c=hi(t,c=mi(n)?pi:fi.current);var s=n.getDerivedStateFromProps,f="function"===typeof s||"function"===typeof u.getSnapshotBeforeUpdate;f||"function"!==typeof u.UNSAFE_componentWillReceiveProps&&"function"!==typeof u.componentWillReceiveProps||(a!==r||l!==c)&&bo(t,u,r,c),io=!1;var d=t.memoizedState;u.state=d,so(t,r,u,i),l=t.memoizedState,a!==r||d!==l||di.current||io?("function"===typeof s&&(mo(t,n,s,r),l=t.memoizedState),(a=io||vo(t,n,a,r,d,l,c))?(f||"function"!==typeof u.UNSAFE_componentWillMount&&"function"!==typeof u.componentWillMount||("function"===typeof u.componentWillMount&&u.componentWillMount(),"function"===typeof u.UNSAFE_componentWillMount&&u.UNSAFE_componentWillMount()),"function"===typeof u.componentDidMount&&(t.effectTag|=4)):("function"===typeof u.componentDidMount&&(t.effectTag|=4),t.memoizedProps=r,t.memoizedState=l),u.props=r,u.state=l,u.context=c,r=a):("function"===typeof u.componentDidMount&&(t.effectTag|=4),r=!1)}else u=t.stateNode,uo(e,t),a=t.memoizedProps,u.props=t.type===t.elementType?a:Yi(t.type,a),l=u.context,"object"===typeof(c=n.contextType)&&null!==c?c=ro(c):c=hi(t,c=mi(n)?pi:fi.current),(f="function"===typeof(s=n.getDerivedStateFromProps)||"function"===typeof u.getSnapshotBeforeUpdate)||"function"!==typeof u.UNSAFE_componentWillReceiveProps&&"function"!==typeof u.componentWillReceiveProps||(a!==r||l!==c)&&bo(t,u,r,c),io=!1,l=t.memoizedState,u.state=l,so(t,r,u,i),d=t.memoizedState,a!==r||l!==d||di.current||io?("function"===typeof s&&(mo(t,n,s,r),d=t.memoizedState),(s=io||vo(t,n,a,r,l,d,c))?(f||"function"!==typeof u.UNSAFE_componentWillUpdate&&"function"!==typeof u.componentWillUpdate||("function"===typeof u.componentWillUpdate&&u.componentWillUpdate(r,d,c),"function"===typeof u.UNSAFE_componentWillUpdate&&u.UNSAFE_componentWillUpdate(r,d,c)),"function"===typeof u.componentDidUpdate&&(t.effectTag|=4),"function"===typeof u.getSnapshotBeforeUpdate&&(t.effectTag|=256)):("function"!==typeof u.componentDidUpdate||a===e.memoizedProps&&l===e.memoizedState||(t.effectTag|=4),"function"!==typeof u.getSnapshotBeforeUpdate||a===e.memoizedProps&&l===e.memoizedState||(t.effectTag|=256),t.memoizedProps=r,t.memoizedState=d),u.props=r,u.state=d,u.context=c,r=s):("function"!==typeof u.componentDidUpdate||a===e.memoizedProps&&l===e.memoizedState||(t.effectTag|=4),"function"!==typeof u.getSnapshotBeforeUpdate||a===e.memoizedProps&&l===e.memoizedState||(t.effectTag|=256),r=!1);return Fu(e,t,n,r,o,i)}function Fu(e,t,n,r,i,o){zu(e,t);var u=0!==(64&t.effectTag);if(!r&&!u)return i&&wi(t,n,!1),Yu(e,t,o);r=t.stateNode,Nu.current=t;var a=u&&"function"!==typeof n.getDerivedStateFromError?null:r.render();return t.effectTag|=1,null!==e&&u?(t.child=Eo(t,e.child,null,o),t.child=Eo(t,null,a,o)):Au(e,t,a,o),t.memoizedState=r.state,i&&wi(t,n,!0),t.child}function Iu(e){var t=e.stateNode;t.pendingContext?vi(0,t.pendingContext,t.pendingContext!==t.context):t.context&&vi(0,t.context,!1),Ro(e,t.containerInfo)}var ju,Hu,Vu,$u={dehydrated:null,retryTime:0};function Wu(e,t,n){var r,i=t.mode,o=t.pendingProps,u=Lo.current,a=!1;if((r=0!==(64&t.effectTag))||(r=0!==(2&u)&&(null===e||null!==e.memoizedState)),r?(a=!0,t.effectTag&=-65):null!==e&&null===e.memoizedState||void 0===o.fallback||!0===o.unstable_avoidThisFallback||(u|=1),ci(Lo,1&u),null===e){if(void 0!==o.fallback&&Eu(t),a){if(a=o.fallback,(o=Ml(null,i,0,null)).return=t,0===(2&t.mode))for(e=null!==t.memoizedState?t.child.child:t.child,o.child=e;null!==e;)e.return=o,e=e.sibling;return(n=Ml(a,i,n,null)).return=t,o.sibling=n,t.memoizedState=$u,t.child=o,n}return i=o.children,t.memoizedState=null,t.child=Co(t,null,i,n)}if(null!==e.memoizedState){if(i=(e=e.child).sibling,a){if(o=o.fallback,(n=Cl(e,e.pendingProps)).return=t,0===(2&t.mode)&&(a=null!==t.memoizedState?t.child.child:t.child)!==e.child)for(n.child=a;null!==a;)a.return=n,a=a.sibling;return(i=Cl(i,o)).return=t,n.sibling=i,n.childExpirationTime=0,t.memoizedState=$u,t.child=n,i}return n=Eo(t,e.child,o.children,n),t.memoizedState=null,t.child=n}if(e=e.child,a){if(a=o.fallback,(o=Ml(null,i,0,null)).return=t,o.child=e,null!==e&&(e.return=o),0===(2&t.mode))for(e=null!==t.memoizedState?t.child.child:t.child,o.child=e;null!==e;)e.return=o,e=e.sibling;return(n=Ml(a,i,n,null)).return=t,o.sibling=n,n.effectTag|=2,o.childExpirationTime=0,t.memoizedState=$u,t.child=o,n}return t.memoizedState=null,t.child=Eo(t,e,o.children,n)}function Qu(e,t){e.expirationTime<t&&(e.expirationTime=t);var n=e.alternate;null!==n&&n.expirationTime<t&&(n.expirationTime=t),to(e.return,t)}function qu(e,t,n,r,i,o){var u=e.memoizedState;null===u?e.memoizedState={isBackwards:t,rendering:null,renderingStartTime:0,last:r,tail:n,tailExpiration:0,tailMode:i,lastEffect:o}:(u.isBackwards=t,u.rendering=null,u.renderingStartTime=0,u.last=r,u.tail=n,u.tailExpiration=0,u.tailMode=i,u.lastEffect=o)}function Bu(e,t,n){var r=t.pendingProps,i=r.revealOrder,o=r.tail;if(Au(e,t,r.children,n),0!==(2&(r=Lo.current)))r=1&r|2,t.effectTag|=64;else{if(null!==e&&0!==(64&e.effectTag))e:for(e=t.child;null!==e;){if(13===e.tag)null!==e.memoizedState&&Qu(e,n);else if(19===e.tag)Qu(e,n);else if(null!==e.child){e.child.return=e,e=e.child;continue}if(e===t)break e;for(;null===e.sibling;){if(null===e.return||e.return===t)break e;e=e.return}e.sibling.return=e.return,e=e.sibling}r&=1}if(ci(Lo,r),0===(2&t.mode))t.memoizedState=null;else switch(i){case"forwards":for(n=t.child,i=null;null!==n;)null!==(e=n.alternate)&&null===Uo(e)&&(i=n),n=n.sibling;null===(n=i)?(i=t.child,t.child=null):(i=n.sibling,n.sibling=null),qu(t,!1,i,n,o,t.lastEffect);break;case"backwards":for(n=null,i=t.child,t.child=null;null!==i;){if(null!==(e=i.alternate)&&null===Uo(e)){t.child=i;break}e=i.sibling,i.sibling=n,n=i,i=e}qu(t,!0,n,null,o,t.lastEffect);break;case"together":qu(t,!1,null,null,void 0,t.lastEffect);break;default:t.memoizedState=null}return t.child}function Yu(e,t,n){null!==e&&(t.dependencies=e.dependencies);var r=t.expirationTime;if(0!==r&&ul(r),t.childExpirationTime<n)return null;if(null!==e&&t.child!==e.child)throw Error(u(153));if(null!==t.child){for(n=Cl(e=t.child,e.pendingProps),t.child=n,n.return=t;null!==e.sibling;)e=e.sibling,(n=n.sibling=Cl(e,e.pendingProps)).return=t;n.sibling=null}return t.child}function Xu(e,t){switch(e.tailMode){case"hidden":t=e.tail;for(var n=null;null!==t;)null!==t.alternate&&(n=t),t=t.sibling;null===n?e.tail=null:n.sibling=null;break;case"collapsed":n=e.tail;for(var r=null;null!==n;)null!==n.alternate&&(r=n),n=n.sibling;null===r?t||null===e.tail?e.tail=null:e.tail.sibling=null:r.sibling=null}}function Ku(e,t,n){var r=t.pendingProps;switch(t.tag){case 2:case 16:case 15:case 0:case 11:case 7:case 8:case 12:case 9:case 14:return null;case 1:return mi(t.type)&&gi(),null;case 3:return Do(),li(di),li(fi),(n=t.stateNode).pendingContext&&(n.context=n.pendingContext,n.pendingContext=null),null!==e&&null!==e.child||!Su(t)||(t.effectTag|=4),null;case 5:zo(t),n=Ao(Po.current);var o=t.type;if(null!==e&&null!=t.stateNode)Hu(e,t,o,r,n),e.ref!==t.ref&&(t.effectTag|=128);else{if(!r){if(null===t.stateNode)throw Error(u(166));return null}if(e=Ao(Mo.current),Su(t)){r=t.stateNode,o=t.type;var a=t.memoizedProps;switch(r[Tn]=t,r[En]=a,o){case"iframe":case"object":case"embed":Bt("load",r);break;case"video":case"audio":for(e=0;e<Ke.length;e++)Bt(Ke[e],r);break;case"source":Bt("error",r);break;case"img":case"image":case"link":Bt("error",r),Bt("load",r);break;case"form":Bt("reset",r),Bt("submit",r);break;case"details":Bt("toggle",r);break;case"input":ke(r,a),Bt("invalid",r),ln(n,"onChange");break;case"select":r._wrapperState={wasMultiple:!!a.multiple},Bt("invalid",r),ln(n,"onChange");break;case"textarea":Ae(r,a),Bt("invalid",r),ln(n,"onChange")}for(var l in on(o,a),e=null,a)if(a.hasOwnProperty(l)){var c=a[l];"children"===l?"string"===typeof c?r.textContent!==c&&(e=["children",c]):"number"===typeof c&&r.textContent!==""+c&&(e=["children",""+c]):T.hasOwnProperty(l)&&null!=c&&ln(n,l)}switch(o){case"input":we(r),Ce(r,a,!0);break;case"textarea":we(r),De(r);break;case"select":case"option":break;default:"function"===typeof a.onClick&&(r.onclick=cn)}n=e,t.updateQueue=n,null!==n&&(t.effectTag|=4)}else{switch(l=9===n.nodeType?n:n.ownerDocument,e===an&&(e=Le(o)),e===an?"script"===o?((e=l.createElement("div")).innerHTML="<script><\/script>",e=e.removeChild(e.firstChild)):"string"===typeof r.is?e=l.createElement(o,{is:r.is}):(e=l.createElement(o),"select"===o&&(l=e,r.multiple?l.multiple=!0:r.size&&(l.size=r.size))):e=l.createElementNS(e,o),e[Tn]=t,e[En]=r,ju(e,t),t.stateNode=e,l=un(o,r),o){case"iframe":case"object":case"embed":Bt("load",e),c=r;break;case"video":case"audio":for(c=0;c<Ke.length;c++)Bt(Ke[c],e);c=r;break;case"source":Bt("error",e),c=r;break;case"img":case"image":case"link":Bt("error",e),Bt("load",e),c=r;break;case"form":Bt("reset",e),Bt("submit",e),c=r;break;case"details":Bt("toggle",e),c=r;break;case"input":ke(e,r),c=_e(e,r),Bt("invalid",e),ln(n,"onChange");break;case"option":c=Me(e,r);break;case"select":e._wrapperState={wasMultiple:!!r.multiple},c=i({},r,{value:void 0}),Bt("invalid",e),ln(n,"onChange");break;case"textarea":Ae(e,r),c=Pe(e,r),Bt("invalid",e),ln(n,"onChange");break;default:c=r}on(o,c);var s=c;for(a in s)if(s.hasOwnProperty(a)){var f=s[a];"style"===a?nn(e,f):"dangerouslySetInnerHTML"===a?null!=(f=f?f.__html:void 0)&&Ie(e,f):"children"===a?"string"===typeof f?("textarea"!==o||""!==f)&&je(e,f):"number"===typeof f&&je(e,""+f):"suppressContentEditableWarning"!==a&&"suppressHydrationWarning"!==a&&"autoFocus"!==a&&(T.hasOwnProperty(a)?null!=f&&ln(n,a):null!=f&&Z(e,a,f,l))}switch(o){case"input":we(e),Ce(e,r,!1);break;case"textarea":we(e),De(e);break;case"option":null!=r.value&&e.setAttribute("value",""+ye(r.value));break;case"select":e.multiple=!!r.multiple,null!=(n=r.value)?Ne(e,!!r.multiple,n,!1):null!=r.defaultValue&&Ne(e,!!r.multiple,r.defaultValue,!0);break;default:"function"===typeof c.onClick&&(e.onclick=cn)}vn(o,r)&&(t.effectTag|=4)}null!==t.ref&&(t.effectTag|=128)}return null;case 6:if(e&&null!=t.stateNode)Vu(0,t,e.memoizedProps,r);else{if("string"!==typeof r&&null===t.stateNode)throw Error(u(166));n=Ao(Po.current),Ao(Mo.current),Su(t)?(n=t.stateNode,r=t.memoizedProps,n[Tn]=t,n.nodeValue!==r&&(t.effectTag|=4)):((n=(9===n.nodeType?n:n.ownerDocument).createTextNode(r))[Tn]=t,t.stateNode=n)}return null;case 13:return li(Lo),r=t.memoizedState,0!==(64&t.effectTag)?(t.expirationTime=n,t):(n=null!==r,r=!1,null===e?void 0!==t.memoizedProps.fallback&&Su(t):(r=null!==(o=e.memoizedState),n||null===o||null!==(o=e.child.sibling)&&(null!==(a=t.firstEffect)?(t.firstEffect=o,o.nextEffect=a):(t.firstEffect=t.lastEffect=o,o.nextEffect=null),o.effectTag=8)),n&&!r&&0!==(2&t.mode)&&(null===e&&!0!==t.memoizedProps.unstable_avoidThisFallback||0!==(1&Lo.current)?Sa===wa&&(Sa=xa):(Sa!==wa&&Sa!==xa||(Sa=_a),0!==Ra&&null!==Ta&&(Dl(Ta,Ca),Ol(Ta,Ra)))),(n||r)&&(t.effectTag|=4),null);case 4:return Do(),null;case 10:return eo(t),null;case 17:return mi(t.type)&&gi(),null;case 19:if(li(Lo),null===(r=t.memoizedState))return null;if(o=0!==(64&t.effectTag),null===(a=r.rendering)){if(o)Xu(r,!1);else if(Sa!==wa||null!==e&&0!==(64&e.effectTag))for(a=t.child;null!==a;){if(null!==(e=Uo(a))){for(t.effectTag|=64,Xu(r,!1),null!==(o=e.updateQueue)&&(t.updateQueue=o,t.effectTag|=4),null===r.lastEffect&&(t.firstEffect=null),t.lastEffect=r.lastEffect,r=t.child;null!==r;)a=n,(o=r).effectTag&=2,o.nextEffect=null,o.firstEffect=null,o.lastEffect=null,null===(e=o.alternate)?(o.childExpirationTime=0,o.expirationTime=a,o.child=null,o.memoizedProps=null,o.memoizedState=null,o.updateQueue=null,o.dependencies=null):(o.childExpirationTime=e.childExpirationTime,o.expirationTime=e.expirationTime,o.child=e.child,o.memoizedProps=e.memoizedProps,o.memoizedState=e.memoizedState,o.updateQueue=e.updateQueue,a=e.dependencies,o.dependencies=null===a?null:{expirationTime:a.expirationTime,firstContext:a.firstContext,responders:a.responders}),r=r.sibling;return ci(Lo,1&Lo.current|2),t.child}a=a.sibling}}else{if(!o)if(null!==(e=Uo(a))){if(t.effectTag|=64,o=!0,null!==(n=e.updateQueue)&&(t.updateQueue=n,t.effectTag|=4),Xu(r,!0),null===r.tail&&"hidden"===r.tailMode&&!a.alternate)return null!==(t=t.lastEffect=r.lastEffect)&&(t.nextEffect=null),null}else 2*Ii()-r.renderingStartTime>r.tailExpiration&&1<n&&(t.effectTag|=64,o=!0,Xu(r,!1),t.expirationTime=t.childExpirationTime=n-1);r.isBackwards?(a.sibling=t.child,t.child=a):(null!==(n=r.last)?n.sibling=a:t.child=a,r.last=a)}return null!==r.tail?(0===r.tailExpiration&&(r.tailExpiration=Ii()+500),n=r.tail,r.rendering=n,r.tail=n.sibling,r.lastEffect=t.lastEffect,r.renderingStartTime=Ii(),n.sibling=null,t=Lo.current,ci(Lo,o?1&t|2:1&t),n):null}throw Error(u(156,t.tag))}function Zu(e){switch(e.tag){case 1:mi(e.type)&&gi();var t=e.effectTag;return 4096&t?(e.effectTag=-4097&t|64,e):null;case 3:if(Do(),li(di),li(fi),0!==(64&(t=e.effectTag)))throw Error(u(285));return e.effectTag=-4097&t|64,e;case 5:return zo(e),null;case 13:return li(Lo),4096&(t=e.effectTag)?(e.effectTag=-4097&t|64,e):null;case 19:return li(Lo),null;case 4:return Do(),null;case 10:return eo(e),null;default:return null}}function Ju(e,t){return{value:e,source:t,stack:ve(t)}}ju=function(e,t){for(var n=t.child;null!==n;){if(5===n.tag||6===n.tag)e.appendChild(n.stateNode);else if(4!==n.tag&&null!==n.child){n.child.return=n,n=n.child;continue}if(n===t)break;for(;null===n.sibling;){if(null===n.return||n.return===t)return;n=n.return}n.sibling.return=n.return,n=n.sibling}},Hu=function(e,t,n,r,o){var u=e.memoizedProps;if(u!==r){var a,l,c=t.stateNode;switch(Ao(Mo.current),e=null,n){case"input":u=_e(c,u),r=_e(c,r),e=[];break;case"option":u=Me(c,u),r=Me(c,r),e=[];break;case"select":u=i({},u,{value:void 0}),r=i({},r,{value:void 0}),e=[];break;case"textarea":u=Pe(c,u),r=Pe(c,r),e=[];break;default:"function"!==typeof u.onClick&&"function"===typeof r.onClick&&(c.onclick=cn)}for(a in on(n,r),n=null,u)if(!r.hasOwnProperty(a)&&u.hasOwnProperty(a)&&null!=u[a])if("style"===a)for(l in c=u[a])c.hasOwnProperty(l)&&(n||(n={}),n[l]="");else"dangerouslySetInnerHTML"!==a&&"children"!==a&&"suppressContentEditableWarning"!==a&&"suppressHydrationWarning"!==a&&"autoFocus"!==a&&(T.hasOwnProperty(a)?e||(e=[]):(e=e||[]).push(a,null));for(a in r){var s=r[a];if(c=null!=u?u[a]:void 0,r.hasOwnProperty(a)&&s!==c&&(null!=s||null!=c))if("style"===a)if(c){for(l in c)!c.hasOwnProperty(l)||s&&s.hasOwnProperty(l)||(n||(n={}),n[l]="");for(l in s)s.hasOwnProperty(l)&&c[l]!==s[l]&&(n||(n={}),n[l]=s[l])}else n||(e||(e=[]),e.push(a,n)),n=s;else"dangerouslySetInnerHTML"===a?(s=s?s.__html:void 0,c=c?c.__html:void 0,null!=s&&c!==s&&(e=e||[]).push(a,s)):"children"===a?c===s||"string"!==typeof s&&"number"!==typeof s||(e=e||[]).push(a,""+s):"suppressContentEditableWarning"!==a&&"suppressHydrationWarning"!==a&&(T.hasOwnProperty(a)?(null!=s&&ln(o,a),e||c===s||(e=[])):(e=e||[]).push(a,s))}n&&(e=e||[]).push("style",n),o=e,(t.updateQueue=o)&&(t.effectTag|=4)}},Vu=function(e,t,n,r){n!==r&&(t.effectTag|=4)};var Gu="function"===typeof WeakSet?WeakSet:Set;function ea(e,t){var n=t.source,r=t.stack;null===r&&null!==n&&(r=ve(n)),null!==n&&ge(n.type),t=t.value,null!==e&&1===e.tag&&ge(e.type);try{console.error(t)}catch(i){setTimeout((function(){throw i}))}}function ta(e){var t=e.ref;if(null!==t)if("function"===typeof t)try{t(null)}catch(n){yl(e,n)}else t.current=null}function na(e,t){switch(t.tag){case 0:case 11:case 15:case 22:return;case 1:if(256&t.effectTag&&null!==e){var n=e.memoizedProps,r=e.memoizedState;t=(e=t.stateNode).getSnapshotBeforeUpdate(t.elementType===t.type?n:Yi(t.type,n),r),e.__reactInternalSnapshotBeforeUpdate=t}return;case 3:case 5:case 6:case 4:case 17:return}throw Error(u(163))}function ra(e,t){if(null!==(t=null!==(t=t.updateQueue)?t.lastEffect:null)){var n=t=t.next;do{if((n.tag&e)===e){var r=n.destroy;n.destroy=void 0,void 0!==r&&r()}n=n.next}while(n!==t)}}function ia(e,t){if(null!==(t=null!==(t=t.updateQueue)?t.lastEffect:null)){var n=t=t.next;do{if((n.tag&e)===e){var r=n.create;n.destroy=r()}n=n.next}while(n!==t)}}function oa(e,t,n){switch(n.tag){case 0:case 11:case 15:case 22:return void ia(3,n);case 1:if(e=n.stateNode,4&n.effectTag)if(null===t)e.componentDidMount();else{var r=n.elementType===n.type?t.memoizedProps:Yi(n.type,t.memoizedProps);e.componentDidUpdate(r,t.memoizedState,e.__reactInternalSnapshotBeforeUpdate)}return void(null!==(t=n.updateQueue)&&fo(n,t,e));case 3:if(null!==(t=n.updateQueue)){if(e=null,null!==n.child)switch(n.child.tag){case 5:e=n.child.stateNode;break;case 1:e=n.child.stateNode}fo(n,t,e)}return;case 5:return e=n.stateNode,void(null===t&&4&n.effectTag&&vn(n.type,n.memoizedProps)&&e.focus());case 6:case 4:case 12:return;case 13:return void(null===n.memoizedState&&(n=n.alternate,null!==n&&(n=n.memoizedState,null!==n&&(n=n.dehydrated,null!==n&&Lt(n)))));case 19:case 17:case 20:case 21:return}throw Error(u(163))}function ua(e,t,n){switch("function"===typeof _l&&_l(t),t.tag){case 0:case 11:case 14:case 15:case 22:if(null!==(e=t.updateQueue)&&null!==(e=e.lastEffect)){var r=e.next;Vi(97<n?97:n,(function(){var e=r;do{var n=e.destroy;if(void 0!==n){var i=t;try{n()}catch(o){yl(i,o)}}e=e.next}while(e!==r)}))}break;case 1:ta(t),"function"===typeof(n=t.stateNode).componentWillUnmount&&function(e,t){try{t.props=e.memoizedProps,t.state=e.memoizedState,t.componentWillUnmount()}catch(n){yl(e,n)}}(t,n);break;case 5:ta(t);break;case 4:sa(e,t,n)}}function aa(e){var t=e.alternate;e.return=null,e.child=null,e.memoizedState=null,e.updateQueue=null,e.dependencies=null,e.alternate=null,e.firstEffect=null,e.lastEffect=null,e.pendingProps=null,e.memoizedProps=null,e.stateNode=null,null!==t&&aa(t)}function la(e){return 5===e.tag||3===e.tag||4===e.tag}function ca(e){e:{for(var t=e.return;null!==t;){if(la(t)){var n=t;break e}t=t.return}throw Error(u(160))}switch(t=n.stateNode,n.tag){case 5:var r=!1;break;case 3:case 4:t=t.containerInfo,r=!0;break;default:throw Error(u(161))}16&n.effectTag&&(je(t,""),n.effectTag&=-17);e:t:for(n=e;;){for(;null===n.sibling;){if(null===n.return||la(n.return)){n=null;break e}n=n.return}for(n.sibling.return=n.return,n=n.sibling;5!==n.tag&&6!==n.tag&&18!==n.tag;){if(2&n.effectTag)continue t;if(null===n.child||4===n.tag)continue t;n.child.return=n,n=n.child}if(!(2&n.effectTag)){n=n.stateNode;break e}}r?function e(t,n,r){var i=t.tag,o=5===i||6===i;if(o)t=o?t.stateNode:t.stateNode.instance,n?8===r.nodeType?r.parentNode.insertBefore(t,n):r.insertBefore(t,n):(8===r.nodeType?(n=r.parentNode).insertBefore(t,r):(n=r).appendChild(t),null!==(r=r._reactRootContainer)&&void 0!==r||null!==n.onclick||(n.onclick=cn));else if(4!==i&&null!==(t=t.child))for(e(t,n,r),t=t.sibling;null!==t;)e(t,n,r),t=t.sibling}(e,n,t):function e(t,n,r){var i=t.tag,o=5===i||6===i;if(o)t=o?t.stateNode:t.stateNode.instance,n?r.insertBefore(t,n):r.appendChild(t);else if(4!==i&&null!==(t=t.child))for(e(t,n,r),t=t.sibling;null!==t;)e(t,n,r),t=t.sibling}(e,n,t)}function sa(e,t,n){for(var r,i,o=t,a=!1;;){if(!a){a=o.return;e:for(;;){if(null===a)throw Error(u(160));switch(r=a.stateNode,a.tag){case 5:i=!1;break e;case 3:case 4:r=r.containerInfo,i=!0;break e}a=a.return}a=!0}if(5===o.tag||6===o.tag){e:for(var l=e,c=o,s=n,f=c;;)if(ua(l,f,s),null!==f.child&&4!==f.tag)f.child.return=f,f=f.child;else{if(f===c)break e;for(;null===f.sibling;){if(null===f.return||f.return===c)break e;f=f.return}f.sibling.return=f.return,f=f.sibling}i?(l=r,c=o.stateNode,8===l.nodeType?l.parentNode.removeChild(c):l.removeChild(c)):r.removeChild(o.stateNode)}else if(4===o.tag){if(null!==o.child){r=o.stateNode.containerInfo,i=!0,o.child.return=o,o=o.child;continue}}else if(ua(e,o,n),null!==o.child){o.child.return=o,o=o.child;continue}if(o===t)break;for(;null===o.sibling;){if(null===o.return||o.return===t)return;4===(o=o.return).tag&&(a=!1)}o.sibling.return=o.return,o=o.sibling}}function fa(e,t){switch(t.tag){case 0:case 11:case 14:case 15:case 22:return void ra(3,t);case 1:return;case 5:var n=t.stateNode;if(null!=n){var r=t.memoizedProps,i=null!==e?e.memoizedProps:r;e=t.type;var o=t.updateQueue;if(t.updateQueue=null,null!==o){for(n[En]=r,"input"===e&&"radio"===r.type&&null!=r.name&&Te(n,r),un(e,i),t=un(e,r),i=0;i<o.length;i+=2){var a=o[i],l=o[i+1];"style"===a?nn(n,l):"dangerouslySetInnerHTML"===a?Ie(n,l):"children"===a?je(n,l):Z(n,a,l,t)}switch(e){case"input":Ee(n,r);break;case"textarea":Re(n,r);break;case"select":t=n._wrapperState.wasMultiple,n._wrapperState.wasMultiple=!!r.multiple,null!=(e=r.value)?Ne(n,!!r.multiple,e,!1):t!==!!r.multiple&&(null!=r.defaultValue?Ne(n,!!r.multiple,r.defaultValue,!0):Ne(n,!!r.multiple,r.multiple?[]:"",!1))}}}return;case 6:if(null===t.stateNode)throw Error(u(162));return void(t.stateNode.nodeValue=t.memoizedProps);case 3:return void((t=t.stateNode).hydrate&&(t.hydrate=!1,Lt(t.containerInfo)));case 12:return;case 13:if(n=t,null===t.memoizedState?r=!1:(r=!0,n=t.child,Oa=Ii()),null!==n)e:for(e=n;;){if(5===e.tag)o=e.stateNode,r?"function"===typeof(o=o.style).setProperty?o.setProperty("display","none","important"):o.display="none":(o=e.stateNode,i=void 0!==(i=e.memoizedProps.style)&&null!==i&&i.hasOwnProperty("display")?i.display:null,o.style.display=tn("display",i));else if(6===e.tag)e.stateNode.nodeValue=r?"":e.memoizedProps;else{if(13===e.tag&&null!==e.memoizedState&&null===e.memoizedState.dehydrated){(o=e.child.sibling).return=e,e=o;continue}if(null!==e.child){e.child.return=e,e=e.child;continue}}if(e===n)break;for(;null===e.sibling;){if(null===e.return||e.return===n)break e;e=e.return}e.sibling.return=e.return,e=e.sibling}return void da(t);case 19:return void da(t);case 17:return}throw Error(u(163))}function da(e){var t=e.updateQueue;if(null!==t){e.updateQueue=null;var n=e.stateNode;null===n&&(n=e.stateNode=new Gu),t.forEach((function(t){var r=wl.bind(null,e,t);n.has(t)||(n.add(t),t.then(r,r))}))}}var pa="function"===typeof WeakMap?WeakMap:Map;function ha(e,t,n){(n=ao(n,null)).tag=3,n.payload={element:null};var r=t.value;return n.callback=function(){La||(La=!0,Ua=r),ea(e,t)},n}function ma(e,t,n){(n=ao(n,null)).tag=3;var r=e.type.getDerivedStateFromError;if("function"===typeof r){var i=t.value;n.payload=function(){return ea(e,t),r(i)}}var o=e.stateNode;return null!==o&&"function"===typeof o.componentDidCatch&&(n.callback=function(){"function"!==typeof r&&(null===Fa?Fa=new Set([this]):Fa.add(this),ea(e,t));var n=t.stack;this.componentDidCatch(t.value,{componentStack:null!==n?n:""})}),n}var ga,va=Math.ceil,ya=K.ReactCurrentDispatcher,ba=K.ReactCurrentOwner,wa=0,xa=3,_a=4,ka=0,Ta=null,Ea=null,Ca=0,Sa=wa,Ma=null,Na=1073741823,Pa=1073741823,Aa=null,Ra=0,Da=!1,Oa=0,za=null,La=!1,Ua=null,Fa=null,Ia=!1,ja=null,Ha=90,Va=null,$a=0,Wa=null,Qa=0;function qa(){return 0!==(48&ka)?1073741821-(Ii()/10|0):0!==Qa?Qa:Qa=1073741821-(Ii()/10|0)}function Ba(e,t,n){if(0===(2&(t=t.mode)))return 1073741823;var r=ji();if(0===(4&t))return 99===r?1073741823:1073741822;if(0!==(16&ka))return Ca;if(null!==n)e=Bi(e,0|n.timeoutMs||5e3,250);else switch(r){case 99:e=1073741823;break;case 98:e=Bi(e,150,100);break;case 97:case 96:e=Bi(e,5e3,250);break;case 95:e=2;break;default:throw Error(u(326))}return null!==Ta&&e===Ca&&--e,e}function Ya(e,t){if(50<$a)throw $a=0,Wa=null,Error(u(185));if(null!==(e=Xa(e,t))){var n=ji();1073741823===t?0!==(8&ka)&&0===(48&ka)?Ga(e):(Za(e),0===ka&&Qi()):Za(e),0===(4&ka)||98!==n&&99!==n||(null===Va?Va=new Map([[e,t]]):(void 0===(n=Va.get(e))||n>t)&&Va.set(e,t))}}function Xa(e,t){e.expirationTime<t&&(e.expirationTime=t);var n=e.alternate;null!==n&&n.expirationTime<t&&(n.expirationTime=t);var r=e.return,i=null;if(null===r&&3===e.tag)i=e.stateNode;else for(;null!==r;){if(n=r.alternate,r.childExpirationTime<t&&(r.childExpirationTime=t),null!==n&&n.childExpirationTime<t&&(n.childExpirationTime=t),null===r.return&&3===r.tag){i=r.stateNode;break}r=r.return}return null!==i&&(Ta===i&&(ul(t),Sa===_a&&Dl(i,Ca)),Ol(i,t)),i}function Ka(e){var t=e.lastExpiredTime;if(0!==t)return t;if(!Rl(e,t=e.firstPendingTime))return t;var n=e.lastPingedTime;return 2>=(e=n>(e=e.nextKnownPendingLevel)?n:e)&&t!==e?0:e}function Za(e){if(0!==e.lastExpiredTime)e.callbackExpirationTime=1073741823,e.callbackPriority=99,e.callbackNode=Wi(Ga.bind(null,e));else{var t=Ka(e),n=e.callbackNode;if(0===t)null!==n&&(e.callbackNode=null,e.callbackExpirationTime=0,e.callbackPriority=90);else{var r=qa();if(1073741823===t?r=99:1===t||2===t?r=95:r=0>=(r=10*(1073741821-t)-10*(1073741821-r))?99:250>=r?98:5250>=r?97:95,null!==n){var i=e.callbackPriority;if(e.callbackExpirationTime===t&&i>=r)return;n!==Ri&&ki(n)}e.callbackExpirationTime=t,e.callbackPriority=r,t=1073741823===t?Wi(Ga.bind(null,e)):$i(r,Ja.bind(null,e),{timeout:10*(1073741821-t)-Ii()}),e.callbackNode=t}}}function Ja(e,t){if(Qa=0,t)return zl(e,t=qa()),Za(e),null;var n=Ka(e);if(0!==n){if(t=e.callbackNode,0!==(48&ka))throw Error(u(327));if(ml(),e===Ta&&n===Ca||nl(e,n),null!==Ea){var r=ka;ka|=16;for(var i=il();;)try{ll();break}catch(l){rl(e,l)}if(Gi(),ka=r,ya.current=i,1===Sa)throw t=Ma,nl(e,n),Dl(e,n),Za(e),t;if(null===Ea)switch(i=e.finishedWork=e.current.alternate,e.finishedExpirationTime=n,r=Sa,Ta=null,r){case wa:case 1:throw Error(u(345));case 2:zl(e,2<n?2:n);break;case xa:if(Dl(e,n),n===(r=e.lastSuspendedTime)&&(e.nextKnownPendingLevel=fl(i)),1073741823===Na&&10<(i=Oa+500-Ii())){if(Da){var o=e.lastPingedTime;if(0===o||o>=n){e.lastPingedTime=n,nl(e,n);break}}if(0!==(o=Ka(e))&&o!==n)break;if(0!==r&&r!==n){e.lastPingedTime=r;break}e.timeoutHandle=bn(dl.bind(null,e),i);break}dl(e);break;case _a:if(Dl(e,n),n===(r=e.lastSuspendedTime)&&(e.nextKnownPendingLevel=fl(i)),Da&&(0===(i=e.lastPingedTime)||i>=n)){e.lastPingedTime=n,nl(e,n);break}if(0!==(i=Ka(e))&&i!==n)break;if(0!==r&&r!==n){e.lastPingedTime=r;break}if(1073741823!==Pa?r=10*(1073741821-Pa)-Ii():1073741823===Na?r=0:(r=10*(1073741821-Na)-5e3,0>(r=(i=Ii())-r)&&(r=0),(n=10*(1073741821-n)-i)<(r=(120>r?120:480>r?480:1080>r?1080:1920>r?1920:3e3>r?3e3:4320>r?4320:1960*va(r/1960))-r)&&(r=n)),10<r){e.timeoutHandle=bn(dl.bind(null,e),r);break}dl(e);break;case 5:if(1073741823!==Na&&null!==Aa){o=Na;var a=Aa;if(0>=(r=0|a.busyMinDurationMs)?r=0:(i=0|a.busyDelayMs,r=(o=Ii()-(10*(1073741821-o)-(0|a.timeoutMs||5e3)))<=i?0:i+r-o),10<r){Dl(e,n),e.timeoutHandle=bn(dl.bind(null,e),r);break}}dl(e);break;default:throw Error(u(329))}if(Za(e),e.callbackNode===t)return Ja.bind(null,e)}}return null}function Ga(e){var t=e.lastExpiredTime;if(t=0!==t?t:1073741823,0!==(48&ka))throw Error(u(327));if(ml(),e===Ta&&t===Ca||nl(e,t),null!==Ea){var n=ka;ka|=16;for(var r=il();;)try{al();break}catch(i){rl(e,i)}if(Gi(),ka=n,ya.current=r,1===Sa)throw n=Ma,nl(e,t),Dl(e,t),Za(e),n;if(null!==Ea)throw Error(u(261));e.finishedWork=e.current.alternate,e.finishedExpirationTime=t,Ta=null,dl(e),Za(e)}return null}function el(e,t){var n=ka;ka|=1;try{return e(t)}finally{0===(ka=n)&&Qi()}}function tl(e,t){var n=ka;ka&=-2,ka|=8;try{return e(t)}finally{0===(ka=n)&&Qi()}}function nl(e,t){e.finishedWork=null,e.finishedExpirationTime=0;var n=e.timeoutHandle;if(-1!==n&&(e.timeoutHandle=-1,wn(n)),null!==Ea)for(n=Ea.return;null!==n;){var r=n;switch(r.tag){case 1:null!==(r=r.type.childContextTypes)&&void 0!==r&&gi();break;case 3:Do(),li(di),li(fi);break;case 5:zo(r);break;case 4:Do();break;case 13:case 19:li(Lo);break;case 10:eo(r)}n=n.return}Ta=e,Ea=Cl(e.current,null),Ca=t,Sa=wa,Ma=null,Pa=Na=1073741823,Aa=null,Ra=0,Da=!1}function rl(e,t){for(;;){try{if(Gi(),Io.current=gu,Qo)for(var n=Vo.memoizedState;null!==n;){var r=n.queue;null!==r&&(r.pending=null),n=n.next}if(Ho=0,Wo=$o=Vo=null,Qo=!1,null===Ea||null===Ea.return)return Sa=1,Ma=t,Ea=null;e:{var i=e,o=Ea.return,u=Ea,a=t;if(t=Ca,u.effectTag|=2048,u.firstEffect=u.lastEffect=null,null!==a&&"object"===typeof a&&"function"===typeof a.then){var l=a;if(0===(2&u.mode)){var c=u.alternate;c?(u.updateQueue=c.updateQueue,u.memoizedState=c.memoizedState,u.expirationTime=c.expirationTime):(u.updateQueue=null,u.memoizedState=null)}var s=0!==(1&Lo.current),f=o;do{var d;if(d=13===f.tag){var p=f.memoizedState;if(null!==p)d=null!==p.dehydrated;else{var h=f.memoizedProps;d=void 0!==h.fallback&&(!0!==h.unstable_avoidThisFallback||!s)}}if(d){var m=f.updateQueue;if(null===m){var g=new Set;g.add(l),f.updateQueue=g}else m.add(l);if(0===(2&f.mode)){if(f.effectTag|=64,u.effectTag&=-2981,1===u.tag)if(null===u.alternate)u.tag=17;else{var v=ao(1073741823,null);v.tag=2,lo(u,v)}u.expirationTime=1073741823;break e}a=void 0,u=t;var y=i.pingCache;if(null===y?(y=i.pingCache=new pa,a=new Set,y.set(l,a)):void 0===(a=y.get(l))&&(a=new Set,y.set(l,a)),!a.has(u)){a.add(u);var b=bl.bind(null,i,l,u);l.then(b,b)}f.effectTag|=4096,f.expirationTime=t;break e}f=f.return}while(null!==f);a=Error((ge(u.type)||"A React component")+" suspended while rendering, but no fallback UI was specified.\n\nAdd a <Suspense fallback=...> component higher in the tree to provide a loading indicator or placeholder to display."+ve(u))}5!==Sa&&(Sa=2),a=Ju(a,u),f=o;do{switch(f.tag){case 3:l=a,f.effectTag|=4096,f.expirationTime=t,co(f,ha(f,l,t));break e;case 1:l=a;var w=f.type,x=f.stateNode;if(0===(64&f.effectTag)&&("function"===typeof w.getDerivedStateFromError||null!==x&&"function"===typeof x.componentDidCatch&&(null===Fa||!Fa.has(x)))){f.effectTag|=4096,f.expirationTime=t,co(f,ma(f,l,t));break e}}f=f.return}while(null!==f)}Ea=sl(Ea)}catch(_){t=_;continue}break}}function il(){var e=ya.current;return ya.current=gu,null===e?gu:e}function ol(e,t){e<Na&&2<e&&(Na=e),null!==t&&e<Pa&&2<e&&(Pa=e,Aa=t)}function ul(e){e>Ra&&(Ra=e)}function al(){for(;null!==Ea;)Ea=cl(Ea)}function ll(){for(;null!==Ea&&!Di();)Ea=cl(Ea)}function cl(e){var t=ga(e.alternate,e,Ca);return e.memoizedProps=e.pendingProps,null===t&&(t=sl(e)),ba.current=null,t}function sl(e){Ea=e;do{var t=Ea.alternate;if(e=Ea.return,0===(2048&Ea.effectTag)){if(t=Ku(t,Ea,Ca),1===Ca||1!==Ea.childExpirationTime){for(var n=0,r=Ea.child;null!==r;){var i=r.expirationTime,o=r.childExpirationTime;i>n&&(n=i),o>n&&(n=o),r=r.sibling}Ea.childExpirationTime=n}if(null!==t)return t;null!==e&&0===(2048&e.effectTag)&&(null===e.firstEffect&&(e.firstEffect=Ea.firstEffect),null!==Ea.lastEffect&&(null!==e.lastEffect&&(e.lastEffect.nextEffect=Ea.firstEffect),e.lastEffect=Ea.lastEffect),1<Ea.effectTag&&(null!==e.lastEffect?e.lastEffect.nextEffect=Ea:e.firstEffect=Ea,e.lastEffect=Ea))}else{if(null!==(t=Zu(Ea)))return t.effectTag&=2047,t;null!==e&&(e.firstEffect=e.lastEffect=null,e.effectTag|=2048)}if(null!==(t=Ea.sibling))return t;Ea=e}while(null!==Ea);return Sa===wa&&(Sa=5),null}function fl(e){var t=e.expirationTime;return t>(e=e.childExpirationTime)?t:e}function dl(e){var t=ji();return Vi(99,pl.bind(null,e,t)),null}function pl(e,t){do{ml()}while(null!==ja);if(0!==(48&ka))throw Error(u(327));var n=e.finishedWork,r=e.finishedExpirationTime;if(null===n)return null;if(e.finishedWork=null,e.finishedExpirationTime=0,n===e.current)throw Error(u(177));e.callbackNode=null,e.callbackExpirationTime=0,e.callbackPriority=90,e.nextKnownPendingLevel=0;var i=fl(n);if(e.firstPendingTime=i,r<=e.lastSuspendedTime?e.firstSuspendedTime=e.lastSuspendedTime=e.nextKnownPendingLevel=0:r<=e.firstSuspendedTime&&(e.firstSuspendedTime=r-1),r<=e.lastPingedTime&&(e.lastPingedTime=0),r<=e.lastExpiredTime&&(e.lastExpiredTime=0),e===Ta&&(Ea=Ta=null,Ca=0),1<n.effectTag?null!==n.lastEffect?(n.lastEffect.nextEffect=n,i=n.firstEffect):i=n:i=n.firstEffect,null!==i){var o=ka;ka|=32,ba.current=null,mn=qt;var a=pn();if(hn(a)){if("selectionStart"in a)var l={start:a.selectionStart,end:a.selectionEnd};else e:{var c=(l=(l=a.ownerDocument)&&l.defaultView||window).getSelection&&l.getSelection();if(c&&0!==c.rangeCount){l=c.anchorNode;var s=c.anchorOffset,f=c.focusNode;c=c.focusOffset;try{l.nodeType,f.nodeType}catch(C){l=null;break e}var d=0,p=-1,h=-1,m=0,g=0,v=a,y=null;t:for(;;){for(var b;v!==l||0!==s&&3!==v.nodeType||(p=d+s),v!==f||0!==c&&3!==v.nodeType||(h=d+c),3===v.nodeType&&(d+=v.nodeValue.length),null!==(b=v.firstChild);)y=v,v=b;for(;;){if(v===a)break t;if(y===l&&++m===s&&(p=d),y===f&&++g===c&&(h=d),null!==(b=v.nextSibling))break;y=(v=y).parentNode}v=b}l=-1===p||-1===h?null:{start:p,end:h}}else l=null}l=l||{start:0,end:0}}else l=null;gn={activeElementDetached:null,focusedElem:a,selectionRange:l},qt=!1,za=i;do{try{hl()}catch(C){if(null===za)throw Error(u(330));yl(za,C),za=za.nextEffect}}while(null!==za);za=i;do{try{for(a=e,l=t;null!==za;){var w=za.effectTag;if(16&w&&je(za.stateNode,""),128&w){var x=za.alternate;if(null!==x){var _=x.ref;null!==_&&("function"===typeof _?_(null):_.current=null)}}switch(1038&w){case 2:ca(za),za.effectTag&=-3;break;case 6:ca(za),za.effectTag&=-3,fa(za.alternate,za);break;case 1024:za.effectTag&=-1025;break;case 1028:za.effectTag&=-1025,fa(za.alternate,za);break;case 4:fa(za.alternate,za);break;case 8:sa(a,s=za,l),aa(s)}za=za.nextEffect}}catch(C){if(null===za)throw Error(u(330));yl(za,C),za=za.nextEffect}}while(null!==za);if(_=gn,x=pn(),w=_.focusedElem,l=_.selectionRange,x!==w&&w&&w.ownerDocument&&function e(t,n){return!(!t||!n)&&(t===n||(!t||3!==t.nodeType)&&(n&&3===n.nodeType?e(t,n.parentNode):"contains"in t?t.contains(n):!!t.compareDocumentPosition&&!!(16&t.compareDocumentPosition(n))))}(w.ownerDocument.documentElement,w)){null!==l&&hn(w)&&(x=l.start,void 0===(_=l.end)&&(_=x),"selectionStart"in w?(w.selectionStart=x,w.selectionEnd=Math.min(_,w.value.length)):(_=(x=w.ownerDocument||document)&&x.defaultView||window).getSelection&&(_=_.getSelection(),s=w.textContent.length,a=Math.min(l.start,s),l=void 0===l.end?a:Math.min(l.end,s),!_.extend&&a>l&&(s=l,l=a,a=s),s=dn(w,a),f=dn(w,l),s&&f&&(1!==_.rangeCount||_.anchorNode!==s.node||_.anchorOffset!==s.offset||_.focusNode!==f.node||_.focusOffset!==f.offset)&&((x=x.createRange()).setStart(s.node,s.offset),_.removeAllRanges(),a>l?(_.addRange(x),_.extend(f.node,f.offset)):(x.setEnd(f.node,f.offset),_.addRange(x))))),x=[];for(_=w;_=_.parentNode;)1===_.nodeType&&x.push({element:_,left:_.scrollLeft,top:_.scrollTop});for("function"===typeof w.focus&&w.focus(),w=0;w<x.length;w++)(_=x[w]).element.scrollLeft=_.left,_.element.scrollTop=_.top}qt=!!mn,gn=mn=null,e.current=n,za=i;do{try{for(w=e;null!==za;){var k=za.effectTag;if(36&k&&oa(w,za.alternate,za),128&k){x=void 0;var T=za.ref;if(null!==T){var E=za.stateNode;switch(za.tag){case 5:x=E;break;default:x=E}"function"===typeof T?T(x):T.current=x}}za=za.nextEffect}}catch(C){if(null===za)throw Error(u(330));yl(za,C),za=za.nextEffect}}while(null!==za);za=null,Oi(),ka=o}else e.current=n;if(Ia)Ia=!1,ja=e,Ha=t;else for(za=i;null!==za;)t=za.nextEffect,za.nextEffect=null,za=t;if(0===(t=e.firstPendingTime)&&(Fa=null),1073741823===t?e===Wa?$a++:($a=0,Wa=e):$a=0,"function"===typeof xl&&xl(n.stateNode,r),Za(e),La)throw La=!1,e=Ua,Ua=null,e;return 0!==(8&ka)||Qi(),null}function hl(){for(;null!==za;){var e=za.effectTag;0!==(256&e)&&na(za.alternate,za),0===(512&e)||Ia||(Ia=!0,$i(97,(function(){return ml(),null}))),za=za.nextEffect}}function ml(){if(90!==Ha){var e=97<Ha?97:Ha;return Ha=90,Vi(e,gl)}}function gl(){if(null===ja)return!1;var e=ja;if(ja=null,0!==(48&ka))throw Error(u(331));var t=ka;for(ka|=32,e=e.current.firstEffect;null!==e;){try{var n=e;if(0!==(512&n.effectTag))switch(n.tag){case 0:case 11:case 15:case 22:ra(5,n),ia(5,n)}}catch(r){if(null===e)throw Error(u(330));yl(e,r)}n=e.nextEffect,e.nextEffect=null,e=n}return ka=t,Qi(),!0}function vl(e,t,n){lo(e,t=ha(e,t=Ju(n,t),1073741823)),null!==(e=Xa(e,1073741823))&&Za(e)}function yl(e,t){if(3===e.tag)vl(e,e,t);else for(var n=e.return;null!==n;){if(3===n.tag){vl(n,e,t);break}if(1===n.tag){var r=n.stateNode;if("function"===typeof n.type.getDerivedStateFromError||"function"===typeof r.componentDidCatch&&(null===Fa||!Fa.has(r))){lo(n,e=ma(n,e=Ju(t,e),1073741823)),null!==(n=Xa(n,1073741823))&&Za(n);break}}n=n.return}}function bl(e,t,n){var r=e.pingCache;null!==r&&r.delete(t),Ta===e&&Ca===n?Sa===_a||Sa===xa&&1073741823===Na&&Ii()-Oa<500?nl(e,Ca):Da=!0:Rl(e,n)&&(0!==(t=e.lastPingedTime)&&t<n||(e.lastPingedTime=n,Za(e)))}function wl(e,t){var n=e.stateNode;null!==n&&n.delete(t),0===(t=0)&&(t=Ba(t=qa(),e,null)),null!==(e=Xa(e,t))&&Za(e)}ga=function(e,t,n){var r=t.expirationTime;if(null!==e){var i=t.pendingProps;if(e.memoizedProps!==i||di.current)Pu=!0;else{if(r<n){switch(Pu=!1,t.tag){case 3:Iu(t),Mu();break;case 5:if(Oo(t),4&t.mode&&1!==n&&i.hidden)return t.expirationTime=t.childExpirationTime=1,null;break;case 1:mi(t.type)&&bi(t);break;case 4:Ro(t,t.stateNode.containerInfo);break;case 10:r=t.memoizedProps.value,i=t.type._context,ci(Xi,i._currentValue),i._currentValue=r;break;case 13:if(null!==t.memoizedState)return 0!==(r=t.child.childExpirationTime)&&r>=n?Wu(e,t,n):(ci(Lo,1&Lo.current),null!==(t=Yu(e,t,n))?t.sibling:null);ci(Lo,1&Lo.current);break;case 19:if(r=t.childExpirationTime>=n,0!==(64&e.effectTag)){if(r)return Bu(e,t,n);t.effectTag|=64}if(null!==(i=t.memoizedState)&&(i.rendering=null,i.tail=null),ci(Lo,Lo.current),!r)return null}return Yu(e,t,n)}Pu=!1}}else Pu=!1;switch(t.expirationTime=0,t.tag){case 2:if(r=t.type,null!==e&&(e.alternate=null,t.alternate=null,t.effectTag|=2),e=t.pendingProps,i=hi(t,fi.current),no(t,n),i=Yo(null,t,r,e,i,n),t.effectTag|=1,"object"===typeof i&&null!==i&&"function"===typeof i.render&&void 0===i.$$typeof){if(t.tag=1,t.memoizedState=null,t.updateQueue=null,mi(r)){var o=!0;bi(t)}else o=!1;t.memoizedState=null!==i.state&&void 0!==i.state?i.state:null,oo(t);var a=r.getDerivedStateFromProps;"function"===typeof a&&mo(t,r,a,e),i.updater=go,t.stateNode=i,i._reactInternalFiber=t,wo(t,r,e,n),t=Fu(null,t,r,!0,o,n)}else t.tag=0,Au(null,t,i,n),t=t.child;return t;case 16:e:{if(i=t.elementType,null!==e&&(e.alternate=null,t.alternate=null,t.effectTag|=2),e=t.pendingProps,function(e){if(-1===e._status){e._status=0;var t=e._ctor;t=t(),e._result=t,t.then((function(t){0===e._status&&(t=t.default,e._status=1,e._result=t)}),(function(t){0===e._status&&(e._status=2,e._result=t)}))}}(i),1!==i._status)throw i._result;switch(i=i._result,t.type=i,o=t.tag=function(e){if("function"===typeof e)return El(e)?1:0;if(void 0!==e&&null!==e){if((e=e.$$typeof)===le)return 11;if(e===fe)return 14}return 2}(i),e=Yi(i,e),o){case 0:t=Lu(null,t,i,e,n);break e;case 1:t=Uu(null,t,i,e,n);break e;case 11:t=Ru(null,t,i,e,n);break e;case 14:t=Du(null,t,i,Yi(i.type,e),r,n);break e}throw Error(u(306,i,""))}return t;case 0:return r=t.type,i=t.pendingProps,Lu(e,t,r,i=t.elementType===r?i:Yi(r,i),n);case 1:return r=t.type,i=t.pendingProps,Uu(e,t,r,i=t.elementType===r?i:Yi(r,i),n);case 3:if(Iu(t),r=t.updateQueue,null===e||null===r)throw Error(u(282));if(r=t.pendingProps,i=null!==(i=t.memoizedState)?i.element:null,uo(e,t),so(t,r,null,n),(r=t.memoizedState.element)===i)Mu(),t=Yu(e,t,n);else{if((i=t.stateNode.hydrate)&&(xu=xn(t.stateNode.containerInfo.firstChild),wu=t,i=_u=!0),i)for(n=Co(t,null,r,n),t.child=n;n;)n.effectTag=-3&n.effectTag|1024,n=n.sibling;else Au(e,t,r,n),Mu();t=t.child}return t;case 5:return Oo(t),null===e&&Eu(t),r=t.type,i=t.pendingProps,o=null!==e?e.memoizedProps:null,a=i.children,yn(r,i)?a=null:null!==o&&yn(r,o)&&(t.effectTag|=16),zu(e,t),4&t.mode&&1!==n&&i.hidden?(t.expirationTime=t.childExpirationTime=1,t=null):(Au(e,t,a,n),t=t.child),t;case 6:return null===e&&Eu(t),null;case 13:return Wu(e,t,n);case 4:return Ro(t,t.stateNode.containerInfo),r=t.pendingProps,null===e?t.child=Eo(t,null,r,n):Au(e,t,r,n),t.child;case 11:return r=t.type,i=t.pendingProps,Ru(e,t,r,i=t.elementType===r?i:Yi(r,i),n);case 7:return Au(e,t,t.pendingProps,n),t.child;case 8:case 12:return Au(e,t,t.pendingProps.children,n),t.child;case 10:e:{r=t.type._context,i=t.pendingProps,a=t.memoizedProps,o=i.value;var l=t.type._context;if(ci(Xi,l._currentValue),l._currentValue=o,null!==a)if(l=a.value,0===(o=Ur(l,o)?0:0|("function"===typeof r._calculateChangedBits?r._calculateChangedBits(l,o):1073741823))){if(a.children===i.children&&!di.current){t=Yu(e,t,n);break e}}else for(null!==(l=t.child)&&(l.return=t);null!==l;){var c=l.dependencies;if(null!==c){a=l.child;for(var s=c.firstContext;null!==s;){if(s.context===r&&0!==(s.observedBits&o)){1===l.tag&&((s=ao(n,null)).tag=2,lo(l,s)),l.expirationTime<n&&(l.expirationTime=n),null!==(s=l.alternate)&&s.expirationTime<n&&(s.expirationTime=n),to(l.return,n),c.expirationTime<n&&(c.expirationTime=n);break}s=s.next}}else a=10===l.tag&&l.type===t.type?null:l.child;if(null!==a)a.return=l;else for(a=l;null!==a;){if(a===t){a=null;break}if(null!==(l=a.sibling)){l.return=a.return,a=l;break}a=a.return}l=a}Au(e,t,i.children,n),t=t.child}return t;case 9:return i=t.type,r=(o=t.pendingProps).children,no(t,n),r=r(i=ro(i,o.unstable_observedBits)),t.effectTag|=1,Au(e,t,r,n),t.child;case 14:return o=Yi(i=t.type,t.pendingProps),Du(e,t,i,o=Yi(i.type,o),r,n);case 15:return Ou(e,t,t.type,t.pendingProps,r,n);case 17:return r=t.type,i=t.pendingProps,i=t.elementType===r?i:Yi(r,i),null!==e&&(e.alternate=null,t.alternate=null,t.effectTag|=2),t.tag=1,mi(r)?(e=!0,bi(t)):e=!1,no(t,n),yo(t,r,i),wo(t,r,i,n),Fu(null,t,r,!0,e,n);case 19:return Bu(e,t,n)}throw Error(u(156,t.tag))};var xl=null,_l=null;function kl(e,t,n,r){this.tag=e,this.key=n,this.sibling=this.child=this.return=this.stateNode=this.type=this.elementType=null,this.index=0,this.ref=null,this.pendingProps=t,this.dependencies=this.memoizedState=this.updateQueue=this.memoizedProps=null,this.mode=r,this.effectTag=0,this.lastEffect=this.firstEffect=this.nextEffect=null,this.childExpirationTime=this.expirationTime=0,this.alternate=null}function Tl(e,t,n,r){return new kl(e,t,n,r)}function El(e){return!(!(e=e.prototype)||!e.isReactComponent)}function Cl(e,t){var n=e.alternate;return null===n?((n=Tl(e.tag,t,e.key,e.mode)).elementType=e.elementType,n.type=e.type,n.stateNode=e.stateNode,n.alternate=e,e.alternate=n):(n.pendingProps=t,n.effectTag=0,n.nextEffect=null,n.firstEffect=null,n.lastEffect=null),n.childExpirationTime=e.childExpirationTime,n.expirationTime=e.expirationTime,n.child=e.child,n.memoizedProps=e.memoizedProps,n.memoizedState=e.memoizedState,n.updateQueue=e.updateQueue,t=e.dependencies,n.dependencies=null===t?null:{expirationTime:t.expirationTime,firstContext:t.firstContext,responders:t.responders},n.sibling=e.sibling,n.index=e.index,n.ref=e.ref,n}function Sl(e,t,n,r,i,o){var a=2;if(r=e,"function"===typeof e)El(e)&&(a=1);else if("string"===typeof e)a=5;else e:switch(e){case ne:return Ml(n.children,i,o,t);case ae:a=8,i|=7;break;case re:a=8,i|=1;break;case ie:return(e=Tl(12,n,t,8|i)).elementType=ie,e.type=ie,e.expirationTime=o,e;case ce:return(e=Tl(13,n,t,i)).type=ce,e.elementType=ce,e.expirationTime=o,e;case se:return(e=Tl(19,n,t,i)).elementType=se,e.expirationTime=o,e;default:if("object"===typeof e&&null!==e)switch(e.$$typeof){case oe:a=10;break e;case ue:a=9;break e;case le:a=11;break e;case fe:a=14;break e;case de:a=16,r=null;break e;case pe:a=22;break e}throw Error(u(130,null==e?e:typeof e,""))}return(t=Tl(a,n,t,i)).elementType=e,t.type=r,t.expirationTime=o,t}function Ml(e,t,n,r){return(e=Tl(7,e,r,t)).expirationTime=n,e}function Nl(e,t,n){return(e=Tl(6,e,null,t)).expirationTime=n,e}function Pl(e,t,n){return(t=Tl(4,null!==e.children?e.children:[],e.key,t)).expirationTime=n,t.stateNode={containerInfo:e.containerInfo,pendingChildren:null,implementation:e.implementation},t}function Al(e,t,n){this.tag=t,this.current=null,this.containerInfo=e,this.pingCache=this.pendingChildren=null,this.finishedExpirationTime=0,this.finishedWork=null,this.timeoutHandle=-1,this.pendingContext=this.context=null,this.hydrate=n,this.callbackNode=null,this.callbackPriority=90,this.lastExpiredTime=this.lastPingedTime=this.nextKnownPendingLevel=this.lastSuspendedTime=this.firstSuspendedTime=this.firstPendingTime=0}function Rl(e,t){var n=e.firstSuspendedTime;return e=e.lastSuspendedTime,0!==n&&n>=t&&e<=t}function Dl(e,t){var n=e.firstSuspendedTime,r=e.lastSuspendedTime;n<t&&(e.firstSuspendedTime=t),(r>t||0===n)&&(e.lastSuspendedTime=t),t<=e.lastPingedTime&&(e.lastPingedTime=0),t<=e.lastExpiredTime&&(e.lastExpiredTime=0)}function Ol(e,t){t>e.firstPendingTime&&(e.firstPendingTime=t);var n=e.firstSuspendedTime;0!==n&&(t>=n?e.firstSuspendedTime=e.lastSuspendedTime=e.nextKnownPendingLevel=0:t>=e.lastSuspendedTime&&(e.lastSuspendedTime=t+1),t>e.nextKnownPendingLevel&&(e.nextKnownPendingLevel=t))}function zl(e,t){var n=e.lastExpiredTime;(0===n||n>t)&&(e.lastExpiredTime=t)}function Ll(e,t,n,r){var i=t.current,o=qa(),a=po.suspense;o=Ba(o,i,a);e:if(n){t:{if(Ge(n=n._reactInternalFiber)!==n||1!==n.tag)throw Error(u(170));var l=n;do{switch(l.tag){case 3:l=l.stateNode.context;break t;case 1:if(mi(l.type)){l=l.stateNode.__reactInternalMemoizedMergedChildContext;break t}}l=l.return}while(null!==l);throw Error(u(171))}if(1===n.tag){var c=n.type;if(mi(c)){n=yi(n,c,l);break e}}n=l}else n=si;return null===t.context?t.context=n:t.pendingContext=n,(t=ao(o,a)).payload={element:e},null!==(r=void 0===r?null:r)&&(t.callback=r),lo(i,t),Ya(i,o),o}function Ul(e){if(!(e=e.current).child)return null;switch(e.child.tag){case 5:default:return e.child.stateNode}}function Fl(e,t){null!==(e=e.memoizedState)&&null!==e.dehydrated&&e.retryTime<t&&(e.retryTime=t)}function Il(e,t){Fl(e,t),(e=e.alternate)&&Fl(e,t)}function jl(e,t,n){var r=new Al(e,t,n=null!=n&&!0===n.hydrate),i=Tl(3,null,null,2===t?7:1===t?3:0);r.current=i,i.stateNode=r,oo(i),e[Cn]=r.current,n&&0!==t&&function(e,t){var n=Je(t);Ct.forEach((function(e){ht(e,t,n)})),St.forEach((function(e){ht(e,t,n)}))}(0,9===e.nodeType?e:e.ownerDocument),this._internalRoot=r}function Hl(e){return!(!e||1!==e.nodeType&&9!==e.nodeType&&11!==e.nodeType&&(8!==e.nodeType||" react-mount-point-unstable "!==e.nodeValue))}function Vl(e,t,n,r,i){var o=n._reactRootContainer;if(o){var u=o._internalRoot;if("function"===typeof i){var a=i;i=function(){var e=Ul(u);a.call(e)}}Ll(t,u,e,i)}else{if(o=n._reactRootContainer=function(e,t){if(t||(t=!(!(t=e?9===e.nodeType?e.documentElement:e.firstChild:null)||1!==t.nodeType||!t.hasAttribute("data-reactroot"))),!t)for(var n;n=e.lastChild;)e.removeChild(n);return new jl(e,0,t?{hydrate:!0}:void 0)}(n,r),u=o._internalRoot,"function"===typeof i){var l=i;i=function(){var e=Ul(u);l.call(e)}}tl((function(){Ll(t,u,e,i)}))}return Ul(u)}function $l(e,t,n){var r=3<arguments.length&&void 0!==arguments[3]?arguments[3]:null;return{$$typeof:te,key:null==r?null:""+r,children:e,containerInfo:t,implementation:n}}function Wl(e,t){var n=2<arguments.length&&void 0!==arguments[2]?arguments[2]:null;if(!Hl(t))throw Error(u(200));return $l(e,t,null,n)}jl.prototype.render=function(e){Ll(e,this._internalRoot,null,null)},jl.prototype.unmount=function(){var e=this._internalRoot,t=e.containerInfo;Ll(null,e,null,(function(){t[Cn]=null}))},mt=function(e){if(13===e.tag){var t=Bi(qa(),150,100);Ya(e,t),Il(e,t)}},gt=function(e){13===e.tag&&(Ya(e,3),Il(e,3))},vt=function(e){if(13===e.tag){var t=qa();Ya(e,t=Ba(t,e,null)),Il(e,t)}},M=function(e,t,n){switch(t){case"input":if(Ee(e,n),t=n.name,"radio"===n.type&&null!=t){for(n=e;n.parentNode;)n=n.parentNode;for(n=n.querySelectorAll("input[name="+JSON.stringify(""+t)+'][type="radio"]'),t=0;t<n.length;t++){var r=n[t];if(r!==e&&r.form===e.form){var i=Pn(r);if(!i)throw Error(u(90));xe(r),Ee(r,i)}}}break;case"textarea":Re(e,n);break;case"select":null!=(t=n.value)&&Ne(e,!!n.multiple,t,!1)}},O=el,z=function(e,t,n,r,i){var o=ka;ka|=4;try{return Vi(98,e.bind(null,t,n,r,i))}finally{0===(ka=o)&&Qi()}},L=function(){0===(49&ka)&&(function(){if(null!==Va){var e=Va;Va=null,e.forEach((function(e,t){zl(t,e),Za(t)})),Qi()}}(),ml())},U=function(e,t){var n=ka;ka|=2;try{return e(t)}finally{0===(ka=n)&&Qi()}};var Ql={Events:[Mn,Nn,Pn,C,k,Un,function(e){it(e,Ln)},R,D,Zt,at,ml,{current:!1}]};!function(e){var t=e.findFiberByHostInstance;(function(e){if("undefined"===typeof __REACT_DEVTOOLS_GLOBAL_HOOK__)return!1;var t=__REACT_DEVTOOLS_GLOBAL_HOOK__;if(t.isDisabled||!t.supportsFiber)return!0;try{var n=t.inject(e);xl=function(e){try{t.onCommitFiberRoot(n,e,void 0,64===(64&e.current.effectTag))}catch(r){}},_l=function(e){try{t.onCommitFiberUnmount(n,e)}catch(r){}}}catch(r){}})(i({},e,{overrideHookState:null,overrideProps:null,setSuspenseHandler:null,scheduleUpdate:null,currentDispatcherRef:K.ReactCurrentDispatcher,findHostInstanceByFiber:function(e){return null===(e=nt(e))?null:e.stateNode},findFiberByHostInstance:function(e){return t?t(e):null},findHostInstancesForRefresh:null,scheduleRefresh:null,scheduleRoot:null,setRefreshHandler:null,getCurrentFiber:null}))}({findFiberByHostInstance:Sn,bundleType:0,version:"16.13.1",rendererPackageName:"react-dom"}),t.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED=Ql,t.createPortal=Wl,t.findDOMNode=function(e){if(null==e)return null;if(1===e.nodeType)return e;var t=e._reactInternalFiber;if(void 0===t){if("function"===typeof e.render)throw Error(u(188));throw Error(u(268,Object.keys(e)))}return e=null===(e=nt(t))?null:e.stateNode},t.flushSync=function(e,t){if(0!==(48&ka))throw Error(u(187));var n=ka;ka|=1;try{return Vi(99,e.bind(null,t))}finally{ka=n,Qi()}},t.hydrate=function(e,t,n){if(!Hl(t))throw Error(u(200));return Vl(null,e,t,!0,n)},t.render=function(e,t,n){if(!Hl(t))throw Error(u(200));return Vl(null,e,t,!1,n)},t.unmountComponentAtNode=function(e){if(!Hl(e))throw Error(u(40));return!!e._reactRootContainer&&(tl((function(){Vl(null,null,e,!1,(function(){e._reactRootContainer=null,e[Cn]=null}))})),!0)},t.unstable_batchedUpdates=el,t.unstable_createPortal=function(e,t){return Wl(e,t,2<arguments.length&&void 0!==arguments[2]?arguments[2]:null)},t.unstable_renderSubtreeIntoContainer=function(e,t,n,r){if(!Hl(n))throw Error(u(200));if(null==e||void 0===e._reactInternalFiber)throw Error(u(38));return Vl(e,t,n,!1,r)},t.version="16.13.1"},function(e,t,n){"use strict";e.exports=n(32)},function(e,t,n){"use strict";var r,i,o,u,a;if("undefined"===typeof window||"function"!==typeof MessageChannel){var l=null,c=null,s=function e(){if(null!==l)try{var n=t.unstable_now();l(!0,n),l=null}catch(r){throw setTimeout(e,0),r}},f=Date.now();t.unstable_now=function(){return Date.now()-f},r=function(e){null!==l?setTimeout(r,0,e):(l=e,setTimeout(s,0))},i=function(e,t){c=setTimeout(e,t)},o=function(){clearTimeout(c)},u=function(){return!1},a=t.unstable_forceFrameRate=function(){}}else{var d=window.performance,p=window.Date,h=window.setTimeout,m=window.clearTimeout;if("undefined"!==typeof console){var g=window.cancelAnimationFrame;"function"!==typeof window.requestAnimationFrame&&console.error("This browser doesn't support requestAnimationFrame. Make sure that you load a polyfill in older browsers. https://fb.me/react-polyfills"),"function"!==typeof g&&console.error("This browser doesn't support cancelAnimationFrame. Make sure that you load a polyfill in older browsers. https://fb.me/react-polyfills")}if("object"===typeof d&&"function"===typeof d.now)t.unstable_now=function(){return d.now()};else{var v=p.now();t.unstable_now=function(){return p.now()-v}}var y=!1,b=null,w=-1,x=5,_=0;u=function(){return t.unstable_now()>=_},a=function(){},t.unstable_forceFrameRate=function(e){0>e||125<e?console.error("forceFrameRate takes a positive int between 0 and 125, forcing framerates higher than 125 fps is not unsupported"):x=0<e?Math.floor(1e3/e):5};var k=new MessageChannel,T=k.port2;k.port1.onmessage=function(){if(null!==b){var e=t.unstable_now();_=e+x;try{b(!0,e)?T.postMessage(null):(y=!1,b=null)}catch(n){throw T.postMessage(null),n}}else y=!1},r=function(e){b=e,y||(y=!0,T.postMessage(null))},i=function(e,n){w=h((function(){e(t.unstable_now())}),n)},o=function(){m(w),w=-1}}function E(e,t){var n=e.length;e.push(t);e:for(;;){var r=n-1>>>1,i=e[r];if(!(void 0!==i&&0<M(i,t)))break e;e[r]=t,e[n]=i,n=r}}function C(e){return void 0===(e=e[0])?null:e}function S(e){var t=e[0];if(void 0!==t){var n=e.pop();if(n!==t){e[0]=n;e:for(var r=0,i=e.length;r<i;){var o=2*(r+1)-1,u=e[o],a=o+1,l=e[a];if(void 0!==u&&0>M(u,n))void 0!==l&&0>M(l,u)?(e[r]=l,e[a]=n,r=a):(e[r]=u,e[o]=n,r=o);else{if(!(void 0!==l&&0>M(l,n)))break e;e[r]=l,e[a]=n,r=a}}}return t}return null}function M(e,t){var n=e.sortIndex-t.sortIndex;return 0!==n?n:e.id-t.id}var N=[],P=[],A=1,R=null,D=3,O=!1,z=!1,L=!1;function U(e){for(var t=C(P);null!==t;){if(null===t.callback)S(P);else{if(!(t.startTime<=e))break;S(P),t.sortIndex=t.expirationTime,E(N,t)}t=C(P)}}function F(e){if(L=!1,U(e),!z)if(null!==C(N))z=!0,r(I);else{var t=C(P);null!==t&&i(F,t.startTime-e)}}function I(e,n){z=!1,L&&(L=!1,o()),O=!0;var r=D;try{for(U(n),R=C(N);null!==R&&(!(R.expirationTime>n)||e&&!u());){var a=R.callback;if(null!==a){R.callback=null,D=R.priorityLevel;var l=a(R.expirationTime<=n);n=t.unstable_now(),"function"===typeof l?R.callback=l:R===C(N)&&S(N),U(n)}else S(N);R=C(N)}if(null!==R)var c=!0;else{var s=C(P);null!==s&&i(F,s.startTime-n),c=!1}return c}finally{R=null,D=r,O=!1}}function j(e){switch(e){case 1:return-1;case 2:return 250;case 5:return 1073741823;case 4:return 1e4;default:return 5e3}}var H=a;t.unstable_IdlePriority=5,t.unstable_ImmediatePriority=1,t.unstable_LowPriority=4,t.unstable_NormalPriority=3,t.unstable_Profiling=null,t.unstable_UserBlockingPriority=2,t.unstable_cancelCallback=function(e){e.callback=null},t.unstable_continueExecution=function(){z||O||(z=!0,r(I))},t.unstable_getCurrentPriorityLevel=function(){return D},t.unstable_getFirstCallbackNode=function(){return C(N)},t.unstable_next=function(e){switch(D){case 1:case 2:case 3:var t=3;break;default:t=D}var n=D;D=t;try{return e()}finally{D=n}},t.unstable_pauseExecution=function(){},t.unstable_requestPaint=H,t.unstable_runWithPriority=function(e,t){switch(e){case 1:case 2:case 3:case 4:case 5:break;default:e=3}var n=D;D=e;try{return t()}finally{D=n}},t.unstable_scheduleCallback=function(e,n,u){var a=t.unstable_now();if("object"===typeof u&&null!==u){var l=u.delay;l="number"===typeof l&&0<l?a+l:a,u="number"===typeof u.timeout?u.timeout:j(e)}else u=j(e),l=a;return e={id:A++,callback:n,priorityLevel:e,startTime:l,expirationTime:u=l+u,sortIndex:-1},l>a?(e.sortIndex=l,E(P,e),null===C(N)&&e===C(P)&&(L?o():L=!0,i(F,l-a))):(e.sortIndex=u,E(N,e),z||O||(z=!0,r(I))),e},t.unstable_shouldYield=function(){var e=t.unstable_now();U(e);var n=C(N);return n!==R&&null!==R&&null!==n&&null!==n.callback&&n.startTime<=e&&n.expirationTime<R.expirationTime||u()},t.unstable_wrapCallback=function(e){var t=D;return function(){var n=D;D=t;try{return e.apply(this,arguments)}finally{D=n}}}}]]);

(this["webpackJsonpvisualisation-d3"]=this["webpackJsonpvisualisation-d3"]||[]).push([[0],{18:function(e,t,a){},28:function(e,t,a){e.exports=a(33)},33:function(e,t,a){"use strict";a.r(t);var r=a(0),n=a.n(r),i=a(8),c=a(2),l=(a(18),a(9)),o=a(10),s=a(11),y=a(13),m=a(12),p=function(e){Object(y.a)(a,e);var t=Object(m.a)(a);function a(){return Object(l.a)(this,a),t.apply(this,arguments)}return Object(o.a)(a,[{key:"render",value:function(){var e="0 0 ".concat(500," ").concat(900),t={paddingBottom:"180%"},a={stroke:"rgba(70, 130, 180,.3)",strokeWidth:1},r={stroke:"rgba(70, 130, 180,.7)",strokeWidth:1},i={stroke:"rgba(70, 130, 180,.7)",strokeWidth:2},l=this.props.data_terms,o=(this.props.itemsDisplay,this.props.data_weight),s=this.props.cat,y=l.length,m=75+27*y,p=230/c.b(l,(function(e){return+Math.round(e.Total)})),u=c.e().domain([c.c(o,(function(e){return+e.proportion})),c.b(o,(function(e){return+e.proportion}))]).range([5,17]);return n.a.createElement(n.a.Fragment,null,n.a.createElement("div",{className:"svg-container",style:t},n.a.createElement("svg",{preserveAspectRatio:"xMidYMidmeet",viewBox:e,className:"svg-content-responsive"},n.a.createElement("line",{x1:5,x2:495,y1:5,y2:5,style:i}),n.a.createElement("text",{className:"svg_h1",x:12,y:50},"Topic weight"),n.a.createElement("text",{className:"svg_h1",x:110,y:50},"Default"==s?"Term frequency - All Topics":"Term frequency - "+s," "),n.a.createElement("line",{x1:5,x2:495,y1:55,y2:55,style:r}),n.a.createElement("line",{x1:5,x2:495,y1:m,y2:m,style:i}),n.a.createElement("rect",{x:345,y:24,width:"12",height:"10",fill:"rgba(243, 157, 65,1)"}),n.a.createElement("text",{x:365,y:34,className:"svg_text_2"},"Frequency within topic"),n.a.createElement("rect",{x:345,y:40,width:"12",height:"10",fill:"rgba(70, 130, 180,.7)"}),n.a.createElement("text",{x:365,y:50,className:"svg_text_2"},"Overall term frequency"),n.a.createElement("line",{x1:230,x2:230,y1:55,y2:m,style:a}),n.a.createElement("line",{x1:5,x2:5,y1:5,y2:m,style:i}),n.a.createElement("line",{x1:100,x2:100,y1:5,y2:m,style:i}),n.a.createElement("line",{x1:495,x2:495,y1:5,y2:m,style:i}),o.map((function(e,t){return n.a.createElement(n.a.Fragment,{key:"frag_"+t},n.a.createElement("line",{key:t+"_1_",x1:5,x2:100,y1:135+80*t,y2:135+80*t,style:r}),n.a.createElement("text",{key:t+"2_",x:50,y:87+80*t,className:"svg_h2"},"Topic "+(+e.topic_id+1)),n.a.createElement("circle",{key:t+"_c",cx:25,cy:95+80*t,r:u(e.proportion),fill:"Topic"+(+e.topic_id+1)==s?"rgba(243, 157, 65,.8)":"rgba(70, 130, 180,.8)"}),n.a.createElement("text",{key:t+"_prc",x:55,y:102+80*t,className:"svg_h1"},Math.round(100*e.proportion,2)+"%"))})),l.map((function(e,t){return n.a.createElement(n.a.Fragment,{key:"frag_2_"+t},n.a.createElement("text",{key:t+"_term_",x:110,y:90+27*t,className:"svg_text"},e.Term),n.a.createElement("line",{key:t+"_l",x1:105,x2:495,y1:91+27*t,y2:91+27*t,style:a}),n.a.createElement("rect",{key:t+"_r1",x:230,y:"Default"==e.Category?80+27*t:82+27*t,width:+e.Total*p,height:"Default"==e.Category?"12":"8",fill:"rgba(70, 130, 180,.8)"}),n.a.createElement("text",{key:t+"_tot_",x:232+ +e.Total*p,y:89+27*t,className:"svg_text_3"},"Default"==e.Category?Number(e.Total).toFixed(0):Number(e.Total).toFixed(1)),n.a.createElement("rect",{key:t+"_r2",x:230,y:72+27*t,width:+e.Freq*p,height:"8",fill:"Default"==e.Category?"rgba(70, 130, 180,0)":"rgba(243, 157, 65,.8)"}),n.a.createElement("text",{key:t+"_tot2_",x:232+ +e.Freq*p,y:79+27*t,className:"svg_text_3",fill:"Default"==e.Category?"rgba(70, 130, 180,0)":"rgb(0, 0, 0)"},Number(e.Freq).toFixed(1)))})))))}}]),a}(n.a.Component),u=function(e){Object(y.a)(a,e);var t=Object(m.a)(a);function a(e){var r;return Object(l.a)(this,a),(r=t.call(this,e)).state={Category:"Default",Color:"rgba(70, 130, 180,.8)"},r.handleChange=r.handleChange.bind(Object(s.a)(r)),r}return Object(o.a)(a,[{key:"handleChange",value:function(e){this.setState({Category:e.target.value})}},{key:"render",value:function(){var e=this.props.topic_info,t=this.props.topic_proportion,a=c.d().key((function(e){return e.Category})).entries(e),r=this.state.Category,i=e.filter((function(e){return e.Category===r})).slice(0,30),l=t;return n.a.createElement(n.a.Fragment,null,n.a.createElement("div",{className:"select_box"},n.a.createElement("select",{value:this.state.Category,onChange:this.handleChange},a.map((function(e){return n.a.createElement("option",{key:e.key},e.key)})))),n.a.createElement("div",{className:"main_container_svg"},n.a.createElement(p,{data_terms:i,data_weight:l,itemsDisplay:30,cat:this.state.Category})))}}]),a}(n.a.Component);Promise.all([c.a("https://raw.githubusercontent.com/DHARPA-Project/TopicModelling-/master/vis-files/tm_1/topic_info.csv"),c.a("https://raw.githubusercontent.com/DHARPA-Project/TopicModelling-/master/vis-files/tm_1/topic_proportion.csv")]).then((function(e){var t=e[0],a=e[1],r=document.getElementById("vis_tm_1");r.hasChildNodes()?Object(i.hydrate)(n.a.createElement(u,{topic_info:t,topic_proportion:a}),r):Object(i.render)(n.a.createElement(u,{topic_info:t,topic_proportion:a}),r)})).catch((function(e){}))}},[[28,1,2]]]);

# kiara\TopicModelling-\vis-files\tm_1\main.8a3a0dbd.chunk.css
@import url(https://fonts.googleapis.com/css2?family=Fira+Sans&display=swap);.svg-container{position:relative;width:100%;vertical-align:top;overflow:hidden;margin:0;padding:0}.svg-content-responsive{margin:0;padding:0;display:inline-block;position:absolute;top:0;left:0}.line_style{stroke:"rgb(70, 130, 180,.3)";stroke-width:1}.svg_h1{font-size:.9em}.svg_h1,.svg_h2{font-family:"Fira Sans",Arial}.svg_h2{font-size:.6em}.svg_text{font-size:.8em}.svg_text,.svg_text_2{font-family:"Fira Sans",Arial}.svg_text_2{font-size:.7em}.svg_text_3{font-family:sans-serif;font-size:.6em}.select_box{width:30%;padding-left:1em;font-family:"Fira Sans",Arial}.select_box select{width:100%;height:2.25em;background-color:#fff;padding:.5em .6em;box-shadow:inset 0 1px 3px #ddd;vertical-align:middle;font-family:"Fira Sans",Arial,sans-serif;letter-spacing:.01em;line-height:1.15;-webkit-writing-mode:horizontal-tb!important;text-rendering:auto;color:#000;letter-spacing:normal;word-spacing:normal;text-transform:none;text-indent:0;text-shadow:none;display:inline-block;text-align:start;-webkit-appearance:menulist;align-items:center;white-space:pre;-webkit-rtl-ordering:logical;background-color:#f8f8f8;cursor:default;margin:0;font:400 11px system-ui;border-radius:5px;border:1px solid #a6a6a6;border-image:none;border-image:initial}.select_box select:focus{border-color:#fff;outline:0;box-shadow:inset 0 1px 1px rgba(0,0,0,.075),0 0 1px rgba(102,175,233,.2)}
/*# sourceMappingURL=main.8a3a0dbd.chunk.css.map */

# kiara\TopicModelling-\vis-files\tm_1\main.c8962a70.chunk.js
(this["webpackJsonpvisualisation-d3"]=this["webpackJsonpvisualisation-d3"]||[]).push([[0],{17:function(e,t,a){},28:function(e,t,a){e.exports=a(33)},33:function(e,t,a){"use strict";a.r(t);var r=a(0),n=a.n(r),c=a(22),i=a.n(c),l=a(2),o=(a(17),a(8)),s=a(9),y=a(10),m=a(12),u=a(11),g=function(e){Object(m.a)(a,e);var t=Object(u.a)(a);function a(){return Object(o.a)(this,a),t.apply(this,arguments)}return Object(s.a)(a,[{key:"render",value:function(){var e="0 0 ".concat(500," ").concat(900),t={paddingBottom:"180%"},a={stroke:"rgba(70, 130, 180,.3)",strokeWidth:1},r={stroke:"rgba(70, 130, 180,.7)",strokeWidth:1},c={stroke:"rgba(70, 130, 180,.7)",strokeWidth:2},i=this.props.data_terms,o=(this.props.itemsDisplay,this.props.data_weight),s=this.props.cat,y=i.length,m=75+27*y,u=230/l.b(i,(function(e){return+Math.round(e.Total)})),g=l.e().domain([l.c(o,(function(e){return+e.proportion})),l.b(o,(function(e){return+e.proportion}))]).range([5,17]);return n.a.createElement(n.a.Fragment,null,n.a.createElement("div",{className:"svg-container",style:t},n.a.createElement("svg",{preserveAspectRatio:"xMidYMidmeet",viewBox:e,className:"svg-content-responsive"},n.a.createElement("line",{x1:5,x2:495,y1:5,y2:5,style:c}),n.a.createElement("text",{className:"svg_h1",x:12,y:50},"Topic weight"),n.a.createElement("text",{className:"svg_h1",x:110,y:50},"Default"==s?"Term frequency - All Topics":"Term frequency - "+s," "),n.a.createElement("line",{x1:5,x2:495,y1:55,y2:55,style:r}),n.a.createElement("line",{x1:5,x2:495,y1:m,y2:m,style:c}),n.a.createElement("rect",{x:345,y:24,width:"12",height:"10",fill:"rgba(243, 157, 65,1)"}),n.a.createElement("text",{x:365,y:34,className:"svg_text_2"},"Frequency within topic"),n.a.createElement("rect",{x:345,y:40,width:"12",height:"10",fill:"rgba(70, 130, 180,.7)"}),n.a.createElement("text",{x:365,y:50,className:"svg_text_2"},"Overall term frequency"),n.a.createElement("line",{x1:230,x2:230,y1:55,y2:m,style:a}),n.a.createElement("line",{x1:5,x2:5,y1:5,y2:m,style:c}),n.a.createElement("line",{x1:100,x2:100,y1:5,y2:m,style:c}),n.a.createElement("line",{x1:495,x2:495,y1:5,y2:m,style:c}),o.map((function(e,t){return n.a.createElement(n.a.Fragment,{key:"frag_"+t},n.a.createElement("line",{key:t+"_1_",x1:5,x2:100,y1:135+80*t,y2:135+80*t,style:r}),n.a.createElement("text",{key:t+"2_",x:50,y:87+80*t,className:"svg_h2"},"Topic "+(+e.topic_id+1)),n.a.createElement("circle",{key:t+"_c",cx:25,cy:95+80*t,r:g(e.proportion),fill:"Topic"+(+e.topic_id+1)==s?"rgba(243, 157, 65,.8)":"rgba(70, 130, 180,.8)"}),n.a.createElement("text",{key:t+"_prc",x:55,y:102+80*t,className:"svg_h1"},Math.round(100*e.proportion,2)+"%"))})),i.map((function(e,t){return n.a.createElement(n.a.Fragment,{key:"frag_2_"+t},n.a.createElement("text",{key:t+"_term_",x:110,y:90+27*t,className:"svg_text"},e.Term),n.a.createElement("line",{key:t+"_l",x1:105,x2:495,y1:91+27*t,y2:91+27*t,style:a}),n.a.createElement("rect",{key:t+"_r1",x:230,y:"Default"==e.Category?80+27*t:82+27*t,width:+e.Total*u,height:"Default"==e.Category?"12":"8",fill:"rgba(70, 130, 180,.8)"}),n.a.createElement("text",{key:t+"_tot_",x:232+ +e.Total*u,y:89+27*t,className:"svg_text_3"},"Default"==e.Category?Number(e.Total).toFixed(0):Number(e.Total).toFixed(1)),n.a.createElement("rect",{key:t+"_r2",x:230,y:72+27*t,width:+e.Freq*u,height:"8",fill:"Default"==e.Category?"rgba(70, 130, 180,0)":"rgba(243, 157, 65,.8)"}),n.a.createElement("text",{key:t+"_tot2_",x:232+ +e.Freq*u,y:79+27*t,className:"svg_text_3",fill:"Default"==e.Category?"rgba(70, 130, 180,0)":"rgb(0, 0, 0)"},Number(e.Freq).toFixed(1)))})))))}}]),a}(n.a.Component),p=function(e){Object(m.a)(a,e);var t=Object(u.a)(a);function a(e){var r;return Object(o.a)(this,a),(r=t.call(this,e)).state={Category:"Default",Color:"rgba(70, 130, 180,.8)"},r.handleChange=r.handleChange.bind(Object(y.a)(r)),r}return Object(s.a)(a,[{key:"handleChange",value:function(e){this.setState({Category:e.target.value})}},{key:"render",value:function(){var e=this.props.topic_info,t=this.props.topic_proportion,a=l.d().key((function(e){return e.Category})).entries(e),r=this.state.Category,c=e.filter((function(e){return e.Category===r})).slice(0,30),i=t;return n.a.createElement(n.a.Fragment,null,n.a.createElement("div",{className:"select_box"},n.a.createElement("select",{value:this.state.Category,onChange:this.handleChange},a.map((function(e){return n.a.createElement("option",{key:e.key},e.key)})))),n.a.createElement("div",{className:"main_container_svg"},n.a.createElement(g,{data_terms:c,data_weight:i,itemsDisplay:30,cat:this.state.Category})))}}]),a}(n.a.Component);Promise.all([l.a("/topic_info.csv"),l.a("/topic_proportion.csv")]).then((function(e){var t=e[0],a=e[1];i.a.render(n.a.createElement(n.a.StrictMode,null,n.a.createElement(p,{topic_info:t,topic_proportion:a})),document.getElementById("vis_tm_1"))})).catch((function(e){}))}},[[28,1,2]]]);
//# sourceMappingURL=main.c8962a70.chunk.js.map

# kiara\TopicModelling-\vis-files\tm_1\tm1_data_prepare_gensim.py
"""
pyLDAvis Prepare
===============
Main transformation functions for preparing LDAdata to the visualization's data structures
"""

from __future__ import absolute_import
from past.builtins import basestring
from collections import namedtuple
import json
import logging
from joblib import Parallel, delayed, cpu_count
import numpy as np
import pandas as pd
from scipy.stats import entropy
from scipy.spatial.distance import pdist, squareform
import funcy as fp
import numpy as np
from scipy.sparse import issparse

try:
    from sklearn.manifold import MDS, TSNE
    sklearn_present = True
except ImportError:
    sklearn_present = False


def __num_dist_rows__(array, ndigits=2):
    return array.shape[0] - int((pd.DataFrame(array).sum(axis=1) < 0.999).sum())


class ValidationError(ValueError):
    pass


def _input_check(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency):
    ttds = topic_term_dists.shape
    dtds = doc_topic_dists.shape
    errors = []

    def err(msg):
        errors.append(msg)

    if dtds[1] != ttds[0]:
        err_msg = ('Number of rows of topic_term_dists does not match number of columns of '
                   'doc_topic_dists; both should be equal to the number of topics in the model.')
        err(err_msg)

    if len(doc_lengths) != dtds[0]:
        err_msg = ('Length of doc_lengths not equal to the number of rows in doc_topic_dists;'
                   'both should be equal to the number of documents in the data.')
        err(err_msg)

    W = len(vocab)
    if ttds[1] != W:
        err_msg = ('Number of terms in vocabulary does not match the number of columns of '
                   'topic_term_dists (where each row of topic_term_dists is a probability '
                   'distribution of terms for a given topic)')
        err(err_msg)
    if len(term_frequency) != W:
        err_msg = ('Length of term_frequency not equal to the number of terms in the '
                   'number of terms in the vocabulary (len of vocab)')
        err(err_msg)

    if __num_dist_rows__(topic_term_dists) != ttds[0]:
        err('Not all rows (distributions) in topic_term_dists sum to 1.')

    if __num_dist_rows__(doc_topic_dists) != dtds[0]:
        err('Not all rows (distributions) in doc_topic_dists sum to 1.')

    if len(errors) > 0:
        return errors


def _input_validate(*args):
    res = _input_check(*args)
    if res:
        raise ValidationError('\n' + '\n'.join([' * ' + s for s in res]))


def _jensen_shannon(_P, _Q):
    _M = 0.5 * (_P + _Q)
    return 0.5 * (entropy(_P, _M) + entropy(_Q, _M))


def _pcoa(pair_dists, n_components=2):
    """Principal Coordinate Analysis,
    aka Classical Multidimensional Scaling
    """
    # code referenced from skbio.stats.ordination.pcoa
    # https://github.com/biocore/scikit-bio/blob/0.5.0/skbio/stats/ordination/_principal_coordinate_analysis.py

    # pairwise distance matrix is assumed symmetric
    pair_dists = np.asarray(pair_dists, np.float64)

    # perform SVD on double centred distance matrix
    n = pair_dists.shape[0]
    H = np.eye(n) - np.ones((n, n)) / n
    B = - H.dot(pair_dists ** 2).dot(H) / 2
    eigvals, eigvecs = np.linalg.eig(B)

    # Take first n_components of eigenvalues and eigenvectors
    # sorted in decreasing order
    ix = eigvals.argsort()[::-1][:n_components]
    eigvals = eigvals[ix]
    eigvecs = eigvecs[:, ix]

    # replace any remaining negative eigenvalues and associated eigenvectors with zeroes
    # at least 1 eigenvalue must be zero
    eigvals[np.isclose(eigvals, 0)] = 0
    if np.any(eigvals < 0):
        ix_neg = eigvals < 0
        eigvals[ix_neg] = np.zeros(eigvals[ix_neg].shape)
        eigvecs[:, ix_neg] = np.zeros(eigvecs[:, ix_neg].shape)

    return np.sqrt(eigvals) * eigvecs


def js_PCoA(distributions):
    """Dimension reduction via Jensen-Shannon Divergence & Principal Coordinate Analysis
    (aka Classical Multidimensional Scaling)

    Parameters
    ----------
    distributions : array-like, shape (`n_dists`, `k`)
        Matrix of distributions probabilities.

    Returns
    -------
    pcoa : array, shape (`n_dists`, 2)
    """
    dist_matrix = squareform(pdist(distributions, metric=_jensen_shannon))
    return _pcoa(dist_matrix)


def js_MMDS(distributions, **kwargs):
    """Dimension reduction via Jensen-Shannon Divergence & Metric Multidimensional Scaling

    Parameters
    ----------
    distributions : array-like, shape (`n_dists`, `k`)
        Matrix of distributions probabilities.

    **kwargs : Keyword argument to be passed to `sklearn.manifold.MDS()`

    Returns
    -------
    mmds : array, shape (`n_dists`, 2)
    """
    dist_matrix = squareform(pdist(distributions, metric=_jensen_shannon))
    model = MDS(n_components=2, random_state=0, dissimilarity='precomputed', **kwargs)
    return model.fit_transform(dist_matrix)


def js_TSNE(distributions, **kwargs):
    """Dimension reduction via Jensen-Shannon Divergence & t-distributed Stochastic Neighbor Embedding

    Parameters
    ----------
    distributions : array-like, shape (`n_dists`, `k`)
        Matrix of distributions probabilities.

    **kwargs : Keyword argument to be passed to `sklearn.manifold.TSNE()`

    Returns
    -------
    tsne : array, shape (`n_dists`, 2)
    """
    dist_matrix = squareform(pdist(distributions, metric=_jensen_shannon))
    model = TSNE(n_components=2, random_state=0, metric='precomputed', **kwargs)
    return model.fit_transform(dist_matrix)


def _df_with_names(data, index_name, columns_name):
    if type(data) == pd.DataFrame:
        # we want our index to be numbered
        df = pd.DataFrame(data.values)
    else:
        df = pd.DataFrame(data)
    df.index.name = index_name
    df.columns.name = columns_name
    return df


def _series_with_name(data, name):
    if type(data) == pd.Series:
        data.name = name
        # ensures a numeric index
        return data.reset_index()[name]
    else:
        return pd.Series(data, name=name)


def _topic_coordinates(mds, topic_term_dists, topic_proportion):
    K = topic_term_dists.shape[0]
    mds_res = mds(topic_term_dists)
    assert mds_res.shape == (K, 2)
    mds_df = pd.DataFrame({'x': mds_res[:, 0], 'y': mds_res[:, 1], 'topics': range(1, K + 1),
                          'cluster': 1, 'Freq': topic_proportion * 100})
    # note: cluster (should?) be deprecated soon. See: https://github.com/cpsievert/LDAvis/issues/26
    return mds_df


def _chunks(l, n):
    """ Yield successive n-sized chunks from l.
    """
    for i in range(0, len(l), n):
        yield l[i:i + n]


def _job_chunks(l, n_jobs):
    n_chunks = n_jobs
    if n_jobs < 0:
        # so, have n chunks if we are using all n cores/cpus
        n_chunks = cpu_count() + 1 - n_jobs

    return _chunks(l, n_chunks)


def _find_relevance(log_ttd, log_lift, R, lambda_):
    relevance = lambda_ * log_ttd + (1 - lambda_) * log_lift
    return relevance.T.apply(lambda s: s.sort_values(ascending=False).index).head(R)


def _find_relevance_chunks(log_ttd, log_lift, R, lambda_seq):
    return pd.concat([_find_relevance(log_ttd, log_lift, R, l) for l in lambda_seq])


def _topic_info(topic_term_dists, topic_proportion, term_frequency, term_topic_freq,
                vocab, lambda_step, R, n_jobs):
    # marginal distribution over terms (width of blue bars)
    term_proportion = term_frequency / term_frequency.sum()

    # compute the distinctiveness and saliency of the terms:
    # this determines the R terms that are displayed when no topic is selected
    topic_given_term = topic_term_dists / topic_term_dists.sum()
    kernel = (topic_given_term * np.log((topic_given_term.T / topic_proportion).T))
    distinctiveness = kernel.sum()
    saliency = term_proportion * distinctiveness
    # Order the terms for the "default" view by decreasing saliency:
    default_term_info = pd.DataFrame({
        'saliency': saliency,
        'Term': vocab,
        'Freq': term_frequency,
        'Total': term_frequency,
        'Category': 'Default'})
    default_term_info = default_term_info.sort_values(
        by='saliency', ascending=False).head(R).drop('saliency', 1)
    # Rounding Freq and Total to integer values to match LDAvis code:
    default_term_info['Freq'] = np.floor(default_term_info['Freq'])
    default_term_info['Total'] = np.floor(default_term_info['Total'])
    ranks = np.arange(R, 0, -1)
    default_term_info['logprob'] = default_term_info['loglift'] = ranks

    # compute relevance and top terms for each topic
    log_lift = np.log(topic_term_dists / term_proportion)
    log_ttd = np.log(topic_term_dists)
    lambda_seq = np.arange(0, 1 + lambda_step, lambda_step)

    def topic_top_term_df(tup):
        new_topic_id, (original_topic_id, topic_terms) = tup
        term_ix = topic_terms.unique()
        return pd.DataFrame({'Term': vocab[term_ix],
                             'Freq': term_topic_freq.loc[original_topic_id, term_ix],
                             'Total': term_frequency[term_ix],
                             'logprob': log_ttd.loc[original_topic_id, term_ix].round(4),
                             'loglift': log_lift.loc[original_topic_id, term_ix].round(4),
                             'Category': 'Topic%d' % new_topic_id})

    top_terms = pd.concat(Parallel(n_jobs=n_jobs)
                          (delayed(_find_relevance_chunks)(log_ttd, log_lift, R, ls)
                          for ls in _job_chunks(lambda_seq, n_jobs)))
    topic_dfs = map(topic_top_term_df, enumerate(top_terms.T.iterrows(), 1))
    return pd.concat([default_term_info] + list(topic_dfs), sort=True)


def _token_table(topic_info, term_topic_freq, vocab, term_frequency):
    # last, to compute the areas of the circles when a term is highlighted
    # we must gather all unique terms that could show up (for every combination
    # of topic and value of lambda) and compute its distribution over topics.

    # term-topic frequency table of unique terms across all topics and all values of lambda
    term_ix = topic_info.index.unique()
    term_ix = np.sort(term_ix)

    top_topic_terms_freq = term_topic_freq[term_ix]
    # use the new ordering for the topics
    K = len(term_topic_freq)
    top_topic_terms_freq.index = range(1, K + 1)
    top_topic_terms_freq.index.name = 'Topic'

    # we filter to Freq >= 0.5 to avoid sending too much data to the browser
    token_table = pd.DataFrame({'Freq': top_topic_terms_freq.unstack()})\
        .reset_index().set_index('term').query('Freq >= 0.5')

    token_table['Freq'] = token_table['Freq'].round()
    token_table['Term'] = vocab[token_table.index.values].values
    # Normalize token frequencies:
    token_table['Freq'] = token_table.Freq / term_frequency[token_table.index]
    return token_table.sort_values(by=['Term', 'Topic'])


def vis_prepare(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency,
            R=30, lambda_step=0.01, mds=js_PCoA, n_jobs=-1,
            plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, sort_topics=False):
    """Transforms the topic model distributions and related corpus data into
    the data structures needed for the visualization.

    Parameters
    ----------
    topic_term_dists : array-like, shape (`n_topics`, `n_terms`)
        Matrix of topic-term probabilities. Where `n_terms` is `len(vocab)`.
    doc_topic_dists : array-like, shape (`n_docs`, `n_topics`)
        Matrix of document-topic probabilities.
    doc_lengths : array-like, shape `n_docs`
        The length of each document, i.e. the number of words in each document.
        The order of the numbers should be consistent with the ordering of the
        docs in `doc_topic_dists`.
    vocab : array-like, shape `n_terms`
        List of all the words in the corpus used to train the model.
    term_frequency : array-like, shape `n_terms`
        The count of each particular term over the entire corpus. The ordering
        of these counts should correspond with `vocab` and `topic_term_dists`.
    R : int
        The number of terms to display in the barcharts of the visualization.
        Default is 30. Recommended to be roughly between 10 and 50.
    lambda_step : float, between 0 and 1
        Determines the interstep distance in the grid of lambda values over
        which to iterate when computing relevance.
        Default is 0.01. Recommended to be between 0.01 and 0.1.
    mds : function or a string representation of function
        A function that takes `topic_term_dists` as an input and outputs a
        `n_topics` by `2`  distance matrix. The output approximates the distance
        between topics. See :func:`js_PCoA` for details on the default function.
        A string representation currently accepts `pcoa` (or upper case variant),
        `mmds` (or upper case variant) and `tsne` (or upper case variant),
        if `sklearn` package is installed for the latter two.
    n_jobs : int
        The number of cores to be used to do the computations. The regular
        joblib conventions are followed so `-1`, which is the default, will
        use all cores.
    plot_opts : dict, with keys 'xlab' and `ylab`
        Dictionary of plotting options, right now only used for the axis labels.
    sort_topics : sort topics by topic proportion (percentage of tokens covered). Set to false to
        to keep original topic order.

    Returns
    -------
    prepared_data : PreparedData
        A named tuple containing all the data structures required to create
        the visualization. To be passed on to functions like :func:`display`.
        This named tuple can be represented as json or a python dictionary.
        There is a helper function 'sorted_terms' that can be used to get
        the terms of a topic using lambda to rank their relevance.


    Notes
    -----
    This implements the method of `Sievert, C. and Shirley, K. (2014):
    LDAvis: A Method for Visualizing and Interpreting Topics, ACL Workshop on
    Interactive Language Learning, Visualization, and Interfaces.`

    http://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf

    See Also
    --------
    :func:`save_json`: save json representation of a figure to file
    :func:`save_html` : save html representation of a figure to file
    :func:`show` : launch a local server and show a figure in a browser
    :func:`display` : embed figure within the IPython notebook
    :func:`enable_notebook` : automatically embed visualizations in IPython notebook
   """
    # parse mds
    if isinstance(mds, basestring):
        mds = mds.lower()
        if mds == 'pcoa':
            mds = js_PCoA
        elif mds in ('mmds', 'tsne'):
            if sklearn_present:
                mds_opts = {'mmds': js_MMDS, 'tsne': js_TSNE}
                mds = mds_opts[mds]
            else:
                logging.warning('sklearn not present, switch to PCoA')
                mds = js_PCoA
        else:
            logging.warning('Unknown mds `%s`, switch to PCoA' % mds)
            mds = js_PCoA

    topic_term_dists = _df_with_names(topic_term_dists, 'topic', 'term')
    doc_topic_dists = _df_with_names(doc_topic_dists, 'doc', 'topic')
    term_frequency = _series_with_name(term_frequency, 'term_frequency')
    doc_lengths = _series_with_name(doc_lengths, 'doc_length')
    vocab = _series_with_name(vocab, 'vocab')
    _input_validate(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency)
    R = min(R, len(vocab))

    topic_freq = (doc_topic_dists.T * doc_lengths).T.sum()
    # topic_freq       = np.dot(doc_topic_dists.T, doc_lengths)
    if (sort_topics):
        topic_proportion = (topic_freq / topic_freq.sum()).sort_values(ascending=False)
    else:
        topic_proportion = (topic_freq / topic_freq.sum())

    topic_order = topic_proportion.index
    # reorder all data based on new ordering of topics
    topic_freq = topic_freq[topic_order]
    topic_term_dists = topic_term_dists.iloc[topic_order]
    doc_topic_dists = doc_topic_dists[topic_order]

    # token counts for each term-topic combination (widths of red bars)
    term_topic_freq = (topic_term_dists.T * topic_freq).T
    # Quick fix for red bar width bug.  We calculate the
    # term frequencies internally, using the topic term distributions and the
    # topic frequencies, rather than using the user-supplied term frequencies.
    # For a detailed discussion, see: https://github.com/cpsievert/LDAvis/pull/41
    term_frequency = np.sum(term_topic_freq, axis=0)

    topic_info = _topic_info(topic_term_dists, topic_proportion,
                             term_frequency, term_topic_freq, vocab, lambda_step, R, n_jobs)
    token_table = _token_table(topic_info, term_topic_freq, vocab, term_frequency)
    topic_coordinates = _topic_coordinates(mds, topic_term_dists, topic_proportion)
    client_topic_order = [x + 1 for x in topic_order]



    return PreparedData(topic_coordinates, topic_info,
                        token_table, R, lambda_step, plot_opts, client_topic_order)


class PreparedData(namedtuple('PreparedData', ['topic_coordinates', 'topic_info', 'token_table',
                                               'R', 'lambda_step', 'plot_opts', 'topic_order'])):
  

    def sorted_terms(self, topic=1, _lambda=1):
        """Retuns a dataframe using _lambda to calculate term relevance of a given topic."""
        tdf = pd.DataFrame(self.topic_info[self.topic_info.Category == 'Topic' + str(topic)])
        if _lambda < 0 or _lambda > 1:
            _lambda = 1
        stdf = tdf.assign(relevance=_lambda * tdf['logprob'] + (1 - _lambda) * tdf['loglift'])

        return stdf.sort_values('relevance', ascending=False)



    def to_dict(self):
        return {'mdsDat': self.topic_coordinates.to_dict(orient='list'),
                'tinfo': self.topic_info.to_dict(orient='list'),
                'token.table': self.token_table.to_dict(orient='list'),
                'R': self.R,
                'lambda.step': self.lambda_step,
                'plot.opts': self.plot_opts,
                'topic.order': self.topic_order}
              

    def to_json(self):
        return json.dumps(self.to_dict(), cls=NumPyEncoder)


def _extract_data(topic_model, corpus, dictionary, doc_topic_dists=None):
    import gensim

    if not gensim.matutils.ismatrix(corpus):
        corpus_csc = gensim.matutils.corpus2csc(corpus, num_terms=len(dictionary))
    else:
        corpus_csc = corpus
        # Need corpus to be a streaming gensim list corpus for len and inference functions below:
        corpus = gensim.matutils.Sparse2Corpus(corpus_csc)

    vocab = list(dictionary.token2id.keys())
    # TODO: add the hyperparam to smooth it out? no beta in online LDA impl.. hmm..
    # for now, I'll just make sure we don't ever get zeros...
    beta = 0.01
    fnames_argsort = np.asarray(list(dictionary.token2id.values()), dtype=np.int_)
    term_freqs = corpus_csc.sum(axis=1).A.ravel()[fnames_argsort]
    term_freqs[term_freqs == 0] = beta
    doc_lengths = corpus_csc.sum(axis=0).A.ravel()

    assert term_freqs.shape[0] == len(dictionary),\
        'Term frequencies and dictionary have different shape {} != {}'.format(
        term_freqs.shape[0], len(dictionary))
    assert doc_lengths.shape[0] == len(corpus),\
        'Document lengths and corpus have different sizes {} != {}'.format(
        doc_lengths.shape[0], len(corpus))

    if hasattr(topic_model, 'lda_alpha'):
        num_topics = len(topic_model.lda_alpha)
    else:
        num_topics = topic_model.num_topics

    if doc_topic_dists is None:
        # If its an HDP model.
        if hasattr(topic_model, 'lda_beta'):
            gamma = topic_model.inference(corpus)
        else:
            gamma, _ = topic_model.inference(corpus)
        doc_topic_dists = gamma / gamma.sum(axis=1)[:, None]
    else:
        if isinstance(doc_topic_dists, list):
            doc_topic_dists = gensim.matutils.corpus2dense(doc_topic_dists, num_topics).T
        elif issparse(doc_topic_dists):
            doc_topic_dists = doc_topic_dists.T.todense()
        doc_topic_dists = doc_topic_dists / doc_topic_dists.sum(axis=1)

    assert doc_topic_dists.shape[1] == num_topics,\
        'Document topics and number of topics do not match {} != {}'.format(
        doc_topic_dists.shape[1], num_topics)

    # get the topic-term distribution straight from gensim without
    # iterating over tuples
    if hasattr(topic_model, 'lda_beta'):
        topic = topic_model.lda_beta
    else:
        topic = topic_model.state.get_lambda()
    topic = topic / topic.sum(axis=1)[:, None]
    topic_term_dists = topic[:, fnames_argsort]

    assert topic_term_dists.shape[0] == doc_topic_dists.shape[1]

    return {'topic_term_dists': topic_term_dists, 'doc_topic_dists': doc_topic_dists,
            'doc_lengths': doc_lengths, 'vocab': vocab, 'term_frequency': term_freqs}


def prepare(topic_model, corpus, dictionary, doc_topic_dist=None, **kwargs):
    """Transforms the Gensim TopicModel and related corpus and dictionary into
    the data structures needed for the visualization.

    Parameters
    ----------
    topic_model : gensim.models.ldamodel.LdaModel
        An already trained Gensim LdaModel. The other gensim model types are
        not supported (PRs welcome).

    corpus : array-like list of bag of word docs in tuple form or scipy CSC matrix
        The corpus in bag of word form, the same docs used to train the model.
        The corpus is transformed into a csc matrix internally, if you intend to
        call prepare multiple times it is a good idea to first call
        `gensim.matutils.corpus2csc(corpus)` and pass in the csc matrix instead.

    For example: [(50, 3), (63, 5), ....]

    dictionary: gensim.corpora.Dictionary
        The dictionary object used to create the corpus. Needed to extract the
        actual terms (not ids).

    doc_topic_dist (optional): Document topic distribution from LDA (default=None)
        The document topic distribution that is eventually visualised, if you will
        be calling `prepare` multiple times it's a good idea to explicitly pass in
        `doc_topic_dist` as inferring this for large corpora can be quite
        expensive.

    **kwargs :
        additional keyword arguments are passed through to :func:`pyldavis.prepare`.

    Returns
    -------
    prepared_data : PreparedData
        the data structures used in the visualization

    Example
    --------
    For example usage please see this notebook:
    http://nbviewer.ipython.org/github/bmabey/pyLDAvis/blob/master/notebooks/Gensim%20Newsgroup.ipynb

    See
    ------
    See `pyLDAvis.prepare` for **kwargs.
    """
    opts = fp.merge(_extract_data(topic_model, corpus, dictionary, doc_topic_dist), kwargs)
    return vis_prepare(**opts)





# kiara\TopicModelling-\vis-files\tm_1\tm1_gensim.py
ldamodel = model
dictionary = id2word
corpus_csc = gensim.matutils.corpus2csc(corpus, num_terms=len(dictionary))
import numpy as np
vocab = list(dictionary.token2id.keys())
vocab = pd.Series(vocab, name='vocab')
fnames_argsort = np.asarray(list(dictionary.token2id.values()), dtype=np.int_)
term_frequency = corpus_csc.sum(axis=1).A.ravel()[fnames_argsort]
term_frequency = pd.Series(term_frequency,name='term_frequency')
doc_length = corpus_csc.sum(axis=0).A.ravel()
doc_length = pd.Series(doc_length, name='doc_length')
# topic weights for each document in the corpus
doc_topic_weights = ldamodel.inference(corpus)[0]
doc_topic_dists = doc_topic_weights / doc_topic_weights.sum(axis=1)[:, None]
# put data into dataframe
doc_topic_dists = pd.DataFrame(doc_topic_dists)
doc_topic_dists.index.name = 'doc'
doc_topic_dists.columns.name = 'topic'
topic_term = ldamodel.state.get_lambda() # topics term matrix: https://stackoverflow.com/questions/42289858/extract-topic-word-probability-matrix-in-gensim-ldamodel
topic_term_dists = topic_term / topic_term.sum(axis=1)[:, None]
topic_term_dists = topic_term_dists[:, fnames_argsort]
topic_term_dists = pd.DataFrame(topic_term_dists)
topic_term_dists.index.name = 'topic'
topic_term_dists.columns.name = 'term'
topic_freq = (doc_topic_dists.T * doc_length).T.sum()
topic_proportion = (topic_freq / topic_freq.sum()).sort_values(ascending=False)
topic_order = topic_proportion.index
topic_freq = topic_freq[topic_order]
topic_term_dists = topic_term_dists.iloc[topic_order]
doc_topic_dists = doc_topic_dists[topic_order]
# token counts for each term-topic combination (widths of red bars)
term_topic_freq = (topic_term_dists.T * topic_freq).T
term_frequency = np.sum(term_topic_freq, axis=0)
term_proportion = term_frequency / term_frequency.sum()
topic_given_term = topic_term_dists / topic_term_dists.sum()
kernel = (topic_given_term * np.log((topic_given_term.T / topic_proportion).T))
distinctiveness = kernel.sum()
saliency = term_proportion * distinctiveness
default_term_info = pd.DataFrame({
  'saliency': saliency,
  'Term': vocab,
  'Freq': term_frequency,
  'Total': term_frequency,
  'Category': 'Default'})
# Sort terms for the "default" view by decreasing saliency and display only the R first lines:
default_term_info = default_term_info.sort_values(
  by='saliency', ascending=False).head(R).drop('saliency', 1)
# Rounding Freq and Total to integer values
default_term_info['Freq'] = np.floor(default_term_info['Freq'])
default_term_info['Total'] = np.floor(default_term_info['Total'])
ranks = np.arange(R, 0, -1)
default_term_info['logprob'] = default_term_info['loglift'] = ranks
log_lift = np.log(topic_term_dists / term_proportion)
log_ttd = np.log(topic_term_dists)
lambda_seq = np.arange(0, 1 + 0.01, 0.01) # lambda_step=0.01
def topic_top_term_df(tup):
        new_topic_id, (original_topic_id, topic_terms) = tup
        term_ix = topic_terms.unique()
        return pd.DataFrame({'Term': vocab[term_ix],
                             'Freq': term_topic_freq.loc[original_topic_id, term_ix],
                             'Total': term_frequency[term_ix],
                             'logprob': log_ttd.loc[original_topic_id, term_ix].round(4),
                             'loglift': log_lift.loc[original_topic_id, term_ix].round(4),
                             'Category': 'Topic%d' % new_topic_id})
from joblib import Parallel, delayed, cpu_count
# Technical parameters for further processing
lambda_step = 0.01 #interstep distance on which to iterate when computing relevance
n_jobs = -1 #number of cores to be used to do the computations (-1 = all cores)
lambda_seq = np.arange(0, 1 + lambda_step, lambda_step)
def _chunks(l, n):
    """ Yield successive n-sized chunks from l.
    """
    for i in range(0, len(l), n):
        yield l[i:i + n]

def _job_chunks(l, n_jobs):
    n_chunks = n_jobs
    if n_jobs < 0:
        n_chunks = cpu_count() + 1 - n_jobs

    return _chunks(l, n_chunks)

def _find_relevance_chunks(log_ttd, log_lift, R, lambda_seq):
    return pd.concat([_find_relevance(log_ttd, log_lift, R, l) for l in lambda_seq])

def _find_relevance(log_ttd, log_lift, R, lambda_):
    relevance = lambda_ * log_ttd + (1 - lambda_) * log_lift
    return relevance.T.apply(lambda s: s.sort_values(ascending=False).index).head(R)
top_terms = pd.concat(Parallel(n_jobs=-1)
                          (delayed(_find_relevance_chunks)(log_ttd, log_lift, R, ls)
                          for ls in _job_chunks(lambda_seq, n_jobs))) #n jobs = -1
topic_dfs = map(topic_top_term_df, enumerate(top_terms.T.iterrows(), 1))
topic_info = pd.concat([default_term_info] + list(topic_dfs), sort=True)
topic_proportion_df = pd.DataFrame(topic_proportion, columns = ['proportion'])
topic_proportion_df.index.name = 'topic_id'
import csv
topic_info.to_csv('topic_info.csv')
topic_proportion_df.to_csv('topic_proportion.csv')

# kiara\TopicModelling-\vis-files\tm_1\tm1_mallet.py
ldamodel = model
dictionary = id2word
corpus_csc = gensim.matutils.corpus2csc(corpus, num_terms=len(dictionary))
import numpy as np
vocab = list(dictionary.token2id.keys())
vocab = pd.Series(vocab, name='vocab')
fnames_argsort = np.asarray(list(dictionary.token2id.values()), dtype=np.int_)
term_frequency = corpus_csc.sum(axis=1).A.ravel()[fnames_argsort]
term_frequency = pd.Series(term_frequency,name='term_frequency')
doc_length = corpus_csc.sum(axis=0).A.ravel()
doc_length = pd.Series(doc_length, name='doc_length')
# topic weights for each document in the corpus
doc_topic_weights = ldamodel.inference(corpus)[0]
doc_topic_dists = doc_topic_weights / doc_topic_weights.sum(axis=1)[:, None]
# put data into dataframe
doc_topic_dists = pd.DataFrame(doc_topic_dists)
doc_topic_dists.index.name = 'doc'
doc_topic_dists.columns.name = 'topic'
topic_term = ldamodel.state.get_lambda() # topics term matrix: https://stackoverflow.com/questions/42289858/extract-topic-word-probability-matrix-in-gensim-ldamodel
topic_term_dists = topic_term / topic_term.sum(axis=1)[:, None]
topic_term_dists = topic_term_dists[:, fnames_argsort]
topic_term_dists = pd.DataFrame(topic_term_dists)
topic_term_dists.index.name = 'topic'
topic_term_dists.columns.name = 'term'
topic_freq = (doc_topic_dists.T * doc_length).T.sum()
topic_proportion = (topic_freq / topic_freq.sum()).sort_values(ascending=False)
topic_order = topic_proportion.index
topic_freq = topic_freq[topic_order]
topic_term_dists = topic_term_dists.iloc[topic_order]
doc_topic_dists = doc_topic_dists[topic_order]
# token counts for each term-topic combination (widths of red bars)
term_topic_freq = (topic_term_dists.T * topic_freq).T
term_frequency = np.sum(term_topic_freq, axis=0)
term_proportion = term_frequency / term_frequency.sum()
topic_given_term = topic_term_dists / topic_term_dists.sum()
kernel = (topic_given_term * np.log((topic_given_term.T / topic_proportion).T))
distinctiveness = kernel.sum()
saliency = term_proportion * distinctiveness
default_term_info = pd.DataFrame({
  'saliency': saliency,
  'Term': vocab,
  'Freq': term_frequency,
  'Total': term_frequency,
  'Category': 'Default'})
# Sort terms for the "default" view by decreasing saliency and display only the R first lines:
default_term_info = default_term_info.sort_values(
  by='saliency', ascending=False).head(R).drop('saliency', 1)
# Rounding Freq and Total to integer values
default_term_info['Freq'] = np.floor(default_term_info['Freq'])
default_term_info['Total'] = np.floor(default_term_info['Total'])
ranks = np.arange(R, 0, -1)
default_term_info['logprob'] = default_term_info['loglift'] = ranks
log_lift = np.log(topic_term_dists / term_proportion)
log_ttd = np.log(topic_term_dists)
lambda_seq = np.arange(0, 1 + 0.01, 0.01) # lambda_step=0.01
def topic_top_term_df(tup):
        new_topic_id, (original_topic_id, topic_terms) = tup
        term_ix = topic_terms.unique()
        return pd.DataFrame({'Term': vocab[term_ix],
                             'Freq': term_topic_freq.loc[original_topic_id, term_ix],
                             'Total': term_frequency[term_ix],
                             'logprob': log_ttd.loc[original_topic_id, term_ix].round(4),
                             'loglift': log_lift.loc[original_topic_id, term_ix].round(4),
                             'Category': 'Topic%d' % new_topic_id})
from joblib import Parallel, delayed, cpu_count
# Technical parameters for further processing
lambda_step = 0.01 #interstep distance on which to iterate when computing relevance
n_jobs = -1 #number of cores to be used to do the computations (-1 = all cores)
lambda_seq = np.arange(0, 1 + lambda_step, lambda_step)
def _chunks(l, n):
    """ Yield successive n-sized chunks from l.
    """
    for i in range(0, len(l), n):
        yield l[i:i + n]

def _job_chunks(l, n_jobs):
    n_chunks = n_jobs
    if n_jobs < 0:
        n_chunks = cpu_count() + 1 - n_jobs

    return _chunks(l, n_chunks)

def _find_relevance_chunks(log_ttd, log_lift, R, lambda_seq):
    return pd.concat([_find_relevance(log_ttd, log_lift, R, l) for l in lambda_seq])

def _find_relevance(log_ttd, log_lift, R, lambda_):
    relevance = lambda_ * log_ttd + (1 - lambda_) * log_lift
    return relevance.T.apply(lambda s: s.sort_values(ascending=False).index).head(R)
top_terms = pd.concat(Parallel(n_jobs=-1)
                          (delayed(_find_relevance_chunks)(log_ttd, log_lift, R, ls)
                          for ls in _job_chunks(lambda_seq, n_jobs))) #n jobs = -1
topic_dfs = map(topic_top_term_df, enumerate(top_terms.T.iterrows(), 1))
topic_info = pd.concat([default_term_info] + list(topic_dfs), sort=True)
topic_proportion_df = pd.DataFrame(topic_proportion, columns = ['proportion'])
topic_proportion_df.index.name = 'topic_id'
import csv
topic_info.to_csv('topic_info.csv')
topic_proportion_df.to_csv('topic_proportion.csv')

# kiara\TopicModelling-\vis-files\tm_1\tm1_tfidf.py
ldamodel = model
dictionary = id2word
corpus_csc = gensim.matutils.corpus2csc(corpus, num_terms=len(dictionary))
import numpy as np
vocab = list(dictionary.token2id.keys())
vocab = pd.Series(vocab, name='vocab')
fnames_argsort = np.asarray(list(dictionary.token2id.values()), dtype=np.int_)
term_frequency = corpus_csc.sum(axis=1).A.ravel()[fnames_argsort]
term_frequency = pd.Series(term_frequency,name='term_frequency')
doc_length = corpus_csc.sum(axis=0).A.ravel()
doc_length = pd.Series(doc_length, name='doc_length')
# topic weights for each document in the corpus
doc_topic_weights = ldamodel.inference(corpus)[0]
doc_topic_dists = doc_topic_weights / doc_topic_weights.sum(axis=1)[:, None]
# put data into dataframe
doc_topic_dists = pd.DataFrame(doc_topic_dists)
doc_topic_dists.index.name = 'doc'
doc_topic_dists.columns.name = 'topic'
topic_term = ldamodel.state.get_lambda()
topic_term_dists = topic_term / topic_term.sum(axis=1)[:, None]
topic_term_dists = topic_term_dists[:, fnames_argsort]
topic_term_dists = pd.DataFrame(topic_term_dists)
topic_term_dists.index.name = 'topic'
topic_term_dists.columns.name = 'term'
topic_freq = (doc_topic_dists.T * doc_length).T.sum()
topic_proportion = (topic_freq / topic_freq.sum()).sort_values(ascending=False)
topic_order = topic_proportion.index
topic_freq = topic_freq[topic_order]
topic_term_dists = topic_term_dists.iloc[topic_order]
doc_topic_dists = doc_topic_dists[topic_order]
term_topic_freq = (topic_term_dists.T * topic_freq).T
term_frequency = np.sum(term_topic_freq, axis=0)
term_proportion = term_frequency / term_frequency.sum()
topic_given_term = topic_term_dists / topic_term_dists.sum()
kernel = (topic_given_term * np.log((topic_given_term.T / topic_proportion).T))
distinctiveness = kernel.sum()
saliency = term_proportion * distinctiveness
default_term_info = pd.DataFrame({
  'saliency': saliency,
  'Term': vocab,
  'Freq': term_frequency*10,
  'Total': term_frequency*10,
  'Category': 'Default'})
# Sort terms for the "default" view by decreasing saliency and display only the R first lines:
default_term_info = default_term_info.sort_values(
  by='saliency', ascending=False).head(R).drop('saliency', 1)
# Rounding Freq and Total to integer values
ranks = np.arange(R, 0, -1)
default_term_info['logprob'] = default_term_info['loglift'] = ranks
log_lift = np.log(topic_term_dists / term_proportion)
log_ttd = np.log(topic_term_dists)
lambda_seq = np.arange(0, 1 + 0.01, 0.01) # lambda_step=0.01
def topic_top_term_df(tup):
        new_topic_id, (original_topic_id, topic_terms) = tup
        term_ix = topic_terms.unique()
        return pd.DataFrame({'Term': vocab[term_ix],
                             'Freq': term_topic_freq.loc[original_topic_id, term_ix]*10,
                             'Total': term_frequency[term_ix]*10,
                             'logprob': log_ttd.loc[original_topic_id, term_ix].round(4),
                             'loglift': log_lift.loc[original_topic_id, term_ix].round(4),
                             'Category': 'Topic%d' % new_topic_id})
from joblib import Parallel, delayed, cpu_count
# Technical parameters for further processing
lambda_step = 0.01 #interstep distance on which to iterate when computing relevance
n_jobs = -1 #number of cores to be used to do the computations (-1 = all cores)
lambda_seq = np.arange(0, 1 + lambda_step, lambda_step)
def _chunks(l, n):
    """ Yield successive n-sized chunks from l.
    """
    for i in range(0, len(l), n):
        yield l[i:i + n]

def _job_chunks(l, n_jobs):
    n_chunks = n_jobs
    if n_jobs < 0:
        n_chunks = cpu_count() + 1 - n_jobs

    return _chunks(l, n_chunks)

def _find_relevance_chunks(log_ttd, log_lift, R, lambda_seq):
    return pd.concat([_find_relevance(log_ttd, log_lift, R, l) for l in lambda_seq])

def _find_relevance(log_ttd, log_lift, R, lambda_):
    relevance = lambda_ * log_ttd + (1 - lambda_) * log_lift
    return relevance.T.apply(lambda s: s.sort_values(ascending=False).index).head(R)
top_terms = pd.concat(Parallel(n_jobs=-1)
                          (delayed(_find_relevance_chunks)(log_ttd, log_lift, R, ls)
                          for ls in _job_chunks(lambda_seq, n_jobs))) #n jobs = -1
topic_dfs = map(topic_top_term_df, enumerate(top_terms.T.iterrows(), 1))
topic_info = pd.concat([default_term_info] + list(topic_dfs), sort=True)
topic_proportion_df = pd.DataFrame(topic_proportion, columns = ['proportion'])
topic_proportion_df.index.name = 'topic_id'
import csv
topic_info.to_csv('topic_info.csv')
topic_proportion_df.to_csv('topic_proportion.csv')