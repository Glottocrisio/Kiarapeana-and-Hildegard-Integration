class ValueSchema(KiaraModel):

    """
    The schema of a value.

    The schema contains the [ValueTypeOrm][kiara.data.values.ValueTypeOrm] of a value, as well as an optional default that
    will be used if no user input was given (yet) for a value.

    For more complex container data_types like array, tables, unions etc, data_types can also be configured with values from the ``type_config`` field.
    """

    _kiara_model_id = "instance.value_schema"

    class Config:
        use_enum_values = True
        # extra = Extra.forbid

    type: str = Field(description="The type of the value.")
    type_config: typing.Dict[str, typing.Any] = Field(
        description="Configuration for the type, in case it's complex.",
        default_factory=dict,
    )
    default: typing.Any = Field(
        description="A default value.", default=SpecialValue.NOT_SET
    )

    optional: bool = Field(
        description="Whether this value is required (True), or whether 'None' value is allowed (False).",
        default=False,
    )
    is_constant: bool = Field(
        description="Whether the value is a constant.", default=False
    )

    doc: DocumentationMetadataModel = Field(
        default_factory=DocumentationMetadataModel,
        description="A description for the value of this input field.",
    )

    @validator("doc", pre=True)
    def validate_doc(cls, value):
        doc = DocumentationMetadataModel.create(value)
        return doc

    def _retrieve_data_to_hash(self) -> typing.Any:

        return {"type": self.type, "type_config": self.type_config}

    def is_required(self):

        if self.optional:
            return False
        else:
            if self.default in [None, SpecialValue.NOT_SET, SpecialValue.NO_VALUE]:
                return True
            else:
                return False

    # def validate_types(self, kiara: "Kiara"):
    #
    #     if self.type not in kiara.value_type_names:
    #         raise ValueError(
    #             f"Invalid value type '{self.type}', available data_types: {kiara.value_type_names}"
    #         )

    def __eq__(self, other):

        if not isinstance(other, ValueSchema):
            return False

        return (self.type, self.default) == (other.type, other.default)

    def __hash__(self):

        return hash((self.type, self.default))

    def __repr__(self):

        return f"ValueSchema(type={self.type}, default={self.default}, optional={self.optional})"

    def __str__(self):

        return self.__repr__()

class Config:
    use_enum_values = True

@validator("doc", pre=True)
def validate_doc(cls, value):
    doc = DocumentationMetadataModel.create(value)
    return doc

def is_required(self):

    if self.optional:
        return False
    else:
        if self.default in [None, SpecialValue.NOT_SET, SpecialValue.NO_VALUE]:
            return True
        else:
            return False
class JobDesc(KiaraModel):
    """An object describing a compute job with both raw or referenced inputs."""

    _kiara_model_id = "instance.job_desc"

    @classmethod
    def create_from_file(cls, path: Union[str, Path]):
        run_data = cls.parse_from_file(path)
        return cls(**run_data)

    @classmethod
    def parse_from_file(cls, path: Union[str, Path]):

        if isinstance(path, str):
            path = Path(path)

        if not path.is_file():
            raise KiaraException(
                f"Can't load job description, invalid file path: '{path}'"
            )

        data = get_data_from_file(path)

        repl_dict: Dict[str, Any] = {"this_dir": path.parent.absolute().as_posix()}

        try:
            run_data = cls.parse_data(
                data=data, var_repl_dict=repl_dict, alias=path.stem
            )
            return run_data
        except Exception as e:
            raise KiaraException(f"Invalid run description in file '{path}': {e}")

    @classmethod
    def parse_data(
        cls,
        data: Mapping[str, Any],
        var_repl_dict: Union[Mapping[str, Any], None] = None,
        alias: Union[str, None] = None,
    ):

        if not isinstance(data, Mapping):
            raise KiaraException("Job description data is not a mapping.")

        if "operation" not in data.keys():
            raise KiaraException("Missing 'operation' key")

        if var_repl_dict:
            run_data = replace_var_names_in_obj(data, repl_dict=var_repl_dict)
        else:
            run_data = data

        if alias:
            run_data["job_alias"] = alias

        return run_data

    @classmethod
    def create_from_data(
        cls,
        data: Mapping[str, Any],
        var_repl_dict: Union[Mapping[str, Any], None] = None,
        alias: Union[str, None] = None,
    ) -> "JobDesc":

        run_data = cls.parse_data(data=data, var_repl_dict=var_repl_dict, alias=alias)
        return cls(**run_data)

    job_alias: str = Field(description="The alias for the job.", default="default")
    operation: str = Field(description="The operation id or module type.")
    module_config: Union[Mapping[str, Any], None] = Field(
        default=None, description="The configuration for the module."
    )
    inputs: Dict[str, Any] = Field(
        description="The inputs for the job.", default_factory=dict
    )
    doc: DocumentationMetadataModel = Field(
        description="A description/doc for this job.",
        default_factory=DocumentationMetadataModel.create,
    )
    save: Dict[str, str] = Field(
        description="Configuration on how/whether to save the job results.",
        default_factory=dict,
    )

    def _retrieve_data_to_hash(self) -> IPLDKind:
        def get_hash(v: Any):
            if hasattr(v, "instance_cid"):
                return v.instance_cid
            elif hasattr(v, "value_id"):
                return str(v.value_id)
            elif isinstance(v, uuid.UUID):
                return str(v)
            elif isinstance(v, Mapping):
                return {get_hash(k): get_hash(v) for k, v in v.items()}
            return v

        inputs_hash = {k: get_hash(v) for k, v in self.inputs.items()}
        return {
            "operation": self.operation,
            "module_config": self.module_config,  # type: ignore
            "inputs": inputs_hash,
            "save": self.save,  # type: ignore
        }

    @root_validator(pre=True)
    def validate_inputs(cls, values):

        if len(values) == 1 and "data" in values.keys():
            data = values["data"]
            if isinstance(data, str):
                if os.path.isfile(data):
                    data = Path(data)

            if isinstance(data, Path):
                run_data = cls.parse_from_file(data)
                return run_data
            else:
                values = data
        return values

    @validator("doc", pre=True)
    def validate_doc(cls, value):
        return DocumentationMetadataModel.create(value)

    def get_operation(self, kiara_api: "KiaraAPI") -> "Operation":

        if not self.module_config:
            operation = kiara_api.get_operation(self.operation, allow_external=True)
        else:
            data = {
                "module_type": self.operation,
                "module_config": self.module_config,
            }
            operation = kiara_api.get_operation(operation=data, allow_external=False)

        return operation

@classmethod
def create_from_file(cls, path: Union[str, Path]):
    run_data = cls.parse_from_file(path)
    return cls(**run_data)

@classmethod
def parse_from_file(cls, path: Union[str, Path]):

    if isinstance(path, str):
        path = Path(path)

    if not path.is_file():
        raise KiaraException(
            f"Can't load job description, invalid file path: '{path}'"
        )

    data = get_data_from_file(path)

    repl_dict: Dict[str, Any] = {"this_dir": path.parent.absolute().as_posix()}

    try:
        run_data = cls.parse_data(
            data=data, var_repl_dict=repl_dict, alias=path.stem
        )
        return run_data
    except Exception as e:
        raise KiaraException(f"Invalid run description in file '{path}': {e}")


@classmethod
def parse_data(
    cls,
    data: Mapping[str, Any],
    var_repl_dict: Union[Mapping[str, Any], None] = None,
    alias: Union[str, None] = None,
):

    if not isinstance(data, Mapping):
        raise KiaraException("Job description data is not a mapping.")

    if "operation" not in data.keys():
        raise KiaraException("Missing 'operation' key")

    if var_repl_dict:
        run_data = replace_var_names_in_obj(data, repl_dict=var_repl_dict)
    else:
        run_data = data

    if alias:
        run_data["job_alias"] = alias

    return run_data

@classmethod
def create_from_data(
    cls,
    data: Mapping[str, Any],
    var_repl_dict: Union[Mapping[str, Any], None] = None,
    alias: Union[str, None] = None,
) -> "JobDesc":

    run_data = cls.parse_data(data=data, var_repl_dict=var_repl_dict, alias=alias)
    return cls(**run_data)

@root_validator(pre=True)
def validate_inputs(cls, values):

    if len(values) == 1 and "data" in values.keys():
        data = values["data"]
        if isinstance(data, str):
            if os.path.isfile(data):
                data = Path(data)

        if isinstance(data, Path):
            run_data = cls.parse_from_file(data)
            return run_data
        else:
            values = data
    return values

@validator("doc", pre=True)
def validate_doc(cls, value):
    return DocumentationMetadataModel.create(value)

def get_operation(self, kiara_api: "KiaraAPI") -> "Operation":

    if not self.module_config:
        operation = kiara_api.get_operation(self.operation, allow_external=True)
    else:
        data = {
            "module_type": self.operation,
            "module_config": self.module_config,
        }
        operation = kiara_api.get_operation(operation=data, allow_external=False)

    return operation

class KiaraModuleConfig(KiaraModel):

    """
    Base class that describes the configuration a [``KiaraModule``][kiara.module.KiaraModule] class accepts.

    This is stored in the ``_config_cls`` class attribute in each ``KiaraModule`` class.

    There are two config options every ``KiaraModule`` supports:

     - ``constants``, and
     - ``defaults``

     Constants are pre-set inputs, and users can't change them and an error is thrown if they try. Defaults are default
     values that override the schema defaults, and those can be overwritten by users. If both a constant and a default
     value is set for an input field, an error is thrown.
    """

    _kiara_model_id = "instance.module_config"

    @classmethod
    def requires_config(cls, config: Union[Mapping[str, Any], None] = None) -> bool:
        """Return whether this class can be used as-is, or requires configuration before an instance can be created."""
        for field_name, field in cls.__fields__.items():
            if field.required and field.default is None:
                if config:
                    if config.get(field_name, None) is None:
                        return True
                else:
                    return True
        return False

    _config_hash: str = PrivateAttr(default=None)
    constants: Dict[str, Any] = Field(
        default_factory=dict, description="Value constants for this module."
    )
    defaults: Dict[str, Any] = Field(
        default_factory=dict, description="Value defaults for this module."
    )

    class Config:
        extra = Extra.forbid
        validate_assignment = True

    def get(self, key: str) -> Any:
        """Get the value for the specified configuation key."""
        if key not in self.__fields__:
            raise Exception(
                f"No config value '{key}' in module config class '{self.__class__.__name__}'."
            )

        return getattr(self, key)

    def create_renderable(self, **config: Any) -> RenderableType:

        my_table = Table(box=box.MINIMAL, show_header=False)
        my_table.add_column("Field name", style="i")
        my_table.add_column("Value")
        for field in self.__fields__:
            attr = getattr(self, field)
            if isinstance(attr, str):
                attr_str = attr
            elif hasattr(attr, "create_renderable"):
                attr_str = attr.create_renderable()
            elif isinstance(attr, BaseModel):
                attr_str = attr.json(option=orjson.orjson.OPT_INDENT_2)
            else:
                attr_str = str(attr)
            my_table.add_row(field, attr_str)

        return my_table

class Config:
    extra = Extra.forbid
    validate_assignment = True

@classmethod
def requires_config(cls, config: Union[Mapping[str, Any], None] = None) -> bool:
    """Return whether this class can be used as-is, or requires configuration before an instance can be created."""
    for field_name, field in cls.__fields__.items():
        if field.required and field.default is None:
            if config:
                if config.get(field_name, None) is None:
                    return True
            else:
                return True
    return False

def get(self, key: str) -> Any:
    """Get the value for the specified configuation key."""
    if key not in self.__fields__:
        raise Exception(
            f"No config value '{key}' in module config class '{self.__class__.__name__}'."
        )

    return getattr(self, key)

def create_renderable(self, **config: Any) -> RenderableType:

    my_table = Table(box=box.MINIMAL, show_header=False)
    my_table.add_column("Field name", style="i")
    my_table.add_column("Value")
    for field in self.__fields__:
        attr = getattr(self, field)
        if isinstance(attr, str):
            attr_str = attr
        elif hasattr(attr, "create_renderable"):
            attr_str = attr.create_renderable()
        elif isinstance(attr, BaseModel):
            attr_str = attr.json(option=orjson.orjson.OPT_INDENT_2)
        else:
            attr_str = str(attr)
        my_table.add_row(field, attr_str)

    return my_table


class Kiara(object):

    """
    The core context of a kiara session.

    The `Kiara` object holds all information related to the current environment the user does works in. This includes:

      - available modules, operations & pipelines
      - available value data_types
      - available metadata schemas
      - available data items
      - available controller and processor data_types
      - misc. configuration options

    It's possible to use *kiara* without ever manually touching the 'Kiara' class, by default all relevant classes and functions
    will use a default instance of this class (available via the `Kiara.instance()` method.

    The Kiara class is highly dependent on the Python environment it lives in, because it auto-discovers available sub-classes
    of its building blocks (modules, value data_types, etc.). So, you can't assume that, for example, a pipeline you create
    will work the same way (or at all) in a different environment. *kiara* will always be able to tell you all the details
    of this environment, though, and it will attach those details to things like data, so there is always a record of
    how something was created, and in which environment.
    """

    @classmethod
    def instance(cls) -> "Kiara":
        """The default *kiara* context. In most cases, it's recommended you create and manage your own, though."""
        from kiara.interfaces.python_api import KiaraAPI

        return KiaraAPI.instance().context

    def __init__(
        self,
        config: Union[KiaraContextConfig, None] = None,
        runtime_config: Union[KiaraRuntimeConfig, None] = None,
    ) -> None:

        kc: Union[KiaraConfig, None] = None
        if not config:
            kc = KiaraConfig()
            config = kc.get_context_config()

        if not runtime_config:
            if kc is None:
                kc = KiaraConfig()
            runtime_config = kc.runtime_config

        self._id: uuid.UUID = ID_REGISTRY.generate(
            id=uuid.UUID(config.context_id), obj=self
        )
        ID_REGISTRY.update_metadata(self._id, kiara_id=self._id)
        self._config: KiaraContextConfig = config
        self._runtime_config: KiaraRuntimeConfig = runtime_config

        self._event_registry: EventRegistry = EventRegistry(kiara=self)
        self._type_registry: TypeRegistry = TypeRegistry(self)
        self._data_registry: DataRegistry = DataRegistry(kiara=self)
        self._job_registry: JobRegistry = JobRegistry(kiara=self)
        self._module_registry: ModuleRegistry = ModuleRegistry(kiara=self)
        self._operation_registry: OperationRegistry = OperationRegistry(kiara=self)

        self._kiara_model_registry: ModelRegistry = ModelRegistry.instance()

        self._alias_registry: AliasRegistry = AliasRegistry(kiara=self)
        self._destiny_registry: DestinyRegistry = DestinyRegistry(kiara=self)

        self._workflow_registry: WorkflowRegistry = WorkflowRegistry(kiara=self)

        self._render_registry = RenderRegistry(kiara=self)

        self._env_mgmt: Union[EnvironmentRegistry, None] = None

        metadata_augmenter = CreateMetadataDestinies(kiara=self)
        self._event_registry.add_listener(
            metadata_augmenter, *metadata_augmenter.supported_event_types()
        )

        self._context_info: Union[KiaraContextInfo, None] = None

        # initialize stores
        self._archive_types = find_all_archive_types()
        self._archives: Dict[str, KiaraArchive] = {}

        for archive_alias, archive in self._config.archives.items():
            archive_cls = self._archive_types.get(archive.archive_type, None)
            if archive_cls is None:
                raise Exception(
                    f"Can't create context: no archive type '{archive.archive_type}' available. Available types: {', '.join(self._archive_types.keys())}"
                )

            config_cls = archive_cls._config_cls
            archive_config = config_cls(**archive.config)
            archive_obj = archive_cls(archive_id=archive.archive_uuid, config=archive_config)  # type: ignore
            for supported_type in archive_obj.supported_item_types():
                if supported_type == "data":
                    self.data_registry.register_data_archive(
                        archive_obj, alias=archive_alias  # type: ignore
                    )
                if supported_type == "job_record":
                    self.job_registry.register_job_archive(archive_obj, alias=archive_alias)  # type: ignore

                if supported_type == "alias":
                    self.alias_registry.register_archive(archive_obj, alias=archive_alias)  # type: ignore

                if supported_type == "destiny":
                    self.destiny_registry.register_destiny_archive(archive_obj, alias=archive_alias)  # type: ignore

                if supported_type == "workflow":
                    self.workflow_registry.register_archive(archive_obj, alias=archive_alias)  # type: ignore

        if self._runtime_config.lock_context:
            self.lock_context()

    def lock_context(self):
        """Lock the context, so that it can't be used by other processes."""
        aquired = ID_REGISTRY.lock_context(self.id)

        if not aquired:
            raise KiaraContextException(
                "Can't lock context: already locked by another process.",
                context_id=self.id,
            )

        atexit.register(self.unlock_context)

    def unlock_context(self):

        ID_REGISTRY.unlock_context(self.id)

    @property
    def id(self) -> uuid.UUID:
        return self._id

    @property
    def context_config(self) -> KiaraContextConfig:
        return self._config

    @property
    def runtime_config(self) -> KiaraRuntimeConfig:
        return self._runtime_config

    def update_runtime_config(self, **settings) -> KiaraRuntimeConfig:

        for k, v in settings.items():
            setattr(self.runtime_config, k, v)

        return self.runtime_config

    @property
    def context_info(self) -> "KiaraContextInfo":

        if self._context_info is None:
            self._context_info = KiaraContextInfo.create_from_kiara_instance(kiara=self)
        return self._context_info

    # ===================================================================================================
    # registry accessors

    @property
    def environment_registry(self) -> EnvironmentRegistry:
        if self._env_mgmt is not None:
            return self._env_mgmt

        self._env_mgmt = EnvironmentRegistry.instance()
        return self._env_mgmt

    @property
    def type_registry(self) -> TypeRegistry:
        return self._type_registry

    @property
    def module_registry(self) -> ModuleRegistry:
        return self._module_registry

    @property
    def kiara_model_registry(self) -> ModelRegistry:
        return self._kiara_model_registry

    @property
    def alias_registry(self) -> AliasRegistry:
        return self._alias_registry

    @property
    def destiny_registry(self) -> DestinyRegistry:
        return self._destiny_registry

    @property
    def job_registry(self) -> JobRegistry:
        return self._job_registry

    @property
    def operation_registry(self) -> OperationRegistry:
        op_registry = self._operation_registry
        return op_registry

    @property
    def data_registry(self) -> DataRegistry:
        return self._data_registry

    @property
    def workflow_registry(self) -> WorkflowRegistry:
        return self._workflow_registry

    @property
    def event_registry(self) -> EventRegistry:
        return self._event_registry

    @property
    def render_registry(self) -> RenderRegistry:
        return self._render_registry

    # ===================================================================================================
    # context specific types & instances

    @property
    def current_environments(self) -> Mapping[str, RuntimeEnvironment]:
        return self.environment_registry.environments

    @property
    def data_type_classes(self) -> Mapping[str, Type[DataType]]:
        return self.type_registry.data_type_classes

    @property
    def data_type_names(self) -> List[str]:
        return self.type_registry.get_data_type_names(include_profiles=True)

    @property
    def module_type_classes(self) -> Mapping[str, Type["KiaraModule"]]:
        return self._module_registry.module_types

    @property
    def module_type_names(self) -> Iterable[str]:
        return self._module_registry.get_module_type_names()

    # ===================================================================================================
    # kiara session API methods

    def create_manifest(
        self, module_or_operation: str, config: Union[Mapping[str, Any], None] = None
    ) -> Manifest:

        if config is None:
            config = {}

        if module_or_operation in self.module_type_names:

            manifest: Manifest = Manifest(
                module_type=module_or_operation, module_config=config
            )

        elif module_or_operation in self.operation_registry.operation_ids:

            if config:
                raise Exception(
                    f"Specified run target '{module_or_operation}' is an operation, additional module configuration is not allowed (yet)."
                )
            manifest = self.operation_registry.get_operation(module_or_operation)

        elif os.path.isfile(module_or_operation):
            raise NotImplementedError()

        else:
            raise Exception(
                f"Can't assemble operation, invalid operation/module name: {module_or_operation}. Must be registered module or operation name, or file."
            )

        return manifest

    # def create_module(self, manifest: Union[Manifest, str]) -> "KiaraModule":
    #     """Create a [KiaraModule][kiara.module.KiaraModule] object from a module configuration.
    #
    #     Arguments:
    #         manifest: the module configuration
    #     """
    #
    #     return self._module_registry.create_module(manifest=manifest)

    def queue(
        self, manifest: Manifest, inputs: Mapping[str, Any], wait: bool = False
    ) -> uuid.UUID:
        """
        Queue a job with the specified manifest and inputs.

        Arguments:
        ---------
           manifest: the job manifest
           inputs: the job inputs
           wait: whether to wait for the job to be finished before returning

        Returns:
        -------
            the job id that can be used to look up job status & results
        """
        return self.job_registry.execute(manifest=manifest, inputs=inputs, wait=wait)

    def process(self, manifest: Manifest, inputs: Mapping[str, Any]) -> ValueMap:
        """
        Queue a job with the specified manifest and inputs.

        Arguments:
        ---------
           manifest: the job manifest
           inputs: the job inputs
           wait: whether to wait for the job to be finished before returning

        Returns:
        -------
        """
        return self.job_registry.execute_and_retrieve(manifest=manifest, inputs=inputs)

    def save_values(
        self, values: ValueMap, alias_map: Mapping[str, Iterable[str]]
    ) -> StoreValuesResult:

        _values = {}
        for field_name in values.field_names:
            value = values.get_value_obj(field_name)
            _values[field_name] = value
            self.data_registry.store_value(value=value)
        stored = {}
        for field_name, field_aliases in alias_map.items():

            value = _values[field_name]
            try:
                if field_aliases:
                    self.alias_registry.register_aliases(value.value_id, *field_aliases)

                stored[field_name] = StoreValueResult(
                    value=value,
                    aliases=sorted(field_aliases),
                    error=None,
                    persisted_data=None,
                )

            except Exception as e:
                log_exception(e)
                stored[field_name] = StoreValueResult(
                    value=value,
                    aliases=sorted(field_aliases),
                    error=str(e),
                    persisted_data=None,
                )

        return StoreValuesResult.construct(__root__=stored)

    def create_context_summary(self) -> ContextInfo:
        return ContextInfo.create_from_context(kiara=self)

    def get_all_archives(self) -> Dict[KiaraArchive, Set[str]]:

        result: Dict[KiaraArchive, Set[str]] = {}

        archive: KiaraArchive
        for alias, archive in self.data_registry.data_archives.items():
            result.setdefault(archive, set()).add(alias)
        for alias, archive in self.alias_registry.alias_archives.items():
            result.setdefault(archive, set()).add(alias)
        for alias, archive in self.destiny_registry.destiny_archives.items():
            result.setdefault(archive, set()).add(alias)
        for alias, archive in self.job_registry.job_archives.items():
            result.setdefault(archive, set()).add(alias)
        for alias, archive in self.workflow_registry.workflow_archives.items():
            result.setdefault(archive, set()).add(alias)

        return result

Create network graph

%env CONSOLE_WIDTH=140

from kiara.interfaces.python_api.workflow import Workflow
from kiara.utils.jupyter import graph_to_image
from kiara.utils.cli import terminal_print_model

doc = """Onboard network data"""
workflow = Workflow.create("create_network_graph", doc=doc, replace_existing_alias=True)

# Creating step: import_edges_file
workflow.add_step(operation="import.file", step_id="import_edges_file")

# Creating step: import_nodes_file
workflow.add_step(operation="import.file", step_id="import_nodes_file")

# Creating step: create_edges_table
step_create_edges_table_config = {'constants': {}, 'defaults': {}, 'source_type': 'csv_file', 'target_type': 'table', 'ignore_errors': False}
workflow.add_step(
    operation="create.table",
    module_config=step_create_edges_table_config,
    step_id="create_edges_table")

# Connecting input(s) of step 'create_edges_table'
workflow.connect_fields("create_edges_table.csv_file", "import_edges_file.file")

# Creating step: create_nodes_table
step_create_nodes_table_config = {'constants': {}, 'defaults': {}, 'source_type': 'csv_file', 'target_type': 'table', 'ignore_errors': False}
workflow.add_step(
    operation="create.table",
    module_config=step_create_nodes_table_config,
    step_id="create_nodes_table")

# Connecting input(s) of step 'create_nodes_table'
workflow.connect_fields("create_nodes_table.csv_file", "import_nodes_file.file")

# Creating step: assemble_network_data
workflow.add_step(operation="create.network_data.from.tables", step_id="assemble_network_data")


# Connecting input(s) of step 'assemble_network_data'
workflow.connect_fields("assemble_network_data.edges", "create_edges_table.table")
workflow.connect_fields("assemble_network_data.nodes", "create_nodes_table.table")

Setting workflow input/output names (optional)

To make our workflow nicer to use, we can set aliases for its inputs and outputs.

workflow.set_input_alias(input_field="import_edges_file.path", alias="edges_file")
workflow.set_input_alias(input_field="import_nodes_file.path", alias="nodes_file")
workflow.set_input_alias(input_field="assemble_network_data.source_column_name", alias="source_column_name")
workflow.set_input_alias(input_field="assemble_network_data.target_column_name", alias="target_column_name")
workflow.set_input_alias(input_field="assemble_network_data.edges_column_map", alias="edges_column_map")
workflow.set_input_alias(input_field="assemble_network_data.id_column_name", alias="id_column_name")
workflow.set_input_alias(input_field="assemble_network_data.label_column_name", alias="label_column_name")
workflow.set_input_alias(input_field="assemble_network_data.nodes_column_map", alias="nodes_column_map")


workflow.set_output_alias(output_field="assemble_network_data.network_data", alias="network_data")

Workflow information

After our workflow is wired up, we look can look at its structure, and other properties.
Workflow status

A workflow consists of a series of 'states', the most relevant is always the most recent one. We can investigate that latest states details like so:

workflow.current_state


graph_to_image(workflow.pipeline.execution_graph)


Workflow inputs

Once a workflow has an assembled pipeline, we can set it's inputs. We use the input field names that we got from the result of the workflow.current_state call.

workflow.set_input("edges_file", "/home/markus/projects/kiara/dev/kiara.examples/examples/pipelines/network_analysis/../../data/journals/JournalEdges1902.csv")
workflow.set_input("nodes_file", "/home/markus/projects/kiara/dev/kiara.examples/examples/pipelines/network_analysis/../../data/journals/JournalNodes1902.csv")
workflow.set_input("source_column_name", "Source")
workflow.set_input("target_column_name", "Target")
workflow.set_input("edges_column_map", None)
workflow.set_input("id_column_name", "Id")
workflow.set_input("label_column_name", "Label")
workflow.set_input("nodes_column_map", None)


# process all workflow steps that can be processed
workflow.process_steps()

# print the current state, after we set our inputs
workflow.current_state

workflow.current_output_values



Workflow snapshot

So far, our workflow only exists in memory. If we want to save it so we can have a look at it again at a later stage, we can snapshot the current state, which will save the current structure of the internal pipeline, as well as all inputs that are currently used. In addition, this will register the workflow under the alias we specified on top of this file when creating the Workflow object (in our case: create_network_graph).

If we would not not specify save=True, the structure of the pipeline and inputs would still be frozen and kept, but only in memory, and we'd only be able to access it in our current session.

workflow.snapshot(save=True)




Now, we can access our workflow in other environments, for example from the commandline:

! kiara workflow list

! kiara workflow explain create_network_graph


# ---
# jupyter:
#   jupytext:
#     cell_markers: region,endregion
#     formats: ipynb,.pct.py:percent,.lgt.py:light,.spx.py:sphinx,md,Rmd,.pandoc.md:pandoc
#     text_representation:
#       extension: .py
#       format_name: light
#       format_version: '1.5'
#       jupytext_version: 1.14.1
#   kernelspec:
#     display_name: Python 3 (ipykernel)
#     language: python
#     name: python3
# ---

# region
# %env CONSOLE_WIDTH=140

from kiara.interfaces.python_api.workflow import Workflow
from kiara.utils.graphs import graph_to_image
from kiara.utils.cli import terminal_print_model
# endregion


# # Creating the workflow object <a class="anchor" id="create_workflow_obj"></a>
#
# As the first step we create a [`Workflow`](https://dharpa.org/kiara/latest/reference/kiara/interfaces/python_api/workflow/) object, which is a convenience class that manages workflow state, internal consistency and history for us:

doc = """Onboard network data"""
workflow = Workflow.create("create_network_graph", doc=doc, replace_existing_alias=True)

# # Assembling the workflow <a class="anchor" id="assembly"></a>
#
# The first step in the creation of our workflow is to create the individual steps from the available *kiara* modules.
#
# A list of available modules and their aliases can be found here: TODO
#
# ## Creating the steps of the workflow <a class="anchor" id="creating_steps"></a>

# Creating step: import_edges_file
workflow.add_step(operation="import.file", step_id="import_edges_file")
# Creating step: import_nodes_file
workflow.add_step(operation="import.file", step_id="import_nodes_file")
# Creating step: create_edges_table
step_create_edges_table_config = {'constants': {}, 'defaults': {}, 'source_type': 'csv_file', 'target_type': 'table', 'ignore_errors': False}
workflow.add_step(
    operation="create.table",
    module_config=step_create_edges_table_config,
    step_id="create_edges_table")
# Connecting input(s) of step 'create_edges_table'
workflow.connect_fields("create_edges_table.csv_file", "import_edges_file.file")
# Creating step: create_nodes_table
step_create_nodes_table_config = {'constants': {}, 'defaults': {}, 'source_type': 'csv_file', 'target_type': 'table', 'ignore_errors': False}
workflow.add_step(
    operation="create.table",
    module_config=step_create_nodes_table_config,
    step_id="create_nodes_table")
# Connecting input(s) of step 'create_nodes_table'
workflow.connect_fields("create_nodes_table.csv_file", "import_nodes_file.file")
# Creating step: assemble_network_data
workflow.add_step(operation="create.network_data.from.tables", step_id="assemble_network_data")
# Connecting input(s) of step 'assemble_network_data'
workflow.connect_fields("assemble_network_data.edges", "create_edges_table.table")
workflow.connect_fields("assemble_network_data.nodes", "create_nodes_table.table")
# ## Setting workflow input/output names (optional)
#
# To make our workflow nicer to use, we can set aliases for its inputs and outputs.
# region
workflow.set_input_alias(input_field="import_edges_file.path", alias="edges_file")
workflow.set_input_alias(input_field="import_nodes_file.path", alias="nodes_file")
workflow.set_input_alias(input_field="assemble_network_data.source_column_name", alias="source_column_name")
workflow.set_input_alias(input_field="assemble_network_data.target_column_name", alias="target_column_name")
workflow.set_input_alias(input_field="assemble_network_data.edges_column_map", alias="edges_column_map")
workflow.set_input_alias(input_field="assemble_network_data.id_column_name", alias="id_column_name")
workflow.set_input_alias(input_field="assemble_network_data.label_column_name", alias="label_column_name")
workflow.set_input_alias(input_field="assemble_network_data.nodes_column_map", alias="nodes_column_map")


workflow.set_output_alias(output_field="assemble_network_data.network_data", alias="network_data")
# endregion
# # Workflow information <a class="anchor" id="pipeline_info"></a>
#
# After our workflow is wired up, we look can look at its structure, and other properties.

#
# ## Workflow status
#
# A workflow consists of a series of 'states', the most relevant is always the most recent one. We can investigate
# that latest states details like so:

workflow.current_state

# ## Pipeline execution graph
#
# Let's look at the current execution graph for the current workflow pipeline:

graph_to_image(workflow.pipeline.execution_graph)

# # Workflow inputs <a class="anchor" id="pipeline_inputs"></a>
#
# Once a workflow has an assembled pipeline, we can set it's inputs. We use the input field
# names that we got from the result of the `workflow.current_state` call.

# region
workflow.set_input("edges_file", "/home/markus/projects/kiara/dev/kiara.examples/examples/pipelines/network_analysis/../../data/journals/JournalEdges1902.csv")
workflow.set_input("nodes_file", "/home/markus/projects/kiara/dev/kiara.examples/examples/pipelines/network_analysis/../../data/journals/JournalNodes1902.csv")
workflow.set_input("source_column_name", "Source")
workflow.set_input("target_column_name", "Target")
workflow.set_input("edges_column_map", None)
workflow.set_input("id_column_name", "Id")
workflow.set_input("label_column_name", "Label")
workflow.set_input("nodes_column_map", None)


# process all workflow steps that can be processed
workflow.process_steps()

# print the current state, after we set our inputs
workflow.current_state
# endregion

# # Workflow outputs <a class="anchor" id="pipeline_outputs"></a>
#
# To print the actual data of the workflows' current outputs, we call the `current_output_values` property of the workflow object:

workflow.current_output_values

# # Workflow snapshot <a class="anchor" id="snapshot"></a>
#
# So far, our workflow only exists in memory. If we want to save it so we can have a look at it again at a later stage, we can snapshot the current state, which will save the current structure of the internal pipeline, as well as all inputs that are currently used. In addition, this will register the workflow under the alias we specified on top of this file when creating the `Workflow` object (in our case: `create_network_graph`).
#
# If we would not not specify `save=True`, the structure of the pipeline and inputs would still be frozen and kept, but only in memory, and we'd only be able to access it in our current session.

workflow.snapshot(save=True)

# Now, we can access our workflow in other environments, for example from the commandline:

# ! kiara workflow list

# ! kiara workflow explain create_network_graph

# ---
# jupyter:
#   jupytext:
#     cell_markers: region,endregion
#     formats: ipynb,.pct.py:percent,.lgt.py:light,.spx.py:sphinx,md,Rmd,.pandoc.md:pandoc
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.14.1
#   kernelspec:
#     display_name: Python 3 (ipykernel)
#     language: python
#     name: python3
# ---

# %%
# %env CONSOLE_WIDTH=140

from kiara.interfaces.python_api.workflow import Workflow
from kiara.utils.jupyter import graph_to_image
from kiara.utils.cli import terminal_print_model


# %% [markdown]
# # Creating the workflow object <a class="anchor" id="create_workflow_obj"></a>
#
# As the first step we create a [`Workflow`](https://dharpa.org/kiara/latest/reference/kiara/interfaces/python_api/workflow/) object, which is a convenience class that manages workflow state, internal consistency and history for us:

# %%
doc = """Onboard network data"""
workflow = Workflow.create("create_network_graph", doc=doc, replace_existing_alias=True)

# %% [markdown]
# # Assembling the workflow <a class="anchor" id="assembly"></a>
#
# The first step in the creation of our workflow is to create the individual steps from the available *kiara* modules.
#
# A list of available modules and their aliases can be found here: TODO
#
# ## Creating the steps of the workflow <a class="anchor" id="creating_steps"></a>

# %%
# Creating step: import_edges_file
workflow.add_step(operation="import.file", step_id="import_edges_file")
# %%
# Creating step: import_nodes_file
workflow.add_step(operation="import.file", step_id="import_nodes_file")
# %%
# Creating step: create_edges_table
step_create_edges_table_config = {'constants': {}, 'defaults': {}, 'source_type': 'csv_file', 'target_type': 'table', 'ignore_errors': False}
workflow.add_step(
    operation="create.table",
    module_config=step_create_edges_table_config,
    step_id="create_edges_table")
# %%
# Connecting input(s) of step 'create_edges_table'
workflow.connect_fields("create_edges_table.csv_file", "import_edges_file.file")
# %%
# Creating step: create_nodes_table
step_create_nodes_table_config = {'constants': {}, 'defaults': {}, 'source_type': 'csv_file', 'target_type': 'table', 'ignore_errors': False}
workflow.add_step(
    operation="create.table",
    module_config=step_create_nodes_table_config,
    step_id="create_nodes_table")
# %%
# Connecting input(s) of step 'create_nodes_table'
workflow.connect_fields("create_nodes_table.csv_file", "import_nodes_file.file")
# %%
# Creating step: assemble_network_data
workflow.add_step(operation="create.network_data.from.tables", step_id="assemble_network_data")
# %%
# Connecting input(s) of step 'assemble_network_data'
workflow.connect_fields("assemble_network_data.edges", "create_edges_table.table")
workflow.connect_fields("assemble_network_data.nodes", "create_nodes_table.table")
# %% [markdown]
# ## Setting workflow input/output names (optional)
#
# To make our workflow nicer to use, we can set aliases for its inputs and outputs.
# %%
workflow.set_input_alias(input_field="import_edges_file.path", alias="edges_file")
workflow.set_input_alias(input_field="import_nodes_file.path", alias="nodes_file")
workflow.set_input_alias(input_field="assemble_network_data.source_column_name", alias="source_column_name")
workflow.set_input_alias(input_field="assemble_network_data.target_column_name", alias="target_column_name")
workflow.set_input_alias(input_field="assemble_network_data.edges_column_map", alias="edges_column_map")
workflow.set_input_alias(input_field="assemble_network_data.id_column_name", alias="id_column_name")
workflow.set_input_alias(input_field="assemble_network_data.label_column_name", alias="label_column_name")
workflow.set_input_alias(input_field="assemble_network_data.nodes_column_map", alias="nodes_column_map")


workflow.set_output_alias(output_field="assemble_network_data.network_data", alias="network_data")
# %% [markdown]
# # Workflow information <a class="anchor" id="pipeline_info"></a>
#
# After our workflow is wired up, we look can look at its structure, and other properties.

# %% [markdown]
#
# ## Workflow status
#
# A workflow consists of a series of 'states', the most relevant is always the most recent one. We can investigate
# that latest states details like so:

# %%
workflow.current_state

# %% [markdown]
# ## Pipeline execution graph
#
# Let's look at the current execution graph for the current workflow pipeline:

# %%
graph_to_image(workflow.pipeline.execution_graph)

# %% [markdown]
# # Workflow inputs <a class="anchor" id="pipeline_inputs"></a>
#
# Once a workflow has an assembled pipeline, we can set it's inputs. We use the input field
# names that we got from the result of the `workflow.current_state` call.

# %%
workflow.set_input("edges_file", "/home/markus/projects/kiara/dev/kiara.examples/examples/pipelines/network_analysis/../../data/journals/JournalEdges1902.csv")
workflow.set_input("nodes_file", "/home/markus/projects/kiara/dev/kiara.examples/examples/pipelines/network_analysis/../../data/journals/JournalNodes1902.csv")
workflow.set_input("source_column_name", "Source")
workflow.set_input("target_column_name", "Target")
workflow.set_input("edges_column_map", None)
workflow.set_input("id_column_name", "Id")
workflow.set_input("label_column_name", "Label")
workflow.set_input("nodes_column_map", None)


# process all workflow steps that can be processed
workflow.process_steps()

# print the current state, after we set our inputs
workflow.current_state

# %% [markdown]
# # Workflow outputs <a class="anchor" id="pipeline_outputs"></a>
#
# To print the actual data of the workflows' current outputs, we call the `current_output_values` property of the workflow object:

# %%
workflow.current_output_values

# %% [markdown]
# # Workflow snapshot <a class="anchor" id="snapshot"></a>
#
# So far, our workflow only exists in memory. If we want to save it so we can have a look at it again at a later stage, we can snapshot the current state, which will save the current structure of the internal pipeline, as well as all inputs that are currently used. In addition, this will register the workflow under the alias we specified on top of this file when creating the `Workflow` object (in our case: `create_network_graph`).
#
# If we would not not specify `save=True`, the structure of the pipeline and inputs would still be frozen and kept, but only in memory, and we'd only be able to access it in our current session.

# %%
workflow.snapshot(save=True)

# %% [markdown]
# Now, we can access our workflow in other environments, for example from the commandline:

# %%
# ! kiara workflow list

# %%
# ! kiara workflow explain create_network_graph

# ---
# jupyter:
#   jupytext:
#     cell_markers: region,endregion
#     formats: ipynb,.pct.py:percent,.lgt.py:light,.spx.py:sphinx,md,Rmd,.pandoc.md:pandoc
#     text_representation:
#       extension: .py
#       format_name: sphinx
#       format_version: '1.1'
#       jupytext_version: 1.14.1
#   kernelspec:
#     display_name: Python 3 (ipykernel)
#     language: python
#     name: python3
# ---

# %env CONSOLE_WIDTH=140

from kiara.interfaces.python_api.workflow import Workflow
from kiara.utils.jupyter import graph_to_image
from kiara.utils.cli import terminal_print_model


"""
# Creating the workflow object <a class="anchor" id="create_workflow_obj"></a>

As the first step we create a [`Workflow`](https://dharpa.org/kiara/latest/reference/kiara/interfaces/python_api/workflow/) object, which is a convenience class that manages workflow state, internal consistency and history for us:
"""

doc = """Onboard network data"""
workflow = Workflow.create("create_network_graph", doc=doc, replace_existing_alias=True)

###############################################################################
# # Assembling the workflow <a class="anchor" id="assembly"></a>
#
# The first step in the creation of our workflow is to create the individual steps from the available *kiara* modules.
#
# A list of available modules and their aliases can be found here: TODO
#
# ## Creating the steps of the workflow <a class="anchor" id="creating_steps"></a>

# Creating step: import_edges_file
workflow.add_step(operation="import.file", step_id="import_edges_file")
""
# Creating step: import_nodes_file
workflow.add_step(operation="import.file", step_id="import_nodes_file")
""
# Creating step: create_edges_table
step_create_edges_table_config = {'constants': {}, 'defaults': {}, 'source_type': 'csv_file', 'target_type': 'table', 'ignore_errors': False}
workflow.add_step(
    operation="create.table",
    module_config=step_create_edges_table_config,
    step_id="create_edges_table")
""
# Connecting input(s) of step 'create_edges_table'
workflow.connect_fields("create_edges_table.csv_file", "import_edges_file.file")
""
# Creating step: create_nodes_table
step_create_nodes_table_config = {'constants': {}, 'defaults': {}, 'source_type': 'csv_file', 'target_type': 'table', 'ignore_errors': False}
workflow.add_step(
    operation="create.table",
    module_config=step_create_nodes_table_config,
    step_id="create_nodes_table")
""
# Connecting input(s) of step 'create_nodes_table'
workflow.connect_fields("create_nodes_table.csv_file", "import_nodes_file.file")
""
# Creating step: assemble_network_data
workflow.add_step(operation="create.network_data.from.tables", step_id="assemble_network_data")
""
# Connecting input(s) of step 'assemble_network_data'
workflow.connect_fields("assemble_network_data.edges", "create_edges_table.table")
workflow.connect_fields("assemble_network_data.nodes", "create_nodes_table.table")
###############################################################################
# ## Setting workflow input/output names (optional)
#
# To make our workflow nicer to use, we can set aliases for its inputs and outputs.
workflow.set_input_alias(input_field="import_edges_file.path", alias="edges_file")
workflow.set_input_alias(input_field="import_nodes_file.path", alias="nodes_file")
workflow.set_input_alias(input_field="assemble_network_data.source_column_name", alias="source_column_name")
workflow.set_input_alias(input_field="assemble_network_data.target_column_name", alias="target_column_name")
workflow.set_input_alias(input_field="assemble_network_data.edges_column_map", alias="edges_column_map")
workflow.set_input_alias(input_field="assemble_network_data.id_column_name", alias="id_column_name")
workflow.set_input_alias(input_field="assemble_network_data.label_column_name", alias="label_column_name")
workflow.set_input_alias(input_field="assemble_network_data.nodes_column_map", alias="nodes_column_map")


workflow.set_output_alias(output_field="assemble_network_data.network_data", alias="network_data")
###############################################################################
# # Workflow information <a class="anchor" id="pipeline_info"></a>
#
# After our workflow is wired up, we look can look at its structure, and other properties.

###############################################################################
#
# ## Workflow status
#
# A workflow consists of a series of 'states', the most relevant is always the most recent one. We can investigate
# that latest states details like so:

workflow.current_state

###############################################################################
# ## Pipeline execution graph
#
# Let's look at the current execution graph for the current workflow pipeline:

graph_to_image(workflow.pipeline.execution_graph)

###############################################################################
# # Workflow inputs <a class="anchor" id="pipeline_inputs"></a>
#
# Once a workflow has an assembled pipeline, we can set it's inputs. We use the input field
# names that we got from the result of the `workflow.current_state` call.

workflow.set_input("edges_file", "/home/markus/projects/kiara/dev/kiara.examples/examples/pipelines/network_analysis/../../data/journals/JournalEdges1902.csv")
workflow.set_input("nodes_file", "/home/markus/projects/kiara/dev/kiara.examples/examples/pipelines/network_analysis/../../data/journals/JournalNodes1902.csv")
workflow.set_input("source_column_name", "Source")
workflow.set_input("target_column_name", "Target")
workflow.set_input("edges_column_map", None)
workflow.set_input("id_column_name", "Id")
workflow.set_input("label_column_name", "Label")
workflow.set_input("nodes_column_map", None)


# process all workflow steps that can be processed
workflow.process_steps()

# print the current state, after we set our inputs
workflow.current_state

###############################################################################
# # Workflow outputs <a class="anchor" id="pipeline_outputs"></a>
#
# To print the actual data of the workflows' current outputs, we call the `current_output_values` property of the workflow object:

workflow.current_output_values

###############################################################################
# # Workflow snapshot <a class="anchor" id="snapshot"></a>
#
# So far, our workflow only exists in memory. If we want to save it so we can have a look at it again at a later stage, we can snapshot the current state, which will save the current structure of the internal pipeline, as well as all inputs that are currently used. In addition, this will register the workflow under the alias we specified on top of this file when creating the `Workflow` object (in our case: `create_network_graph`).
#
# If we would not not specify `save=True`, the structure of the pipeline and inputs would still be frozen and kept, but only in memory, and we'd only be able to access it in our current session.

workflow.snapshot(save=True)

###############################################################################
# Now, we can access our workflow in other environments, for example from the commandline:

# ! kiara workflow list

""
# ! kiara workflow explain create_network_graph
# ---
# jupyter:
#   jupytext:
#     cell_markers: region,endregion
#     formats: ipynb,.pct.py:percent,.lgt.py:light,.spx.py:sphinx,md,Rmd,.pandoc.md:pandoc
#     text_representation:
#       extension: .py
#       format_name: light
#       format_version: '1.5'
#       jupytext_version: 1.14.1
#   kernelspec:
#     display_name: Python 3 (ipykernel)
#     language: python
#     name: python3
# ---

# region
# %env CONSOLE_WIDTH=140

from kiara.interfaces.python_api.workflow import Workflow
from kiara.utils.jupyter import graph_to_image
from kiara.utils.cli import terminal_print_model
# endregion


# # Creating the workflow object <a class="anchor" id="create_workflow_obj"></a>
#
# As the first step we create a [`Workflow`](https://dharpa.org/kiara/latest/reference/kiara/interfaces/python_api/workflow/) object, which is a convenience class that manages workflow state, internal consistency and history for us:

doc = """Example topic-modeling end-to-end workflow."""
workflow = Workflow.create("topic_modeling", doc=doc, replace_existing_alias=True)

# # Assembling the workflow <a class="anchor" id="assembly"></a>
#
# The first step in the creation of our workflow is to create the individual steps from the available *kiara* modules.
#
# A list of available modules and their aliases can be found here: TODO
#
# ## Creating the steps of the workflow <a class="anchor" id="creating_steps"></a>

# Creating step: import_text_corpus
workflow.add_step(operation="import.file_bundle", step_id="import_text_corpus")
# Creating step: create_stopwords_list
workflow.add_step(operation="create.stopwords_list", step_id="create_stopwords_list")
# Creating step: create_text_corpus
step_create_text_corpus_config = {'constants': {}, 'defaults': {}, 'source_type': 'text_file_bundle', 'target_type': 'table', 'ignore_errors': False}
workflow.add_step(
    operation="create.table",
    module_config=step_create_text_corpus_config,
    step_id="create_text_corpus")
# Connecting input(s) of step 'create_text_corpus'
workflow.connect_fields("create_text_corpus.text_file_bundle", "import_text_corpus.file_bundle")
# Creating step: extract_texts_column
workflow.add_step(operation="table.cut_column", step_id="extract_texts_column")
# Connecting input(s) of step 'extract_texts_column'
workflow.connect_fields("extract_texts_column.table", "create_text_corpus.table")
# Creating step: extract_filename_column
workflow.add_step(operation="table.cut_column", step_id="extract_filename_column")
# Connecting input(s) of step 'extract_filename_column'
workflow.connect_fields("extract_filename_column.table", "create_text_corpus.table")
# Creating step: create_date_array
workflow.add_step(operation="parse.date_array", step_id="create_date_array")
# Connecting input(s) of step 'create_date_array'
workflow.connect_fields("create_date_array.array", "extract_filename_column.array")
# Creating step: tokenize_content
workflow.add_step(operation="tokenize.texts_array", step_id="tokenize_content")
# Connecting input(s) of step 'tokenize_content'
workflow.connect_fields("tokenize_content.texts_array", "extract_texts_column.array")
# Creating step: preprocess_corpus
workflow.add_step(operation="preprocess.tokens_array", step_id="preprocess_corpus")
# Connecting input(s) of step 'preprocess_corpus'
workflow.connect_fields("preprocess_corpus.tokens_array", "tokenize_content.tokens_array")
workflow.connect_fields("preprocess_corpus.remove_stopwords", "create_stopwords_list.stopwords_list")
# Creating step: generate_lda
workflow.add_step(operation="generate.LDA.for.tokens_array", step_id="generate_lda")
# Connecting input(s) of step 'generate_lda'
workflow.connect_fields("generate_lda.tokens_array", "preprocess_corpus.tokens_array")
# ## Setting workflow input/output names (optional)
#
# To make our workflow nicer to use, we can set aliases for its inputs and outputs.
# region
workflow.set_input_alias(input_field="extract_texts_column.column_name", alias="content_column_name")
workflow.set_input_alias(input_field="extract_filename_column.column_name", alias="filename_column_name")
workflow.set_input_alias(input_field="import_text_corpus.path", alias="text_corpus_folder_path")
workflow.set_input_alias(input_field="create_date_array.min_index", alias="date_parse_min")
workflow.set_input_alias(input_field="create_date_array.max_index", alias="date_parse_max")
workflow.set_input_alias(input_field="create_date_array.force_non_null", alias="date_force_non_null")
workflow.set_input_alias(input_field="create_date_array.remove_tokens", alias="date_remove_tokensl")
workflow.set_input_alias(input_field="tokenize_content.tokenize_by_word", alias="tokenize_by_word")
workflow.set_input_alias(input_field="generate_lda.num_topics_min", alias="num_topics_min")
workflow.set_input_alias(input_field="generate_lda.num_topics_max", alias="num_topics_max")
workflow.set_input_alias(input_field="generate_lda.compute_coherence", alias="compute_coherence")
workflow.set_input_alias(input_field="generate_lda.words_per_topic", alias="words_per_topic")
workflow.set_input_alias(input_field="create_stopwords_list.languages", alias="languages")
workflow.set_input_alias(input_field="create_stopwords_list.stopword_lists", alias="stopword_lists")
workflow.set_input_alias(input_field="preprocess_corpus.to_lowercase", alias="to_lowercase")
workflow.set_input_alias(input_field="preprocess_corpus.remove_alphanumeric", alias="remove_alphanumeric")
workflow.set_input_alias(input_field="preprocess_corpus.remove_non_alpha", alias="remove_non_alpha")
workflow.set_input_alias(input_field="preprocess_corpus.remove_all_numeric", alias="remove_all_numeric")
workflow.set_input_alias(input_field="preprocess_corpus.remove_short_tokens", alias="remove_short_tokens")
workflow.set_input_alias(input_field="preprocess_corpus.remove_stopwords", alias="remove_stopwords")


workflow.set_output_alias(output_field="import_text_corpus.file_bundle", alias="text_corpus_file_bundle")
workflow.set_output_alias(output_field="create_text_corpus.table", alias="text_corpus_table")
workflow.set_output_alias(output_field="extract_texts_column.array", alias="content_array")
workflow.set_output_alias(output_field="tokenize_content.tokens_array", alias="tokenized_corpus")
workflow.set_output_alias(output_field="preprocess_corpus.tokens_array", alias="preprocessed_corpus")
workflow.set_output_alias(output_field="generate_lda.topic_models", alias="topic_models")
workflow.set_output_alias(output_field="generate_lda.coherence_map", alias="coherence_map")
workflow.set_output_alias(output_field="generate_lda.coherence_table", alias="coherence_table")
workflow.set_output_alias(output_field="create_date_array.date_array", alias="date_array")
# endregion
# # Workflow information <a class="anchor" id="pipeline_info"></a>
#
# After our workflow is wired up, we look can look at its structure, and other properties.

#
# ## Workflow status
#
# A workflow consists of a series of 'states', the most relevant is always the most recent one. We can investigate
# that latest states details like so:

workflow.current_state

# ## Pipeline execution graph
#
# Let's look at the current execution graph for the current workflow pipeline:

graph_to_image(workflow.pipeline.execution_graph)

# # Workflow inputs <a class="anchor" id="pipeline_inputs"></a>
#
# Once a workflow has an assembled pipeline, we can set it's inputs. We use the input field
# names that we got from the result of the `workflow.current_state` call.

# region
workflow.set_input("text_corpus_folder_path", "/home/markus/projects/kiara/dev/kiara.examples/examples/pipelines/topic_modeling/../../data/text_corpus/data")
workflow.set_input("content_column_name", "content")
workflow.set_input("filename_column_name", "file_name")
workflow.set_input("date_force_non_null", None)
workflow.set_input("date_parse_min", 11)
workflow.set_input("date_parse_max", 21)
workflow.set_input("date_remove_tokensl", None)
workflow.set_input("tokenize_by_word", None)
workflow.set_input("languages", ['italian'])
workflow.set_input("stopword_lists", [])
workflow.set_input("to_lowercase", None)
workflow.set_input("remove_alphanumeric", None)
workflow.set_input("remove_non_alpha", None)
workflow.set_input("remove_all_numeric", None)
workflow.set_input("remove_short_tokens", None)
workflow.set_input("num_topics_min", 7)
workflow.set_input("num_topics_max", 9)
workflow.set_input("compute_coherence", True)
workflow.set_input("words_per_topic", None)


# process all workflow steps that can be processed
workflow.process_steps()

# print the current state, after we set our inputs
workflow.current_state
# endregion

# # Workflow outputs <a class="anchor" id="pipeline_outputs"></a>
#
# To print the actual data of the workflows' current outputs, we call the `current_output_values` property of the workflow object:

workflow.current_output_values

# # Workflow snapshot <a class="anchor" id="snapshot"></a>
#
# So far, our workflow only exists in memory. If we want to save it so we can have a look at it again at a later stage, we can snapshot the current state, which will save the current structure of the internal pipeline, as well as all inputs that are currently used. In addition, this will register the workflow under the alias we specified on top of this file when creating the `Workflow` object (in our case: `topic_modeling`).
#
# If we would not not specify `save=True`, the structure of the pipeline and inputs would still be frozen and kept, but only in memory, and we'd only be able to access it in our current session.

workflow.snapshot(save=True)

# Now, we can access our workflow in other environments, for example from the commandline:

# ! kiara workflow list

# ! kiara workflow explain topic_modeling

# ---
# jupyter:
#   jupytext:
#     cell_markers: region,endregion
#     formats: ipynb,.pct.py:percent,.lgt.py:light,.spx.py:sphinx,md,Rmd,.pandoc.md:pandoc
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.14.1
#   kernelspec:
#     display_name: Python 3 (ipykernel)
#     language: python
#     name: python3
# ---

# %%
# %env CONSOLE_WIDTH=140

from kiara.interfaces.python_api.workflow import Workflow
from kiara.utils.jupyter import graph_to_image
from kiara.utils.cli import terminal_print_model


# %% [markdown]
# # Creating the workflow object <a class="anchor" id="create_workflow_obj"></a>
#
# As the first step we create a [`Workflow`](https://dharpa.org/kiara/latest/reference/kiara/interfaces/python_api/workflow/) object, which is a convenience class that manages workflow state, internal consistency and history for us:

# %%
doc = """Example topic-modeling end-to-end workflow."""
workflow = Workflow.create("topic_modeling", doc=doc, replace_existing_alias=True)

# %% [markdown]
# # Assembling the workflow <a class="anchor" id="assembly"></a>
#
# The first step in the creation of our workflow is to create the individual steps from the available *kiara* modules.
#
# A list of available modules and their aliases can be found here: TODO
#
# ## Creating the steps of the workflow <a class="anchor" id="creating_steps"></a>

# %%
# Creating step: import_text_corpus
workflow.add_step(operation="import.file_bundle", step_id="import_text_corpus")
# %%
# Creating step: create_stopwords_list
workflow.add_step(operation="create.stopwords_list", step_id="create_stopwords_list")
# %%
# Creating step: create_text_corpus
step_create_text_corpus_config = {'constants': {}, 'defaults': {}, 'source_type': 'text_file_bundle', 'target_type': 'table', 'ignore_errors': False}
workflow.add_step(
    operation="create.table",
    module_config=step_create_text_corpus_config,
    step_id="create_text_corpus")
# %%
# Connecting input(s) of step 'create_text_corpus'
workflow.connect_fields("create_text_corpus.text_file_bundle", "import_text_corpus.file_bundle")
# %%
# Creating step: extract_texts_column
workflow.add_step(operation="table.cut_column", step_id="extract_texts_column")
# %%
# Connecting input(s) of step 'extract_texts_column'
workflow.connect_fields("extract_texts_column.table", "create_text_corpus.table")
# %%
# Creating step: extract_filename_column
workflow.add_step(operation="table.cut_column", step_id="extract_filename_column")
# %%
# Connecting input(s) of step 'extract_filename_column'
workflow.connect_fields("extract_filename_column.table", "create_text_corpus.table")
# %%
# Creating step: create_date_array
workflow.add_step(operation="parse.date_array", step_id="create_date_array")
# %%
# Connecting input(s) of step 'create_date_array'
workflow.connect_fields("create_date_array.array", "extract_filename_column.array")
# %%
# Creating step: tokenize_content
workflow.add_step(operation="tokenize.texts_array", step_id="tokenize_content")
# %%
# Connecting input(s) of step 'tokenize_content'
workflow.connect_fields("tokenize_content.texts_array", "extract_texts_column.array")
# %%
# Creating step: preprocess_corpus
workflow.add_step(operation="preprocess.tokens_array", step_id="preprocess_corpus")
# %%
# Connecting input(s) of step 'preprocess_corpus'
workflow.connect_fields("preprocess_corpus.tokens_array", "tokenize_content.tokens_array")
workflow.connect_fields("preprocess_corpus.remove_stopwords", "create_stopwords_list.stopwords_list")
# %%
# Creating step: generate_lda
workflow.add_step(operation="generate.LDA.for.tokens_array", step_id="generate_lda")
# %%
# Connecting input(s) of step 'generate_lda'
workflow.connect_fields("generate_lda.tokens_array", "preprocess_corpus.tokens_array")
# %% [markdown]
# ## Setting workflow input/output names (optional)
#
# To make our workflow nicer to use, we can set aliases for its inputs and outputs.
# %%
workflow.set_input_alias(input_field="extract_texts_column.column_name", alias="content_column_name")
workflow.set_input_alias(input_field="extract_filename_column.column_name", alias="filename_column_name")
workflow.set_input_alias(input_field="import_text_corpus.path", alias="text_corpus_folder_path")
workflow.set_input_alias(input_field="create_date_array.min_index", alias="date_parse_min")
workflow.set_input_alias(input_field="create_date_array.max_index", alias="date_parse_max")
workflow.set_input_alias(input_field="create_date_array.force_non_null", alias="date_force_non_null")
workflow.set_input_alias(input_field="create_date_array.remove_tokens", alias="date_remove_tokensl")
workflow.set_input_alias(input_field="tokenize_content.tokenize_by_word", alias="tokenize_by_word")
workflow.set_input_alias(input_field="generate_lda.num_topics_min", alias="num_topics_min")
workflow.set_input_alias(input_field="generate_lda.num_topics_max", alias="num_topics_max")
workflow.set_input_alias(input_field="generate_lda.compute_coherence", alias="compute_coherence")
workflow.set_input_alias(input_field="generate_lda.words_per_topic", alias="words_per_topic")
workflow.set_input_alias(input_field="create_stopwords_list.languages", alias="languages")
workflow.set_input_alias(input_field="create_stopwords_list.stopword_lists", alias="stopword_lists")
workflow.set_input_alias(input_field="preprocess_corpus.to_lowercase", alias="to_lowercase")
workflow.set_input_alias(input_field="preprocess_corpus.remove_alphanumeric", alias="remove_alphanumeric")
workflow.set_input_alias(input_field="preprocess_corpus.remove_non_alpha", alias="remove_non_alpha")
workflow.set_input_alias(input_field="preprocess_corpus.remove_all_numeric", alias="remove_all_numeric")
workflow.set_input_alias(input_field="preprocess_corpus.remove_short_tokens", alias="remove_short_tokens")
workflow.set_input_alias(input_field="preprocess_corpus.remove_stopwords", alias="remove_stopwords")


workflow.set_output_alias(output_field="import_text_corpus.file_bundle", alias="text_corpus_file_bundle")
workflow.set_output_alias(output_field="create_text_corpus.table", alias="text_corpus_table")
workflow.set_output_alias(output_field="extract_texts_column.array", alias="content_array")
workflow.set_output_alias(output_field="tokenize_content.tokens_array", alias="tokenized_corpus")
workflow.set_output_alias(output_field="preprocess_corpus.tokens_array", alias="preprocessed_corpus")
workflow.set_output_alias(output_field="generate_lda.topic_models", alias="topic_models")
workflow.set_output_alias(output_field="generate_lda.coherence_map", alias="coherence_map")
workflow.set_output_alias(output_field="generate_lda.coherence_table", alias="coherence_table")
workflow.set_output_alias(output_field="create_date_array.date_array", alias="date_array")
# %% [markdown]
# # Workflow information <a class="anchor" id="pipeline_info"></a>
#
# After our workflow is wired up, we look can look at its structure, and other properties.

# %% [markdown]
#
# ## Workflow status
#
# A workflow consists of a series of 'states', the most relevant is always the most recent one. We can investigate
# that latest states details like so:

# %%
workflow.current_state

# %% [markdown]
# ## Pipeline execution graph
#
# Let's look at the current execution graph for the current workflow pipeline:

# %%
graph_to_image(workflow.pipeline.execution_graph)

# %% [markdown]
# # Workflow inputs <a class="anchor" id="pipeline_inputs"></a>
#
# Once a workflow has an assembled pipeline, we can set it's inputs. We use the input field
# names that we got from the result of the `workflow.current_state` call.

# %%
workflow.set_input("text_corpus_folder_path", "/home/markus/projects/kiara/dev/kiara.examples/examples/pipelines/topic_modeling/../../data/text_corpus/data")
workflow.set_input("content_column_name", "content")
workflow.set_input("filename_column_name", "file_name")
workflow.set_input("date_force_non_null", None)
workflow.set_input("date_parse_min", 11)
workflow.set_input("date_parse_max", 21)
workflow.set_input("date_remove_tokensl", None)
workflow.set_input("tokenize_by_word", None)
workflow.set_input("languages", ['italian'])
workflow.set_input("stopword_lists", [])
workflow.set_input("to_lowercase", None)
workflow.set_input("remove_alphanumeric", None)
workflow.set_input("remove_non_alpha", None)
workflow.set_input("remove_all_numeric", None)
workflow.set_input("remove_short_tokens", None)
workflow.set_input("num_topics_min", 7)
workflow.set_input("num_topics_max", 9)
workflow.set_input("compute_coherence", True)
workflow.set_input("words_per_topic", None)


# process all workflow steps that can be processed
workflow.process_steps()

# print the current state, after we set our inputs
workflow.current_state

# %% [markdown]
# # Workflow outputs <a class="anchor" id="pipeline_outputs"></a>
#
# To print the actual data of the workflows' current outputs, we call the `current_output_values` property of the workflow object:

# %%
workflow.current_output_values

# %% [markdown]
# # Workflow snapshot <a class="anchor" id="snapshot"></a>
#
# So far, our workflow only exists in memory. If we want to save it so we can have a look at it again at a later stage, we can snapshot the current state, which will save the current structure of the internal pipeline, as well as all inputs that are currently used. In addition, this will register the workflow under the alias we specified on top of this file when creating the `Workflow` object (in our case: `topic_modeling`).
#
# If we would not not specify `save=True`, the structure of the pipeline and inputs would still be frozen and kept, but only in memory, and we'd only be able to access it in our current session.

# %%
workflow.snapshot(save=True)

# %% [markdown]
# Now, we can access our workflow in other environments, for example from the commandline:

# %%
# ! kiara workflow list

# %%
# ! kiara workflow explain topic_modeling


# ---
# jupyter:
#   jupytext:
#     cell_markers: region,endregion
#     formats: ipynb,.pct.py:percent,.lgt.py:light,.spx.py:sphinx,md,Rmd,.pandoc.md:pandoc
#     text_representation:
#       extension: .py
#       format_name: sphinx
#       format_version: '1.1'
#       jupytext_version: 1.14.1
#   kernelspec:
#     display_name: Python 3 (ipykernel)
#     language: python
#     name: python3
# ---

# %env CONSOLE_WIDTH=140

from kiara.interfaces.python_api.workflow import Workflow
from kiara.utils.jupyter import graph_to_image
from kiara.utils.cli import terminal_print_model


"""
# Creating the workflow object <a class="anchor" id="create_workflow_obj"></a>

As the first step we create a [`Workflow`](https://dharpa.org/kiara/latest/reference/kiara/interfaces/python_api/workflow/) object, which is a convenience class that manages workflow state, internal consistency and history for us:
"""

doc = """Example topic-modeling end-to-end workflow."""
workflow = Workflow.create("topic_modeling", doc=doc, replace_existing_alias=True)

###############################################################################
# # Assembling the workflow <a class="anchor" id="assembly"></a>
#
# The first step in the creation of our workflow is to create the individual steps from the available *kiara* modules.
#
# A list of available modules and their aliases can be found here: TODO
#
# ## Creating the steps of the workflow <a class="anchor" id="creating_steps"></a>

# Creating step: import_text_corpus
workflow.add_step(operation="import.file_bundle", step_id="import_text_corpus")
""
# Creating step: create_stopwords_list
workflow.add_step(operation="create.stopwords_list", step_id="create_stopwords_list")
""
# Creating step: create_text_corpus
step_create_text_corpus_config = {'constants': {}, 'defaults': {}, 'source_type': 'text_file_bundle', 'target_type': 'table', 'ignore_errors': False}
workflow.add_step(
    operation="create.table",
    module_config=step_create_text_corpus_config,
    step_id="create_text_corpus")
""
# Connecting input(s) of step 'create_text_corpus'
workflow.connect_fields("create_text_corpus.text_file_bundle", "import_text_corpus.file_bundle")
""
# Creating step: extract_texts_column
workflow.add_step(operation="table.cut_column", step_id="extract_texts_column")
""
# Connecting input(s) of step 'extract_texts_column'
workflow.connect_fields("extract_texts_column.table", "create_text_corpus.table")
""
# Creating step: extract_filename_column
workflow.add_step(operation="table.cut_column", step_id="extract_filename_column")
""
# Connecting input(s) of step 'extract_filename_column'
workflow.connect_fields("extract_filename_column.table", "create_text_corpus.table")
""
# Creating step: create_date_array
workflow.add_step(operation="parse.date_array", step_id="create_date_array")
""
# Connecting input(s) of step 'create_date_array'
workflow.connect_fields("create_date_array.array", "extract_filename_column.array")
""
# Creating step: tokenize_content
workflow.add_step(operation="tokenize.texts_array", step_id="tokenize_content")
""
# Connecting input(s) of step 'tokenize_content'
workflow.connect_fields("tokenize_content.texts_array", "extract_texts_column.array")
""
# Creating step: preprocess_corpus
workflow.add_step(operation="preprocess.tokens_array", step_id="preprocess_corpus")
""
# Connecting input(s) of step 'preprocess_corpus'
workflow.connect_fields("preprocess_corpus.tokens_array", "tokenize_content.tokens_array")
workflow.connect_fields("preprocess_corpus.remove_stopwords", "create_stopwords_list.stopwords_list")
""
# Creating step: generate_lda
workflow.add_step(operation="generate.LDA.for.tokens_array", step_id="generate_lda")
""
# Connecting input(s) of step 'generate_lda'
workflow.connect_fields("generate_lda.tokens_array", "preprocess_corpus.tokens_array")
###############################################################################
# ## Setting workflow input/output names (optional)
#
# To make our workflow nicer to use, we can set aliases for its inputs and outputs.
workflow.set_input_alias(input_field="extract_texts_column.column_name", alias="content_column_name")
workflow.set_input_alias(input_field="extract_filename_column.column_name", alias="filename_column_name")
workflow.set_input_alias(input_field="import_text_corpus.path", alias="text_corpus_folder_path")
workflow.set_input_alias(input_field="create_date_array.min_index", alias="date_parse_min")
workflow.set_input_alias(input_field="create_date_array.max_index", alias="date_parse_max")
workflow.set_input_alias(input_field="create_date_array.force_non_null", alias="date_force_non_null")
workflow.set_input_alias(input_field="create_date_array.remove_tokens", alias="date_remove_tokensl")
workflow.set_input_alias(input_field="tokenize_content.tokenize_by_word", alias="tokenize_by_word")
workflow.set_input_alias(input_field="generate_lda.num_topics_min", alias="num_topics_min")
workflow.set_input_alias(input_field="generate_lda.num_topics_max", alias="num_topics_max")
workflow.set_input_alias(input_field="generate_lda.compute_coherence", alias="compute_coherence")
workflow.set_input_alias(input_field="generate_lda.words_per_topic", alias="words_per_topic")
workflow.set_input_alias(input_field="create_stopwords_list.languages", alias="languages")
workflow.set_input_alias(input_field="create_stopwords_list.stopword_lists", alias="stopword_lists")
workflow.set_input_alias(input_field="preprocess_corpus.to_lowercase", alias="to_lowercase")
workflow.set_input_alias(input_field="preprocess_corpus.remove_alphanumeric", alias="remove_alphanumeric")
workflow.set_input_alias(input_field="preprocess_corpus.remove_non_alpha", alias="remove_non_alpha")
workflow.set_input_alias(input_field="preprocess_corpus.remove_all_numeric", alias="remove_all_numeric")
workflow.set_input_alias(input_field="preprocess_corpus.remove_short_tokens", alias="remove_short_tokens")
workflow.set_input_alias(input_field="preprocess_corpus.remove_stopwords", alias="remove_stopwords")


workflow.set_output_alias(output_field="import_text_corpus.file_bundle", alias="text_corpus_file_bundle")
workflow.set_output_alias(output_field="create_text_corpus.table", alias="text_corpus_table")
workflow.set_output_alias(output_field="extract_texts_column.array", alias="content_array")
workflow.set_output_alias(output_field="tokenize_content.tokens_array", alias="tokenized_corpus")
workflow.set_output_alias(output_field="preprocess_corpus.tokens_array", alias="preprocessed_corpus")
workflow.set_output_alias(output_field="generate_lda.topic_models", alias="topic_models")
workflow.set_output_alias(output_field="generate_lda.coherence_map", alias="coherence_map")
workflow.set_output_alias(output_field="generate_lda.coherence_table", alias="coherence_table")
workflow.set_output_alias(output_field="create_date_array.date_array", alias="date_array")
###############################################################################
# # Workflow information <a class="anchor" id="pipeline_info"></a>
#
# After our workflow is wired up, we look can look at its structure, and other properties.

###############################################################################
#
# ## Workflow status
#
# A workflow consists of a series of 'states', the most relevant is always the most recent one. We can investigate
# that latest states details like so:

workflow.current_state

###############################################################################
# ## Pipeline execution graph
#
# Let's look at the current execution graph for the current workflow pipeline:

graph_to_image(workflow.pipeline.execution_graph)

###############################################################################
# # Workflow inputs <a class="anchor" id="pipeline_inputs"></a>
#
# Once a workflow has an assembled pipeline, we can set it's inputs. We use the input field
# names that we got from the result of the `workflow.current_state` call.

workflow.set_input("text_corpus_folder_path", "/home/markus/projects/kiara/dev/kiara.examples/examples/pipelines/topic_modeling/../../data/text_corpus/data")
workflow.set_input("content_column_name", "content")
workflow.set_input("filename_column_name", "file_name")
workflow.set_input("date_force_non_null", None)
workflow.set_input("date_parse_min", 11)
workflow.set_input("date_parse_max", 21)
workflow.set_input("date_remove_tokensl", None)
workflow.set_input("tokenize_by_word", None)
workflow.set_input("languages", ['italian'])
workflow.set_input("stopword_lists", [])
workflow.set_input("to_lowercase", None)
workflow.set_input("remove_alphanumeric", None)
workflow.set_input("remove_non_alpha", None)
workflow.set_input("remove_all_numeric", None)
workflow.set_input("remove_short_tokens", None)
workflow.set_input("num_topics_min", 7)
workflow.set_input("num_topics_max", 9)
workflow.set_input("compute_coherence", True)
workflow.set_input("words_per_topic", None)


# process all workflow steps that can be processed
workflow.process_steps()

# print the current state, after we set our inputs
workflow.current_state

###############################################################################
# # Workflow outputs <a class="anchor" id="pipeline_outputs"></a>
#
# To print the actual data of the workflows' current outputs, we call the `current_output_values` property of the workflow object:

workflow.current_output_values

###############################################################################
# # Workflow snapshot <a class="anchor" id="snapshot"></a>
#
# So far, our workflow only exists in memory. If we want to save it so we can have a look at it again at a later stage, we can snapshot the current state, which will save the current structure of the internal pipeline, as well as all inputs that are currently used. In addition, this will register the workflow under the alias we specified on top of this file when creating the `Workflow` object (in our case: `topic_modeling`).
#
# If we would not not specify `save=True`, the structure of the pipeline and inputs would still be frozen and kept, but only in memory, and we'd only be able to access it in our current session.

workflow.snapshot(save=True)

###############################################################################
# Now, we can access our workflow in other environments, for example from the commandline:

# ! kiara workflow list

""
# ! kiara workflow explain topic_modeling

PIPELINES
NETWORK ANALYSIS

pipeline_name: create_network_graph
doc: Onboard network data
steps:
  - module_type: import.file
    module_config:
      constants:
        attach_metadata: true
        file_name: ""
    step_id: import_edges_file
  - module_type: create.table.from.file
    step_id: create_edges_table
    input_links:
      file: import_edges_file.file
  - module_type: import.file
    module_config:
      constants:
        attach_metadata: true
        file_name: ""
    step_id: import_nodes_file
  - module_type: create.table.from.file
    step_id: create_nodes_table
    input_links:
      file: import_nodes_file.file
  - module_type: assemble.network_data
    step_id: assemble_network_data
    input_links:
      edges: create_edges_table.table
      nodes: create_nodes_table.table

input_aliases:
  import_edges_file.source: edges_file
  import_edges_file.onboard_type: edges_file_onboard_type
  import_nodes_file.source: nodes_file
  import_nodes_file.onboard_type: nodes_file_onboard_type
  assemble_network_data.source_column: source_column
  assemble_network_data.target_column: target_column
  assemble_network_data.edges_column_map: edges_column_map
  assemble_network_data.id_column: id_column
  assemble_network_data.label_column: label_column
  assemble_network_data.nodes_column_map: nodes_column_map
  create_edges_table.first_row_is_header: first_edges_row_is_header
  create_nodes_table.first_row_is_header: first_nodes_row_is_header
output_aliases:
  assemble_network_data.network_data: network_data

#defaults:
#  edges_file: "${pipeline_dir}/../data/journals/JournalEdges1902.csv"
#  nodes_file: "${pipeline_dir}/../data/journals/JournalNodes1902.csv"

pipeline_name: corpus_onboarding
doc: Onboard a text corpus.
steps:
  - module_type: import.local.file_bundle
    step_id: import_text_corpus
  - module_type: create.table.from.text_file_bundle
    step_id: create_corpus_table
    input_links:
      text_file_bundle: import_text_corpus.file_bundle
  - module_type: table.pick.column
    step_id: extract_filename_column
    module_config:
      column_name: "file_name"
    input_links:
      table: create_corpus_table.table
  - module_type: parse.date_array
    step_id: create_date_array
    input_links:
      array: extract_filename_column.array
  - module_type: table.merge
    step_id: merge_table
    module_config:
      inputs_schema:
        source_table:
          type: table
          doc: The original table.
        date_array:
          type: array
          doc: The array containing the parsed date items.
      column_map:
        date: date_array
        content: source_table.content
        file_name: source_table.file_name
    input_links:
      source_table: create_corpus_table.table
      date_array: create_date_array.date_array

input_aliases:
#  extract_filename_column.column_name: filename_column_name
  import_text_corpus.path: text_corpus_folder_path
  create_date_array.min_index: date_parse_min
  create_date_array.max_index: date_parse_max
  create_date_array.force_non_null: force_parsed_date
  create_date_array.remove_tokens: remove_tokens

output_aliases:
  merge_table.table: merged_table

defaults:
#  filename_column_name: "file_name"
  date_parse_min: 11
  date_parse_max: 21
  text_corpus_folder_path: "${pipeline_dir}/../../data/language_processing/text_corpus/data"
#  text_corpus_folder_path: "/home/markus/projects/kiara_new/kiara_plugin.tabular/examples/data/text_corpus/data"

pipeline_name: topic_modeling
doc: Example topic-modeling end-to-end workflow.
steps:
  - module_type: import.local.file_bundle
    step_id: import_text_corpus
  - module_type: create.table.from.file_bundle
    step_id: create_text_corpus
    input_links:
      file_bundle: import_text_corpus.file_bundle
  - module_type: table.pick.column
    step_id: extract_texts_column
    input_links:
      table: create_text_corpus.table
  - module_type: table.pick.column
    step_id: extract_filename_column
    input_links:
      table: create_text_corpus.table
  - module_type: parse.date_array
    step_id: create_date_array
    input_links:
      array: extract_filename_column.array
  - module_type: tokenize.texts_array
    step_id: tokenize_content
    input_links:
      texts_array: extract_texts_column.array
  - module_type: create.stopwords_list
    step_id: create_stopwords_list
  - module_type: preprocess.tokens_array
    step_id: preprocess_corpus
    input_links:
      tokens_array: tokenize_content.tokens_array
      remove_stopwords: create_stopwords_list.stopwords_list
  - module_type: generate.LDA.for.tokens_array
    step_id: generate_lda
    input_links:
      tokens_array: preprocess_corpus.tokens_array

input_aliases:
  extract_texts_column.column_name: content_column_name
  extract_filename_column.column_name: filename_column_name
  import_text_corpus.path: text_corpus_folder_path
  create_date_array.min_index: date_parse_min
  create_date_array.max_index: date_parse_max
  create_date_array.force_non_null: date_force_non_null
  create_date_array.remove_tokens: date_remove_tokensl
  tokenize_content.tokenize_by_word: tokenize_by_word
  generate_lda.num_topics_min: num_topics_min
  generate_lda.num_topics_max: num_topics_max
  generate_lda.compute_coherence: compute_coherence
  generate_lda.words_per_topic: words_per_topic
  create_stopwords_list.languages: languages
  create_stopwords_list.stopwords: stopwords
  preprocess_corpus.to_lowercase: to_lowercase
  preprocess_corpus.remove_alphanumeric: remove_alphanumeric
  preprocess_corpus.remove_non_alpha: remove_non_alpha
  preprocess_corpus.remove_all_numeric: remove_all_numeric
  preprocess_corpus.remove_short_tokens: remove_short_tokens
  preprocess_corpus.remove_stopwords: remove_stopwords

output_aliases:
  import_text_corpus.file_bundle: text_corpus_file_bundle
  create_text_corpus.table: text_corpus_table
  extract_texts_column.array: content_array
  tokenize_content.tokens_array: tokenized_corpus
  preprocess_corpus.tokens_array: preprocessed_corpus
  generate_lda.topic_models: topic_models
  generate_lda.coherence_map: coherence_map
  generate_lda.coherence_table: coherence_table
  create_date_array.date_array: date_array

defaults:
  content_column_name: "content"
  filename_column_name: "file_name"
  date_parse_min: 11
  date_parse_max: 21
  text_corpus_folder_path: "${pipeline_dir}/../../data/language_processing/text_corpus/data"
  languages: ["italian"]
  stopwords: []
  num_topics_min: 7
  num_topics_max: 9
  compute_coherence: true

pipeline_name: topic_modeling
doc: Example topic-modeling end-to-end workflow.
steps:
  - module_type: import.local.file_bundle
    step_id: import_text_corpus
  - module_type: create.table.from.file_bundle
    step_id: create_text_corpus
    input_links:
      file_bundle: import_text_corpus.file_bundle
  - module_type: table.pick.column
    step_id: extract_texts_column
    input_links:
      table: create_text_corpus.table
  - module_type: tokenize.texts_array
    step_id: tokenize_content
    input_links:
      texts_array: extract_texts_column.array
  - module_type: create.stopwords_list
    step_id: create_stopwords_list
  - module_type: preprocess.tokens_array
    step_id: preprocess_corpus
    input_links:
      tokens_array: tokenize_content.tokens_array
      remove_stopwords: create_stopwords_list.stopwords_list
  - module_type: generate.LDA.for.tokens_array
    step_id: generate_lda
    input_links:
      tokens_array: preprocess_corpus.tokens_array

input_aliases:
  extract_texts_column.column_name: content_column_name
  import_text_corpus.path: text_corpus_folder_path
  tokenize_content.tokenize_by_word: tokenize_by_word
  generate_lda.num_topics_min: num_topics_min
  generate_lda.num_topics_max: num_topics_max
  generate_lda.compute_coherence: compute_coherence
  generate_lda.words_per_topic: words_per_topic
  create_stopwords_list.languages: languages
  create_stopwords_list.stopwords: stopwords
  preprocess_corpus.to_lowercase: to_lowercase
  preprocess_corpus.remove_alphanumeric: remove_alphanumeric
  preprocess_corpus.remove_non_alpha: remove_non_alpha
  preprocess_corpus.remove_all_numeric: remove_all_numeric
  preprocess_corpus.remove_short_tokens: remove_short_tokens
  preprocess_corpus.remove_stopwords: remove_stopwords

output_aliases:
  generate_lda.topic_models: topic_models
  generate_lda.coherence_map: coherence_map
  generate_lda.coherence_table: coherence_table

defaults:
  content_column_name: "content"
  text_corpus_folder_path: "${pipeline_dir}/../../data/language_processing/text_corpus/data"
  languages: ["italian"]
  stopwords: []
  num_topics_min: 7
  num_topics_max: 9
  compute_coherence: true


pipeline_name: operations_doc_onboarding
doc: Onboard example datasets to be used in the operations doc app.
steps:
  - module_type: import.file_bundle
    step_id: download_example_data
  - module_type: create.table.from.file_bundle
    step_id: create_files_table
    input_links:
      file_bundle: download_example_data.file_bundle
  - module_type: file_bundle.pick.file
    step_id: pick_journal_nodes
    input_links:
      file_bundle: download_example_data.file_bundle
  - module_type: file_bundle.pick.file
    step_id: pick_journal_edges
    input_links:
      file_bundle: download_example_data.file_bundle
  - module_type: create.table.from.file
    step_id: create_nodes_table
    input_links:
      file: pick_journal_nodes.file
  - module_type: create.table.from.file
    step_id: create_edges_table
    input_links:
      file: pick_journal_edges.file
  - module_type: assemble.tables
    step_id: assemble_tables
    module_config:
      constants:
        table_name_1: "nodes"
        table_name_2: "edges"
    input_links:
      table_1: create_nodes_table.table
      table_2: create_edges_table.table
  - module_type: create.database.from.tables
    step_id: create_database_from_tables
    input_links:
      tables: assemble_tables.tables


input_aliases:
  download_example_data.source: "example_data_source"
  download_example_data.sub_path: "sub_folder"
  download_example_data.include_files: "include_files"
  download_example_data.exclude_files: "exclude_files"
  download_example_data.onboard_type: "onboard_type"
  download_example_data.exclude_dirs: "exclude_dirs"
  pick_journal_edges.path: "journal_edges_path"
  pick_journal_nodes.path: "journal_nodes_path"
  create_nodes_table.first_row_is_header: "nodes_first_row_is_header"
  create_edges_table.first_row_is_header: "edges_first_row_is_header"

output_aliases:
  download_example_data.file_bundle: "file_bundle"
  create_files_table.table: "files_table"
  pick_journal_nodes.file: "nodes_file"
  pick_journal_edges.file: "edges_file"
  create_nodes_table.table: "nodes_table"
  create_edges_table.table: "edges_table"
  assemble_tables.tables: "journals_tables"
  create_database_from_tables.database: "journals_database"

STREAMLIT

# -*- coding: utf-8 -*-
import os

import kiara_plugin.streamlit as kst
from kiara.interfaces.python_api import JobDesc

st = kst.init(page_config={"layout": "wide"})

st.kiara.api.set_active_context("components_doc", create=True)

if "file_bundle" not in st.kiara.api.list_alias_names():

    with st.spinner("Downloading example data ..."):
        job_file = os.path.join(
            os.path.dirname(__file__), "jobs", "download_journals.yaml"
        )
        job_desc = JobDesc.create_from_file(path=job_file)

        results = st.kiara.api.run_job(job_desc)

        for field_name, value in results.items():
            st.kiara.api.store_value(value, field_name)


st.kiara.component_info()

# -*- coding: utf-8 -*-
import os

from streamlit_option_menu import option_menu

import kiara_plugin.streamlit as kst
from kiara.interfaces.python_api import JobDesc

st = kst.init(page_config={"layout": "wide"})

st.kiara.api.set_active_context("_operation_doc", create=True)

if "file_bundle" not in st.kiara.api.list_alias_names():

    with st.spinner("Downloading example data ..."):
        job_file = os.path.join(
            os.path.dirname(__file__), "jobs", "download_journals.yaml"
        )
        job_desc = JobDesc.create_from_file(path=job_file)

        results = st.kiara.api.run_job(job_desc)

        for field_name, value in results.items():
            st.kiara.api.store_value(value, field_name)

selected = option_menu(None, ["Components", "Operations"], orientation="horizontal")
if selected == "Components":
    st.kiara.component_info(key="components_doc", height=600)
elif selected == "Operations":
    st.kiara.operation_documentation(key="operations_doc", height=600)

# -*- coding: utf-8 -*-


import kiara_plugin.streamlit as kst
from kiara.interfaces.python_api import KiaraAPI

st = kst.init(page_config={"layout": "wide"})

api: KiaraAPI = st.kiara
api.kiara_api_help()

# -*- coding: utf-8 -*-
import os

import kiara_plugin.streamlit as kst
from kiara.interfaces.python_api import JobDesc

st = kst.init(page_config={"layout": "wide"})

st.kiara.api.set_active_context("_operation_doc", create=True)

if "file_bundle" not in st.kiara.api.list_alias_names():

    with st.spinner("Downloading example data ..."):
        job_file = os.path.join(
            os.path.dirname(__file__), "jobs", "download_journals.yaml"
        )
        job_desc = JobDesc.create_from_file(path=job_file)

        results = st.kiara.api.run_job(job_desc)

        for field_name, value in results.items():
            st.kiara.api.store_value(value, field_name)

st.kiara.operation_documentation()


data onboarding

# -*- coding: utf-8 -*-

"""This module contains the metadata (and other) models that are used in the ``kiara_plugin.onboarding`` package.

Those models are convenience wrappers that make it easier for *kiara* to find, create, manage and version metadata -- but also
other type of models -- that is attached to data, as well as *kiara* modules.

Metadata models must be a sub-class of [kiara.metadata.MetadataModel][kiara.metadata.MetadataModel]. Other models usually
sub-class a pydantic BaseModel or implement custom base classes.
"""
import os.path
from abc import abstractmethod
from typing import ClassVar, List, Tuple, Union

from kiara.exceptions import KiaraException
from kiara.models import KiaraModel
from kiara.models.filesystem import FolderImportConfig, KiaraFile, KiaraFileBundle


class OnboardDataModel(KiaraModel):

    _kiara_model_id: ClassVar[str] = None  # type: ignore

    @classmethod
    def get_config_fields(cls) -> List[str]:
        return sorted(cls.model_fields.keys())

    @classmethod
    @abstractmethod
    def accepts_uri(cls, uri: str) -> Tuple[bool, str]:
        pass

    @classmethod
    def accepts_bundle_uri(cls, uri: str) -> Tuple[bool, str]:
        return cls.accepts_uri(uri)

    @abstractmethod
    def retrieve(
        self, uri: str, file_name: Union[None, str], attach_metadata: bool
    ) -> KiaraFile:
        pass

    def retrieve_bundle(
        self, uri: str, import_config: FolderImportConfig, attach_metadata: bool
    ) -> KiaraFileBundle:
        raise NotImplementedError()


class FileFromLocalModel(OnboardDataModel):

    _kiara_model_id: ClassVar[str] = "onboarding.file.from.local_file"

    @classmethod
    def accepts_uri(cls, uri: str) -> Tuple[bool, str]:

        if os.path.isfile(os.path.abspath(uri)):
            return True, "local file exists and is file"
        else:
            return False, "local file does not exist or is not a file"

    @classmethod
    def accepts_bundle_uri(cls, uri: str) -> Tuple[bool, str]:

        if os.path.isdir(os.path.abspath(uri)):
            return True, "local folder exists and is folder"
        else:
            return False, "local folder does not exist or is not a folder"

    def retrieve(
        self, uri: str, file_name: Union[None, str], attach_metadata: bool
    ) -> KiaraFile:

        if not os.path.exists(os.path.abspath(uri)):
            raise KiaraException(
                f"Can't create file from path '{uri}': path does not exist."
            )
        if not os.path.isfile(os.path.abspath(uri)):
            raise KiaraException(
                f"Can't create file from path '{uri}': path is not a file."
            )

        return KiaraFile.load_file(uri)

    def retrieve_bundle(
        self, uri: str, import_config: FolderImportConfig, attach_metadata: bool
    ) -> KiaraFileBundle:

        if not os.path.exists(os.path.abspath(uri)):
            raise KiaraException(
                f"Can't create file from path '{uri}': path does not exist."
            )
        if not os.path.isdir(os.path.abspath(uri)):
            raise KiaraException(
                f"Can't create file from path '{uri}': path is not a directory."
            )

        return KiaraFileBundle.import_folder(source=uri, import_config=import_config)


class FileFromRemoteModel(OnboardDataModel):

    _kiara_model_id: ClassVar[str] = "onboarding.file.from.url"

    @classmethod
    def accepts_uri(cls, uri: str) -> Tuple[bool, str]:

        accepted_protocols = ["http", "https"]
        for protocol in accepted_protocols:
            if uri.startswith(f"{protocol}://"):
                return True, "url is valid (starts with http or https)"

        return False, "url is not valid (does not start with http or https)"

    def retrieve(
        self, uri: str, file_name: Union[None, str], attach_metadata: bool
    ) -> KiaraFile:
        from kiara_plugin.onboarding.utils.download import download_file

        result_file: KiaraFile = download_file(  # type: ignore
            url=uri, file_name=file_name, attach_metadata=attach_metadata
        )
        return result_file

    def retrieve_bundle(
        self, uri: str, import_config: FolderImportConfig, attach_metadata: bool
    ) -> KiaraFileBundle:
        from kiara_plugin.onboarding.utils.download import download_file_bundle

        result_bundle = download_file_bundle(
            url=uri, import_config=import_config, attach_metadata=attach_metadata
        )
        return result_bundle


class FileFromZenodoModel(OnboardDataModel):

    _kiara_model_id: ClassVar[str] = "onboarding.file.from.zenodo"

    @classmethod
    def accepts_uri(cls, uri: str) -> Tuple[bool, str]:

        if uri.startswith("zenodo:"):
            return True, "url is valid (follows format 'zenodo:<doi>')"

        elif "/zenodo." in uri:
            return True, "url is valid (contains '/zenodo.')"

        return False, "url is not valid (does not follow format 'zenodo:<doi>')"

    def retrieve(
        self, uri: str, file_name: Union[None, str], attach_metadata: bool
    ) -> KiaraFile:

        import pyzenodo3

        from kiara_plugin.onboarding.utils.download import download_file

        if uri.startswith("zenodo:"):
            doi = uri[len("zenodo:") :]
        elif "/zenodo." in uri:
            doi = uri

        tokens = doi.split("/zenodo.")
        if len(tokens) != 2:
            raise KiaraException(
                msg=f"Can't parse Zenodo DOI from URI for single file download: {doi}"
            )

        path_components = tokens[1].split("/", maxsplit=1)
        if len(path_components) != 2:
            raise KiaraException(
                msg=f"Can't parse Zenodo DOI from URI for single file download: {doi}"
            )

        file_path = path_components[1]
        _doi = f"{tokens[0]}/zenodo.{path_components[0]}"

        zen = pyzenodo3.Zenodo()
        record = zen.find_record_by_doi(_doi)

        match = None
        for _available_file in record.data["files"]:
            if file_path == _available_file["key"]:
                match = _available_file
                break

        if not match:
            msg = "Available files:\n"
            for key in record.data["files"]:
                msg += f"  - {key['key']}\n"
            raise KiaraException(
                msg=f"Can't find file '{file_path}' in Zenodo record. {msg}"
            )

        url = match["links"]["self"]
        checksum = match["checksum"][4:]

        file_name = file_path.split("/")[-1]

        file_model: KiaraFile
        file_model, md5_digest = download_file(  # type: ignore
            url=url,
            target=None,
            file_name=file_name,
            attach_metadata=attach_metadata,
            return_md5_hash=True,
        )

        if checksum != md5_digest:
            raise KiaraException(
                msg=f"Can't download file '{file_name}', invalid checksum: {checksum} != {md5_digest}"
            )

        if attach_metadata:
            file_model.metadata["zenodo_record_data"] = record.data

        return file_model

    def retrieve_bundle(
        self, uri: str, import_config: FolderImportConfig, attach_metadata: bool
    ) -> KiaraFileBundle:

        import shutil

        import pyzenodo3

        from kiara_plugin.onboarding.utils.download import download_file

        if uri.startswith("zenodo:"):
            doi = uri[len("zenodo:") :]
        elif "/zenodo." in uri:
            doi = uri

        tokens = doi.split("/zenodo.")
        if len(tokens) != 2:
            raise KiaraException(
                msg=f"Can't parse Zenodo DOI from URI for single file download: {doi}"
            )

        path_components = tokens[1].split("/", maxsplit=1)

        if len(path_components) == 2:
            zid = path_components[0]
            file_path = path_components[1]
        else:
            zid = path_components[0]
            file_path = None

        _doi = f"{tokens[0]}/zenodo.{zid}"

        if not file_path:

            zen = pyzenodo3.Zenodo()

            record = zen.find_record_by_doi(_doi)

            path = KiaraFileBundle.create_tmp_dir()
            shutil.rmtree(path, ignore_errors=True)
            path.mkdir()

            for file_data in record.data["files"]:
                url = file_data["links"]["self"]
                file_name = file_data["key"]
                checksum = file_data["checksum"][4:]

                target = os.path.join(path, file_name)
                file_model: KiaraFile
                file_model, md5_digest = download_file(  # type: ignore
                    url=url,
                    target=target,
                    file_name=file_name,
                    attach_metadata=attach_metadata,
                    return_md5_hash=True,
                )

                if checksum != md5_digest:
                    raise KiaraException(
                        msg=f"Can't download file '{file_name}', invalid checksum: {checksum} != {md5_digest}"
                    )

            bundle = KiaraFileBundle.import_folder(path.as_posix())
            if attach_metadata:
                bundle.metadata["zenodo_record_data"] = record.data

        else:

            zen = pyzenodo3.Zenodo()
            record = zen.find_record_by_doi(_doi)

            match = None
            for _available_file in record.data["files"]:
                if file_path == _available_file["key"]:
                    match = _available_file
                    break

            if not match:
                msg = "Available files:\n"
                for key in record.data["files"]:
                    msg += f"  - {key['key']}\n"
                raise KiaraException(
                    msg=f"Can't find file '{file_path}' in Zenodo record. {msg}"
                )

            url = match["links"]["self"]
            checksum = match["checksum"][4:]

            file_name = file_path.split("/")[-1]

            file_model, md5_digest = download_file(  # type: ignore
                url=url,
                target=None,
                file_name=file_name,
                attach_metadata=attach_metadata,
                return_md5_hash=True,
            )

            if checksum != md5_digest:
                raise KiaraException(
                    msg=f"Can't download file '{file_name}', invalid checksum: {checksum} != {md5_digest}"
                )

            bundle = KiaraFileBundle.from_archive_file(
                archive_file=file_model, import_config=import_config
            )
            if attach_metadata:
                bundle.metadata["zenodo_record_data"] = record.data

        return bundle


class FileFromZoteroModel(OnboardDataModel):

    _kiara_model_id: ClassVar[str] = "onboarding.file.from.zotero"

    @classmethod
    def accepts_uri(cls, uri: str) -> Tuple[bool, str]:
        if uri.startswith("zotero:"):
            return True, "uri is a zotero uri"
        else:
            return False, "uri is not a zotero uri, must start with 'zotero:'"


class FileFromGithubModel(OnboardDataModel):

    _kiara_model_id: ClassVar[str] = "onboarding.file.from.github"

    @classmethod
    def accepts_uri(cls, uri: str) -> Tuple[bool, str]:

        if uri.startswith("gh:") or uri.startswith("github:"):
            return True, "uri is a github uri"

        return False, "uri is not a github uri, must start with 'gh:' or 'github:'"

    def retrieve(
        self, uri: str, file_name: Union[None, str], attach_metadata: bool
    ) -> KiaraFile:
        from kiara_plugin.onboarding.utils.download import download_file

        tokens = uri.split(":")[1].split("/", maxsplit=3)
        if len(tokens) != 4:
            raise KiaraException(
                msg=f"Can't parse github uri '{uri}' for single file download. Required format: 'gh:<user>/<repo>/<branch_or_tag>/<path>'"
            )

        url = f"https://raw.githubusercontent.com/{tokens[0]}/{tokens[1]}/{tokens[2]}/{tokens[3]}"

        result_file: KiaraFile = download_file(  # type: ignore
            url=url, attach_metadata=attach_metadata
        )
        return result_file

    def retrieve_bundle(
        self, uri: str, import_config: FolderImportConfig, attach_metadata: bool
    ) -> KiaraFileBundle:

        from kiara_plugin.onboarding.utils.download import download_file

        tokens = uri.split(":")[1].split("/", maxsplit=3)
        if len(tokens) == 3:
            sub_path = None
        elif len(tokens) != 4:
            raise KiaraException(
                msg=f"Can't parse github uri '{uri}' for single file download. Required format: 'gh:<user>/<repo>/<branch_or_tag>/<path>'"
            )
        else:
            sub_path = tokens[3]

        url = f"https://github.com/{tokens[0]}/{tokens[1]}/archive/refs/heads/{tokens[2]}.zip"
        file_name = f"{tokens[1]}-{tokens[2]}.zip"

        archive_zip: KiaraFile
        archive_zip = download_file(  # type: ignore
            url=url,
            attach_metadata=attach_metadata,
            file_name=file_name,
            return_md5_hash=False,
        )

        base_sub_path = f"{tokens[1]}-{tokens[2]}"

        if sub_path:
            if import_config.sub_path:
                new_sub_path = "/".join([base_sub_path, sub_path, import_config.sub_path])  # type: ignore
            else:
                new_sub_path = "/".join([base_sub_path, sub_path])
        elif import_config.sub_path:
            new_sub_path = "/".join([base_sub_path, import_config.sub_path])
        else:
            new_sub_path = base_sub_path

        import_config_new = import_config.copy(update={"sub_path": new_sub_path})

        result_bundle = KiaraFileBundle.from_archive_file(
            archive_file=archive_zip, import_config=import_config_new
        )

        return result_bundle

# -*- coding: utf-8 -*-

"""Generate the code reference pages and navigation."""

from pathlib import Path

import mkdocs_gen_files

nav = mkdocs_gen_files.Nav()

for path in sorted(Path("src").rglob("*.py")):
    module_path = path.relative_to("src").with_suffix("")
    doc_path = path.relative_to("src").with_suffix(".md")
    full_doc_path = Path("reference", doc_path)

    parts = list(module_path.parts)

    if parts[-1] == "__init__":
        parts = parts[:-1]
    elif parts[-1] == "__main__":
        continue

    nav[parts] = doc_path  #

    with mkdocs_gen_files.open(full_doc_path, "w") as fd:
        ident = ".".join(parts)
        print("::: " + ident, file=fd)

    mkdocs_gen_files.set_edit_path(full_doc_path, path)

with mkdocs_gen_files.open("reference/SUMMARY.md", "w") as nav_file:  #
    nav_file.writelines(nav.build_literate_nav())  #


# -*- coding: utf-8 -*-
#  Copyright (c) 2022-2022, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import builtins

from kiara.context import Kiara, KiaraContextInfo
from kiara.doc.gen_info_pages import generate_detail_pages

pkg_name = "kiara_plugin.onboarding"

kiara: Kiara = Kiara.instance()
context_info = KiaraContextInfo.create_from_kiara_instance(
    kiara=kiara, package_filter=pkg_name
)

generate_detail_pages(context_info=context_info)

builtins.plugin_package_context_info = context_info

# -*- coding: utf-8 -*-
import os

import mkdocs_gen_files
from kiara.context import Kiara

kiara = Kiara.instance()

modules_file_path = os.path.join("modules_list.md")
modules_page_content = """# Available module types

This page contains a list of all available *Kiara* module types, and their details.

!!! note
The formatting here will be improved later on, for now this should be enough to get the important details of each module type.

"""

BASE_PACKAGE = "kiara_plugin.onboarding"


for module_type in kiara.module_mgmt.find_modules_for_package(
    BASE_PACKAGE, include_pipelines=False
).keys():

    if module_type == "pipeline":
        continue

    modules_page_content = modules_page_content + f"## ``{module_type}``\n\n"
    modules_page_content = (
        modules_page_content
        + "```\n{{ get_module_info('"
        + module_type
        + "') }}\n```\n\n"
    )

with mkdocs_gen_files.open(modules_file_path, "w") as f:
    f.write(modules_page_content)

pipelines_file_path = os.path.join("pipelines_list.md")
pipelines_page_content = """# Available pipeline module types

This page contains a list of all available *Kiara* pipeline module types, and their details.

!!! note
The formatting here will be improved later on, for now this should be enough to get the important details of each module type.

"""

for module_type in kiara.module_mgmt.find_modules_for_package(
    BASE_PACKAGE, include_core_modules=False
):

    if module_type == "pipeline":
        continue

    pipelines_page_content = pipelines_page_content + f"## ``{module_type}``\n\n"
    pipelines_page_content = (
        pipelines_page_content
        + "```\n{{ get_module_info('"
        + module_type
        + "') }}\n```\n\n"
    )

with mkdocs_gen_files.open(pipelines_file_path, "w") as f:
    f.write(pipelines_page_content)

# -*- coding: utf-8 -*-
import atexit
import os
import tempfile
from typing import TYPE_CHECKING, Any, Dict, Union

from kiara.models.values.value import ValueMap
from kiara_plugin.onboarding.modules import OnboardFileBundleModule, OnboardFileModule

if TYPE_CHECKING:
    from kiara.models.filesystem import FolderImportConfig, KiaraFile, KiaraFileBundle


class DownloadFileModule(OnboardFileModule):
    """Download a single file from a remote location.

    The result of this operation is a single value of type 'file' (basically an array of raw bytes + some light metadata), which can then be used in other modules to create more meaningful data structures.
    """

    _module_type_name = "download.file"

    def create_onboard_inputs_schema(self) -> Dict[str, Any]:

        result: Dict[str, Dict[str, Any]] = {
            "url": {"type": "string", "doc": "The url of the file to download."},
        }
        return result

    def retrieve_file(
        self, inputs: ValueMap, file_name: Union[str, None], attach_metadata: bool
    ) -> Any:

        from kiara_plugin.onboarding.utils.download import download_file

        url = inputs.get_value_data("url")

        result_file = download_file(
            url=url,
            file_name=file_name,
            attach_metadata=attach_metadata,
        )
        return result_file


class DownloadFileBundleModule(OnboardFileBundleModule):
    """Download a file bundle from a remote location.

    This is basically just a convenience module that incorporates unpacking of the downloaded file into a folder structure, and then wrapping it into a *kiara* `file_bundle` data type.

    If the `sub_path` input is set, the whole data is downloaded anyway, but before wrapping into a `file_bundle` value, the files not in the sub-path are ignored (and thus not available later on). Make sure you
    decided whether this is ok for your use-case, if not, rather filter the `file_bundle` later in an
    extra step (for example using the `file_bundle.pick.sub_folder` operation).
    """

    _module_type_name = "download.file_bundle"

    def create_onboard_inputs_schema(self) -> Dict[str, Any]:

        result: Dict[str, Dict[str, Any]] = {
            "url": {
                "type": "string",
                "doc": "The url of an archive/zip file to download.",
            }
        }

        return result

    def retrieve_archive(
        self,
        inputs: ValueMap,
        bundle_name: Union[str, None],
        attach_metadata_to_bundle: bool,
        attach_metadata_to_files: bool,
        import_config: "FolderImportConfig",
    ) -> Union["KiaraFile", "KiaraFileBundle"]:

        from urllib.parse import urlparse

        from kiara.models.filesystem import KiaraFile
        from kiara_plugin.onboarding.utils.download import download_file

        url = inputs.get_value_data("url")
        suffix = None
        try:
            parsed_url = urlparse(url)
            _, suffix = os.path.splitext(parsed_url.path)
        except Exception:
            pass
        if not suffix:
            suffix = ""

        tmp_file = tempfile.NamedTemporaryFile(delete=False, suffix=suffix)

        def rm_tmp_file():
            tmp_file.close()
            os.unlink(tmp_file.name)

        atexit.register(rm_tmp_file)
        kiara_file: KiaraFile

        kiara_file = download_file(  # type: ignore
            url, target=tmp_file.name, attach_metadata=True, return_md5_hash=False
        )

        assert kiara_file.path == tmp_file.name

        return kiara_file

# -*- coding: utf-8 -*-
from typing import TYPE_CHECKING, Any, Dict, Union

from kiara.models.values.value import ValueMap
from kiara_plugin.onboarding.modules import OnboardFileBundleModule, OnboardFileModule

if TYPE_CHECKING:
    from kiara.models.filesystem import FolderImportConfig, KiaraFile, KiaraFileBundle


class DownloadGithubFileModule(OnboardFileModule):
    """Download a single file from a github repo."""

    _module_type_name = "download.file.from.github"

    def create_onboard_inputs_schema(self) -> Dict[str, Any]:

        result: Dict[str, Dict[str, Any]] = {
            "user": {"type": "string", "doc": "The username/org-name."},
            "repo": {"type": "string", "doc": "The repository name."},
            "branch": {
                "type": "string",
                "doc": "The branch (or tag) name. If not specified, the 'main' branch name will be used.",
                "default": "main",
            },
            "path": {
                "type": "string",
                "doc": "The path to the file in the repository. Make sure not to specify a directory here, only a file path.",
            },
        }
        return result

    def retrieve_file(
        self, inputs: ValueMap, file_name: Union[str, None], attach_metadata: bool
    ) -> Any:

        from kiara_plugin.onboarding.utils.download import download_file

        user = inputs.get_value_data("user")
        repo = inputs.get_value_data("repo")
        branch = inputs.get_value_data("branch")
        sub_path = inputs.get_value_data("path")

        url = f"https://raw.githubusercontent.com/{user}/{repo}/{branch}/{sub_path}"

        result_file: KiaraFile = download_file(  # type: ignore
            url=url, file_name=file_name, attach_metadata=attach_metadata
        )
        return result_file


class DownloadGithbFileBundleModule(OnboardFileBundleModule):
    """Download a file bundle from a remote github repository.

    If 'sub_path' is not specified, the whole repo will be used.

    """

    _module_type_name = "download.file_bundle.from.github"

    def create_onboard_inputs_schema(self) -> Dict[str, Any]:
        result: Dict[str, Dict[str, Any]] = {
            "user": {"type": "string", "doc": "The username/org-name."},
            "repo": {"type": "string", "doc": "The repository name."},
            "branch": {
                "type": "string",
                "doc": "The branch (or tag) name. If not specified, the default branch will be used.",
                "optional": True,
            },
        }
        return result

    def retrieve_archive(
        self,
        inputs: ValueMap,
        bundle_name: Union[str, None],
        attach_metadata_to_bundle: bool,
        attach_metadata_to_files: bool,
        import_config: "FolderImportConfig",
    ) -> Union["KiaraFile", "KiaraFileBundle"]:

        from kiara_plugin.onboarding.utils.download import download_file

        user = inputs.get_value_data("user")
        repo = inputs.get_value_data("repo")
        branch = inputs.get_value_data("branch")
        if not branch:
            branch = "main"

        url = f"https://github.com/{user}/{repo}/archive/refs/heads/{branch}.zip"

        file_name = f"{repo}-{branch}.zip"
        result_file: KiaraFile = download_file(  # type: ignore
            url=url, file_name=file_name, attach_metadata=True, return_md5_hash=False
        )
        return result_file

# -*- coding: utf-8 -*-
from typing import TYPE_CHECKING, Any, Dict, Union

from kiara.api import ValueMap
from kiara.exceptions import KiaraException
from kiara_plugin.onboarding.modules import OnboardFileBundleModule, OnboardFileModule

if TYPE_CHECKING:
    from kiara.models.filesystem import FolderImportConfig, KiaraFile, KiaraFileBundle


class DownloadZenodoFileModule(OnboardFileModule):
    """Download a single file from a Zenodo record."""

    _module_type_name = "download.file.from.zenodo"

    def create_onboard_inputs_schema(self) -> Dict[str, Any]:

        result: Dict[str, Dict[str, Any]] = {
            "doi": {"type": "string", "doc": "The DOI."},
            "version": {
                "type": "string",
                "doc": "The version of the record to download.",
                "optional": True,
            },
            "path": {
                "type": "string",
                "doc": "The path to the file/file name within the dataset.",
                "optional": True,
            },
        }
        return result

    def retrieve_file(
        self, inputs: ValueMap, file_name: Union[str, None], attach_metadata: bool
    ) -> Any:

        import pyzenodo3

        from kiara_plugin.onboarding.utils.download import download_file

        doi = inputs.get_value_data("doi")

        version = inputs.get_value_data("version")
        if version:
            raise NotImplementedError(
                "Downloading versioned records is not yet supported."
            )

        file_path = inputs.get_value_data("path")

        if "/zenodo." not in doi:
            doi = f"10.5281/zenodo.{doi}"

        zen = pyzenodo3.Zenodo()
        record = zen.find_record_by_doi(doi)

        if not file_path:
            if len(record.data["files"]) == 1:
                file_path = record.data["files"][0]["key"]
            else:
                msg = "Available files:\n"
                for key in record.data["files"]:
                    msg += f"  - {key['key']}\n"

                raise KiaraException(
                    msg=f"Multiple files available in Zenodo record, please specify 'path' input.\n\n{msg}"
                )

        match = None
        for _available_file in record.data["files"]:
            if file_path == _available_file["key"]:
                match = _available_file
                break

        if not match:
            msg = "Available files:\n"
            for key in record.data["files"]:
                msg += f"  - {key['key']}\n"
            raise KiaraException(
                msg=f"Can't find file '{file_path}' in Zenodo record. {msg}"
            )

        url = match["links"]["self"]
        checksum = match["checksum"][4:]

        file_name = file_path.split("/")[-1]

        result_file: KiaraFile
        result_file, result_checksum = download_file(  # type: ignore
            url=url,
            file_name=file_name,
            attach_metadata=attach_metadata,
            return_md5_hash=True,
        )

        if checksum != result_checksum:
            raise KiaraException(
                msg=f"Can't download file '{file_name}' from zenodo, invalid checksum: {checksum} != {checksum}"
            )

        if attach_metadata:
            result_file.metadata["zenodo_record_data"] = record.data

        return result_file


class DownloadZenodoFileBundleModule(OnboardFileBundleModule):
    """Download a file bundle from a remote github repository.

    If 'sub_path' is not specified, the whole repo will be used.

    """

    _module_type_name = "download.file_bundle.from.zenodo"

    def create_onboard_inputs_schema(self) -> Dict[str, Any]:
        result: Dict[str, Dict[str, Any]] = {
            "doi": {"type": "string", "doc": "The DOI."},
            "version": {
                "type": "string",
                "doc": "The version of the record to download. By default, the latest version will be used.",
                "optional": True,
            },
        }
        return result

    def retrieve_archive(
        self,
        inputs: ValueMap,
        bundle_name: Union[str, None],
        attach_metadata_to_bundle: bool,
        attach_metadata_to_files: bool,
        import_config: "FolderImportConfig",
    ) -> Union["KiaraFile", "KiaraFileBundle"]:

        from kiara_plugin.onboarding.utils.download import download_zenodo_file_bundle

        doi = inputs.get_value_data("doi")
        version = inputs.get_value_data("version")

        result = download_zenodo_file_bundle(
            doi=doi,
            version=version,
            attach_metadata_to_bundle=attach_metadata_to_bundle,
            attach_metadata_to_files=attach_metadata_to_files,
            bundle_name=bundle_name,
            import_config=import_config,
        )
        return result


# -*- coding: utf-8 -*-
import atexit
import os
import tempfile
from datetime import datetime
from functools import lru_cache
from pathlib import Path
from typing import Any, Dict, List, Mapping, Tuple, Type, Union

from pydantic import BaseModel, Field

from kiara.exceptions import KiaraException
from kiara.models.filesystem import FolderImportConfig, KiaraFile, KiaraFileBundle
from kiara.utils.dates import get_current_time_incl_timezone
from kiara.utils.files import unpack_archive
from kiara.utils.json import orjson_dumps
from kiara_plugin.onboarding.models import OnboardDataModel


class DownloadMetadata(BaseModel):
    url: str = Field(description="The url of the download request.")
    response_headers: List[Dict[str, str]] = Field(
        description="The response headers of the download request."
    )
    request_time: datetime = Field(description="The time the request was made.")
    download_time_in_seconds: float = Field(
        description="How long the download took in seconds."
    )


class DownloadBundleMetadata(DownloadMetadata):
    import_config: FolderImportConfig = Field(
        description="The import configuration that was used to import the files from the source bundle."
    )


@lru_cache()
def get_onboard_model_cls(
    onboard_type: Union[str, None]
) -> Union[None, Type[OnboardDataModel]]:

    if not onboard_type:
        return None

    from kiara.registries.models import ModelRegistry

    model_registry = ModelRegistry.instance()
    model_cls = model_registry.get_model_cls(onboard_type, OnboardDataModel)
    return model_cls  # type: ignore


def download_file(
    url: str,
    target: Union[str, None] = None,
    file_name: Union[str, None] = None,
    attach_metadata: bool = True,
    return_md5_hash: bool = False,
) -> Union[KiaraFile, Tuple[KiaraFile, str]]:

    import hashlib

    import httpx

    if not file_name:
        # TODO: make this smarter, using content-disposition headers if available
        file_name = url.split("/")[-1]

    if not target:
        tmp_file = tempfile.NamedTemporaryFile(delete=False, suffix=file_name)

        def rm_tmp_file():
            tmp_file.close()
            os.unlink(tmp_file.name)

        atexit.register(rm_tmp_file)

        _target = Path(tmp_file.name)
    else:
        _target = Path(target)
        _target.parent.mkdir(parents=True, exist_ok=True)

    if return_md5_hash:
        hash_md5 = hashlib.md5()  # noqa

    history = []

    request_time = get_current_time_incl_timezone()

    with open(_target, "wb") as f:
        with httpx.stream("GET", url, follow_redirects=True) as r:
            if r.status_code < 200 or r.status_code >= 399:
                raise KiaraException(
                    f"Could not download file from {url}: status code {r.status_code}."
                )
            history.append(dict(r.headers))
            for h in r.history:
                history.append(dict(h.headers))
            for data in r.iter_bytes():
                if return_md5_hash:
                    hash_md5.update(data)
                f.write(data)

    result_file = KiaraFile.load_file(_target.as_posix(), file_name)
    now_time = get_current_time_incl_timezone()
    delta = (now_time - request_time).total_seconds()
    if attach_metadata:
        metadata = {
            "url": url,
            "response_headers": history,
            "request_time": request_time,
            "download_time_in_seconds": delta,
        }
        _metadata: DownloadMetadata = DownloadMetadata(**metadata)
        result_file.metadata["download_info"] = _metadata.model_dump()
        result_file.metadata_schemas["download_info"] = orjson_dumps(
            DownloadMetadata.model_json_schema()
        )

    if return_md5_hash:
        return result_file, hash_md5.hexdigest()
    else:
        return result_file


def download_file_bundle(
    url: str,
    attach_metadata: bool = True,
    import_config: Union[FolderImportConfig, None] = None,
) -> KiaraFileBundle:

    import shutil
    from datetime import datetime
    from urllib.parse import urlparse

    import httpx
    import pytz

    suffix = None
    try:
        parsed_url = urlparse(url)
        _, suffix = os.path.splitext(parsed_url.path)
    except Exception:
        pass
    if not suffix:
        suffix = ""

    tmp_file = tempfile.NamedTemporaryFile(delete=False, suffix=suffix)

    def rm_tmp_file():
        tmp_file.close()
        os.unlink(tmp_file.name)

    atexit.register(rm_tmp_file)

    history = []
    # datetime.utcnow().replace(tzinfo=pytz.utc)
    with open(tmp_file.name, "wb") as f:
        with httpx.stream("GET", url, follow_redirects=True) as r:
            history.append(dict(r.headers))
            for h in r.history:
                history.append(dict(h.headers))
            for data in r.iter_bytes():
                f.write(data)

    out_dir = tempfile.mkdtemp()

    def del_out_dir():
        shutil.rmtree(out_dir, ignore_errors=True)

    atexit.register(del_out_dir)

    # error = None
    # try:
    #     shutil.unpack_archive(tmp_file.name, out_dir)
    # except Exception:
    #     # try patool, maybe we're lucky
    #     try:
    #         import patoolib
    #
    #         patoolib.extract_archive(tmp_file.name, outdir=out_dir)
    #     except Exception as e:
    #         error = e
    #
    # if error is not None:
    #     raise KiaraException(msg=f"Could not extract archive: {error}.")

    unpack_archive(tmp_file.name, out_dir)
    bundle = KiaraFileBundle.import_folder(out_dir, import_config=import_config)

    if import_config is None:
        ic_dict = {}
    elif isinstance(import_config, FolderImportConfig):
        ic_dict = import_config.dict()
    else:
        ic_dict = import_config
    if attach_metadata:
        metadata = {
            "url": url,
            "response_headers": history,
            "request_time": datetime.utcnow().replace(tzinfo=pytz.utc).isoformat(),
            "import_config": ic_dict,
        }
        _metadata = DownloadBundleMetadata(**metadata)
        bundle.metadata["download_info"] = _metadata.model_dump()
        bundle.metadata_schemas["download_info"] = DownloadMetadata.schema_json()

    return bundle


def find_matching_onboard_models(
    uri: str, for_bundle: bool = False
) -> Mapping[Type[OnboardDataModel], Tuple[bool, str]]:

    from kiara.registries.models import ModelRegistry

    model_registry = ModelRegistry.instance()
    onboard_models = model_registry.get_models_of_type(
        OnboardDataModel
    ).item_infos.values()

    result = {}
    onboard_model: Type[OnboardDataModel]
    for onboard_model in onboard_models:  # type: ignore

        python_cls: Type[OnboardDataModel] = onboard_model.python_class.get_class()  # type: ignore
        if for_bundle:
            result[python_cls] = python_cls.accepts_bundle_uri(uri)
        else:
            result[python_cls] = python_cls.accepts_uri(uri)

    return result


def onboard_file(
    source: str,
    file_name: Union[str, None] = None,
    onboard_type: Union[str, None] = None,
    attach_metadata: bool = True,
) -> KiaraFile:

    if not onboard_type:

        model_clsses = find_matching_onboard_models(source)
        matches = [k for k, v in model_clsses.items() if v[0]]
        if not matches:
            raise KiaraException(
                msg=f"Can't onboard file from '{source}': no onboard models found that accept this source type."
            )
        elif len(matches) > 1:
            msg = "Valid onboarding types for this uri:\n\n"
            for k, v in model_clsses.items():
                if not v[0]:
                    continue
                msg += f"  - {k._kiara_model_id}: {v[1]}\n"
            raise KiaraException(
                msg=f"Can't onboard file from '{source}': multiple onboard models found that accept this source type.\n\n{msg}"
            )

        model_cls: Type[OnboardDataModel] = matches[0]

    else:

        model_cls = get_onboard_model_cls(onboard_type=onboard_type)  # type: ignore
        if not model_cls:
            raise KiaraException(msg=f"Can't onboard file from '{source}' using onboard type '{onboard_type}': no onboard model found with this name.")  # type: ignore

        valid, msg = model_cls.accepts_uri(source)
        if not valid:
            raise KiaraException(msg=f"Can't onboard file from '{source}' using onboard type '{model_cls._kiara_model_id}': {msg}")  # type: ignore

    if not model_cls.get_config_fields():
        model = model_cls()
    else:
        raise NotImplementedError()

    result = model.retrieve(
        uri=source, file_name=file_name, attach_metadata=attach_metadata
    )
    if not result:
        raise KiaraException(msg=f"Can't onboard file from '{source}' using onboard type '{model_cls._kiara_model_id}': no result data retrieved. This is most likely a bug.")  # type: ignore

    if isinstance(result, str):
        data = KiaraFile.load_file(result, file_name=file_name)
    elif not isinstance(result, KiaraFile):
        raise KiaraException(
            "Can't onboard file: onboard model returned data that is not a file. This is most likely a bug."
        )
    else:
        data = result

    return data


def onboard_file_bundle(
    source: str,
    import_config: Union[FolderImportConfig, None],
    onboard_type: Union[str, None] = None,
    attach_metadata: bool = True,
) -> KiaraFileBundle:

    if not onboard_type:

        model_clsses = find_matching_onboard_models(uri=source, for_bundle=True)
        matches = [k for k, v in model_clsses.items() if v[0]]
        if not matches:
            raise KiaraException(
                msg=f"Can't onboard file from '{source}': no onboard models found that accept this source type."
            )
        elif len(matches) > 1:
            msg = "Valid onboarding types for this uri:\n\n"
            for k, v in model_clsses.items():
                if not v[0]:
                    continue
                msg += f"  - {k._kiara_model_id}: {v[1]}\n"
            raise KiaraException(
                msg=f"Can't onboard file from '{source}': multiple onboard models found that accept this source type.\n\n{msg}"
            )

        model_cls: Type[OnboardDataModel] = matches[0]

    else:
        model_cls = get_onboard_model_cls(onboard_type=onboard_type)  # type: ignore
        if not model_cls:
            raise KiaraException(msg=f"Can't onboard file from '{source}' using onboard type '{onboard_type}': no onboard model found with this name.")  # type: ignore
        valid, msg = model_cls.accepts_bundle_uri(source)
        if not valid:
            raise KiaraException(msg=f"Can't onboard file from '{source}' using onboard type '{model_cls._kiara_model_id}': {msg}")  # type: ignore

    if not model_cls.get_config_fields():
        model = model_cls()
    else:
        raise NotImplementedError()

    if not import_config:
        import_config = FolderImportConfig()

    try:
        result: Union[None, KiaraFileBundle] = model.retrieve_bundle(
            uri=source, import_config=import_config, attach_metadata=attach_metadata
        )

        if not result:
            raise KiaraException(msg=f"Can't onboard file bundle from '{source}' using onboard type '{model_cls._kiara_model_id}': no result data retrieved. This is most likely a bug.")  # type: ignore

        if isinstance(result, str):
            result = KiaraFileBundle.import_folder(source=result)

    except NotImplementedError:
        result = None

    if not result:
        result_file = model.retrieve(
            uri=source, file_name=None, attach_metadata=attach_metadata
        )
        if not result_file:
            raise KiaraException(msg=f"Can't onboard file bundle from '{source}' using onboard type '{model_cls._kiara_model_id}': no result data retrieved. This is most likely a bug.")  # type: ignore

        if isinstance(result, str):
            imported_bundle_file = KiaraFile.load_file(result_file)  # type: ignore
        elif not isinstance(result_file, KiaraFile):
            raise KiaraException(
                "Can't onboard file: onboard model returned data that is not a file. This is most likely a bug."
            )
        else:
            imported_bundle_file = result_file

        imported_bundle = KiaraFileBundle.from_archive_file(
            imported_bundle_file, import_config=import_config
        )
    else:
        imported_bundle = result

    return imported_bundle


def download_zenodo_file_bundle(
    doi: str,
    version: Union[None, str],
    attach_metadata_to_bundle: bool,
    attach_metadata_to_files: bool,
    bundle_name: Union[str, None] = None,
    import_config: Union[None, Mapping[str, Any], FolderImportConfig] = None,
) -> KiaraFileBundle:

    import pyzenodo3

    from kiara.models.filesystem import KiaraFile, KiaraFileBundle

    if "/zenodo." not in doi:
        doi = f"10.5281/zenodo.{doi}"

    zen = pyzenodo3.Zenodo()

    if version:
        raise NotImplementedError("Downloading versioned records is not yet supported.")

    record = zen.find_record_by_doi(doi)

    base_path = KiaraFileBundle.create_tmp_dir()

    for _available_file in record.data["files"]:
        match = _available_file

        url = match["links"]["self"]
        checksum = match["checksum"][4:]

        file_path = _available_file["key"]
        full_path = base_path / file_path

        file_name = file_path.split("/")[-1]

        # TODO: filter here already, so we don't need to download files we don't want

        result_file: KiaraFile
        result_file, result_checksum = download_file(  # type: ignore
            url=url,
            target=full_path.as_posix(),
            file_name=file_name,
            attach_metadata=True,
            return_md5_hash=True,
        )

        if checksum != result_checksum:
            raise KiaraException(
                msg=f"Can't download file '{file_name}' from zenodo, invalid checksum: {checksum} != {result_checksum}"
            )

    if not bundle_name:
        bundle_name = doi
    result = KiaraFileBundle.import_folder(
        source=base_path.as_posix(),
        bundle_name=bundle_name,
        import_config=import_config,
    )
    if attach_metadata_to_bundle:
        result.metadata["zenodo_record_data"] = record.data

    if attach_metadata_to_files:
        for file in result.included_files.values():
            file.metadata["zenodo_record_data"] = record.data

    return result


# -*- coding: utf-8 -*-

"""This module contains the metadata (and other) models that are used in the ``kiara_plugin.onboarding`` package.

Those models are convenience wrappers that make it easier for *kiara* to find, create, manage and version metadata -- but also
other type of models -- that is attached to data, as well as *kiara* modules.

Metadata models must be a sub-class of [kiara.metadata.MetadataModel][kiara.metadata.MetadataModel]. Other models usually
sub-class a pydantic BaseModel or implement custom base classes.
"""
import os.path
from abc import abstractmethod
from typing import ClassVar, List, Tuple, Union

from kiara.exceptions import KiaraException
from kiara.models import KiaraModel
from kiara.models.filesystem import FolderImportConfig, KiaraFile, KiaraFileBundle


class OnboardDataModel(KiaraModel):

    _kiara_model_id: ClassVar[str] = None  # type: ignore

    @classmethod
    def get_config_fields(cls) -> List[str]:
        return sorted(cls.model_fields.keys())

    @classmethod
    @abstractmethod
    def accepts_uri(cls, uri: str) -> Tuple[bool, str]:
        pass

    @classmethod
    def accepts_bundle_uri(cls, uri: str) -> Tuple[bool, str]:
        return cls.accepts_uri(uri)

    @abstractmethod
    def retrieve(
        self, uri: str, file_name: Union[None, str], attach_metadata: bool
    ) -> KiaraFile:
        pass

    def retrieve_bundle(
        self, uri: str, import_config: FolderImportConfig, attach_metadata: bool
    ) -> KiaraFileBundle:
        raise NotImplementedError()


class FileFromLocalModel(OnboardDataModel):

    _kiara_model_id: ClassVar[str] = "onboarding.file.from.local_file"

    @classmethod
    def accepts_uri(cls, uri: str) -> Tuple[bool, str]:

        if os.path.isfile(os.path.abspath(uri)):
            return True, "local file exists and is file"
        else:
            return False, "local file does not exist or is not a file"

    @classmethod
    def accepts_bundle_uri(cls, uri: str) -> Tuple[bool, str]:

        if os.path.isdir(os.path.abspath(uri)):
            return True, "local folder exists and is folder"
        else:
            return False, "local folder does not exist or is not a folder"

    def retrieve(
        self, uri: str, file_name: Union[None, str], attach_metadata: bool
    ) -> KiaraFile:

        if not os.path.exists(os.path.abspath(uri)):
            raise KiaraException(
                f"Can't create file from path '{uri}': path does not exist."
            )
        if not os.path.isfile(os.path.abspath(uri)):
            raise KiaraException(
                f"Can't create file from path '{uri}': path is not a file."
            )

        return KiaraFile.load_file(uri)

    def retrieve_bundle(
        self, uri: str, import_config: FolderImportConfig, attach_metadata: bool
    ) -> KiaraFileBundle:

        if not os.path.exists(os.path.abspath(uri)):
            raise KiaraException(
                f"Can't create file from path '{uri}': path does not exist."
            )
        if not os.path.isdir(os.path.abspath(uri)):
            raise KiaraException(
                f"Can't create file from path '{uri}': path is not a directory."
            )

        return KiaraFileBundle.import_folder(source=uri, import_config=import_config)


class FileFromRemoteModel(OnboardDataModel):

    _kiara_model_id: ClassVar[str] = "onboarding.file.from.url"

    @classmethod
    def accepts_uri(cls, uri: str) -> Tuple[bool, str]:

        accepted_protocols = ["http", "https"]
        for protocol in accepted_protocols:
            if uri.startswith(f"{protocol}://"):
                return True, "url is valid (starts with http or https)"

        return False, "url is not valid (does not start with http or https)"

    def retrieve(
        self, uri: str, file_name: Union[None, str], attach_metadata: bool
    ) -> KiaraFile:
        from kiara_plugin.onboarding.utils.download import download_file

        result_file: KiaraFile = download_file(  # type: ignore
            url=uri, file_name=file_name, attach_metadata=attach_metadata
        )
        return result_file

    def retrieve_bundle(
        self, uri: str, import_config: FolderImportConfig, attach_metadata: bool
    ) -> KiaraFileBundle:
        from kiara_plugin.onboarding.utils.download import download_file_bundle

        result_bundle = download_file_bundle(
            url=uri, import_config=import_config, attach_metadata=attach_metadata
        )
        return result_bundle


class FileFromZenodoModel(OnboardDataModel):

    _kiara_model_id: ClassVar[str] = "onboarding.file.from.zenodo"

    @classmethod
    def accepts_uri(cls, uri: str) -> Tuple[bool, str]:

        if uri.startswith("zenodo:"):
            return True, "url is valid (follows format 'zenodo:<doi>')"

        elif "/zenodo." in uri:
            return True, "url is valid (contains '/zenodo.')"

        return False, "url is not valid (does not follow format 'zenodo:<doi>')"

    def retrieve(
        self, uri: str, file_name: Union[None, str], attach_metadata: bool
    ) -> KiaraFile:

        import pyzenodo3

        from kiara_plugin.onboarding.utils.download import download_file

        if uri.startswith("zenodo:"):
            doi = uri[len("zenodo:") :]
        elif "/zenodo." in uri:
            doi = uri

        tokens = doi.split("/zenodo.")
        if len(tokens) != 2:
            raise KiaraException(
                msg=f"Can't parse Zenodo DOI from URI for single file download: {doi}"
            )

        path_components = tokens[1].split("/", maxsplit=1)
        if len(path_components) != 2:
            raise KiaraException(
                msg=f"Can't parse Zenodo DOI from URI for single file download: {doi}"
            )

        file_path = path_components[1]
        _doi = f"{tokens[0]}/zenodo.{path_components[0]}"

        zen = pyzenodo3.Zenodo()
        record = zen.find_record_by_doi(_doi)

        match = None
        for _available_file in record.data["files"]:
            if file_path == _available_file["key"]:
                match = _available_file
                break

        if not match:
            msg = "Available files:\n"
            for key in record.data["files"]:
                msg += f"  - {key['key']}\n"
            raise KiaraException(
                msg=f"Can't find file '{file_path}' in Zenodo record. {msg}"
            )

        url = match["links"]["self"]
        checksum = match["checksum"][4:]

        file_name = file_path.split("/")[-1]

        file_model: KiaraFile
        file_model, md5_digest = download_file(  # type: ignore
            url=url,
            target=None,
            file_name=file_name,
            attach_metadata=attach_metadata,
            return_md5_hash=True,
        )

        if checksum != md5_digest:
            raise KiaraException(
                msg=f"Can't download file '{file_name}', invalid checksum: {checksum} != {md5_digest}"
            )

        if attach_metadata:
            file_model.metadata["zenodo_record_data"] = record.data

        return file_model

    def retrieve_bundle(
        self, uri: str, import_config: FolderImportConfig, attach_metadata: bool
    ) -> KiaraFileBundle:

        import shutil

        import pyzenodo3

        from kiara_plugin.onboarding.utils.download import download_file

        if uri.startswith("zenodo:"):
            doi = uri[len("zenodo:") :]
        elif "/zenodo." in uri:
            doi = uri

        tokens = doi.split("/zenodo.")
        if len(tokens) != 2:
            raise KiaraException(
                msg=f"Can't parse Zenodo DOI from URI for single file download: {doi}"
            )

        path_components = tokens[1].split("/", maxsplit=1)

        if len(path_components) == 2:
            zid = path_components[0]
            file_path = path_components[1]
        else:
            zid = path_components[0]
            file_path = None

        _doi = f"{tokens[0]}/zenodo.{zid}"

        if not file_path:

            zen = pyzenodo3.Zenodo()

            record = zen.find_record_by_doi(_doi)

            path = KiaraFileBundle.create_tmp_dir()
            shutil.rmtree(path, ignore_errors=True)
            path.mkdir()

            for file_data in record.data["files"]:
                url = file_data["links"]["self"]
                file_name = file_data["key"]
                checksum = file_data["checksum"][4:]

                target = os.path.join(path, file_name)
                file_model: KiaraFile
                file_model, md5_digest = download_file(  # type: ignore
                    url=url,
                    target=target,
                    file_name=file_name,
                    attach_metadata=attach_metadata,
                    return_md5_hash=True,
                )

                if checksum != md5_digest:
                    raise KiaraException(
                        msg=f"Can't download file '{file_name}', invalid checksum: {checksum} != {md5_digest}"
                    )

            bundle = KiaraFileBundle.import_folder(path.as_posix())
            if attach_metadata:
                bundle.metadata["zenodo_record_data"] = record.data

        else:

            zen = pyzenodo3.Zenodo()
            record = zen.find_record_by_doi(_doi)

            match = None
            for _available_file in record.data["files"]:
                if file_path == _available_file["key"]:
                    match = _available_file
                    break

            if not match:
                msg = "Available files:\n"
                for key in record.data["files"]:
                    msg += f"  - {key['key']}\n"
                raise KiaraException(
                    msg=f"Can't find file '{file_path}' in Zenodo record. {msg}"
                )

            url = match["links"]["self"]
            checksum = match["checksum"][4:]

            file_name = file_path.split("/")[-1]

            file_model, md5_digest = download_file(  # type: ignore
                url=url,
                target=None,
                file_name=file_name,
                attach_metadata=attach_metadata,
                return_md5_hash=True,
            )

            if checksum != md5_digest:
                raise KiaraException(
                    msg=f"Can't download file '{file_name}', invalid checksum: {checksum} != {md5_digest}"
                )

            bundle = KiaraFileBundle.from_archive_file(
                archive_file=file_model, import_config=import_config
            )
            if attach_metadata:
                bundle.metadata["zenodo_record_data"] = record.data

        return bundle


class FileFromZoteroModel(OnboardDataModel):

    _kiara_model_id: ClassVar[str] = "onboarding.file.from.zotero"

    @classmethod
    def accepts_uri(cls, uri: str) -> Tuple[bool, str]:
        if uri.startswith("zotero:"):
            return True, "uri is a zotero uri"
        else:
            return False, "uri is not a zotero uri, must start with 'zotero:'"


class FileFromGithubModel(OnboardDataModel):

    _kiara_model_id: ClassVar[str] = "onboarding.file.from.github"

    @classmethod
    def accepts_uri(cls, uri: str) -> Tuple[bool, str]:

        if uri.startswith("gh:") or uri.startswith("github:"):
            return True, "uri is a github uri"

        return False, "uri is not a github uri, must start with 'gh:' or 'github:'"

    def retrieve(
        self, uri: str, file_name: Union[None, str], attach_metadata: bool
    ) -> KiaraFile:
        from kiara_plugin.onboarding.utils.download import download_file

        tokens = uri.split(":")[1].split("/", maxsplit=3)
        if len(tokens) != 4:
            raise KiaraException(
                msg=f"Can't parse github uri '{uri}' for single file download. Required format: 'gh:<user>/<repo>/<branch_or_tag>/<path>'"
            )

        url = f"https://raw.githubusercontent.com/{tokens[0]}/{tokens[1]}/{tokens[2]}/{tokens[3]}"

        result_file: KiaraFile = download_file(  # type: ignore
            url=url, attach_metadata=attach_metadata
        )
        return result_file

    def retrieve_bundle(
        self, uri: str, import_config: FolderImportConfig, attach_metadata: bool
    ) -> KiaraFileBundle:

        from kiara_plugin.onboarding.utils.download import download_file

        tokens = uri.split(":")[1].split("/", maxsplit=3)
        if len(tokens) == 3:
            sub_path = None
        elif len(tokens) != 4:
            raise KiaraException(
                msg=f"Can't parse github uri '{uri}' for single file download. Required format: 'gh:<user>/<repo>/<branch_or_tag>/<path>'"
            )
        else:
            sub_path = tokens[3]

        url = f"https://github.com/{tokens[0]}/{tokens[1]}/archive/refs/heads/{tokens[2]}.zip"
        file_name = f"{tokens[1]}-{tokens[2]}.zip"

        archive_zip: KiaraFile
        archive_zip = download_file(  # type: ignore
            url=url,
            attach_metadata=attach_metadata,
            file_name=file_name,
            return_md5_hash=False,
        )

        base_sub_path = f"{tokens[1]}-{tokens[2]}"

        if sub_path:
            if import_config.sub_path:
                new_sub_path = "/".join([base_sub_path, sub_path, import_config.sub_path])  # type: ignore
            else:
                new_sub_path = "/".join([base_sub_path, sub_path])
        elif import_config.sub_path:
            new_sub_path = "/".join([base_sub_path, import_config.sub_path])
        else:
            new_sub_path = base_sub_path

        import_config_new = import_config.copy(update={"sub_path": new_sub_path})

        result_bundle = KiaraFileBundle.from_archive_file(
            archive_file=archive_zip, import_config=import_config_new
        )

        return result_bundle


# -*- coding: utf-8 -*-

"""Generate the code reference pages and navigation."""

from pathlib import Path

import mkdocs_gen_files

nav = mkdocs_gen_files.Nav()

for path in sorted(Path("src/kiara").rglob("*.py")):
    if "resources" in path.as_posix():
        continue
    module_path = path.relative_to("src").with_suffix("")
    doc_path = path.relative_to("src").with_suffix(".md")
    full_doc_path = Path("reference", doc_path)

    parts = list(module_path.parts)

    if parts[-1] == "__init__":
        parts = parts[:-1]
    elif parts[-1] == "__main__":
        continue

    nav[parts] = doc_path  #

    with mkdocs_gen_files.open(full_doc_path, "w") as fd:
        ident = ".".join(parts)
        print("::: " + ident, file=fd)

    mkdocs_gen_files.set_edit_path(full_doc_path, path)

with mkdocs_gen_files.open("reference/SUMMARY.md", "w") as nav_file:  #
    nav_file.writelines(nav.build_literate_nav())  #

# -*- coding: utf-8 -*-

import builtins

from kiara.context import Kiara, KiaraContextInfo
from kiara.doc.gen_info_pages import generate_detail_pages

pkg_name = "kiara"

kiara: Kiara = Kiara.instance()
context_info = KiaraContextInfo.create_from_kiara_instance(
    kiara=kiara, package_filter=pkg_name
)

generate_detail_pages(
    context_info=context_info, sub_path="included_components", add_summary_page=True
)

builtins.plugin_package_context_info = context_info

# -*- coding: utf-8 -*-
import inspect
import os
import typing

import mkdocs_gen_files

from kiara.doc.mkdocs_macros_kiara import KIARA_MODEL_CLASSES


def class_namespace(cls: typing.Type):

    module = cls.__module__
    if module is None or module == str.__class__.__module__:
        return cls.__name__
    else:
        return module + "." + cls.__name__


overview_file_path = os.path.join("development", "entities", "index.md")
overview = """# Schemas overviews

This page contains an overview of the available models and their associated schemas used in *kiara*.

"""

for category, classes in KIARA_MODEL_CLASSES.items():

    overview = overview + f"## {category.capitalize()}\n\n"

    file_path = os.path.join("development", "entities", f"{category}.md")

    content = f"# {category.capitalize()}\n\n"

    for cls in classes:

        doc = cls.__doc__

        if doc is None:
            doc = ""

        doc = inspect.cleandoc(doc)

        doc_short = doc.split("\n")[0]
        if doc_short:
            doc_str = f": {doc_short}"
        else:
            doc_str = ""

        overview = (
            overview
            + f"  - [``{cls.__name__}``]({category}{os.path.sep}#{cls.__name__.lower()}){doc_str}\n"
        )

        namescace = class_namespace(cls)
        download_link = f'<a href="{cls.__name__}.json">{cls.__name__}.json</a>'

        # content = content + f"## {cls.__name__}\n\n" + "{{ get_schema_for_model('" + class_namespace(cls) + ") }}\n\n"
        content = content + f"## {cls.__name__}\n\n"
        content = content + doc + "\n\n"
        content = content + "#### References\n\n"
        content = (
            content + f"  - model class reference: [{cls.__name__}][{namescace}]\n"
        )
        content = content + f"  - JSON schema file: {download_link}\n\n"
        content = content + "#### JSON schema\n\n"
        content = (
            content
            + "``` json\n{{ get_schema_for_model('"
            + namescace
            + "') }}\n```\n\n"
        )

    with mkdocs_gen_files.open(file_path, "w") as f:
        f.write(content)

with mkdocs_gen_files.open(overview_file_path, "w") as f:
    f.write(overview)

# -*- coding: utf-8 -*-
from kiara.api import Kiara

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)


def test_operation_type_list(kiara: Kiara):

    assert "pretty_print" in kiara.operation_registry.operation_types.keys()

    op_xor = kiara.operation_registry.get_operation("logic.xor")
    assert op_xor.module.is_pipeline()
    assert op_xor.operation_id == "logic.xor"

import sys

from setuptools import setup

try:
    from pkg_resources import VersionConflict, require

    require("setuptools>=38.3")
except VersionConflict:
    print("Error: version of setuptools is too old (<38.3)!")
    sys.exit(1)


def get_extra_requires(add_all=True, add_all_dev=True):

    from distutils.dist import Distribution

    dist = Distribution()
    dist.parse_config_files()
    dist.parse_command_line()

    extras = {}
    extra_deps = dist.get_option_dict("options.extras_require")

    for extra_name, data in extra_deps.items():

        _, dep_string = data
        deps = []
        d = dep_string.split("\n")
        for line in d:
            if not line:
                continue
            deps.append(line)
        extras[extra_name] = deps

    if add_all:
        all = set()
        for e_n, deps in extras.items():
            if not e_n.startswith("dev_"):
                all.update(deps)
        extras["all"] = all

    # add tag `all` at the end
    if add_all_dev:
        extras["all_dev"] = set(vv for v in extras.values() for vv in v)
        extras["dev_all"] = extras["all_dev"]

    return extras


if __name__ in ["__main__", "builtins", "__builtin__"]:
    setup(
        use_scm_version={"write_to": "src/kiara_modules/core/version.txt"},
        extras_require=get_extra_requires(),
    )

# -*- coding: utf-8 -*-
import typing
from pathlib import Path

from kiara.data import Value
from kiara.operations.store_value import StoreValueTypeModule

from kiara_modules.core.generic import JsonSerializationConfig


class SaveDictModule(StoreValueTypeModule):

    _config_cls = JsonSerializationConfig
    _module_type_name = "store"

    @classmethod
    def retrieve_supported_types(cls) -> typing.Union[str, typing.Iterable[str]]:
        return "dict"

    def store_value(self, value: Value, base_path: str) -> typing.Dict[str, typing.Any]:

        import orjson

        options = self.get_config_value("options")
        file_name = self.get_config_value("file_name")
        json_str = orjson.dumps(value.get_value_data(), option=options)

        bp = Path(base_path)
        bp.mkdir(parents=True, exist_ok=True)

        full_path = bp / file_name
        full_path.write_bytes(json_str)

        load_config = {
            "module_type": "generic.restore_from_json",
            "base_path_input_name": "base_path",
            "inputs": {
                "base_path": base_path,
                "file_name": self.get_config_value("file_name"),
            },
            "output_name": "value_item",
        }

        return load_config

# -*- coding: utf-8 -*-
#  Copyright (c) 2022, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)
import typing

from kiara import KiaraModule
from kiara.data import ValueSet
from kiara.data.values import ValueSchema
from kiara.module_config import ModuleTypeConfigSchema
from pydantic import Field
from sqlalchemy import create_engine

from kiara_modules.core.metadata_models import KiaraDatabase


class QueryDatabaseSQLModuleConfig(ModuleTypeConfigSchema):

    query: typing.Optional[str] = Field(
        description="The query to execute. If not specified, the user will be able to provide their own.",
        default=None,
    )


class QueryTableSQL(KiaraModule):
    """Execute a sql query against an (Arrow) table."""

    _module_type_name = "sql"
    _config_cls = QueryDatabaseSQLModuleConfig

    def create_input_schema(
        self,
    ) -> typing.Mapping[
        str, typing.Union[ValueSchema, typing.Mapping[str, typing.Any]]
    ]:

        inputs = {
            "database": {
                "type": "database",
                "doc": "The database to query",
            }
        }

        if self.get_config_value("query") is None:
            inputs["query"] = {"type": "string", "doc": "The query."}

        return inputs

    def create_output_schema(
        self,
    ) -> typing.Mapping[
        str, typing.Union[ValueSchema, typing.Mapping[str, typing.Any]]
    ]:

        return {"query_result": {"type": "table", "doc": "The query result."}}

    def process(self, inputs: ValueSet, outputs: ValueSet) -> None:

        import pandas as pd
        import pyarrow as pa

        if self.get_config_value("query") is None:
            _query: str = inputs.get_value_data("query")
        else:
            _query = self.get_config_value("query")

        _database: KiaraDatabase = inputs.get_value_data("database")

        # can't re-use the default engine, because pandas does not support having the 'future' flag set to 'True'
        engine = create_engine(_database.db_url)
        df = pd.read_sql(_query, con=engine)
        table = pa.Table.from_pandas(df)

        outputs.set_value("query_result", table)

# -*- coding: utf-8 -*-
#  Copyright (c) 2022, University of Luxembourg / DHARPA project
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import typing
from pathlib import Path
from typing import Iterable, Optional

from jinja2 import BaseLoader, Environment
from kiara.utils.output import DictTabularWrap, TabularWrap
from pydantic import BaseModel, Field
from sqlite_utils.cli import insert_upsert_implementation

from kiara_modules.core.defaults import TEMPLATES_FOLDER
from kiara_modules.core.metadata_models import KiaraDatabase, KiaraFile


def create_sqlite_table_from_file(
    target_db_file: str,
    file_item: KiaraFile,
    table_name: Optional[str] = None,
    is_csv: bool = True,
    is_tsv: bool = False,
    is_nl: bool = False,
    primary_key_column_names: Optional[Iterable[str]] = None,
    flatten_nested_json_objects: bool = False,
    csv_delimiter: str = None,
    quotechar: str = None,
    sniff: bool = True,
    no_headers: bool = False,
    encoding: str = "utf-8",
    batch_size: int = 100,
    detect_types: bool = True,
):

    if not table_name:
        table_name = file_item.file_name_without_extension

    with open(file_item.path, "rb") as f:

        insert_upsert_implementation(
            path=target_db_file,
            table=table_name,
            file=f,
            pk=primary_key_column_names,
            flatten=flatten_nested_json_objects,
            nl=is_nl,
            csv=is_csv,
            tsv=is_tsv,
            lines=False,
            text=False,
            convert=None,
            imports=None,
            delimiter=csv_delimiter,
            quotechar=quotechar,
            sniff=sniff,
            no_headers=no_headers,
            encoding=encoding,
            batch_size=batch_size,
            alter=False,
            upsert=False,
            ignore=False,
            replace=False,
            truncate=False,
            not_null=None,
            default=None,
            detect_types=detect_types,
            analyze=False,
            load_extension=None,
            silent=False,
            bulk_sql=None,
        )


class SqliteTabularWrap(TabularWrap):
    def __init__(self, db: "KiaraDatabase", table_name: str):
        self._db: KiaraDatabase = db
        self._table_name: str = table_name
        super().__init__()

    def retrieve_number_of_rows(self) -> int:

        from sqlalchemy import text

        with self._db.get_sqlalchemy_engine().connect() as con:
            result = con.execute(text(f"SELECT count(*) from {self._table_name}"))
            num_rows = result.fetchone()[0]

        return num_rows

    def retrieve_column_names(self) -> typing.Iterable[str]:

        from sqlalchemy import inspect

        engine = self._db.get_sqlalchemy_engine()
        inspector = inspect(engine)
        columns = inspector.get_columns(self._table_name)
        result = [column["name"] for column in columns]
        return result

    def slice(
        self, offset: int = 0, length: typing.Optional[int] = None
    ) -> "TabularWrap":

        from sqlalchemy import text

        query = f"SELECT * FROM {self._table_name}"
        if length:
            query = f"{query} LIMIT {length}"
        else:
            query = f"{query} LIMIT {self.num_rows}"
        if offset > 0:
            query = f"{query} OFFSET {offset}"
        with self._db.get_sqlalchemy_engine().connect() as con:
            result = con.execute(text(query))
            result_dict: typing.Dict[str, typing.List[typing.Any]] = {}
            for cn in self.column_names:
                result_dict[cn] = []
            for r in result:
                for i, cn in enumerate(self.column_names):
                    result_dict[cn].append(r[i])

        return DictTabularWrap(result_dict)

    def to_pydict(self) -> typing.Mapping:

        from sqlalchemy import text

        query = f"SELECT * FROM {self._table_name}"

        with self._db.get_sqlalchemy_engine().connect() as con:
            result = con.execute(text(query))
            result_dict: typing.Dict[str, typing.List[typing.Any]] = {}
            for cn in self.column_names:
                result_dict[cn] = []
            for r in result:
                for i, cn in enumerate(self.column_names):
                    result_dict[cn].append(r[i])

        return result_dict


class SqliteColumnAttributes(BaseModel):

    data_type: str = Field(
        description="The type of the data in this column.", default="ANY"
    )
    extra_column_info: typing.List[str] = Field(
        description="Additional init information for the column.", default_factory=list
    )
    create_index: bool = Field(
        description="Whether to create an index for this column or not.", default=False
    )


class SqliteTableSchema(BaseModel):

    columns: typing.Dict[str, SqliteColumnAttributes] = Field(
        description="The table columns and their attributes."
    )
    extra_schema: typing.List[str] = Field(
        description="Extra schema information for this table.", default_factory=list
    )
    column_map: typing.Dict[str, str] = Field(
        description="A dictionary describing how to map incoming data column names. Values in this dict point to keys in this models 'columns' attribute.",
        default_factory=dict,
    )


def create_table_init_sql(
    table_name: str,
    table_schema: SqliteTableSchema,
    schema_template_str: typing.Optional[str] = None,
):
    """Create an sql script to initialize a table.

    Arguments:
        column_attrs: a map with the column name as key, and column details ('type', 'extra_column_info', 'create_index') as values
    """

    if schema_template_str is None:
        template_path = Path(TEMPLATES_FOLDER) / "sqlite_schama.sql.j2"
        schema_template_str = template_path.read_text()

    template = Environment(loader=BaseLoader()).from_string(schema_template_str)

    edges_columns = []
    edge_indexes = []
    lines = []
    for cn, details in table_schema.columns.items():
        cn_type = details.data_type
        cn_extra = details.extra_column_info

        line = f"    {cn}    {cn_type}"
        if cn_extra:
            line = f"{line}    {' '.join(cn_extra)}"

        edges_columns.append(line)
        if details.create_index:
            edge_indexes.append(cn)
        lines.append(line)

    lines.extend(table_schema.extra_schema)

    rendered = template.render(
        table_name=table_name, column_info=lines, index_columns=edge_indexes
    )
    return rendered

# -*- coding: utf-8 -*-
import time
import typing

from kiara.data import ValueSet
from kiara.data.values import ValueSchema
from kiara.module import KiaraModule
from kiara.module_config import ModuleTypeConfigSchema
from pydantic import Field


class LogicProcessingModuleConfig(ModuleTypeConfigSchema):
    """Config class for all the 'logic'-related modules."""

    delay: float = Field(
        default=0,
        description="the delay in seconds from processing start to when the output is returned.",
    )


class LogicProcessingModule(KiaraModule):

    _config_cls = LogicProcessingModuleConfig


class NotModule(LogicProcessingModule):
    """Negates the input."""

    def create_input_schema(
        self,
    ) -> typing.Mapping[
        str, typing.Union[ValueSchema, typing.Mapping[str, typing.Any]]
    ]:
        """The not module only has one input, a boolean that will be negated by the module."""

        return {
            "a": {"type": "boolean", "doc": "A boolean describing this input state."}
        }

    def create_output_schema(
        self,
    ) -> typing.Mapping[
        str, typing.Union[ValueSchema, typing.Mapping[str, typing.Any]]
    ]:

        """The output of this module is a single boolean, the negated input."""

        return {
            "y": {
                "type": "boolean",
                "doc": "A boolean describing the module output state.",
            }
        }

    def process(self, inputs: ValueSet, outputs: ValueSet) -> None:
        """Negates the input boolean."""

        time.sleep(self.config.get("delay"))  # type: ignore

        outputs.set_value("y", not inputs.get_value_data("a"))


class AndModule(LogicProcessingModule):
    """Returns 'True' if both inputs are 'True'."""

    def create_input_schema(
        self,
    ) -> typing.Mapping[
        str, typing.Union[ValueSchema, typing.Mapping[str, typing.Any]]
    ]:

        return {
            "a": {"type": "boolean", "doc": "A boolean describing this input state."},
            "b": {"type": "boolean", "doc": "A boolean describing this input state."},
        }

    def create_output_schema(
        self,
    ) -> typing.Mapping[
        str, typing.Union[ValueSchema, typing.Mapping[str, typing.Any]]
    ]:

        return {
            "y": {
                "type": "boolean",
                "doc": "A boolean describing the module output state.",
            }
        }

    def process(self, inputs: ValueSet, outputs: ValueSet) -> None:

        time.sleep(self.config.delay)  # type: ignore

        outputs.set_value(
            "y", inputs.get_value_data("a") and inputs.get_value_data("b")
        )


class OrModule(LogicProcessingModule):
    """Returns 'True' if one of the inputs is 'True'."""

    def create_input_schema(
        self,
    ) -> typing.Mapping[
        str, typing.Union[ValueSchema, typing.Mapping[str, typing.Any]]
    ]:

        return {
            "a": {"type": "boolean", "doc": "A boolean describing this input state."},
            "b": {"type": "boolean", "doc": "A boolean describing this input state."},
        }

    def create_output_schema(
        self,
    ) -> typing.Mapping[
        str, typing.Union[ValueSchema, typing.Mapping[str, typing.Any]]
    ]:

        return {
            "y": {
                "type": "boolean",
                "doc": "A boolean describing the module output state.",
            }
        }

    def process(self, inputs: ValueSet, outputs: ValueSet) -> None:

        time.sleep(self.config.get("delay"))  # type: ignore
        outputs.set_value("y", inputs.get_value_data("a") or inputs.get_value_data("b"))

# -*- coding: utf-8 -*-
import typing
from pathlib import Path

from kiara.data import Value
from kiara.operations.store_value import StoreValueTypeModule

from kiara_modules.core.generic import JsonSerializationConfig


class SaveDictModule(StoreValueTypeModule):

    _config_cls = JsonSerializationConfig
    _module_type_name = "store"

    @classmethod
    def retrieve_supported_types(cls) -> typing.Union[str, typing.Iterable[str]]:
        return "dict"

    def store_value(self, value: Value, base_path: str) -> typing.Dict[str, typing.Any]:

        import orjson

        options = self.get_config_value("options")
        file_name = self.get_config_value("file_name")
        json_str = orjson.dumps(value.get_value_data(), option=options)

        bp = Path(base_path)
        bp.mkdir(parents=True, exist_ok=True)

        full_path = bp / file_name
        full_path.write_bytes(json_str)

        load_config = {
            "module_type": "generic.restore_from_json",
            "base_path_input_name": "base_path",
            "inputs": {
                "base_path": base_path,
                "file_name": self.get_config_value("file_name"),
            },
            "output_name": "value_item",
        }

        return load_config


# -*- coding: utf-8 -*-
import re
import typing
from abc import abstractmethod
from pprint import pformat

from kiara import KiaraModule
from kiara.data import ValueSet
from kiara.data.values import Value, ValueSchema
from kiara.defaults import DEFAULT_NO_DESC_VALUE
from kiara.exceptions import KiaraProcessingException
from kiara.metadata.data import DeserializeConfig
from kiara.module_config import ModuleTypeConfigSchema
from kiara.operations.serialize import SerializeValueModule
from kiara.utils import StringYAML
from kiara.utils.output import ArrowTabularWrap
from pydantic import Field
from rich import box
from rich.console import RenderableType, RenderGroup
from rich.markdown import Markdown
from rich.panel import Panel


def convert_to_renderable(
    value_type: str, data: typing.Any, render_config: typing.Mapping[str, typing.Any]
) -> typing.List[RenderableType]:

    if value_type == "table":

        max_rows = render_config.get("max_no_rows")
        max_row_height = render_config.get("max_row_height")
        max_cell_length = render_config.get("max_cell_length")

        half_lines: typing.Optional[int] = None
        if max_rows:
            half_lines = int(max_rows / 2)

        atw = ArrowTabularWrap(data)
        result = [
            atw.pretty_print(
                rows_head=half_lines,
                rows_tail=half_lines,
                max_row_height=max_row_height,
                max_cell_length=max_cell_length,
            )
        ]
    elif value_type == "value_set":
        value: Value
        result_dict = {}
        for field_name, value in data.items():
            _vt = value.value_schema.type
            _data = value.get_value_data()
            if value.value_schema.doc == DEFAULT_NO_DESC_VALUE:
                _temp: typing.List[RenderableType] = []
            else:
                md = Markdown(value.value_schema.doc)
                _temp = [Panel(md, box=box.SIMPLE)]
            _strings = convert_to_renderable(
                value_type=_vt, data=_data, render_config=render_config
            )
            _temp.extend(_strings)
            result_dict[(field_name, _vt)] = _temp

        result = []
        for k, v in result_dict.items():
            result.append(
                Panel(
                    RenderGroup(*v),
                    title=f"field: [b]{k[0]}[/b] (type: [i]{k[1]}[/i])",
                    title_align="left",
                )
            )

    else:
        result = [pformat(data)]

    return result


def convert_to_string(
    value_type: str, data: typing.Any, render_config: typing.Mapping[str, typing.Any]
) -> str:

    if value_type == "dict":
        yaml = StringYAML()
        result_string = yaml.dump(data)
    if value_type == "value_set":
        value: Value
        result = {}
        for field_name, value in data.items():
            _vt = value.value_schema.type
            _data = value.get_value_data()
            _string = convert_to_string(
                value_type=_vt, data=_data, render_config=render_config
            )
            result[field_name] = _string

        result_string = convert_to_string(
            value_type="dict", data=result, render_config=render_config
        )
    else:
        result_string = pformat(data)

    return result_string


class StringManipulationModule(KiaraModule):
    """Base module to simplify creating other modules that do string manipulation."""

    def create_input_schema(
        self,
    ) -> typing.Mapping[
        str, typing.Union[ValueSchema, typing.Mapping[str, typing.Any]]
    ]:

        return {"text": {"type": "string", "doc": "The input string."}}

    def create_output_schema(
        self,
    ) -> typing.Mapping[
        str, typing.Union[ValueSchema, typing.Mapping[str, typing.Any]]
    ]:
        return {"text": {"type": "string", "doc": "The processed string."}}

    def process(self, inputs: ValueSet, outputs: ValueSet) -> None:

        input_string = inputs.get_value_data("text")
        result = self.process_string(input_string)
        outputs.set_value("text", result)

    @abstractmethod
    def process_string(self, text: str) -> str:
        pass


class RegexModuleConfig(ModuleTypeConfigSchema):

    regex: str = Field(description="The regex to apply.")
    only_first_match: bool = Field(
        description="Whether to only return the first match, or all matches.",
        default=False,
    )


class RegexModule(KiaraModule):
    """Match a string using a regular expression."""

    _config_cls = RegexModuleConfig
    _module_type_name = "match_regex"

    def create_input_schema(
        self,
    ) -> typing.Mapping[
        str, typing.Union[ValueSchema, typing.Mapping[str, typing.Any]]
    ]:
        return {"text": {"type": "string", "doc": "The text to match."}}

    def create_output_schema(
        self,
    ) -> typing.Mapping[
        str, typing.Union[ValueSchema, typing.Mapping[str, typing.Any]]
    ]:

        if self.get_config_value("only_first_match"):
            output_schema = {"text": {"type": "string", "doc": "The first match."}}
        else:
            raise NotImplementedError()

        return output_schema

    def process(self, inputs: ValueSet, outputs: ValueSet) -> None:

        text = inputs.get_value_data("text")
        regex = self.get_config_value("regex")
        matches = re.findall(regex, text)

        if not matches:
            raise KiaraProcessingException(f"No match for regex: {regex}")

        if self.get_config_value("only_first_match"):
            result = matches[0]
        else:
            result = matches

        outputs.set_value("text", result)


class ReplaceModuleConfig(ModuleTypeConfigSchema):

    replacement_map: typing.Dict[str, str] = Field(
        description="A map, containing the strings to be replaced as keys, and the replacements as values."
    )
    default_value: typing.Optional[str] = Field(
        description="The default value to use if the string to be replaced is not in the replacement map. By default, this just returns the string itself.",
        default=None,
    )


class ReplaceStringModule(KiaraModule):
    """Replace a string if it matches a key in a mapping dictionary."""

    _config_cls = ReplaceModuleConfig
    _module_type_name = "replace"

    def create_input_schema(
        self,
    ) -> typing.Mapping[
        str, typing.Union[ValueSchema, typing.Mapping[str, typing.Any]]
    ]:

        return {"text": {"type": "string", "doc": "The input string."}}

    def create_output_schema(
        self,
    ) -> typing.Mapping[
        str, typing.Union[ValueSchema, typing.Mapping[str, typing.Any]]
    ]:
        return {"text": {"type": "string", "doc": "The replaced string."}}

    def process(self, inputs: ValueSet, outputs: ValueSet) -> None:

        text = inputs.get_value_data("text")
        repl_map = self.get_config_value("replacement_map")
        default = self.get_config_value("default_value")

        if text not in repl_map.keys():
            if default is None:
                result = text
            else:
                result = default
        else:
            result = repl_map[text]

        outputs.set_value("text", result)


# class MagicModuleConfig(ModuleTypeConfigSchema):
#
#     source_id: str = Field(description="The id of the source value.")
#     target_type: str = Field(description="The target type.")
#
#
# class MagicModule(KiaraModule):
#     def create_input_schema(
#         self,
#     ) -> typing.Mapping[
#         str, typing.Union[ValueSchema, typing.Mapping[str, typing.Any]]
#     ]:
#
#         return {
#             "description": {
#                 "type": "string",
#                 "doc": "The description of the value, and where it comes from.",
#                 "default": DEFAULT_NO_DESC_VALUE,
#             }
#         }
#
#     def create_output_schema(
#         self,
#     ) -> typing.Mapping[
#         str, typing.Union[ValueSchema, typing.Mapping[str, typing.Any]]
#     ]:
#
#         return {"value_item": {"type": self.get_config_value("target_type")}}
#
#     def process(self, inputs: ValueSet, outputs: ValueSet) -> None:
#
#         pass


class SerializeStringModule(SerializeValueModule):

    _module_type_name = "serialize"

    @classmethod
    def get_value_type(cls) -> str:

        return "string"

    def to_json(self, value: Value):

        input_data = {
            "serialized": value.get_value_data(),
        }
        ds_conf = DeserializeConfig(
            module_type="string.deserialize",
            module_config={"serialization_type": "json"},
            serialization_type="json",
            output_name="value_item",
            input=input_data,
        )
        return ds_conf


class DeserializeStringModuleConfig(ModuleTypeConfigSchema):

    serialization_type: str = Field(
        description="The serialization type that was used to serialize the value."
    )


class DeserializeStringModule(KiaraModule):

    _module_type_name = "deserialize"
    _config_cls = DeserializeStringModuleConfig

    def create_input_schema(
        self,
    ) -> typing.Mapping[
        str, typing.Union[ValueSchema, typing.Mapping[str, typing.Any]]
    ]:

        return {
            "serialized": {
                "type": "string",
                "doc": "The serialized form of the string.",
            }
        }

    def create_output_schema(
        self,
    ) -> typing.Mapping[
        str, typing.Union[ValueSchema, typing.Mapping[str, typing.Any]]
    ]:

        return {"value_item": {"type": "string", "doc": "The string data."}}

    def process(self, inputs: ValueSet, outputs: ValueSet) -> None:

        serialization_type = self.get_config_value("serialization_type")
        if serialization_type not in ["json"]:
            raise KiaraProcessingException(
                f"Can't deserialize string: serialisation type '{serialization_type}' not supported."
            )

        serialized = inputs.get_value_data("serialized")
        outputs.set_value("value_item", serialized)


# class PythonScalarSerializationConfig(SaveValueModuleConfig):
#
#     file_name: str = Field(
#         description="The name of the serialized file.", default="dict.json"
#     )


# class SaveStringModule(SaveValueTypeModule):
#
#     _module_type_name = "save"
#     # _config_cls = PythonScalarSerializationConfig
#
#     @classmethod
#     def retrieve_supported_types(cls) -> typing.Union[str, typing.Iterable[str]]:
#         return ["string", "file_path", "folder_path"]
#
#     def save_value(self, value: Value, base_path: str) -> typing.Dict[str, typing.Any]:
#
#         file_name = self.get_config_value("file_name")
#
#         bp = Path(base_path)
#         bp.mkdir(parents=True, exist_ok=True)
#
#         full_path = bp / file_name
#         full_path.write_text(value.get_value_data())
#
#         load_config = {
#             "module_type": "generic.load_scalar",
#             "base_path_input_name": None,
#             "inputs": {
#                 "data": value.get_value_data()
#             },
#             "output_name": "value_item",
#         }
#
#         return load_config

# -*- coding: utf-8 -*-
import json
import typing

from kiara import Kiara
from kiara.data.types import ValueType
from kiara.data.values import Value
from kiara.modules.type_conversion import OldTypeConversionModule

from kiara_modules.core.python import convert_to_py_obj


class JsonType(ValueType):

    pass


JSON_SUPPORTED_SOURCE_TYPES: typing.Iterable[str] = ["value_set", "table"]


def convert_to_json(
    kiara: Kiara,
    data: typing.Any,
    convert_config: typing.Mapping[str, typing.Any],
    data_type: typing.Optional[str] = None,
) -> str:

    if data_type:
        value_type_name: typing.Optional[str] = data_type
    else:
        value_type_name = None
        if isinstance(data, Value):
            value_type_name = data.value_schema.type
        else:
            _value_type = kiara.determine_type(data)
            if _value_type:
                value_type_name = _value_type._value_type_name  # type: ignore

        if not value_type_name:
            value_type_name = "any"

    if value_type_name == "value_set":
        result = {}
        for field_name, value in data.items():
            _data = value.get_value_data()
            obj = convert_to_py_obj(
                kiara=kiara, data=_data, convert_config=convert_config
            )
            result[field_name] = obj
        return convert_to_json(
            kiara=kiara, data=result, convert_config=convert_config, data_type="dict"
        )
    elif value_type_name == "dict":

        indent = convert_config.get("indent", None)
        sort_keys = convert_config.get("sort_keys", None)

        json_str = json.dumps(data, indent=indent, sort_keys=sort_keys)
        return json_str
    else:
        raise Exception(
            f"Can't convert data into json, value type '{value_type_name}' not supported."
        )


class ToJsonModuleOld(OldTypeConversionModule):
    """Convert arbitrary types into json.

    Very early days for this module, it doesn't support a lot of types yet.
    """

    _module_type_name = "to_json"

    @classmethod
    def _get_supported_source_types(self) -> typing.Union[typing.Iterable[str], str]:
        return JSON_SUPPORTED_SOURCE_TYPES

    @classmethod
    def _get_target_types(self) -> typing.Union[typing.Iterable[str], str]:
        return ["json"]

    def convert(
        self, value: Value, config: typing.Mapping[str, typing.Any]
    ) -> typing.Any:

        input_value: typing.Any = value.get_value_data()

        input_value_str = convert_to_json(
            self._kiara, data=input_value, convert_config=config
        )
        return input_value_str

#!/usr/bin/env python
# -*- coding: utf-8 -*-

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

"""
    Dummy conftest.py for kiara.

    If you don't know what this is for, just leave it empty.
    Read more about conftest.py under:
    https://pytest.org/latest/plugins.html
"""
import os
import tempfile
import uuid
from pathlib import Path

import pytest

from kiara.context import Kiara
from kiara.context.config import KiaraConfig
from kiara.interfaces.python_api.base_api import BaseAPI
from kiara.interfaces.python_api.batch import BatchOperation

from .utils import INVALID_PIPELINES_FOLDER, MODULE_CONFIGS_FOLDER, PIPELINES_FOLDER

ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))

DATA_FOLDER = os.path.join(ROOT_DIR, "examples", "data")
TEST_RESOURCES_FOLDER = os.path.join(ROOT_DIR, "tests", "resources")


def create_temp_dir():
    session_id = str(uuid.uuid4())
    TEMP_DIR = Path(os.path.join(tempfile.gettempdir(), "kiara_tests"))

    instance_path = os.path.join(
        TEMP_DIR.resolve().absolute(), f"instance_{session_id}"
    )
    return instance_path


@pytest.fixture
def pipeline_paths():

    result = {}
    for root, dirnames, filenames in os.walk(PIPELINES_FOLDER, topdown=True):

        for f in filenames:
            full = os.path.join(root, f)
            if os.path.isfile(full) and f.endswith(".json"):
                result[os.path.splitext(f)[0]] = full

    return result


@pytest.fixture
def invalid_pipeline_paths():

    result = {}
    for root, dirnames, filenames in os.walk(INVALID_PIPELINES_FOLDER, topdown=True):

        for f in filenames:
            full = os.path.join(root, f)
            if os.path.isfile(full) and f.endswith(".json"):
                result[os.path.splitext(f)[0]] = full

    return result


@pytest.fixture
def module_config_paths():

    result = {}
    for root, dirnames, filenames in os.walk(MODULE_CONFIGS_FOLDER, topdown=True):

        for f in filenames:
            full = os.path.join(root, f)
            if os.path.isfile(full) and f.endswith(".json"):
                result[os.path.splitext(f)[0]] = full

    return result


@pytest.fixture
def kiara() -> Kiara:

    instance_path = create_temp_dir()
    kc = KiaraConfig.create_in_folder(instance_path)
    kc.runtime_config.runtime_profile = "default"

    kiara = kc.create_context()
    return kiara


@pytest.fixture
def api() -> BaseAPI:

    instance_path = create_temp_dir()
    kc = KiaraConfig.create_in_folder(instance_path)
    kc.runtime_config.runtime_profile = "default"
    api = BaseAPI(kc)
    return api


@pytest.fixture(scope="function")
def presseeded_data_store_minimal() -> Kiara:

    instance_path = create_temp_dir()

    pipeline_file = os.path.join(PIPELINES_FOLDER, "table_import.json")

    kc = KiaraConfig.create_in_folder(instance_path)
    kc.runtime_config.runtime_profile = "default"

    kiara = kc.create_context()

    batch_op = BatchOperation.from_file(pipeline_file, kiara=kiara)

    inputs = {
        "import_file__path": os.path.join(
            ROOT_DIR, "examples", "data", "journals", "JournalNodes1902.csv"
        ),
        "create_table_from_files__first_row_is_header": True,
    }

    batch_op.run(inputs=inputs, save="preseed_minimal")

    return kiara


@pytest.fixture(scope="function")
def preseeded_data_store() -> Kiara:

    instance_path = create_temp_dir()
    kc = KiaraConfig.create_in_folder(instance_path)
    kc.runtime_config.runtime_profile = "default"

    kiara = kc.create_context()

    pipeline = os.path.join(PIPELINES_FOLDER, "test_preseed_1.yaml")
    batch_op = BatchOperation.from_file(pipeline, kiara=kiara)

    inputs = {
        "edges_file_path": os.path.join(DATA_FOLDER, "journals/JournalEdges1902.csv"),
        "nodes_file_path": os.path.join(DATA_FOLDER, "journals/JournalNodes1902.csv"),
        "journals_folder_path": os.path.join(DATA_FOLDER, "journals"),
        "text_corpus_folder_path": os.path.join(DATA_FOLDER, "text_corpus"),
        "city_column_name": "City",
        "label_column_name": "Label",
    }

    batch_op.run(inputs=inputs, save="preseed")

    print(f"kiara data store: {kiara.data_registry.get_archive()}")  # noqa

    return kiara

# -*- coding: utf-8 -*-
import pytest

from kiara.api import Kiara
from kiara.exceptions import (
    InvalidManifestException,
    InvalidValuesException,
    KiaraException,
)
from kiara.models.module.manifest import Manifest
from kiara_plugin.core_types.modules.boolean import AndModule

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)


def test_module_instance_creation(kiara: Kiara):

    l_and = kiara.module_registry.create_module("logic.and")
    assert l_and.config.model_dump() == {"constants": {}, "defaults": {}, "delay": 0}

    with pytest.raises(InvalidManifestException):
        kiara.module_registry.create_module("logic.xor")

    l_and = kiara.module_registry.create_module("logic.and")
    assert "delay" in l_and.config.model_dump().keys()

    with pytest.raises(InvalidManifestException) as e_info:
        manifest = Manifest(module_type="logic.and", module_config={"xxx": "fff"})
        kiara.module_registry.create_module(manifest)

    msg = KiaraException.get_root_details(e_info.value)
    assert "xxx" in msg
    assert "Extra inputs" in msg


def test_module_instance_run(kiara: Kiara):

    l_and = kiara.module_registry.create_module("logic.and")

    result = l_and.run(kiara, a=True, b=True)
    assert result.get_all_value_data() == {"y": True}

    with pytest.raises(InvalidValuesException) as e_info:
        l_and.run(kiara=kiara)

    assert "Invalid inputs for module" in str(e_info.value)

    with pytest.raises(InvalidValuesException) as e_info:
        l_and.run(kiara=kiara, x=True)

    assert "Invalid inputs for module" in str(e_info.value)


def test_module_instance_direct(kiara: Kiara):

    and_module = AndModule()
    result = and_module.run(kiara=kiara, a=True, b=True)
    assert result.get_all_value_data() == {"y": True}

# -*- coding: utf-8 -*-
from kiara.api import Kiara

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)


def test_module_types_exist(kiara: Kiara):

    assert "logic.and" in kiara.module_registry.get_module_type_names()
    assert "logic.xor" not in kiara.module_registry.get_module_type_names()


def test_module_type_metadata(kiara: Kiara):

    l_and = kiara.module_registry.get_module_class("logic.and")
    assert not l_and.is_pipeline()

    pipeline = kiara.module_registry.get_module_class("pipeline")
    assert pipeline.is_pipeline()

    assert hasattr(l_and, "_module_type_name")
    assert hasattr(pipeline, "_module_type_name")

# -*- coding: utf-8 -*-
from kiara.interfaces.python_api.base_api import BaseAPI

#  Copyright (c) 2021, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)


def test_render_python_script(api: BaseAPI):

    render_config = {"inputs": {"a": True, "b": False}}

    rendered = api.render(
        "logic.xor",
        source_type="pipeline",
        target_type="python_script",
        render_config=render_config,
    )

    compile(rendered, "<string>", "exec")

    local_vars = {}
    exec(rendered, {}, local_vars)  # noqa
    assert local_vars["pipeline_result_y"].data is True

import os

KIARA_TEST_RESOURCES = os.path.join(os.path.dirname(__file__), "resources")
PIPELINES_FOLDER = os.path.join(KIARA_TEST_RESOURCES, "pipelines")
INVALID_PIPELINES_FOLDER = os.path.join(KIARA_TEST_RESOURCES, "invalid_pipelines")
MODULE_CONFIGS_FOLDER = os.path.join(KIARA_TEST_RESOURCES, "module_configs")


def get_workflow_config_path(workflow_name: str):
    return os.path.join(PIPELINES_FOLDER, workflow_name)


def get_module_config_path(module_config_name: str):
    return os.path.join(MODULE_CONFIGS_FOLDER, module_config_name)

# -*- coding: utf-8 -*-
import abc
from typing import TYPE_CHECKING, Any, Generic, Mapping, Union

from jinja2 import Environment, Template
from pydantic import BaseModel, Field, PrivateAttr

from kiara.exceptions import KiaraException
from kiara.renderers import (
    INPUTS_SCHEMA,
    SOURCE_TYPE,
    KiaraRenderer,
    KiaraRendererConfig,
    RenderInputsSchema,
)

if TYPE_CHECKING:
    from kiara.context import Kiara
    from kiara.registries.rendering import RenderRegistry


class JinjaEnv(BaseModel):

    template_base: Union[str, None] = Field(
        description="The alias of the base loader to use. Defaults to a special loader that combines all template sources.",
        default=None,
    )

    _render_registry: "RenderRegistry" = PrivateAttr(default=None)

    @property
    def instance(self) -> Environment:

        return self._render_registry.retrieve_jinja_env(self.template_base)


class JinjaRndererConfig(KiaraRendererConfig):

    env: JinjaEnv = Field(description="The loader to use for the jinja environment.")


class BaseJinjaRenderer(
    KiaraRenderer[SOURCE_TYPE, INPUTS_SCHEMA, str, JinjaRndererConfig],
    Generic[SOURCE_TYPE, INPUTS_SCHEMA],
):

    _renderer_config_cls = JinjaRndererConfig

    def __init__(
        self,
        kiara: "Kiara",
        renderer_config: Union[None, Mapping[str, Any], KiaraRendererConfig] = None,
    ):

        self._jinja_env: Union[Environment, None] = None
        super().__init__(kiara=kiara, renderer_config=renderer_config)

    def get_jinja_env(self) -> Environment:

        if self._jinja_env is None:
            je = self.retrieve_jinja_env()
            je._render_registry = self._kiara.render_registry
            self._jinja_env = je.instance
        return self._jinja_env

    def retrieve_jinja_env(self) -> JinjaEnv:
        return JinjaEnv()

    @abc.abstractmethod
    def get_template(self, render_config: INPUTS_SCHEMA) -> Template:
        pass

    @abc.abstractmethod
    def assemble_render_inputs(
        self, instance: Any, render_config: INPUTS_SCHEMA
    ) -> Mapping[str, Any]:
        pass

    def _render(self, instance: SOURCE_TYPE, render_config: INPUTS_SCHEMA) -> str:

        template = self.get_template(render_config=render_config)
        if template is None:
            msg = "Available templates:\n"
            for template_name in self.get_jinja_env().list_templates():
                msg += f" - {template_name}\n"
            raise KiaraException(msg=f"Could not find requested template for renderer '{self.__class__._renderer_name}'", details=msg)  # type: ignore

        inputs = self.assemble_render_inputs(instance, render_config=render_config)
        rendered: str = template.render(**inputs)
        return rendered


class JinjaRenderInputsSchema(RenderInputsSchema):
    template: str = Field(description="The template to use for rendering.")


class JinjaRenderer(BaseJinjaRenderer[Any, JinjaRenderInputsSchema]):

    _renderer_name = "jinja"
    _inputs_schema_cls = JinjaRenderInputsSchema

    def get_template(self, render_config: JinjaRenderInputsSchema) -> Template:

        try:
            template = self.get_jinja_env().get_template(render_config.template)
            return template
        except Exception as e:
            raise Exception(f"Error loading template '{render_config.template}': {e}")

# -*- coding: utf-8 -*-

#  Copyright (c) 2022, University of Luxembourg / DHARPA project
#  Copyright (c) 2021, Markus Binsteiner
#
#  Mozilla Public License, version 2.0 (see LICENSE or https://www.mozilla.org/en-US/MPL/2.0/)

import abc
import inspect
from typing import (
    TYPE_CHECKING,
    Any,
    Generic,
    Iterable,
    List,
    Mapping,
    Set,
    Type,
    TypeVar,
    Union,
)

from jinja2 import TemplateNotFound

from kiara.exceptions import KiaraException
from kiara.models import KiaraModel
from kiara.models.documentation import DocumentationMetadataModel

if TYPE_CHECKING:
    from kiara.context import Kiara


class KiaraRendererConfig(KiaraModel):
    pass


class RenderInputsSchema(KiaraModel):
    pass


RENDERER_CONFIG = TypeVar("RENDERER_CONFIG", bound=KiaraRendererConfig)
SOURCE_TYPE = TypeVar("SOURCE_TYPE")
INPUTS_SCHEMA = TypeVar("INPUTS_SCHEMA", bound=RenderInputsSchema)
TARGET_TYPE = TypeVar("TARGET_TYPE")


class SourceTransformer(Generic[SOURCE_TYPE]):
    def __init__(self) -> None:
        self._doc: Union[DocumentationMetadataModel, None] = None

    @abc.abstractmethod
    def retrieve_supported_python_classes(self) -> Iterable[Type]:
        pass

    @abc.abstractmethod
    def validate_and_transform(self, source: Any) -> Union[SOURCE_TYPE, None]:
        pass

    @abc.abstractmethod
    def retrieve_supported_inputs_descs(self) -> Union[str, Iterable[str]]:
        pass


class NoOpSourceTransformer(SourceTransformer):
    def retrieve_supported_python_classes(self) -> Iterable[Type]:
        return [object]

    def validate_and_transform(self, source: Any) -> Any:
        return source

    def retrieve_supported_inputs_descs(self) -> Union[str, Iterable[str]]:
        return "any Python input, unchecked"


class KiaraRenderer(
    abc.ABC, Generic[SOURCE_TYPE, INPUTS_SCHEMA, TARGET_TYPE, RENDERER_CONFIG]
):

    _renderer_config_cls: Type[RENDERER_CONFIG] = KiaraRendererConfig  # type: ignore
    _inputs_schema: Type[INPUTS_SCHEMA] = RenderInputsSchema  # type: ignore

    def __init__(
        self,
        kiara: "Kiara",
        renderer_config: Union[None, Mapping[str, Any], KiaraRendererConfig] = None,
    ):

        self._kiara: "Kiara" = kiara
        self._source_transformers: Union[None, Iterable[SourceTransformer]] = None
        self._doc: Union[DocumentationMetadataModel, None] = None
        self._supported_inputs_desc: Union[None, Iterable[str]] = None

        if renderer_config is None:
            self._config: RENDERER_CONFIG = self.__class__._renderer_config_cls()
        elif isinstance(renderer_config, Mapping):
            self._config = self.__class__._renderer_config_cls(**renderer_config)
        elif not isinstance(renderer_config, self.__class__._renderer_config_cls):
            raise Exception(
                f"Can't create renderer instance, invalid config type: {type(renderer_config)}, must be: {self.__class__._renderer_config_cls.__name__}"
            )
        else:
            self._config = renderer_config

    @property
    def renderer_config(self) -> RENDERER_CONFIG:
        return self._config

    @property
    def supported_inputs_descs(self) -> Iterable[str]:

        if self._supported_inputs_desc is not None:
            return self._supported_inputs_desc

        transformers: List[str] = []
        for transformer in self.source_transformers:
            descs = transformer.retrieve_supported_inputs_descs()
            if isinstance(descs, str):
                descs = [descs]
            transformers.extend(descs)
        return transformers

    def retrieve_doc(self) -> Union[str, None]:
        return None

    @property
    def doc(self) -> DocumentationMetadataModel:
        if self._doc is not None:
            return self._doc

        doc = self.retrieve_doc()
        if doc is None:
            doc = self.__class__.__doc__
            if not doc:
                doc = ""
            doc = f"{inspect.cleandoc(doc)}\n\n"

        transformers_list = (
            "## Supported inputs:\n\nThis renderer supports the following inputs:\n\n"
        )
        for transformer in self.supported_inputs_descs:
            transformers_list += f"- {transformer}\n"

        doc = f"{doc}\n\n{transformers_list}"

        self._doc = DocumentationMetadataModel.create(doc)
        return self._doc

    @property
    def source_transformers(self) -> Iterable[SourceTransformer]:
        if self._source_transformers is None:
            self._source_transformers = self.retrieve_source_transformers()
        return self._source_transformers

    def retrieve_source_transformers(self) -> Iterable[SourceTransformer]:
        return [NoOpSourceTransformer()]

    def retrieve_supported_python_classes(self) -> Set[Type]:
        """Retrieve the set of Python classes that this renderer supports as inputs."""
        result: Set[Type] = set()
        for x in self.source_transformers:
            result.update(x.retrieve_supported_python_classes())
        return result

    def get_renderer_alias(self) -> str:
        return self.__class__._renderer_name  # type: ignore

    @abc.abstractmethod
    def retrieve_supported_render_sources(self) -> Union[Iterable[str], str]:
        pass

    @abc.abstractmethod
    def retrieve_supported_render_targets(self) -> Union[Iterable[str], str]:
        pass

    @abc.abstractmethod
    def _render(
        self, instance: SOURCE_TYPE, render_config: INPUTS_SCHEMA
    ) -> TARGET_TYPE:
        pass

    def _post_process(self, rendered: TARGET_TYPE) -> TARGET_TYPE:
        return rendered

    def render(self, instance: SOURCE_TYPE, render_config: INPUTS_SCHEMA) -> Any:

        transformed = None
        for transformer in self.source_transformers:
            try:

                for cls in transformer.retrieve_supported_python_classes():
                    if isinstance(instance, cls):
                        transformed = transformer.validate_and_transform(instance)
                        if transformed is not None:
                            break
            except Exception as e:
                raise KiaraException("Error transforming source object.", parent=e)

        if not transformed:
            raise Exception(f"Can't transform input object: {instance}.")

        try:
            rendered: TARGET_TYPE = self._render(
                instance=transformed, render_config=render_config
            )
        except Exception as e:

            if isinstance(e, TemplateNotFound):
                details = f"Template not found: {e}"
                raise KiaraException("Error while rendering item.", details=details)
            else:
                raise e

        try:
            post_processed: TARGET_TYPE = self._post_process(rendered=rendered)
        except Exception as e:
            raise KiaraException("Error post-processing rendered item.", parent=e)

        return post_processed

